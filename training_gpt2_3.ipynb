{"cells":[{"attachments":{},"cell_type":"markdown","metadata":{"id":"YTsVWwCgLAJl"},"source":["First Training result with the GPT-2 decoder 🤖 (after random method)\n","-----------------------------------\n","\n","In this notebook, we will continue the fine-tuning of the pre-trained GPT-2 model provided by OPEN-AI. We obtained, after a hyperparameter tuning with `wandb`, a model with a minimal evaluation cross-entropy-loss of **0.9**. Let us load the model with the best hyperparameter setting and continue the training. We will add EarlyStopping to not make the model over-fits. \n","\n","Parallel coordinates from panel:\n","![parallel_coordinates](W%26B%20Chart%2028_04_2023%2012_40_23.png)\n"]},{"attachments":{},"cell_type":"markdown","metadata":{"id":"LpFGxo7ULAJp"},"source":["We also see that the evaluation loss depends more on the probability of modifying words from a french sentence (fr_word_p) with the following `Parameter importance char` (from [panel](https://wandb.ai/oumar-kane-team/gpt2-french-wolof-sweeps/reports/undefined-23-04-28-13-02-21---Vmlldzo0MjA3MzY5)):\n","\n","![parameter_importance](Parameter%20importance%20screenshot.png)"]},{"attachments":{},"cell_type":"markdown","metadata":{},"source":["The evaluation loss is also negatively correlated to the learning rate and positively to the wf_word_p (probability of modifying words from a wolof sentence)."]},{"cell_type":"code","execution_count":1,"metadata":{"executionInfo":{"elapsed":755,"status":"ok","timestamp":1682646171674,"user":{"displayName":"Oumar Kane","userId":"16897379128044949771"},"user_tz":0},"id":"0NmODpFxX2Ik"},"outputs":[],"source":["# let us extend the paths of the system \n","import sys\n","\n","# path = \"/content/drive/MyDrive/Memoire/subject2/\" # (for colab only)\n","path = \"data/extractions/\"\n","\n","# sys.path.extend([f\"{path}new_data\", f\"{path}wolof-translate\"]) # (for colab only)"]},{"cell_type":"code","execution_count":2,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":4,"status":"ok","timestamp":1682646174166,"user":{"displayName":"Oumar Kane","userId":"16897379128044949771"},"user_tz":0},"id":"JqaQuyv3XvHl","outputId":"29525853-af9e-4e4e-f7e3-d547883af1de"},"outputs":[{"name":"stdout","output_type":"stream","text":["env: WANDB_PROJECT=gpt2_french_wolof_sweeps\n","env: WANDB_LOG_MODEL=true\n","env: WANDB_NOTEBOOK_NAME=training_gpt2_2.ipynb\n","env: WANDB_API_KEY=237a8450cd2568ea1c8e1f8e0400708e79b6b4ee\n"]}],"source":["# define environment\n","%env WANDB_LOG_MODEL=true\n","%env WANDB_NOTEBOOK_NAME=training_gpt2_2.ipynb\n","%env WANDB_API_KEY=237a8450cd2568ea1c8e1f8e0400708e79b6b4ee"]},{"cell_type":"code","execution_count":3,"metadata":{"executionInfo":{"elapsed":14312,"status":"ok","timestamp":1682646190029,"user":{"displayName":"Oumar Kane","userId":"16897379128044949771"},"user_tz":0},"id":"rOALYu0I1th2"},"outputs":[],"source":["!pip install -qq wandb --upgrade"]},{"cell_type":"code","execution_count":4,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":109735,"status":"ok","timestamp":1682646299757,"user":{"displayName":"Oumar Kane","userId":"16897379128044949771"},"user_tz":0},"id":"YqElKFkPLAJq","outputId":"c5b0f023-22db-4b55-9abf-cdf3c8e401fd"},"outputs":[{"name":"stdout","output_type":"stream","text":["^C\n"]}],"source":["!pip install evaluate -qq\n","!pip install sacrebleu -qq\n","!pip install optuna -qq\n","!pip install transformers -qq \n","!pip install tokenizers -qq\n","!pip install nlpaug -qq\n","!pip install ray[tune] -qq\n","!python -m spacy download fr_core_news_lg "]},{"cell_type":"code","execution_count":3,"metadata":{"executionInfo":{"elapsed":36045,"status":"ok","timestamp":1682646338944,"user":{"displayName":"Oumar Kane","userId":"16897379128044949771"},"user_tz":0},"id":"dF37F8_nLAJr"},"outputs":[{"name":"stderr","output_type":"stream","text":["c:\\Users\\Oumar Kane\\AppData\\Local\\pypoetry\\Cache\\virtualenvs\\pytorch1-HleOW5am-py3.10\\lib\\site-packages\\tqdm\\auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n","  from .autonotebook import tqdm as notebook_tqdm\n","\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33moumar-kane\u001b[0m (\u001b[33moumar-kane-team\u001b[0m). Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n","\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m If you're specifying your api key in code, ensure this code is not shared publicly.\n","\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Consider setting the WANDB_API_KEY environment variable, or running `wandb login` from the command line.\n","\u001b[34m\u001b[1mwandb\u001b[0m: Appending key for api.wandb.ai to your netrc file: C:\\Users\\Oumar Kane/.netrc\n"]},{"data":{"text/plain":["True"]},"execution_count":3,"metadata":{},"output_type":"execute_result"}],"source":["# let us import all necessary libraries\n","from transformers import GPT2LMHeadModel, TrainingArguments, Trainer, EarlyStoppingCallback\n","from wolof_translate.utils.sent_transformers import TransformerSequences\n","from wolof_translate.data.dataset_v1 import SentenceDataset\n","from wolof_translate.utils.sent_corrections import *\n","from sklearn.model_selection import train_test_split\n","from nlpaug.augmenter import char as nac\n","from torch.utils.data import DataLoader\n","# from datasets  import load_metric # make pip install evaluate instead\n","# and pip install sacrebleu for instance\n","from functools import partial\n","from tqdm import tqdm\n","import pandas as pd\n","import numpy as np\n","import evaluate\n","import torch\n","import wandb\n","\n","wandb.login(key=\"237a8450cd2568ea1c8e1f8e0400708e79b6b4ee\")\n"]},{"cell_type":"markdown","metadata":{"id":"ypAj4KXBLAJs"},"source":["We will create two models: \n","\n","- One translating the french corpus to a wolof corpus [french_to_wolof](#french-to-wolof)\n","- One translating the wolof corpus to a french corpus [wolof_to_french](#wolof-to-french)"]},{"cell_type":"markdown","metadata":{"id":"mtgeyZoxLAJs"},"source":["--------------"]},{"cell_type":"markdown","metadata":{"id":"19MVywzSLAJt"},"source":["## French to wolof"]},{"cell_type":"markdown","metadata":{"id":"n4tP0YGyLAJt"},"source":["### Configure dataset 🔠"]},{"cell_type":"markdown","metadata":{"id":"e6dLQ3poLAJu"},"source":["We can use the same custom dataset that we created in [text_augmentation](text_augmentation.ipynb). But we need to split the data between train and test sets and save them."]},{"cell_type":"code","execution_count":4,"metadata":{"executionInfo":{"elapsed":482,"status":"ok","timestamp":1682646342844,"user":{"displayName":"Oumar Kane","userId":"16897379128044949771"},"user_tz":0},"id":"GyCZiVSvLAJu"},"outputs":[],"source":["# # load the corpora and split into train and test sets\n","# corpora = pd.read_csv(\"sent_extraction.csv\")\n","\n","# train_set, test_set = train_test_split(corpora, test_size=0.1, random_state=50)\n","\n","# # let us save the sets\n","# train_set.to_csv(\"train_set.csv\", index=False)\n","\n","# test_set.to_csv(\"data/extractions/new_data/test_set.csv\", index=False)"]},{"cell_type":"markdown","metadata":{"id":"WahLNKJ0LAJv"},"source":["Let us recuperate the datasets with and without augmentation."]},{"cell_type":"code","execution_count":5,"metadata":{"executionInfo":{"elapsed":477,"status":"ok","timestamp":1682646361528,"user":{"displayName":"Oumar Kane","userId":"16897379128044949771"},"user_tz":0},"id":"BIjksuH9LAJv"},"outputs":[],"source":["def recuperate_datasets(fr_char_p: float, wf_char_p: float, fr_word_p: float, wf_word_p):\n","\n","  # without augmentation\n","  # train_dataset = SentenceDataset(f\"{path}new_data/train_set.csv\", \n","  #                                 tokenizer_path = f\"{path}wolof-translate/wolof_translate/tokenizers/tokenizer_v1.json\")\n","\n","  # test_dataset = SentenceDataset(f\"{path}new_data/test_set.csv\",\n","                                # tokenizer_path = f\"{path}wolof-translate/wolof_translate/tokenizers/tokenizer_v1.json\")\n","\n","  # with augmentation\n","  fr_augmentation = TransformerSequences(nac.KeyboardAug(aug_char_p=fr_char_p, aug_word_p=fr_word_p),\n","                                        remove_mark_space, delete_guillemet_space)\n","\n","  wf_augmentation = TransformerSequences(nac.KeyboardAug(aug_char_p=wf_char_p, aug_word_p=wf_word_p),\n","                                        remove_mark_space, delete_guillemet_space)\n","\n","  train_dataset_aug = SentenceDataset(f\"{path}new_data/train_set.csv\", \n","                                  # tokenizer_path = f\"{path}wolof-translate/wolof_translate/tokenizers/tokenizer_v1.json\", # (only for colab)\n","                                  tokenizer_path = f\"wolof-translate/wolof_translate/tokenizers/tokenizer_v1.json\",\n","                                  cp1_transformer=fr_augmentation, truncation=True,\n","                                  cp2_transformer=wf_augmentation, max_len=579)\n","\n","  test_dataset_aug = SentenceDataset(f\"{path}new_data/test_set.csv\",\n","                                # tokenizer_path = f\"{path}wolof-translate/wolof_translate/tokenizers/tokenizer_v1.json\", # (only for colab)\n","                                tokenizer_path = f\"wolof-translate/wolof_translate/tokenizers/tokenizer_v1.json\",\n","                                cp1_transformer=fr_augmentation, truncation=True,\n","                                cp2_transformer=wf_augmentation, max_len=579)\n","  \n","  return train_dataset_aug, test_dataset_aug\n","  # return {\n","  #     'False': {\n","  #         'train_dataset': train_dataset,\n","  #         'test_dataset': test_dataset,\n","  #     },\n","  #     'True': {\n","  #         'train_dataset': train_dataset_aug,\n","  #         'test_dataset': test_dataset_aug\n","  #     }\n","  #     }"]},{"cell_type":"markdown","metadata":{"id":"0vhzP3IaLAJv"},"source":["### Configure the model and the evaluation function ⚙️"]},{"cell_type":"markdown","metadata":{"id":"Ts_cesDLLAJw"},"source":["Let us recuperate the model and resize the token embeddings."]},{"cell_type":"code","execution_count":6,"metadata":{"executionInfo":{"elapsed":546,"status":"ok","timestamp":1682646380276,"user":{"displayName":"Oumar Kane","userId":"16897379128044949771"},"user_tz":0},"id":"CO1jx85eLAJw"},"outputs":[],"source":["def gpt2_model_init(tokenizer):\n","  # set the mode name\n","  model_name = \"gpt2\"\n","\n","  # recuperate the tokenizer from the dataset\n","  tokenizer = tokenizer\n","\n","  # configure the model\n","  model = GPT2LMHeadModel.from_pretrained(model_name).cuda()\n","\n","  # resize the token embeddings\n","  model.resize_token_embeddings(len(tokenizer))\n","\n","  return model"]},{"cell_type":"markdown","metadata":{"id":"R8I3tm4WLAJx"},"source":["Let us evaluate the predictions with the `bleu` metric."]},{"cell_type":"code","execution_count":7,"metadata":{"executionInfo":{"elapsed":1207,"status":"ok","timestamp":1682646383893,"user":{"displayName":"Oumar Kane","userId":"16897379128044949771"},"user_tz":0},"id":"IerZolDNLAJx"},"outputs":[],"source":["# %%writefile wolof-translate/wolof_translate/utils/evaluation.py\n","from tokenizers import Tokenizer\n","from typing import *\n","import numpy as np\n","import evaluate\n","\n","class TranslationEvaluation:\n","    \n","    def __init__(self, \n","                 tokenizer: Tokenizer,\n","                 decoder: Union[Callable, None] = None,\n","                 metric = evaluate.load('sacrebleu'),\n","                 ):\n","        \n","        self.tokenizer = tokenizer\n","        \n","        self.decoder = decoder\n","        \n","        self.metric = metric\n","    \n","    def postprocess_text(self, preds, labels):\n","        \n","        preds = [pred.strip() for pred in preds]\n","        \n","        labels = [[label.strip()] for label in labels]\n","        \n","        return preds, labels\n","\n","    def compute_metrics(self, eval_preds):\n","        \n","        preds, labels = eval_preds.preds.detach().cpu(), labels.detach().cpu()\n","        \n","        if isinstance(preds, tuple):\n","            \n","            preds = preds[0]\n","        \n","        if self.decoder is None:\n","            \n","            decoded_preds = self.tokenizer.batch_decode(preds, skip_special_tokens=True)\n","            \n","            decoded_labels = self.tokenizer.batch_decode(labels, skip_special_tokens=True)\n","            \n","            decoded_preds, decoded_labels = self.postprocess_text(decoded_preds, decoded_labels)\n","            \n","            result = self.metric.compute(predictions=decoded_preds, references=decoded_labels)\n","            \n","            result = {\"bleu\": result[\"score\"]}\n","            \n","            prediction_lens = [np.count_nonzero(pred != self.tokenizer.pad_token_id) for pred in preds]\n","            \n","            result[\"gen_len\"] = np.mean(prediction_lens)\n","        \n","        else:\n","            \n","            predictions = list(self.decoder(preds))\n","            \n","            labels = list(self.decoder(labels))\n","      \n","            decoded_preds, decoded_labels = self.postprocess_text(predictions, labels)\n","            \n","            result = self.metric.compute(predictions=predictions, references=labels)\n","            \n","            result = {\"bleu\": result[\"score\"]}\n","        \n","        result = {k:round(v, 4) for k, v in result.items()}\n","\n","        wandb.log(\"bleu\", result[\"bleu\"])\n","            \n","        return result"]},{"cell_type":"code","execution_count":8,"metadata":{"executionInfo":{"elapsed":2,"status":"ok","timestamp":1682646385622,"user":{"displayName":"Oumar Kane","userId":"16897379128044949771"},"user_tz":0},"id":"OEvlO5mtLAJx"},"outputs":[],"source":["# %run wolof-translate/wolof_translate/utils/evaluation.py"]},{"cell_type":"markdown","metadata":{"id":"IuppKYiyLAJx"},"source":["Let us initialize the evaluation object."]},{"cell_type":"code","execution_count":9,"metadata":{"executionInfo":{"elapsed":2,"status":"ok","timestamp":1682646387580,"user":{"displayName":"Oumar Kane","userId":"16897379128044949771"},"user_tz":0},"id":"a7Bpd4UPLAJy"},"outputs":[],"source":["# translation_eval = TranslationEvaluation(test_dataset.tokenizer)"]},{"cell_type":"markdown","metadata":{"id":"xT17hB19LAJy"},"source":["### Searching for the best parameters 🕖"]},{"cell_type":"markdown","metadata":{"id":"XQ5evOG5LAJw"},"source":["Let us define the data collator."]},{"cell_type":"code","execution_count":10,"metadata":{"executionInfo":{"elapsed":3,"status":"ok","timestamp":1682646389302,"user":{"displayName":"Oumar Kane","userId":"16897379128044949771"},"user_tz":0},"id":"SgVN115tLAJw"},"outputs":[],"source":["def data_collator(batch):\n","    \"\"\"Generate a batch of data to provide to trainer\n","\n","    Args:\n","        batch (_type_): The batch\n","\n","    Returns:\n","        dict: A dictionary containing the ids, the attention mask and the labels\n","    \"\"\"\n","    input_ids = torch.stack([b[0] for b in batch])\n","    \n","    attention_mask = torch.stack([b[1] for b in batch])\n","    \n","    labels = torch.stack([b[0] for b in batch])\n","    \n","    return {'input_ids': input_ids, 'attention_mask': attention_mask,\n","            'labels': labels}"]},{"cell_type":"markdown","metadata":{"id":"Ry3DmkBuLAJy"},"source":["Let us initialize the training arguments and make random search."]},{"cell_type":"code","execution_count":12,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":1000},"id":"D_yP2Ny6LAJy","outputId":"8f9e96d7-ac63-400f-9fe9-02373f7f612f"},"outputs":[{"ename":"OutOfMemoryError","evalue":"CUDA out of memory. Tried to allocate 148.00 MiB (GPU 0; 6.00 GiB total capacity; 5.01 GiB already allocated; 0 bytes free; 5.20 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF","output_type":"error","traceback":["\u001b[1;31m---------------------------------------------------------------------------\u001b[0m","\u001b[1;31mOutOfMemoryError\u001b[0m                          Traceback (most recent call last)","\u001b[1;32mc:\\Users\\Oumar Kane\\OneDrive\\Documents\\subject2\\training_gpt2_3.ipynb Cell 29\u001b[0m in \u001b[0;36m5\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/Oumar%20Kane/OneDrive/Documents/subject2/training_gpt2_3.ipynb#X40sZmlsZQ%3D%3D?line=29'>30</a>\u001b[0m training_args \u001b[39m=\u001b[39m TrainingArguments(\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m{\u001b[39;00mpath\u001b[39m}\u001b[39;00m\u001b[39mtraining2/results\u001b[39m\u001b[39m\"\u001b[39m,\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/Oumar%20Kane/OneDrive/Documents/subject2/training_gpt2_3.ipynb#X40sZmlsZQ%3D%3D?line=30'>31</a>\u001b[0m                                   report_to \u001b[39m=\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mwandb\u001b[39m\u001b[39m\"\u001b[39m,\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/Oumar%20Kane/OneDrive/Documents/subject2/training_gpt2_3.ipynb#X40sZmlsZQ%3D%3D?line=31'>32</a>\u001b[0m                                   num_train_epochs\u001b[39m=\u001b[39m\u001b[39m5\u001b[39m,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/Oumar%20Kane/OneDrive/Documents/subject2/training_gpt2_3.ipynb#X40sZmlsZQ%3D%3D?line=45'>46</a>\u001b[0m                                   greater_is_better\u001b[39m=\u001b[39m\u001b[39mFalse\u001b[39;00m,\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/Oumar%20Kane/OneDrive/Documents/subject2/training_gpt2_3.ipynb#X40sZmlsZQ%3D%3D?line=46'>47</a>\u001b[0m                                   )   \n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/Oumar%20Kane/OneDrive/Documents/subject2/training_gpt2_3.ipynb#X40sZmlsZQ%3D%3D?line=48'>49</a>\u001b[0m \u001b[39m# define training loop\u001b[39;00m\n\u001b[1;32m---> <a href='vscode-notebook-cell:/c%3A/Users/Oumar%20Kane/OneDrive/Documents/subject2/training_gpt2_3.ipynb#X40sZmlsZQ%3D%3D?line=49'>50</a>\u001b[0m trainer \u001b[39m=\u001b[39m Trainer(model_init\u001b[39m=\u001b[39;49mpartial(gpt2_model_init, tokenizer \u001b[39m=\u001b[39;49m train_dataset\u001b[39m.\u001b[39;49mtokenizer),\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/Oumar%20Kane/OneDrive/Documents/subject2/training_gpt2_3.ipynb#X40sZmlsZQ%3D%3D?line=50'>51</a>\u001b[0m                   args\u001b[39m=\u001b[39;49mtraining_args,\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/Oumar%20Kane/OneDrive/Documents/subject2/training_gpt2_3.ipynb#X40sZmlsZQ%3D%3D?line=51'>52</a>\u001b[0m                   train_dataset\u001b[39m=\u001b[39;49mtrain_dataset, \n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/Oumar%20Kane/OneDrive/Documents/subject2/training_gpt2_3.ipynb#X40sZmlsZQ%3D%3D?line=52'>53</a>\u001b[0m                   eval_dataset\u001b[39m=\u001b[39;49mtest_dataset,\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/Oumar%20Kane/OneDrive/Documents/subject2/training_gpt2_3.ipynb#X40sZmlsZQ%3D%3D?line=53'>54</a>\u001b[0m                   data_collator\u001b[39m=\u001b[39;49mdata_collator,\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/Oumar%20Kane/OneDrive/Documents/subject2/training_gpt2_3.ipynb#X40sZmlsZQ%3D%3D?line=54'>55</a>\u001b[0m                   \u001b[39m# compute_metrics=translation_eval.compute_metrics\u001b[39;49;00m\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/Oumar%20Kane/OneDrive/Documents/subject2/training_gpt2_3.ipynb#X40sZmlsZQ%3D%3D?line=55'>56</a>\u001b[0m                   callbacks\u001b[39m=\u001b[39;49m[early_stopping]\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/Oumar%20Kane/OneDrive/Documents/subject2/training_gpt2_3.ipynb#X40sZmlsZQ%3D%3D?line=56'>57</a>\u001b[0m                   )\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/Oumar%20Kane/OneDrive/Documents/subject2/training_gpt2_3.ipynb#X40sZmlsZQ%3D%3D?line=58'>59</a>\u001b[0m \u001b[39m# load last checkpoint\u001b[39;00m\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/Oumar%20Kane/OneDrive/Documents/subject2/training_gpt2_3.ipynb#X40sZmlsZQ%3D%3D?line=59'>60</a>\u001b[0m \u001b[39m# trainer._load_from_checkpoint(\"data/training2/results/checkpoint-147\")\u001b[39;00m\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/Oumar%20Kane/OneDrive/Documents/subject2/training_gpt2_3.ipynb#X40sZmlsZQ%3D%3D?line=60'>61</a>\u001b[0m \n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/Oumar%20Kane/OneDrive/Documents/subject2/training_gpt2_3.ipynb#X40sZmlsZQ%3D%3D?line=61'>62</a>\u001b[0m \u001b[39m# start training loop\u001b[39;00m\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/Oumar%20Kane/OneDrive/Documents/subject2/training_gpt2_3.ipynb#X40sZmlsZQ%3D%3D?line=62'>63</a>\u001b[0m \u001b[39m# trainer.train()\u001b[39;00m\n","File \u001b[1;32mc:\\Users\\Oumar Kane\\AppData\\Local\\pypoetry\\Cache\\virtualenvs\\pytorch1-HleOW5am-py3.10\\lib\\site-packages\\transformers\\trainer.py:354\u001b[0m, in \u001b[0;36mTrainer.__init__\u001b[1;34m(self, model, args, data_collator, train_dataset, eval_dataset, tokenizer, model_init, compute_metrics, callbacks, optimizers, preprocess_logits_for_metrics)\u001b[0m\n\u001b[0;32m    352\u001b[0m \u001b[39mif\u001b[39;00m model_init \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m    353\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mmodel_init \u001b[39m=\u001b[39m model_init\n\u001b[1;32m--> 354\u001b[0m     model \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mcall_model_init()\n\u001b[0;32m    355\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m    356\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mRuntimeError\u001b[39;00m(\u001b[39m\"\u001b[39m\u001b[39m`Trainer` requires either a `model` or `model_init` argument\u001b[39m\u001b[39m\"\u001b[39m)\n","File \u001b[1;32mc:\\Users\\Oumar Kane\\AppData\\Local\\pypoetry\\Cache\\virtualenvs\\pytorch1-HleOW5am-py3.10\\lib\\site-packages\\transformers\\trainer.py:1314\u001b[0m, in \u001b[0;36mTrainer.call_model_init\u001b[1;34m(self, trial)\u001b[0m\n\u001b[0;32m   1312\u001b[0m model_init_argcount \u001b[39m=\u001b[39m number_of_arguments(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mmodel_init)\n\u001b[0;32m   1313\u001b[0m \u001b[39mif\u001b[39;00m model_init_argcount \u001b[39m==\u001b[39m \u001b[39m0\u001b[39m:\n\u001b[1;32m-> 1314\u001b[0m     model \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mmodel_init()\n\u001b[0;32m   1315\u001b[0m \u001b[39melif\u001b[39;00m model_init_argcount \u001b[39m==\u001b[39m \u001b[39m1\u001b[39m:\n\u001b[0;32m   1316\u001b[0m     model \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mmodel_init(trial)\n","\u001b[1;32mc:\\Users\\Oumar Kane\\OneDrive\\Documents\\subject2\\training_gpt2_3.ipynb Cell 29\u001b[0m in \u001b[0;36m9\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/Oumar%20Kane/OneDrive/Documents/subject2/training_gpt2_3.ipynb#X40sZmlsZQ%3D%3D?line=5'>6</a>\u001b[0m tokenizer \u001b[39m=\u001b[39m tokenizer\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/Oumar%20Kane/OneDrive/Documents/subject2/training_gpt2_3.ipynb#X40sZmlsZQ%3D%3D?line=7'>8</a>\u001b[0m \u001b[39m# configure the model\u001b[39;00m\n\u001b[1;32m----> <a href='vscode-notebook-cell:/c%3A/Users/Oumar%20Kane/OneDrive/Documents/subject2/training_gpt2_3.ipynb#X40sZmlsZQ%3D%3D?line=8'>9</a>\u001b[0m model \u001b[39m=\u001b[39m GPT2LMHeadModel\u001b[39m.\u001b[39;49mfrom_pretrained(model_name)\u001b[39m.\u001b[39;49mcuda()\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/Oumar%20Kane/OneDrive/Documents/subject2/training_gpt2_3.ipynb#X40sZmlsZQ%3D%3D?line=10'>11</a>\u001b[0m \u001b[39m# resize the token embeddings\u001b[39;00m\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/Oumar%20Kane/OneDrive/Documents/subject2/training_gpt2_3.ipynb#X40sZmlsZQ%3D%3D?line=11'>12</a>\u001b[0m model\u001b[39m.\u001b[39mresize_token_embeddings(\u001b[39mlen\u001b[39m(tokenizer))\n","File \u001b[1;32mc:\\Users\\Oumar Kane\\AppData\\Local\\pypoetry\\Cache\\virtualenvs\\pytorch1-HleOW5am-py3.10\\lib\\site-packages\\torch\\nn\\modules\\module.py:747\u001b[0m, in \u001b[0;36mModule.cuda\u001b[1;34m(self, device)\u001b[0m\n\u001b[0;32m    730\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mcuda\u001b[39m(\u001b[39mself\u001b[39m: T, device: Optional[Union[\u001b[39mint\u001b[39m, device]] \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m T:\n\u001b[0;32m    731\u001b[0m \u001b[39m    \u001b[39m\u001b[39mr\u001b[39m\u001b[39m\"\"\"Moves all model parameters and buffers to the GPU.\u001b[39;00m\n\u001b[0;32m    732\u001b[0m \n\u001b[0;32m    733\u001b[0m \u001b[39m    This also makes associated parameters and buffers different objects. So\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    745\u001b[0m \u001b[39m        Module: self\u001b[39;00m\n\u001b[0;32m    746\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[1;32m--> 747\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_apply(\u001b[39mlambda\u001b[39;49;00m t: t\u001b[39m.\u001b[39;49mcuda(device))\n","File \u001b[1;32mc:\\Users\\Oumar Kane\\AppData\\Local\\pypoetry\\Cache\\virtualenvs\\pytorch1-HleOW5am-py3.10\\lib\\site-packages\\torch\\nn\\modules\\module.py:639\u001b[0m, in \u001b[0;36mModule._apply\u001b[1;34m(self, fn)\u001b[0m\n\u001b[0;32m    637\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_apply\u001b[39m(\u001b[39mself\u001b[39m, fn):\n\u001b[0;32m    638\u001b[0m     \u001b[39mfor\u001b[39;00m module \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mchildren():\n\u001b[1;32m--> 639\u001b[0m         module\u001b[39m.\u001b[39;49m_apply(fn)\n\u001b[0;32m    641\u001b[0m     \u001b[39mdef\u001b[39;00m \u001b[39mcompute_should_use_set_data\u001b[39m(tensor, tensor_applied):\n\u001b[0;32m    642\u001b[0m         \u001b[39mif\u001b[39;00m torch\u001b[39m.\u001b[39m_has_compatible_shallow_copy_type(tensor, tensor_applied):\n\u001b[0;32m    643\u001b[0m             \u001b[39m# If the new tensor has compatible tensor type as the existing tensor,\u001b[39;00m\n\u001b[0;32m    644\u001b[0m             \u001b[39m# the current behavior is to change the tensor in-place using `.data =`,\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    649\u001b[0m             \u001b[39m# global flag to let the user control whether they want the future\u001b[39;00m\n\u001b[0;32m    650\u001b[0m             \u001b[39m# behavior of overwriting the existing tensor or not.\u001b[39;00m\n","File \u001b[1;32mc:\\Users\\Oumar Kane\\AppData\\Local\\pypoetry\\Cache\\virtualenvs\\pytorch1-HleOW5am-py3.10\\lib\\site-packages\\torch\\nn\\modules\\module.py:639\u001b[0m, in \u001b[0;36mModule._apply\u001b[1;34m(self, fn)\u001b[0m\n\u001b[0;32m    637\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_apply\u001b[39m(\u001b[39mself\u001b[39m, fn):\n\u001b[0;32m    638\u001b[0m     \u001b[39mfor\u001b[39;00m module \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mchildren():\n\u001b[1;32m--> 639\u001b[0m         module\u001b[39m.\u001b[39;49m_apply(fn)\n\u001b[0;32m    641\u001b[0m     \u001b[39mdef\u001b[39;00m \u001b[39mcompute_should_use_set_data\u001b[39m(tensor, tensor_applied):\n\u001b[0;32m    642\u001b[0m         \u001b[39mif\u001b[39;00m torch\u001b[39m.\u001b[39m_has_compatible_shallow_copy_type(tensor, tensor_applied):\n\u001b[0;32m    643\u001b[0m             \u001b[39m# If the new tensor has compatible tensor type as the existing tensor,\u001b[39;00m\n\u001b[0;32m    644\u001b[0m             \u001b[39m# the current behavior is to change the tensor in-place using `.data =`,\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    649\u001b[0m             \u001b[39m# global flag to let the user control whether they want the future\u001b[39;00m\n\u001b[0;32m    650\u001b[0m             \u001b[39m# behavior of overwriting the existing tensor or not.\u001b[39;00m\n","File \u001b[1;32mc:\\Users\\Oumar Kane\\AppData\\Local\\pypoetry\\Cache\\virtualenvs\\pytorch1-HleOW5am-py3.10\\lib\\site-packages\\torch\\nn\\modules\\module.py:662\u001b[0m, in \u001b[0;36mModule._apply\u001b[1;34m(self, fn)\u001b[0m\n\u001b[0;32m    658\u001b[0m \u001b[39m# Tensors stored in modules are graph leaves, and we don't want to\u001b[39;00m\n\u001b[0;32m    659\u001b[0m \u001b[39m# track autograd history of `param_applied`, so we have to use\u001b[39;00m\n\u001b[0;32m    660\u001b[0m \u001b[39m# `with torch.no_grad():`\u001b[39;00m\n\u001b[0;32m    661\u001b[0m \u001b[39mwith\u001b[39;00m torch\u001b[39m.\u001b[39mno_grad():\n\u001b[1;32m--> 662\u001b[0m     param_applied \u001b[39m=\u001b[39m fn(param)\n\u001b[0;32m    663\u001b[0m should_use_set_data \u001b[39m=\u001b[39m compute_should_use_set_data(param, param_applied)\n\u001b[0;32m    664\u001b[0m \u001b[39mif\u001b[39;00m should_use_set_data:\n","File \u001b[1;32mc:\\Users\\Oumar Kane\\AppData\\Local\\pypoetry\\Cache\\virtualenvs\\pytorch1-HleOW5am-py3.10\\lib\\site-packages\\torch\\nn\\modules\\module.py:747\u001b[0m, in \u001b[0;36mModule.cuda.<locals>.<lambda>\u001b[1;34m(t)\u001b[0m\n\u001b[0;32m    730\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mcuda\u001b[39m(\u001b[39mself\u001b[39m: T, device: Optional[Union[\u001b[39mint\u001b[39m, device]] \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m T:\n\u001b[0;32m    731\u001b[0m \u001b[39m    \u001b[39m\u001b[39mr\u001b[39m\u001b[39m\"\"\"Moves all model parameters and buffers to the GPU.\u001b[39;00m\n\u001b[0;32m    732\u001b[0m \n\u001b[0;32m    733\u001b[0m \u001b[39m    This also makes associated parameters and buffers different objects. So\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    745\u001b[0m \u001b[39m        Module: self\u001b[39;00m\n\u001b[0;32m    746\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[1;32m--> 747\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_apply(\u001b[39mlambda\u001b[39;00m t: t\u001b[39m.\u001b[39;49mcuda(device))\n","\u001b[1;31mOutOfMemoryError\u001b[0m: CUDA out of memory. Tried to allocate 148.00 MiB (GPU 0; 6.00 GiB total capacity; 5.01 GiB already allocated; 0 bytes free; 5.20 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF"]}],"source":["# %%wandb\n","\n","\"\"\"Best parameters\n","learning_rate = 0.00003702\n","weight_decay = 0.4\n","train_batch_size = 5\n","fr_char_p = 0.5\n","fr_word_p = 0\n","wf_char_p = 0.5\n","wf_word_p = 0\n","eval/loss = 0.9086\n","\"\"\"\n","\n","# seed\n","torch.manual_seed(50)\n","\n","# let us recuperate the datasets\n","train_dataset, test_dataset = recuperate_datasets(0.5, 0.5, 0, 0)\n","\n","# let us initialize a early stopping callback\n","early_stopping = EarlyStoppingCallback(3, 0.1)\n","\n","# get train and test datasets according to the config\n","\n","# train_dataset = datasets[config.dataset_aug]['train_dataset']\n","\n","# test_dataset = datasets[config.dataset_aug]['test_dataset']\n","\n","# set training arguments\n","training_args = TrainingArguments(f\"{path}training2/results\",\n","                                  report_to = \"wandb\",\n","                                  num_train_epochs=5,\n","                                  # logging_steps=100,\n","                                  load_best_model_at_end=True,\n","                                  save_strategy=\"epoch\",\n","                                  evaluation_strategy=\"epoch\",\n","                                  logging_strategy = 'epoch',\n","                                  per_device_train_batch_size=5, \n","                                  per_device_eval_batch_size=5,\n","                                  learning_rate = 0.00003702,\n","                                  weight_decay=0.4,\n","                                  logging_dir=f'{path}gpt2_training_logs2',\n","                                #   remove_unused_columns = False,\n","                                  fp16 = True,\n","                                  metric_for_best_model=\"eval_loss\",\n","                                  greater_is_better=False,\n","                                  )   \n","\n","# define training loop\n","trainer = Trainer(model_init=partial(gpt2_model_init, tokenizer = train_dataset.tokenizer),\n","                  args=training_args,\n","                  train_dataset=train_dataset, \n","                  eval_dataset=test_dataset,\n","                  data_collator=data_collator,\n","                  # compute_metrics=translation_eval.compute_metrics\n","                  callbacks=[early_stopping]\n","                  )\n","\n","# load last checkpoint\n","# trainer._load_from_checkpoint(\"data/training2/results/checkpoint-147\")\n","\n","# start training loop\n","# trainer.train()\n"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":35},"id":"bbwmeRfadllT","outputId":"a6b5353e-7a02-4dfb-f273-c52bd30fd033"},"outputs":[{"data":{"application/vnd.google.colaboratory.intrinsic+json":{"type":"string"},"text/plain":["'data/training1/results/run-9/checkpoint-368'"]},"execution_count":13,"metadata":{},"output_type":"execute_result"}],"source":["# let us get the best model\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"k6Lk652XLAJy"},"outputs":[],"source":["# load from a checkpoint and continue the training\n","# trainer._load_from_checkpoint('data/training1/results/checkpoint-734/')\n","\n"]},{"cell_type":"markdown","metadata":{"id":"maU7JF7tLAJz"},"source":["We see that the model is over-fitted. We must fine-tune the model and augment it to add some noise into the training step."]},{"cell_type":"markdown","metadata":{"id":"sQVOIX4lqGMg"},"source":["### Predictions"]},{"cell_type":"markdown","metadata":{"id":"W_iGdw-LLAJz"},"source":["Let us generate texts and store into a DataFrame."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Lxev3DttLAJz"},"outputs":[],"source":["\n","# set the model to eval mode\n","_ = model.eval()\n","\n","# run model inference on all test data\n","original_traduction, predicted_traduction, original_text, scores = [], [], [], {}\n","\n","for data in tqdm(DataLoader(test_dataset)):\n","    \n","    # recuperate the two part of the sentence\n","    sents = list(test_dataset.decode(data[0]))\n","    \n","    cp1_sent, cp2_sent = sents[0][0], sents[0][1] \n","    \n","    # create the sentence to traduce\n","    sent1 = f'{test_dataset.cls_token}{cp1_sent}{test_dataset.sep_token}'\n","    \n","    # generate tokens\n","    encoding = tokenizer(sent1, return_tensors='pt')\n","    \n","    generated = encoding.input_ids.cuda()\n","    \n","    attention_mask = encoding.attention_mask.cuda()\n","    \n","    # recuperate the pad token id\n","    pad_token_id = tokenizer.pad_token_id\n","    \n","    # perform prediction\n","    sample_outputs = model.generate(generated, do_sample = False, top_k = 50, max_length = test_dataset.max_len, top_p = 0.90,\n","                                    temperature = 0, num_return_sequences = 0, attention_mask = attention_mask, pad_token_id = pad_token_id)\n","    \n","    # calculate the score and add it to the score\n","    result = translation_eval.compute_metrics((sample_outputs, generated))\n","    \n","    if not scores: scores.update({k: v for k, v in result.items()})\n","    \n","    else: scores.update({k: round((scores[k] + v) / 2, 4) for k, v in result.items()})\n","    \n","    # decode the predicted tokens into texts\n","    sent2 = list(test_dataset.decode(sample_outputs, True))[0]\n","    \n","    print(sent2)\n","    # append results\n","    original_traduction.append(cp2_sent)\n","    predicted_traduction.append(sent2)\n","    original_text.append(cp1_sent)\n","\n","# transform result into data frame\n","df_ft_to_wf = pd.DataFrame({'original_text': original_text,\n","                            'original_label': original_traduction,\n","                            'predicted_label': predicted_traduction})\n","\n","# print the result\n","df_ft_to_wf.head()"]}],"metadata":{"accelerator":"GPU","colab":{"provenance":[]},"gpuClass":"standard","kernelspec":{"display_name":"pytorch1-HleOW5am-py3.10","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.10.8"},"orig_nbformat":4},"nbformat":4,"nbformat_minor":0}
