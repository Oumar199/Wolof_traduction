{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Building a Unigram and custom Dataset for training with `T5`\n",
    "--------------------------------\n",
    "\n",
    "Like we did in [processing_2](text_processing2.ipynb) to build a tokenizer for GPT-2 we will need to create one for the T5 model. We will train a Unigram Tokenizer for both of the French and Wolof corpora and finally a custom dataset to recuperate the tokenized sentences.\n",
    "\n",
    "To understand how is working the Unigram tokenizer, see the following tutorial [Unigram_tokenizer](https://huggingface.co/learn/nlp-course/chapter6/7?fw=pt)."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following steps will be necessary to achieve our task:\n",
    "\n",
    "- Creating a batch generator to load the sentences when training the tokenizer.\n",
    "- Load the Unigram Tokenizer from the `tokenizers` library.\n",
    "- Add a normalizer to the tokenizer: See the following link for an explanation of the different types of normalizers [normalizer](https://unicode.org/reports/tr15/). But we will only need to remove too much space that will be found inside the sentences since we have already replaced any weird signs in the corpora (see [extract_sentence](extract_sentences.ipynb) and [extract_text](text_extraction.ipynb)).\n",
    "- Initialize the pre-tokenizer.\n",
    "- Initialize the trainer: we will need to furnish the special tokens that will be used and the vocab size. Let us take, for the latter, 10000 tokens for each corpus.\n",
    "- Train the tokenizer.\n",
    "- Initialize the post-processor `TemplateProcessing`: we will define the types' ids.\n",
    "- Initialize the decoder: `Metaspace.`\n",
    "- Make an example with some sentences.\n",
    "- Save the tokenizers\n",
    "- Create the custom dataset for the T5 model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let us import the necessary libraries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for creating the tokenizer\n",
    "from tokenizers import (\n",
    "    decoders,\n",
    "    models,\n",
    "    pre_tokenizers,\n",
    "    processors,\n",
    "    trainers,\n",
    "    Tokenizer,\n",
    "    normalizers\n",
    ")\n",
    "\n",
    "# for importing and manipulating the sentences\n",
    "import pandas as pd\n",
    "import random\n",
    "\n",
    "# for loading sentences with the custom dataset\n",
    "from torch.utils.data import DataLoader"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Load dataset and create generator"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will create one tokenizer for both of the French and Wolof corpora because the `T5` model' understand only one embedding layer. So we must create one generator for both of the French and Wolof corpora. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load sentences\n",
    "sentences = pd.read_csv(\"data/extractions/new_data/corpora_v3.csv\")\n",
    "\n",
    "# initialize a batch size\n",
    "BATCH_SIZE = 120\n",
    "\n",
    "# create generators (for the corpora)\n",
    "def generate_sents():\n",
    "    \n",
    "    # recuperate the sentences\n",
    "    french_sents = sentences['french_corpus'].to_list() \n",
    "    \n",
    "    wolof_sents = sentences['wolof_corpus'].to_list() \n",
    "    \n",
    "    sents = french_sents + wolof_sents\n",
    "    \n",
    "    for i in range(1, len(sents), BATCH_SIZE):\n",
    "        \n",
    "        yield sents[i:i+BATCH_SIZE]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Initialize the tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = Tokenizer(models.Unigram())"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Add normalizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer.normalizer = normalizers.Replace(\" {2,}\", \" \")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Configure the pre-tokenizers"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will use the Metaspace pre-tokenizer which separates the words considering the spaces between them. It will replace the space by a character (by default the underscore \"_\")."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer.pre_tokenizer = pre_tokenizers.Metaspace()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Initialize the trainers"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will provide all of the necessary special tokens to the Trainer. \n",
    "\n",
    "**Notice that a sentence can be a group of words separated by ending marks and not only one group of words. Then we can, for example, tokenize the following sentences**: `<sep>sentence1.sentence2.sentence3<cls>` **or** `<sep>sentence1.<sep>sentence2.<cls>`. **But, the second sentence is composed of two separate groups. Then the two sentences will have different type ids.** "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [],
   "source": [
    "special_tokens = [\"<cls>\", \"<sep>\", \"<unk>\", \"<pad>\", \"<mask>\", \"<s>\", \"</s>\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer = trainers.UnigramTrainer(vocab_size=10000, special_tokens=special_tokens, unk_token = \"<unk>\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Train the tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer.train_from_iterator(generate_sents(), trainer)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let us print the vocab size."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of tokens: 7611\n"
     ]
    }
   ],
   "source": [
    "print(f\"Number of tokens: {tokenizer.get_vocab_size()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Initialize the post-processor"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can not need the TemplateProcessor to train our corpora in a Sequence To Sequence model, but we will add it to our tokenizer. We can use it for another type of model. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 1\n"
     ]
    }
   ],
   "source": [
    "# let us recuperate the sep and cls ids\n",
    "cls_token_id = tokenizer.token_to_id(\"<cls>\")\n",
    "\n",
    "sep_token_id = tokenizer.token_to_id(\"<sep>\")\n",
    "\n",
    "print(cls_token_id, sep_token_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize the post processor\n",
    "tokenizer.post_process = processors.TemplateProcessing(\n",
    "    single=\"$A:0 <sep>:0 <cls>:2\",\n",
    "    pair=\"$A:0 <sep>:0 $B:1 <sep>:1 <cls>:2\",\n",
    "    special_tokens=[(\"<sep>\", sep_token_id), (\"<cls>\", cls_token_id)]\n",
    ")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Initialize the decoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer.decoder = decoders.Metaspace()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Save the tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer.save(\"wolof-translate/wolof_translate/tokenizers/t5_tokenizers/tokenizer_v1.json\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Make a little example"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let us recuperate random sentences from the corpora and tokenize them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [],
   "source": [
    "random.seed(100)\n",
    "\n",
    "french_sentence = random.choice(sentences['french_corpus'])\n",
    "\n",
    "wolof_sentence = random.choice(sentences['wolof_corpus'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"Nous frappions à nouveau, jusqu'à en avoir mal aux mains, comme si nous combattions un ennemi invisible.\""
      ]
     },
     "execution_count": 121,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# print the french sentence\n",
    "french_sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Mu ngi dale digaloom ak Kamerun gi ci moomeelu Farañse yi, wuti bëj-gànnaar, ca cati Aadamawa, ëmbaale dëkk yu ndaw-ndawaan yi Àngale yi jotul woon a teg loxo ba ñu fa dàqee Almaŋ yi. Muy dëkk yu mel ni : Kantu, Aboŋ, Nkom, Bum, Fumbaŋ ak Baali.'"
      ]
     },
     "execution_count": 122,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# print the wolof sentence\n",
    "wolof_sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "French tokens\n",
      "['▁N', 'ous', '▁frappions', '▁à', '▁nouveau', ',', \"▁jusqu'\", 'à', '▁en', '▁', 'avoir', '▁mal', '▁aux', '▁mains', ',', '▁comme', '▁si', '▁nous', '▁combatt', 'ions', '▁un', '▁ennemi', '▁invisible', '.']\n",
      "French ids\n",
      "[188, 441, 2158, 20, 925, 7, 329, 105, 40, 8, 738, 707, 206, 957, 7, 137, 127, 110, 3442, 309, 41, 4829, 2648, 9]\n"
     ]
    }
   ],
   "source": [
    "french_encoding = tokenizer.encode(french_sentence)\n",
    "\n",
    "print(\"French tokens\")\n",
    "print(french_encoding.tokens)\n",
    "\n",
    "print(\"French ids\")\n",
    "print(french_encoding.ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wolof tokens\n",
      "['▁Mu', '▁ngi', '▁dal', 'e', '▁digal', 'oom', '▁', 'ak', '▁Kamerun', '▁gi', '▁ci', '▁moom', 'eelu', '▁Far', 'añse', '▁yi', ',', '▁wuti', '▁bëj-gànnaar', ',', '▁ca', '▁c', 'ati', '▁Aadamawa', ',', '▁ëmb', 'aale', '▁dëkk', '▁yu', '▁ndaw', '-', 'ndaw', 'aan', '▁yi', '▁Àngale', '▁yi', '▁jot', 'ul', '▁woon', '▁a', '▁teg', '▁loxo', '▁ba', '▁ñu', '▁fa', '▁dàqee', '▁Alma', 'ŋ', '▁yi.', '▁Mu', 'y', '▁dëkk', '▁yu', '▁mel', '▁ni', '▁', ':', '▁Kan', 'tu', ',', '▁Abo', 'ŋ', ',', '▁Nko', 'm', ',', '▁Bu', 'm', ',', '▁Fu', 'm', 'baŋ', '▁', 'ak', '▁Baali', '.']\n",
      "Wolof ids\n",
      "[251, 72, 141, 13, 5129, 1187, 8, 31, 297, 81, 15, 129, 2346, 3010, 2319, 37, 7, 966, 995, 7, 56, 86, 865, 2663, 7, 1790, 1551, 176, 39, 341, 120, 7233, 524, 37, 512, 37, 331, 124, 55, 17, 216, 310, 29, 38, 50, 3166, 970, 222, 203, 251, 14, 176, 39, 112, 23, 8, 53, 979, 419, 7, 1611, 222, 7, 1399, 89, 7, 449, 89, 7, 1438, 89, 4973, 8, 31, 4224, 9]\n"
     ]
    }
   ],
   "source": [
    "wolof_encoding = tokenizer.encode(wolof_sentence)\n",
    "\n",
    "print(\"Wolof tokens\")\n",
    "print(wolof_encoding.tokens)\n",
    "\n",
    "print(\"Wolof ids\")\n",
    "print(wolof_encoding.ids)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Creating the T5 custom dataset"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have two possibilities to use the tokenizer for fine-tuning a T5 model. "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- We can use the `PreTrainedTokenizerFast` class for which we will provide the different special tokens."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import PreTrainedTokenizerFast\n",
    "\n",
    "wrapped_tokenizer1 = PreTrainedTokenizerFast(\n",
    "    tokenizer_object=tokenizer,\n",
    "    bos_token=\"<s>\",\n",
    "    eos_token=\"</s>\",\n",
    "    unk_token=\"<unk>\",\n",
    "    pad_token=\"<pad>\",\n",
    "    cls_token=\"<cls>\",\n",
    "    sep_token=\"<sep>\",\n",
    "    mask_token=\"<mask>\",\n",
    "    padding_side=\"left\",\n",
    ")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Or give directly the tokenizer to the `T5TokenizerFast` class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import T5TokenizerFast\n",
    "\n",
    "wrapped_tokenizer2 = T5TokenizerFast(\n",
    "    tokenizer_object=tokenizer\n",
    ")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let us give them the sentences that we use as example. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input_ids': [3, 3, 3, 3, 3, 3, 188, 441, 2158, 20, 925, 7, 329, 105, 40, 8, 738, 707, 206, 957, 7, 137, 127, 110, 3442, 309, 41, 4829, 2648, 9], 'token_type_ids': [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'attention_mask': [0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]}"
      ]
     },
     "execution_count": 127,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fr_encoding = wrapped_tokenizer1(french_sentence, max_length=30, padding='max_length', truncation=True)\n",
    "\n",
    "fr_encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input_ids': [251, 72, 141, 13, 5129, 1187, 8, 31, 297, 81, 15, 129, 2346, 3010, 2319, 37, 7, 966, 995, 7, 56, 86, 865, 2663, 7, 1790, 1551, 176, 39, 341, 120, 7233, 524, 37, 512, 37, 331, 124, 55, 17, 216, 310, 29, 38, 50, 3166, 970, 222, 203, 251, 14, 176, 39, 112, 23, 8, 53, 979, 419, 7, 1611, 222, 7, 1399, 89, 7, 449, 89, 7, 1438, 89, 4973, 8, 31, 4224, 9, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]}"
      ]
     },
     "execution_count": 128,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wf_encoding = wrapped_tokenizer2(wolof_sentence, max_length=100, padding='max_length', truncation=True)\n",
    "\n",
    "wf_encoding"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let us decode the wolof sentence."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Mu ngi dale digaloom ak Kamerun gi ci moomeelu Farañse yi, wuti bëj-gànnaar, ca cati Aadamawa, ëmbaale dëkk yu ndaw-ndawaan yi Àngale yi jotul woon a teg loxo ba ñu fa dàqee Almaŋ yi. Muy dëkk yu mel ni : Kantu, Aboŋ, Nkom, Bum, Fumbaŋ ak Baali.'"
      ]
     },
     "execution_count": 129,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wrapped_tokenizer1.decode(wf_encoding.input_ids, skip_special_tokens=True)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see that the `T5Tokenizer` add padding to the right side of the sequence while the `PretrainedTokenizer` add the padding to the left side. We can change the padding side from the settings. But, for the next steps, let us directly use the `T5Tokenizer`."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Note that we can augment the sentences when generating them like we did when using the `GPT2 model`.** See the following notebook for discussion on the augmentation method that we will use [augmentation](text_augmentation.ipynb). And for a more clear explanation of the augmentation methods in NLP tasks and training look at the following article [augment_or_not](https://direct.mit.edu/coli/article/48/1/5/108844/To-Augment-or-Not-to-Augment-A-Comparative-Study)."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let us verify, before creating the custom dataset, the max length that we can get from the corpora' tokens without considering the augmentation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_len = 0\n",
    "\n",
    "for sent in sentences['french_corpus'].to_list() + sentences['wolof_corpus'].to_list():\n",
    "    \n",
    "    len_ids = len(wrapped_tokenizer2(sent).input_ids)\n",
    "    \n",
    "    if len_ids > max_len:\n",
    "        \n",
    "        max_len = len_ids\n",
    "\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "279"
      ]
     },
     "execution_count": 131,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# let us print the max lengths\n",
    "max_len"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We find a maximum length of **279** tokens. But considering the augmentation we can obtain more than 279 tokens because it will add modifications on the words and then it can recognize only parts of them and divide them in multiple other tokens. Let us add to the max length the fifth of it. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "334"
      ]
     },
     "execution_count": 132,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "max_len += max_len // 5\n",
    "\n",
    "max_len"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It is time to create our custom dataset.\n",
    "\n",
    "Signature:\n",
    "```python\n",
    "class T5SentenceDataset(Dataset):\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        data_path: str, \n",
    "        tokenizer: PreTrainedTokenizerFast\n",
    "        corpus_1: str = \"french_corpus\",\n",
    "        corpus_2: str = \"wolof_corpus\",\n",
    "        max_len: int = 334,\n",
    "        cp1_truncation: bool = False,\n",
    "        cp2_truncation: bool = False,\n",
    "        file_sep: str = \",\",\n",
    "        cp1_transformer: Union[TransformerSequences, None] = None,\n",
    "        cp2_transformer: Union[TransformerSequences, None] = None,\n",
    "        **kwargs):\n",
    "\n",
    "        pass\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting wolof-translate/wolof_translate/data/dataset_v2.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile wolof-translate/wolof_translate/data/dataset_v2.py\n",
    "from wolof_translate.utils.sent_transformers import TransformerSequences\n",
    "from transformers import PreTrainedTokenizerFast\n",
    "from torch.utils.data import Dataset\n",
    "from typing import *\n",
    "import pandas as pd\n",
    "import torch\n",
    "import re\n",
    "\n",
    "class T5SentenceDataset(Dataset):\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        data_path: str, \n",
    "        tokenizer: PreTrainedTokenizerFast,\n",
    "        corpus_1: str = \"french_corpus\",\n",
    "        corpus_2: str = \"wolof_corpus\",\n",
    "        max_len: int = 334,\n",
    "        truncation: bool = False,\n",
    "        file_sep: str = \",\",\n",
    "        cp1_transformer: Union[TransformerSequences, None] = None,\n",
    "        cp2_transformer: Union[TransformerSequences, None] = None,\n",
    "        **kwargs):\n",
    "        \n",
    "        # let us recuperate the data frame\n",
    "        self.__sentences = pd.read_csv(data_path, sep=file_sep, **kwargs)\n",
    "        \n",
    "        # let us recuperate the tokenizer\n",
    "        self.tokenizer = tokenizer\n",
    "        \n",
    "        # recuperate the first corpus' sentences\n",
    "        self.__sentences_1 = self.__sentences[corpus_1].to_list()\n",
    "        \n",
    "        # recuperate the second corpus' sentences\n",
    "        self.__sentences_2 = self.__sentences[corpus_2].to_list()\n",
    "        \n",
    "        # recuperate the length\n",
    "        self.__length = len(self.__sentences_1)\n",
    "        \n",
    "        # let us recuperate the max len\n",
    "        self.max_len = max_len\n",
    "        \n",
    "        # let us recuperate the truncation argument\n",
    "        self.truncation = truncation\n",
    "        \n",
    "        # let us initialize the transformer\n",
    "        self.cp1_transformer = cp1_transformer\n",
    "        \n",
    "        self.cp2_transformer = cp2_transformer\n",
    "        \n",
    "    def __getitem__(self, index):\n",
    "        \"\"\"Recuperate ids and attention masks of sentences at index\n",
    "\n",
    "        Args:\n",
    "            index (int): The index of the sentences to recuperate\n",
    "\n",
    "        Returns:\n",
    "            tuple: The `sentence to translate' ids`, `the attention mask of the sentence to translate`\n",
    "            `the labels' ids`\n",
    "        \"\"\"\n",
    "        sentence_1 = self.__sentences_1[index]\n",
    "        \n",
    "        sentence_2 = self.__sentences_2[index]\n",
    "        \n",
    "        # apply transformers if necessary\n",
    "        if not self.cp1_transformer is None:\n",
    "            \n",
    "            sentence_1 = self.cp1_transformer(sentence_1) \n",
    "        \n",
    "        if not self.cp2_transformer is None:\n",
    "            \n",
    "            sentence_2 = self.cp2_transformer(sentence_2)\n",
    "        \n",
    "        # let us encode the sentences (we provide the second sentence as labels to the tokenizer)\n",
    "        data = self.tokenizer(\n",
    "            sentence_1,\n",
    "            truncation=self.truncation,\n",
    "            max_length=self.max_len, \n",
    "            padding='max_length', \n",
    "            return_tensors=\"pt\",\n",
    "            text_target=sentence_2)\n",
    "            \n",
    "        \n",
    "        return data.input_ids.squeeze(0), data.attention_mask.squeeze(0), data.labels.squeeze(0)\n",
    "        \n",
    "    def __len__(self):\n",
    "        \n",
    "        return self.__length\n",
    "    \n",
    "    def decode(self, labels: torch.Tensor):\n",
    "        \n",
    "        if labels.ndim < 2:\n",
    "            \n",
    "            labels = labels.unsqueeze(0)\n",
    "\n",
    "        sentences = self.tokenizer.batch_decode(labels, skip_special_tokens=True)\n",
    "\n",
    "        return sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {},
   "outputs": [],
   "source": [
    "%run wolof-translate/wolof_translate/data/dataset_v2.py"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let us generate some data with their masks and decode the labels.\n",
    "\n",
    "**Note that we will use, when training the `T5 model`, train and test sets and not directly the full dataset**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize our custom dataset\n",
    "dataset = T5SentenceDataset(\"data/extractions/new_data/corpora_v3.csv\", wrapped_tokenizer2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_ids, mask, labels = next(iter(DataLoader(dataset, 10))) # generate 10 sentences"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let us print the input ids."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 782,  336, 2555,  ...,    3,    3,    3],\n",
       "        [4700,  683,   44,  ...,    3,    3,    3],\n",
       "        [ 153,  178,   85,  ...,    3,    3,    3],\n",
       "        ...,\n",
       "        [ 286,  647,   10,  ...,    3,    3,    3],\n",
       "        [ 411,  104, 1090,  ...,    3,    3,    3],\n",
       "        [ 184, 6877,    7,  ...,    3,    3,    3]])"
      ]
     },
     "execution_count": 149,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input_ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1, 1, 1,  ..., 0, 0, 0],\n",
       "        [1, 1, 1,  ..., 0, 0, 0],\n",
       "        [1, 1, 1,  ..., 0, 0, 0],\n",
       "        ...,\n",
       "        [1, 1, 1,  ..., 0, 0, 0],\n",
       "        [1, 1, 1,  ..., 0, 0, 0],\n",
       "        [1, 1, 1,  ..., 0, 0, 0]])"
      ]
     },
     "execution_count": 150,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[4157, 3715,   43,  ...,    3,    3,    3],\n",
       "        [5189,  530,   78,  ...,    3,    3,    3],\n",
       "        [ 224,  343,   72,  ...,    3,    3,    3],\n",
       "        ...,\n",
       "        [ 733, 5268,  159,  ...,    3,    3,    3],\n",
       "        [ 979,   84, 1215,  ...,    3,    3,    3],\n",
       "        [ 235,   15,  522,  ...,    3,    3,    3]])"
      ]
     },
     "execution_count": 151,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "labels"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let us decode the labels."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Doomu-aadama bu, ne ci ndey ak baay nga jóge.',\n",
       " 'Mënunu leen a baň a gërëm ak a bëgg, doonte sax mën nanoo am xel ňaar ci ňoom.',\n",
       " 'Waaye ňu ngi fi, ak seen xar-kanam, seen taxawaay, seen defin ak seen jikko, seeni njuumte, seeni yaakaar, seen melokaanu loxook baaraami tànk, seen meloy bët ak karaw, seen waxin, seeni xalaat, amaana sax at ma ňuy nar a génne àddina. Loolu lépp, day àgg fu sore ci nun.',\n",
       " 'Bi ma delloo dëkk ba ma juddoo, dama faa meloon ni gan. Du kenn ku ma fa xam, safatul dara ci man. Li nu jóge Afrig jur ci man tiis wu réy. Su ma yaboo sax ni mënuma woon a nangu ni maak samay way-jur dëkkëtuñu Afrig. Ca laa tàmbalee gént ni sama yaay nit ku ñuul la, di sàkkal it sama bopp cosaan lu bees.',\n",
       " 'Àddinay dox ba Baay tollu ci noppalug liggéey, dellusi Tugal dëkk ak ňun. Ci la ma leere ni moom moomoo doon doomu Afrig.',\n",
       " 'Mu doon nag lu naqadee nangu.',\n",
       " 'Damaa mujjoon a delloo sama xel démb ngir lijjanti lépp la ca léjoon.',\n",
       " 'Kon fàttalikoo meññ téere bu ndaw bii.',\n",
       " 'Kanam gii ma judduwaale, am na lu bari lu ma ci mën a wax.',\n",
       " 'Li ci jiitu moo di ne dama dem ba jàppe ko nattu bu ma war a nangu.']"
      ]
     },
     "execution_count": 152,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset.decode(labels)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pytorch1-HleOW5am-py3.10",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
