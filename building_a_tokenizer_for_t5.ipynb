{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Building a Unigram and custom Dataset for training with `T5`\n",
    "--------------------------------\n",
    "\n",
    "Like we did in [processing_2](text_processing2.ipynb) to build a tokenizer for GPT-2 we will need to create one for the T5 model. We will train a Unigram Tokenizer with each of the French and Wolof corpus and finally a custom dataset to recuperate the tokenized sentences.\n",
    "\n",
    "To understand how is working the Unigram tokenizer, see the following tutorial [Unigram_tokenizer](https://huggingface.co/learn/nlp-course/chapter6/7?fw=pt)."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following steps will be necessary to achieve our task:\n",
    "\n",
    "- Creating a batch generators for each corpus.\n",
    "- Load the Unigram Tokenizer from the `tokenizers` library.\n",
    "- Add a normalizer to the tokenizers: See the following link for explanation on the different type of normalizers [normalizer](https://unicode.org/reports/tr15/). But we will only need to remove too much space that will be find inside the sentences since we have already replace any type of weird signs in the corpora (see [extract_sentence](extract_sentences.ipynb) and [extract_text](text_extraction.ipynb)).\n",
    "- Initialize the pre-tokenizer.\n",
    "- Initialize the trainer: we will need to furnish the special tokens that will be used and the vocab size. Let us take, for the latter, 10000 tokens for each corpus.\n",
    "- Train the tokenizers.\n",
    "- Initialize the post-processor `TemplateProcessing`: we will define the types' ids.\n",
    "- Initialize the decoder: `Metaspace`.\n",
    "- Make a example with some sentences.\n",
    "- Save the tokenizers\n",
    "- Create the custom dataset for the T5 model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let us import the necessary libraries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for creating the tokenizer\n",
    "from tokenizers import (\n",
    "    decoders,\n",
    "    models,\n",
    "    pre_tokenizers,\n",
    "    processors,\n",
    "    trainers,\n",
    "    Tokenizer,\n",
    "    normalizers\n",
    ")\n",
    "\n",
    "# for importing and manipulating the sentences\n",
    "import pandas as pd\n",
    "import random\n",
    "\n",
    "# for loading sentences with the custom dataset\n",
    "from torch.utils.data import DataLoader"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Load dataset and create generator"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will create one tokenizer for each of the French and Wolof corpora. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load sentences\n",
    "sentences = pd.read_csv(\"data/extractions/new_data/corpora_v3.csv\")\n",
    "\n",
    "# initialize a batch size\n",
    "BATCH_SIZE = 60\n",
    "\n",
    "# create generators (for the corpora)\n",
    "def generate_french_sents():\n",
    "    \n",
    "    # recuperate the sentences\n",
    "    french_sents = sentences['french_corpus'].to_list() \n",
    "    \n",
    "    for i in range(1, len(french_sents), BATCH_SIZE):\n",
    "        \n",
    "        yield french_sents[i:i+BATCH_SIZE]\n",
    "        \n",
    "def generate_wolof_sents():\n",
    "    \n",
    "    # recuperate the sentences\n",
    "    wolof_sents = sentences['wolof_corpus'].to_list() \n",
    "    \n",
    "    for i in range(1, len(wolof_sents), BATCH_SIZE):\n",
    "        \n",
    "        yield wolof_sents[i:i+BATCH_SIZE]\n",
    "        "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Initialize the tokenizers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "french_tokenizer = Tokenizer(models.Unigram())\n",
    "\n",
    "wolof_tokenizer = Tokenizer(models.Unigram())"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Add normalizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# french_tokenizer.normalizer = normalizers.Replace(\" {2,}\", \" \")\n",
    "\n",
    "# wolof_tokenizer.normalizer = normalizers.Replace(\" {2,}\", \" \")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Configure the pre-tokenizers"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will use the Metaspace pre-tokenizer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "french_tokenizer.pre_tokenizer = pre_tokenizers.Metaspace()\n",
    "\n",
    "wolof_tokenizer.pre_tokenizer = pre_tokenizers.Metaspace()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Initialize the trainers"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will provide all of the necessary special tokens to the Trainer. \n",
    "\n",
    "**Notice that a sentence can be a groups of words separated by ending marks and not only one group of words. Then we can, for example, tokenize the following sentences**: `<sep>sentence1.sentence2.sentence3<cls>` **or** `<sep>sentence1.<sep>sentence2.<cls>`. **But, the second sentence is composed of two separated groups. Then the two sentences will have different type ids.** "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "special_tokens = [\"<cls>\", \"<sep>\", \"<unk>\", \"<pad>\", \"<mask>\", \"<s>\", \"</s>\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "french_trainer = trainers.UnigramTrainer(vocab_size=10000, special_tokens=special_tokens, unk_token = \"<unk>\")\n",
    "\n",
    "wolof_trainer = trainers.UnigramTrainer(vocab_size=10000, special_tokens=special_tokens, unk_token = \"<unk>\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Train the tokenizers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "french_tokenizer.train_from_iterator(generate_french_sents(), french_trainer)\n",
    "\n",
    "wolof_tokenizer.train_from_iterator(generate_wolof_sents(), wolof_trainer)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let us print the vocab sizes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of tokens in the french corpus: 4714\n",
      "Number of tokens in the wolof corpus: 3336\n"
     ]
    }
   ],
   "source": [
    "# for the french corpus\n",
    "print(f\"Number of tokens in the french corpus: {french_tokenizer.get_vocab_size()}\")\n",
    "\n",
    "print(f\"Number of tokens in the wolof corpus: {wolof_tokenizer.get_vocab_size()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Initialize the post-processor"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can not need the TemplateProcessor to train our corpora in a Sequence To Sequence model but we will add it in our tokenizer. We can use it for another type of model. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 1\n"
     ]
    }
   ],
   "source": [
    "# let us recuperate the sep and cls ids\n",
    "cls_token_id = french_tokenizer.token_to_id(\"<cls>\")\n",
    "\n",
    "sep_token_id = french_tokenizer.token_to_id(\"<sep>\")\n",
    "\n",
    "print(cls_token_id, sep_token_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize the post processors\n",
    "french_tokenizer.post_process = processors.TemplateProcessing(\n",
    "    single=\"$A:0 <sep>:0 <cls>:2\",\n",
    "    pair=\"$A:0 <sep>:0 $B:1 <sep>:1 <cls>:2\",\n",
    "    special_tokens=[(\"<sep>\", sep_token_id), (\"<cls>\", cls_token_id)]\n",
    ")\n",
    "\n",
    "wolof_tokenizer.post_process = processors.TemplateProcessing(\n",
    "    single=\"$A:0 <sep>:0 <cls>:2\",\n",
    "    pair=\"$A:0 <sep>:0 $B:1 <sep>:1 <cls>:2\",\n",
    "    special_tokens=[(\"<sep>\", sep_token_id), (\"<cls>\", cls_token_id)]\n",
    ")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Initialize the decoders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "french_tokenizer.decoder = decoders.Metaspace()\n",
    "\n",
    "wolof_tokenizer.decoder = decoders.Metaspace()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Save the tokenizers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "french_tokenizer.save(\"wolof-translate/wolof_translate/tokenizers/t5_tokenizers/fr_tokenizer_v1.json\")\n",
    "\n",
    "wolof_tokenizer.save(\"wolof-translate/wolof_translate/tokenizers/t5_tokenizers/wf_tokenizer_v1.json\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Make a little example"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let us recuperate random sentences from the corpora and tokenize them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "random.seed(50)\n",
    "\n",
    "french_sentence = random.choice(sentences['french_corpus'])\n",
    "\n",
    "wolof_sentence = random.choice(sentences['wolof_corpus'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Ils vont de campement en campement, dans des villages dont mon père note les noms sur sa carte : Nikom, Babungo, Nji Nikom, Luakom Ndye, Ngi, Obukun.'"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# print the french sentence\n",
    "french_sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Di noyyi xet gu bon gi ci ban bi'"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# print the wolof sentence\n",
    "wolof_sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "French tokens\n",
      "['▁Il', 's', '▁vo', 'nt', '▁de', '▁campement', '▁en', '▁campement', ',', '▁dans', '▁des', '▁villages', '▁d', 'ont', '▁mon', '▁père', '▁not', 'e', '▁les', '▁noms', '▁sur', '▁sa', '▁carte', '▁', ':', '▁Ni', 'kom', ',', '▁Ba', 'b', 'ungo', ',', '▁Nj', 'i', '▁Ni', 'kom', ',', '▁Lu', 'a', 'kom', '▁N', 'd', 'y', 'e', ',', '▁Ng', 'i', ',', '▁', 'Obukun', '.']\n",
      "French ids\n",
      "[49, 10, 674, 57, 9, 607, 23, 607, 7, 21, 17, 292, 59, 70, 29, 35, 558, 13, 15, 552, 39, 54, 347, 8, 71, 1632, 778, 7, 481, 729, 1580, 7, 2835, 45, 1632, 778, 7, 1898, 68, 778, 205, 90, 120, 13, 7, 1507, 45, 7, 8, 2123, 11]\n"
     ]
    }
   ],
   "source": [
    "french_encoding = french_tokenizer.encode(french_sentence)\n",
    "\n",
    "print(\"French tokens\")\n",
    "print(french_encoding.tokens)\n",
    "\n",
    "print(\"French ids\")\n",
    "print(french_encoding.ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wolof tokens\n",
      "['▁Di', '▁noyyi', '▁xet', '▁gu', '▁bon', '▁gi', '▁ci', '▁ban', '▁bi']\n",
      "Wolof ids\n",
      "[551, 980, 923, 82, 910, 48, 10, 505, 23]\n"
     ]
    }
   ],
   "source": [
    "wolof_encoding = wolof_tokenizer.encode(wolof_sentence)\n",
    "\n",
    "print(\"Wolof tokens\")\n",
    "print(wolof_encoding.tokens)\n",
    "\n",
    "print(\"Wolof ids\")\n",
    "print(wolof_encoding.ids)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Creating the T5 custom dataset"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have two possibilities to use the tokenizer for fine-tuning a T5 model. "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- We can use the `PreTrainedTokenizerFast` class for which we will provide the different special tokens."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import PreTrainedTokenizerFast\n",
    "\n",
    "fr_wrapped_tokenizer1 = PreTrainedTokenizerFast(\n",
    "    tokenizer_object=french_tokenizer,\n",
    "    bos_token=\"<s>\",\n",
    "    eos_token=\"</s>\",\n",
    "    unk_token=\"<unk>\",\n",
    "    pad_token=\"<pad>\",\n",
    "    cls_token=\"<cls>\",\n",
    "    sep_token=\"<sep>\",\n",
    "    mask_token=\"<mask>\",\n",
    "    padding_side=\"left\",\n",
    ")\n",
    "\n",
    "wf_wrapped_tokenizer1 = PreTrainedTokenizerFast(\n",
    "    tokenizer_object=wolof_tokenizer,\n",
    "    bos_token=\"<s>\",\n",
    "    eos_token=\"</s>\",\n",
    "    unk_token=\"<unk>\",\n",
    "    pad_token=\"<pad>\",\n",
    "    cls_token=\"<cls>\",\n",
    "    sep_token=\"<sep>\",\n",
    "    mask_token=\"<mask>\",\n",
    "    padding_side=\"left\",\n",
    ")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Or give directly the tokenizer to the `T5TokenizerFast` class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import T5TokenizerFast\n",
    "\n",
    "fr_wrapped_tokenizer2 = T5TokenizerFast(\n",
    "    tokenizer_object=french_tokenizer\n",
    ")\n",
    "\n",
    "wf_wrapped_tokenizer2 = T5TokenizerFast(\n",
    "    tokenizer_object=wolof_tokenizer\n",
    ")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let us give them the sentences that we use as example. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input_ids': [3, 3, 3, 3, 3, 3, 3, 3, 3, 49, 10, 674, 57, 9, 607, 23, 607, 7, 21, 17, 292, 59, 70, 29, 35, 558, 13, 15, 552, 39, 54, 347, 8, 71, 1632, 778, 7, 481, 729, 1580, 7, 2835, 45, 1632, 778, 7, 1898, 68, 778, 205, 90, 120, 13, 7, 1507, 45, 7, 8, 2123, 11], 'token_type_ids': [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'attention_mask': [0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]}"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fr_encoding = fr_wrapped_tokenizer1(french_sentence, max_length=60, padding='max_length', truncation=True)\n",
    "\n",
    "fr_encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input_ids': [551, 980, 923, 82, 910, 48, 10, 505, 23, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]}"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wf_encoding = wf_wrapped_tokenizer2(wolof_sentence, max_length=20, padding='max_length', truncation=True)\n",
    "\n",
    "wf_encoding"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let us decode the wolof sentence."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Di noyyi xet gu bon gi ci ban bi'"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wf_wrapped_tokenizer1.decode(wf_encoding.input_ids, skip_special_tokens=True)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Note that for the tokenization we could use the `text_target` argument to obtain the labels with padding but in our case we decided to use two different tokenizers. So it is impossible.** \n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see that the `T5Tokenizer` add padding to the right side of the sequence while the `PretrainedTokenizer` add the padding to the left side. We can change the padding side from the settings. But, for the next steps of the `T5Tokenizer`, let us directly use the `T5Tokenizer`."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Note that we can add augmentation when generating the sentences like we did when using the `GPT2 model`.** See the following notebook for discussion on the augmentation method that we will use [augmentation](text_augmentation.ipynb). And for a more clear explanation of the augmentation methods in NLP tasks and training look at the following article [augment_or_not](https://direct.mit.edu/coli/article/48/1/5/108844/To-Augment-or-Not-to-Augment-A-Comparative-Study)."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let us verify, before creating the custom dataset, the max lengths that we can get from the French corpus' tokens and the Wolof corpus' tokens without considering the augmentation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "fr_max_len = 0\n",
    "\n",
    "for sent in sentences['french_corpus']:\n",
    "    \n",
    "    len_ids = len(fr_wrapped_tokenizer2(sent).input_ids)\n",
    "    \n",
    "    if len_ids > fr_max_len:\n",
    "        \n",
    "        fr_max_len = len_ids\n",
    "        \n",
    "wf_max_len = 0\n",
    "\n",
    "for sent in sentences['wolof_corpus']:\n",
    "    \n",
    "    len_ids = len(wf_wrapped_tokenizer2(sent).input_ids)\n",
    "    \n",
    "    if len_ids > wf_max_len:\n",
    "        \n",
    "        wf_max_len = len_ids\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(242, 282)"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# let us print the max lengths\n",
    "fr_max_len, wf_max_len"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We find a maximum length of **242** tokens in the french corpus and **282** tokens in the wolof corpus. But considering the augmentation we can obtain more than 242 and 282 tokens because it will add modification on the words and then it can recognize only parts of them and so divide them in multiple other tokens. Let us add to the max lengths the fifth of them. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(290, 338)"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fr_max_len += fr_max_len // 5\n",
    "\n",
    "wf_max_len += wf_max_len // 5\n",
    "\n",
    "fr_max_len, wf_max_len"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It is time to create our custom dataset.\n",
    "\n",
    "Signature:\n",
    "```python\n",
    "class T5SentenceDataset(Dataset):\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        data_path: str, \n",
    "        tokenizer1: PreTrainedTokenizerFast,\n",
    "        tokenizer2: Union[str, None] = None,\n",
    "        corpus_1: str = \"french_corpus\",\n",
    "        corpus_2: str = \"wolof_corpus\",\n",
    "        cp1_max_len: int = 290,\n",
    "        cp2_max_len: int = 338,\n",
    "        cp1_truncation: bool = False,\n",
    "        cp2_truncation: bool = False,\n",
    "        file_sep: str = \",\",\n",
    "        cp1_transformer: Union[TransformerSequences, None] = None,\n",
    "        cp2_transformer: Union[TransformerSequences, None] = None,\n",
    "        **kwargs):\n",
    "\n",
    "        pass\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %%writefile wolof-translate/wolof_translate/data/dataset_v2.py\n",
    "from wolof_translate.utils.sent_transformers import TransformerSequences\n",
    "from transformers import PreTrainedTokenizerFast\n",
    "from torch.utils.data import Dataset\n",
    "from typing import *\n",
    "import pandas as pd\n",
    "import torch\n",
    "import re\n",
    "\n",
    "class T5SentenceDataset(Dataset):\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        data_path: str, \n",
    "        tokenizer1: PreTrainedTokenizerFast,\n",
    "        tokenizer2: Union[PreTrainedTokenizerFast, None] = None,\n",
    "        corpus_1: str = \"french_corpus\",\n",
    "        corpus_2: str = \"wolof_corpus\",\n",
    "        cp1_max_len: int = 290,\n",
    "        cp2_max_len: int = 338,\n",
    "        cp1_truncation: bool = False,\n",
    "        cp2_truncation: bool = False,\n",
    "        file_sep: str = \",\",\n",
    "        cp1_transformer: Union[TransformerSequences, None] = None,\n",
    "        cp2_transformer: Union[TransformerSequences, None] = None,\n",
    "        **kwargs):\n",
    "        \n",
    "        # let us recuperate the data frame\n",
    "        self.__sentences = pd.read_csv(data_path, sep=file_sep, **kwargs)\n",
    "        \n",
    "        # let us recuperate the tokenizers\n",
    "        self.tokenizer1 = tokenizer1\n",
    "        \n",
    "        self.tokenizer2 = tokenizer2\n",
    "        \n",
    "        # recuperate the first corpus' sentences\n",
    "        self.__sentences_1 = self.__sentences[corpus_1].to_list()\n",
    "        \n",
    "        # recuperate the second corpus' sentences\n",
    "        self.__sentences_2 = self.__sentences[corpus_2].to_list()\n",
    "        \n",
    "        # recuperate the length\n",
    "        self.__length = len(self.__sentences_1)\n",
    "        \n",
    "        # let us recuperate the max len\n",
    "        self.cp1_max_len = cp1_max_len\n",
    "        \n",
    "        self.cp2_max_len = cp2_max_len\n",
    "        \n",
    "        # let us recuperate the truncation argument\n",
    "        self.cp1_truncation = cp1_truncation\n",
    "        \n",
    "        self.cp2_truncation = cp2_truncation\n",
    "        \n",
    "        # let us initialize the transformer\n",
    "        self.cp1_transformer = cp1_transformer\n",
    "        \n",
    "        self.cp2_transformer = cp2_transformer\n",
    "        \n",
    "    def __getitem__(self, index):\n",
    "        \"\"\"Recuperate ids and attention masks of sentences at index\n",
    "\n",
    "        Args:\n",
    "            index (int): The index of the sentences to recuperate\n",
    "\n",
    "        Returns:\n",
    "            tuple: The `sentence to translate' ids`, `the attention mask of the sentence to translate`\n",
    "            `the labels' ids`, `the attention mask of the labels` \n",
    "        \"\"\"\n",
    "        sentence_1 = self.__sentences_1[index]\n",
    "        \n",
    "        sentence_2 = self.__sentences_2[index]\n",
    "        \n",
    "        # apply transformers if necessary\n",
    "        if not self.cp1_transformer is None:\n",
    "            \n",
    "            sentence_1 = self.cp1_transformer(sentence_1) \n",
    "        \n",
    "        if not self.cp2_transformer is None:\n",
    "            \n",
    "            sentence_2 = self.cp2_transformer(sentence_2)\n",
    "        \n",
    "        # let us encode the first sentence\n",
    "        data = self.tokenizer1(\n",
    "            sentence_1,\n",
    "            truncation=self.cp1_truncation,\n",
    "            max_length=self.cp1_max_len, \n",
    "            padding='max_length', \n",
    "            return_tensors=\"pt\")\n",
    "        \n",
    "        # let us encode the second sentence\n",
    "        if not self.tokenizer2 is None:\n",
    "            \n",
    "            labels = self.tokenizer2(\n",
    "                sentence_2, \n",
    "                truncation=self.cp2_truncation,\n",
    "                max_length=self.cp2_max_len, \n",
    "                padding='max_length', \n",
    "                return_tensors=\"pt\")\n",
    "            \n",
    "        else:\n",
    "            \n",
    "            labels = self.tokenizer1(\n",
    "                sentence_2, \n",
    "                truncation=self.cp2_truncation,\n",
    "                max_length=self.cp2_max_len, \n",
    "                padding='max_length', \n",
    "                return_tensors=\"pt\")\n",
    "        \n",
    "        return data.input_ids.squeeze(0),\\\n",
    "            data.attention_mask.squeeze(0),\\\n",
    "                labels.input_ids.squeeze(0),\\\n",
    "                    labels.attention_mask.squeeze(0) \n",
    "        \n",
    "    def __len__(self):\n",
    "        \n",
    "        return self.__length\n",
    "    \n",
    "    def decode(self, labels: torch.Tensor):\n",
    "        \n",
    "        if labels.ndim < 2:\n",
    "            \n",
    "            labels = labels.unsqueeze(0)\n",
    "        \n",
    "        ids = labels.tolist()\n",
    "        \n",
    "        sentences = []\n",
    "        \n",
    "        for id in ids:\n",
    "            \n",
    "            sentence = self.tokenizer1.decode(id, skip_special_tokens=True)\\\n",
    "                if self.tokenizer2 is None \\\n",
    "                    else self.tokenizer2.decode(id, skip_special_tokens=True)\n",
    "\n",
    "            sentences.append(sentence)\n",
    "\n",
    "        return sentences"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let us generate some data with their masks and decode the labels.\n",
    "\n",
    "**Note that we will use, when training the `T5 model`, train and test sets and not directly the full dataset**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize our custom dataset\n",
    "dataset = T5SentenceDataset(\"data/extractions/new_data/corpora_v3.csv\", fr_wrapped_tokenizer2, wf_wrapped_tokenizer2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "data, data_mask, labels, labels_mask = next(iter(DataLoader(dataset, 10))) # generate 10 sentences"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let us print the gotten data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 358,  161, 1312,  ...,    3,    3,    3],\n",
       "        [1594,  349,   48,  ...,    3,    3,    3],\n",
       "        [ 126,  103,   40,  ...,    3,    3,    3],\n",
       "        ...,\n",
       "        [ 176,  293,    9,  ...,    3,    3,    3],\n",
       "        [ 204,   52,  507,  ...,    3,    3,    3],\n",
       "        [ 166, 4575,    7,  ...,    3,    3,    3]])"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1, 1, 1,  ..., 0, 0, 0],\n",
       "        [1, 1, 1,  ..., 0, 0, 0],\n",
       "        [1, 1, 1,  ..., 0, 0, 0],\n",
       "        ...,\n",
       "        [1, 1, 1,  ..., 0, 0, 0],\n",
       "        [1, 1, 1,  ..., 0, 0, 0],\n",
       "        [1, 1, 1,  ..., 0, 0, 0]])"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[2461,  410, 2816,  ...,    3,    3,    3],\n",
       "        [2385,  232,   45,  ...,    3,    3,    3],\n",
       "        [ 127,  208,   44,  ...,    3,    3,    3],\n",
       "        ...,\n",
       "        [ 401, 2344,   76,  ...,    3,    3,    3],\n",
       "        [1052,   47,  791,  ...,    3,    3,    3],\n",
       "        [ 126,   10,  314,  ...,    3,    3,    3]])"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1, 1, 1,  ..., 0, 0, 0],\n",
       "        [1, 1, 1,  ..., 0, 0, 0],\n",
       "        [1, 1, 1,  ..., 0, 0, 0],\n",
       "        ...,\n",
       "        [1, 1, 1,  ..., 0, 0, 0],\n",
       "        [1, 1, 1,  ..., 0, 0, 0],\n",
       "        [1, 1, 1,  ..., 0, 0, 0]])"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "labels_mask"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let us decode the labels."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Doomu-aadama bu, ne ci ndey ak baay nga jóge.',\n",
       " 'Mënunu leen a baň a gërëm ak a bëgg, doonte sax mën nanoo am xel ňaar ci ňoom.',\n",
       " 'Waaye ňu ngi fi, ak seen xar-kanam, seen taxawaay, seen defin ak seen jikko, seeni njuumte, seeni yaakaar, seen melokaanu loxook baaraami tànk, seen meloy bët ak karaw, seen waxin, seeni xalaat, amaana sax at ma ňuy nar a génne àddina. Loolu lépp, day àgg fu sore ci nun.',\n",
       " 'Bi ma delloo dëkk ba ma juddoo, dama faa meloon ni gan. Du kenn ku ma fa xam, safatul dara ci man. Li nu jóge Afrig jur ci man tiis wu réy. Su ma yaboo sax ni mënuma woon a nangu ni maak samay way-jur dëkkëtuñu Afrig. Ca laa tàmbalee gént ni sama yaay nit ku ñuul la, di sàkkal it sama bopp cosaan lu bees.',\n",
       " 'Àddinay dox ba Baay tollu ci noppalug liggéey, dellusi Tugal dëkk ak ňun. Ci la ma leere ni moom moomoo doon doomu Afrig.',\n",
       " 'Mu doon nag lu naqadee nangu.',\n",
       " 'Damaa mujjoon a delloo sama xel démb ngir lijjanti lépp la ca léjoon.',\n",
       " 'Kon fàttalikoo meññ téere bu ndaw bii.',\n",
       " 'Kanam gii ma judduwaale, am na lu bari lu ma ci mën a wax.',\n",
       " 'Li ci jiitu moo di ne dama dem ba jàppe ko nattu bu ma war a nangu.']"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset.decode(labels)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pytorch1-HleOW5am-py3.10",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
