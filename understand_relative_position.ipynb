{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-------------"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Relative Positional Encoding Example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Parameter containing:\n",
       "tensor([[ 0.1043, -0.0500, -0.0120,  ..., -0.0818, -0.0159,  0.0040],\n",
       "        [ 0.0261,  0.0660, -0.1183,  ...,  0.0991, -0.1208, -0.1169],\n",
       "        [ 0.0271,  0.1220, -0.0060,  ..., -0.0501, -0.0651,  0.0510],\n",
       "        ...,\n",
       "        [-0.0042,  0.0151, -0.0370,  ..., -0.0921, -0.0884,  0.0806],\n",
       "        [ 0.1129,  0.0979, -0.1043,  ..., -0.0752,  0.0542,  0.0857],\n",
       "        [ 0.0499,  0.0409, -0.0933,  ...,  0.0630,  0.0718,  0.0561]],\n",
       "       requires_grad=True)"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from torch import nn\n",
    "import torch\n",
    "\n",
    "# Initialize the d model\n",
    "d_model = 100\n",
    "\n",
    "# Give the max position\n",
    "max_relative_position = 150\n",
    "\n",
    "# Create a lookup table (if it were inside a model we must initialize as weights)\n",
    "embeddings_table = nn.Parameter(torch.Tensor(max_relative_position * 2 + 1, d_model))\n",
    "\n",
    "# Initialize the value of the lookup table with xavier uniform\n",
    "nn.init.xavier_uniform_(embeddings_table)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([301, 100])"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "embeddings_table.size() # size = (number of positions, embedding dimension)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize a range with same size as the query sequence length (200)\n",
    "range_vec_q = torch.arange(200)\n",
    "\n",
    "# Initialize a range with same size as the key sequence length (200)\n",
    "range_vec_k = torch.arange(200)\n",
    "\n",
    "# Calculate the distance between the positions\n",
    "distance_mat = range_vec_k[None, :] - range_vec_q[:, None]\n",
    "\n",
    "# Clip the distances between the -max distance and max distance\n",
    "distance_mat_clipped = torch.clamp(distance_mat, -150, 150)\n",
    "\n",
    "# Add max distance to the clipped distance in order to obtain positive indices\n",
    "final_mat = distance_mat_clipped + 150\n",
    "\n",
    "# Transform final matrix to Long in order to map the indices to their vectors\n",
    "final_mat = torch.LongTensor(final_mat)\n",
    "\n",
    "# Determine the positional embeddings\n",
    "embeddings = embeddings_table[final_mat]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([200, 200, 100])"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "embeddings.size() # The embedding is of dimension [query sequence length, keys sequence length, embedding dim]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Note since we are using the following formula from [relative_position](https://arxiv.org/pdf/1803.02155v2)**:\n",
    "\n",
    "$$\n",
    "E = \\frac{Attention + Additional\\_attention}{\\sqrt{d\\_model}}\n",
    "$$\n",
    "\n",
    "Where $Additional\\_attention = Linear(Q) \\times position\\_embeddings$\n",
    "\n",
    "And that we don't need to determine a first relative positional embedding to add to the linear transformation of the values because we are making machine translation. Then the key sequence length that we used in `range_vec_k` can be taken as the query sequence length. But for more comprehension of the process we will maintain the name."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize a query with size = (batch = 4, sequence, d_model)\n",
    "query = torch.randn((4, 200, 100))\n",
    "\n",
    "# Calculate the additional attention\n",
    "add_attention = query.transpose(0, 1).matmul(embeddings.transpose(1, 2)).transpose(0, 1)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notice that we transpose the batch dimension with the sequence dimension of the query matrix and we also transposed the dimension of the key sequence with the embedding dimension in order to make matrix multiplication between the query and the position embeddings. After we replaced the batch dimension on its position."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([4, 200, 200])"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "add_attention.size() "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The result will be then added to the original attention and all together divided by $\\sqrt{d\\_model}$."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pytorch1-HleOW5am-py3.10",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
