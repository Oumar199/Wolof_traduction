{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Creating a Unigram Tokenizer on the new sentences\n",
    "----------------------------------\n",
    "We extracted new sentences from the book <i style=\"color: cyan\">Grammaire de Wolof Moderne</i> by Pathé Diagne. Since we want to test the relevancy of the new sentences to our translation task, let us create a tokenizer for them. It is done in order to train the T5 model on it and see if we obtain a better performance that with the original corpora. "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The process is almost the same as in [processing_4](text_processing4.ipynb) excepted that we will create another custom dataset for the custom transformer model."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let us import the necessary libraries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Oumar Kane\\AppData\\Local\\pypoetry\\Cache\\virtualenvs\\pytorch1-HleOW5am-py3.10\\lib\\site-packages\\tqdm\\auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "# for creating the tokenizer\n",
    "from tokenizers import (\n",
    "    decoders,\n",
    "    models,\n",
    "    pre_tokenizers,\n",
    "    processors,\n",
    "    trainers,\n",
    "    Tokenizer,\n",
    "    normalizers\n",
    ")\n",
    "\n",
    "# for importing and manipulating the sentences\n",
    "import pandas as pd\n",
    "import random\n",
    "\n",
    "# for loading sentences with the custom dataset\n",
    "from torch.utils.data import DataLoader"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Load dataset and create generator"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will create one tokenizer for both of the French and Wolof corpora because the `T5` model' understand only one embedding layer. So we must create one generator for both of the French and Wolof corpora. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load sentences\n",
    "sentences = pd.read_csv(\"data/additional_documents/diagne_sentences/extractions.csv\")\n",
    "\n",
    "# initialize a batch size\n",
    "BATCH_SIZE = 400\n",
    "\n",
    "# create generators (for the corpora)\n",
    "def generate_sents():\n",
    "    \n",
    "    # recuperate the sentences\n",
    "    french_sents = sentences['french'].to_list() \n",
    "    \n",
    "    wolof_sents = sentences['wolof'].to_list() \n",
    "    \n",
    "    sents = french_sents + wolof_sents\n",
    "    \n",
    "    for i in range(1, len(sents), BATCH_SIZE):\n",
    "        \n",
    "        yield sents[i:i+BATCH_SIZE]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Initialize the tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = Tokenizer(models.Unigram())"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Add normalizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer.normalizer = normalizers.Replace(\" {2,}\", \" \")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Configure the pre-tokenizers"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will use the Metaspace pre-tokenizer which separates the words considering the spaces between them. It will replace the space by a character (by default the underscore \"_\")."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer.pre_tokenizer = pre_tokenizers.Metaspace()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Initialize the trainers"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will provide all of the necessary special tokens to the Trainer. \n",
    "\n",
    "**Notice that a sentence can be a group of words separated by ending marks and not only one group of words. Then we can, for example, tokenize the following sentences**: `<sep>sentence1.sentence2.sentence3<cls>` **or** `<sep>sentence1.<sep>sentence2.<cls>`. **But, the second sentence is composed of two separate groups. Then the two sentences will have different type ids.** "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "special_tokens = [\"<cls>\", \"<sep>\", \"<unk>\", \"<pad>\", \"<mask>\", \"<s>\", \"</s>\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer = trainers.UnigramTrainer(special_tokens=special_tokens, unk_token = \"<unk>\") # let us take the default vocab size"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Train the tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer.train_from_iterator(generate_sents(), trainer)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let us print the vocab size."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of tokens: 2961\n"
     ]
    }
   ],
   "source": [
    "print(f\"Number of tokens: {tokenizer.get_vocab_size()}\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Initialize the post-processor"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can not need the TemplateProcessor to train our corpora in a Sequence To Sequence model, but we will add it to our tokenizer. We can use it for another type of model. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 1\n"
     ]
    }
   ],
   "source": [
    "# let us recuperate the sep and cls ids\n",
    "cls_token_id = tokenizer.token_to_id(\"<cls>\")\n",
    "\n",
    "sep_token_id = tokenizer.token_to_id(\"<sep>\")\n",
    "\n",
    "print(cls_token_id, sep_token_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize the post processor\n",
    "tokenizer.post_process = processors.TemplateProcessing(\n",
    "    single=\"$A:0 <sep>:0 <cls>:2\",\n",
    "    pair=\"$A:0 <sep>:0 $B:1 <sep>:1 <cls>:2\",\n",
    "    special_tokens=[(\"<sep>\", sep_token_id), (\"<cls>\", cls_token_id)]\n",
    ")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Initialize the decoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer.decoder = decoders.Metaspace()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Save the tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer.save(\"wolof-translate/wolof_translate/tokenizers/t5_tokenizers/tokenizer_v2.json\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Make a little example"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let us recuperate random sentences from the corpora and tokenize them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "random.seed(200)\n",
    "\n",
    "french_sentence = random.choice(sentences['french'])\n",
    "\n",
    "wolof_sentence = random.choice(sentences['wolof'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"s'étendre de tout son long\""
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# print the french sentence\n",
    "french_sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Nit kookuu ci sa wet.'"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# print the wolof sentence\n",
    "wolof_sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "French tokens\n",
      "[\"▁s'\", 'étend', 're', '▁de', '▁tout', '▁son', '▁', 'long']\n",
      "French ids\n",
      "[694, 1585, 102, 14, 80, 279, 7, 1590]\n"
     ]
    }
   ],
   "source": [
    "french_encoding = tokenizer.encode(french_sentence)\n",
    "\n",
    "print(\"French tokens\")\n",
    "print(french_encoding.tokens)\n",
    "\n",
    "print(\"French ids\")\n",
    "print(french_encoding.ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wolof tokens\n",
      "['▁Nit', '▁kookuu', '▁ci', '▁sa', '▁wet', '.']\n",
      "Wolof ids\n",
      "[72, 669, 52, 140, 815, 8]\n"
     ]
    }
   ],
   "source": [
    "wolof_encoding = tokenizer.encode(wolof_sentence)\n",
    "\n",
    "print(\"Wolof tokens\")\n",
    "print(wolof_encoding.tokens)\n",
    "\n",
    "print(\"Wolof ids\")\n",
    "print(wolof_encoding.ids)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Creating the T5 custom dataset for the new sentences"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have two possibilities to use the tokenizer for fine-tuning a T5 model. "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- We can use the `PreTrainedTokenizerFast` class for which we will provide the different special tokens."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import PreTrainedTokenizerFast\n",
    "\n",
    "wrapped_tokenizer1 = PreTrainedTokenizerFast(\n",
    "    tokenizer_object=tokenizer,\n",
    "    bos_token=\"<s>\",\n",
    "    eos_token=\"</s>\",\n",
    "    unk_token=\"<unk>\",\n",
    "    pad_token=\"<pad>\",\n",
    "    cls_token=\"<cls>\",\n",
    "    sep_token=\"<sep>\",\n",
    "    mask_token=\"<mask>\",\n",
    "    padding_side=\"left\",\n",
    ")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Or give directly the tokenizer to the `T5TokenizerFast` class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import T5TokenizerFast\n",
    "\n",
    "wrapped_tokenizer2 = T5TokenizerFast(\n",
    "    tokenizer_object=tokenizer\n",
    ")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let us give them the sentences that we use as example. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input_ids': [3, 3, 3, 3, 3, 3, 3, 694, 1585, 102, 14, 80, 279, 7, 1590], 'token_type_ids': [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'attention_mask': [0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1]}"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fr_encoding = wrapped_tokenizer1(french_sentence, max_length=15, padding='max_length', truncation=True)\n",
    "\n",
    "fr_encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input_ids': [72, 669, 52, 140, 815, 8, 3, 3, 3, 3, 3, 3, 3, 3, 3], 'attention_mask': [1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0]}"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wf_encoding = wrapped_tokenizer2(wolof_sentence, max_length=15, padding='max_length', truncation=True)\n",
    "\n",
    "wf_encoding"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let us decode the wolof sentence."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Nit kookuu ci sa wet.'"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wrapped_tokenizer1.decode(wf_encoding.input_ids, skip_special_tokens=True)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see that the `T5Tokenizer` add padding to the right side of the sequence while the `PretrainedTokenizer` add the padding to the left side. We can change the padding side from the settings. But, for the next steps, let us directly use the `T5Tokenizer`."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Note that we can augment the sentences when generating them like we did when using the `GPT2 model`.** See the following notebook, [augmentation](text_augmentation.ipynb), for discussion on the augmentation method that we will use. And for a more clear explanation of the augmentation methods in NLP tasks and training, look at the following article from the web [augment_or_not](https://direct.mit.edu/coli/article/48/1/5/108844/To-Augment-or-Not-to-Augment-A-Comparative-Study)."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let us verify, before creating the custom dataset, the max length that we can get from the corpora' tokens without considering the augmentation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_len = 0\n",
    "\n",
    "for sent in sentences['french'].to_list() + sentences['wolof'].to_list():\n",
    "    \n",
    "    len_ids = len(wrapped_tokenizer2(sent).input_ids)\n",
    "    \n",
    "    if len_ids > max_len:\n",
    "        \n",
    "        max_len = len_ids\n",
    "\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "36"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# let us print the max lengths\n",
    "max_len"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We find a maximum length of **36** tokens. But considering the augmentation we can obtain more than 36 tokens because it will add modifications on the words and then it can recognize only parts of them and divide them in multiple other tokens. Let us add to the max length the fifth of it. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "43"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "max_len += max_len // 5\n",
    "\n",
    "max_len"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It is time to create our custom dataset.\n",
    "\n",
    "Signature:\n",
    "```python\n",
    "class T5SentenceDataset(Dataset):\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        data_path: str, \n",
    "        tokenizer: PreTrainedTokenizerFast\n",
    "        corpus_1: str = \"french\",\n",
    "        corpus_2: str = \"wolof\",\n",
    "        max_len: int = 51,\n",
    "        cp1_truncation: bool = False,\n",
    "        cp2_truncation: bool = False,\n",
    "        file_sep: str = \",\",\n",
    "        cp1_transformer: Union[TransformerSequences, None] = None,\n",
    "        cp2_transformer: Union[TransformerSequences, None] = None,\n",
    "        **kwargs):\n",
    "\n",
    "        pass\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting wolof-translate/wolof_translate/data/dataset_v2.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile wolof-translate/wolof_translate/data/dataset_v2.py\n",
    "from wolof_translate.utils.sent_transformers import TransformerSequences\n",
    "from transformers import PreTrainedTokenizerFast\n",
    "from torch.utils.data import Dataset\n",
    "from typing import *\n",
    "import pandas as pd\n",
    "import torch\n",
    "import re\n",
    "\n",
    "class T5SentenceDataset(Dataset):\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        data_path: str, \n",
    "        tokenizer: PreTrainedTokenizerFast,\n",
    "        corpus_1: str = \"french\",\n",
    "        corpus_2: str = \"wolof\",\n",
    "        max_len: int = 51,\n",
    "        truncation: bool = False,\n",
    "        file_sep: str = \",\",\n",
    "        cp1_transformer: Union[TransformerSequences, None] = None,\n",
    "        cp2_transformer: Union[TransformerSequences, None] = None,\n",
    "        **kwargs):\n",
    "        \n",
    "        # let us recuperate the data frame\n",
    "        self.__sentences = pd.read_csv(data_path, sep=file_sep, **kwargs)\n",
    "        \n",
    "        # let us recuperate the tokenizer\n",
    "        self.tokenizer = tokenizer\n",
    "        \n",
    "        # recuperate the first corpus' sentences\n",
    "        self.sentences_1 = self.__sentences[corpus_1].to_list()\n",
    "        \n",
    "        # recuperate the second corpus' sentences\n",
    "        self.sentences_2 = self.__sentences[corpus_2].to_list()\n",
    "        \n",
    "        # recuperate the length\n",
    "        self.length = len(self.sentences_1)\n",
    "        \n",
    "        # let us recuperate the max len\n",
    "        self.max_len = max_len\n",
    "        \n",
    "        # let us recuperate the truncation argument\n",
    "        self.truncation = truncation\n",
    "        \n",
    "        # let us initialize the transformer\n",
    "        self.cp1_transformer = cp1_transformer\n",
    "        \n",
    "        self.cp2_transformer = cp2_transformer\n",
    "        \n",
    "    def __getitem__(self, index):\n",
    "        \"\"\"Recuperate ids and attention masks of sentences at index\n",
    "\n",
    "        Args:\n",
    "            index (int): The index of the sentences to recuperate\n",
    "\n",
    "        Returns:\n",
    "            tuple: The `sentence to translate' ids`, `the attention mask of the sentence to translate`\n",
    "            `the labels' ids`\n",
    "        \"\"\"\n",
    "        sentence_1 = self.sentences_1[index]\n",
    "        \n",
    "        sentence_2 = self.sentences_2[index]\n",
    "        \n",
    "        # apply transformers if necessary\n",
    "        if not self.cp1_transformer is None:\n",
    "            \n",
    "            sentence_1 = self.cp1_transformer(sentence_1) \n",
    "        \n",
    "        if not self.cp2_transformer is None:\n",
    "            \n",
    "            sentence_2 = self.cp2_transformer(sentence_2)\n",
    "        \n",
    "        # let us encode the sentences (we provide the second sentence as labels to the tokenizer)\n",
    "        data = self.tokenizer(\n",
    "            sentence_1,\n",
    "            truncation=self.truncation,\n",
    "            max_length=self.max_len, \n",
    "            padding='max_length', \n",
    "            return_tensors=\"pt\",\n",
    "            text_target=sentence_2)\n",
    "            \n",
    "        \n",
    "        return data.input_ids.squeeze(0), data.attention_mask.squeeze(0), data.labels.squeeze(0)\n",
    "        \n",
    "    def __len__(self):\n",
    "        \n",
    "        return self.length\n",
    "    \n",
    "    def decode(self, labels: torch.Tensor):\n",
    "        \n",
    "        if labels.ndim < 2:\n",
    "            \n",
    "            labels = labels.unsqueeze(0)\n",
    "\n",
    "        sentences = self.tokenizer.batch_decode(labels, skip_special_tokens=True)\n",
    "\n",
    "        return sentences\n",
    "\n",
    "\n",
    "class SentenceDataset(T5SentenceDataset):\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        data_path: str, \n",
    "        tokenizer: PreTrainedTokenizerFast,\n",
    "        corpus_1: str = \"french\",\n",
    "        corpus_2: str = \"wolof\",\n",
    "        max_len: int = 51,\n",
    "        truncation: bool = False,\n",
    "        file_sep: str = \",\",\n",
    "        cp1_transformer: Union[TransformerSequences, None] = None,\n",
    "        cp2_transformer: Union[TransformerSequences, None] = None,\n",
    "        **kwargs):\n",
    "        \n",
    "        super().__init__(data_path, \n",
    "                        tokenizer,\n",
    "                        corpus_1,\n",
    "                        corpus_2,\n",
    "                        max_len,\n",
    "                        truncation,\n",
    "                        file_sep,\n",
    "                        cp1_transformer,\n",
    "                        cp2_transformer,\n",
    "                        **kwargs)\n",
    "        \n",
    "    def __getitem__(self, index):\n",
    "        \"\"\"Recuperate ids and attention masks of sentences at index\n",
    "\n",
    "        Args:\n",
    "            index (int): The index of the sentences to recuperate\n",
    "\n",
    "        Returns:\n",
    "            tuple: The `sentence to translate' ids`, `the attention mask of the sentence to translate`\n",
    "            `the labels' ids`\n",
    "        \"\"\"\n",
    "        sentence_1 = self.sentences_1[index]\n",
    "        \n",
    "        sentence_2 = self.sentences_2[index]\n",
    "        \n",
    "        # apply transformers if necessary\n",
    "        if not self.cp1_transformer is None:\n",
    "            \n",
    "            sentence_1 = self.cp1_transformer(sentence_1) \n",
    "        \n",
    "        if not self.cp2_transformer is None:\n",
    "            \n",
    "            sentence_2 = self.cp2_transformer(sentence_2)\n",
    "        \n",
    "        # let us encode the sentences (we provide the second sentence as labels to the tokenizer)\n",
    "        data = self.tokenizer(\n",
    "            sentence_1,\n",
    "            truncation=self.truncation,\n",
    "            max_length=self.max_len, \n",
    "            padding='max_length', \n",
    "            return_tensors=\"pt\")\n",
    "        \n",
    "        # let us encode the sentences (we provide the second sentence as labels to the tokenizer)\n",
    "        labels = self.tokenizer(\n",
    "            sentence_2,\n",
    "            truncation=self.truncation,\n",
    "            max_length=self.max_len, \n",
    "            padding='max_length', \n",
    "            return_tensors=\"pt\")\n",
    "            \n",
    "        \n",
    "        return (data.input_ids.squeeze(0),\n",
    "                data.attention_mask.squeeze(0), \n",
    "                labels.input_ids.squeeze(0),\n",
    "                labels.attention_mask.squeeze(0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "%run wolof-translate/wolof_translate/data/dataset_v2.py"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let us generate some data with their masks and decode the labels.\n",
    "\n",
    "**Note that we will use, when training the `T5 model`, train and test sets and not directly the full dataset**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize our custom dataset\n",
    "dataset = T5SentenceDataset(\"data/additional_documents/diagne_sentences/extractions.csv\", wrapped_tokenizer2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_ids, mask, labels = next(iter(DataLoader(dataset, 10, shuffle=True))) # generate 10 sentences with shuffling"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let us print the input ids."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 686,    3,    3,    3,    3,    3,    3,    3,    3,    3,    3,    3,\n",
       "            3,    3,    3,    3,    3,    3,    3,    3,    3,    3,    3,    3,\n",
       "            3,    3,    3,    3,    3,    3,    3,    3,    3,    3,    3,    3,\n",
       "            3,    3,    3,    3,    3,    3,    3,    3,    3,    3,    3,    3,\n",
       "            3,    3,    3],\n",
       "        [ 225,   14,  872,   21,    3,    3,    3,    3,    3,    3,    3,    3,\n",
       "            3,    3,    3,    3,    3,    3,    3,    3,    3,    3,    3,    3,\n",
       "            3,    3,    3,    3,    3,    3,    3,    3,    3,    3,    3,    3,\n",
       "            3,    3,    3,    3,    3,    3,    3,    3,    3,    3,    3,    3,\n",
       "            3,    3,    3],\n",
       "        [  44,  475,   14,  269,  222, 1038,  841, 1033,    7,    9,    3,    3,\n",
       "            3,    3,    3,    3,    3,    3,    3,    3,    3,    3,    3,    3,\n",
       "            3,    3,    3,    3,    3,    3,    3,    3,    3,    3,    3,    3,\n",
       "            3,    3,    3,    3,    3,    3,    3,    3,    3,    3,    3,    3,\n",
       "            3,    3,    3],\n",
       "        [  56,  105,   14,   75,  593,  209,    7,    9,    3,    3,    3,    3,\n",
       "            3,    3,    3,    3,    3,    3,    3,    3,    3,    3,    3,    3,\n",
       "            3,    3,    3,    3,    3,    3,    3,    3,    3,    3,    3,    3,\n",
       "            3,    3,    3,    3,    3,    3,    3,    3,    3,    3,    3,    3,\n",
       "            3,    3,    3],\n",
       "        [ 499,   61, 1576,   12,   68,  141,   58, 1001,  998,   12,  109, 1728,\n",
       "           22,   12,  282,   11, 1728,  998,    8,    3,    3,    3,    3,    3,\n",
       "            3,    3,    3,    3,    3,    3,    3,    3,    3,    3,    3,    3,\n",
       "            3,    3,    3,    3,    3,    3,    3,    3,    3,    3,    3,    3,\n",
       "            3,    3,    3],\n",
       "        [ 284,  129, 1475,    3,    3,    3,    3,    3,    3,    3,    3,    3,\n",
       "            3,    3,    3,    3,    3,    3,    3,    3,    3,    3,    3,    3,\n",
       "            3,    3,    3,    3,    3,    3,    3,    3,    3,    3,    3,    3,\n",
       "            3,    3,    3,    3,    3,    3,    3,    3,    3,    3,    3,    3,\n",
       "            3,    3,    3],\n",
       "        [1000,  405,   46,    3,    3,    3,    3,    3,    3,    3,    3,    3,\n",
       "            3,    3,    3,    3,    3,    3,    3,    3,    3,    3,    3,    3,\n",
       "            3,    3,    3,    3,    3,    3,    3,    3,    3,    3,    3,    3,\n",
       "            3,    3,    3,    3,    3,    3,    3,    3,    3,    3,    3,    3,\n",
       "            3,    3,    3],\n",
       "        [  40,   51,   36,   18,  391,    8,    3,    3,    3,    3,    3,    3,\n",
       "            3,    3,    3,    3,    3,    3,    3,    3,    3,    3,    3,    3,\n",
       "            3,    3,    3,    3,    3,    3,    3,    3,    3,    3,    3,    3,\n",
       "            3,    3,    3,    3,    3,    3,    3,    3,    3,    3,    3,    3,\n",
       "            3,    3,    3],\n",
       "        [  50,  122,    3,    3,    3,    3,    3,    3,    3,    3,    3,    3,\n",
       "            3,    3,    3,    3,    3,    3,    3,    3,    3,    3,    3,    3,\n",
       "            3,    3,    3,    3,    3,    3,    3,    3,    3,    3,    3,    3,\n",
       "            3,    3,    3,    3,    3,    3,    3,    3,    3,    3,    3,    3,\n",
       "            3,    3,    3],\n",
       "        [ 311,    7,  283,   23,    3,    3,    3,    3,    3,    3,    3,    3,\n",
       "            3,    3,    3,    3,    3,    3,    3,    3,    3,    3,    3,    3,\n",
       "            3,    3,    3,    3,    3,    3,    3,    3,    3,    3,    3,    3,\n",
       "            3,    3,    3,    3,    3,    3,    3,    3,    3,    3,    3,    3,\n",
       "            3,    3,    3]])"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input_ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0],\n",
       "        [1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0],\n",
       "        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0],\n",
       "        [1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0],\n",
       "        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0],\n",
       "        [1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0],\n",
       "        [1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0],\n",
       "        [1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0],\n",
       "        [1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0],\n",
       "        [1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0]])"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1348,    3,    3,    3,    3,    3,    3,    3,    3,    3,    3,    3,\n",
       "            3,    3,    3,    3,    3,    3,    3,    3,    3,    3,    3,    3,\n",
       "            3,    3,    3,    3,    3,    3,    3,    3,    3,    3,    3,    3,\n",
       "            3,    3,    3,    3,    3,    3,    3,    3,    3,    3,    3,    3,\n",
       "            3,    3,    3],\n",
       "        [2536,   70,  402,    3,    3,    3,    3,    3,    3,    3,    3,    3,\n",
       "            3,    3,    3,    3,    3,    3,    3,    3,    3,    3,    3,    3,\n",
       "            3,    3,    3,    3,    3,    3,    3,    3,    3,    3,    3,    3,\n",
       "            3,    3,    3,    3,    3,    3,    3,    3,    3,    3,    3,    3,\n",
       "            3,    3,    3],\n",
       "        [ 401,   13,  506,  428,   31,   53,    7,    9,    3,    3,    3,    3,\n",
       "            3,    3,    3,    3,    3,    3,    3,    3,    3,    3,    3,    3,\n",
       "            3,    3,    3,    3,    3,    3,    3,    3,    3,    3,    3,    3,\n",
       "            3,    3,    3,    3,    3,    3,    3,    3,    3,    3,    3,    3,\n",
       "            3,    3,    3],\n",
       "        [2007,   92,  144,   53,    7,    9,    3,    3,    3,    3,    3,    3,\n",
       "            3,    3,    3,    3,    3,    3,    3,    3,    3,    3,    3,    3,\n",
       "            3,    3,    3,    3,    3,    3,    3,    3,    3,    3,    3,    3,\n",
       "            3,    3,    3,    3,    3,    3,    3,    3,    3,    3,    3,    3,\n",
       "            3,    3,    3],\n",
       "        [ 635,   23,   65,   29,  439,   12,   92,   82,   88,  100,   12,   63,\n",
       "          445,   12,  314,  445,    8,    3,    3,    3,    3,    3,    3,    3,\n",
       "            3,    3,    3,    3,    3,    3,    3,    3,    3,    3,    3,    3,\n",
       "            3,    3,    3,    3,    3,    3,    3,    3,    3,    3,    3,    3,\n",
       "            3,    3,    3],\n",
       "        [   7, 2379,    3,    3,    3,    3,    3,    3,    3,    3,    3,    3,\n",
       "            3,    3,    3,    3,    3,    3,    3,    3,    3,    3,    3,    3,\n",
       "            3,    3,    3,    3,    3,    3,    3,    3,    3,    3,    3,    3,\n",
       "            3,    3,    3,    3,    3,    3,    3,    3,    3,    3,    3,    3,\n",
       "            3,    3,    3],\n",
       "        [1201, 1208,    3,    3,    3,    3,    3,    3,    3,    3,    3,    3,\n",
       "            3,    3,    3,    3,    3,    3,    3,    3,    3,    3,    3,    3,\n",
       "            3,    3,    3,    3,    3,    3,    3,    3,    3,    3,    3,    3,\n",
       "            3,    3,    3,    3,    3,    3,    3,    3,    3,    3,    3,    3,\n",
       "            3,    3,    3],\n",
       "        [  42,   37, 2146,  237,   93,    8,    3,    3,    3,    3,    3,    3,\n",
       "            3,    3,    3,    3,    3,    3,    3,    3,    3,    3,    3,    3,\n",
       "            3,    3,    3,    3,    3,    3,    3,    3,    3,    3,    3,    3,\n",
       "            3,    3,    3,    3,    3,    3,    3,    3,    3,    3,    3,    3,\n",
       "            3,    3,    3],\n",
       "        [  52,  121,    3,    3,    3,    3,    3,    3,    3,    3,    3,    3,\n",
       "            3,    3,    3,    3,    3,    3,    3,    3,    3,    3,    3,    3,\n",
       "            3,    3,    3,    3,    3,    3,    3,    3,    3,    3,    3,    3,\n",
       "            3,    3,    3,    3,    3,    3,    3,    3,    3,    3,    3,    3,\n",
       "            3,    3,    3],\n",
       "        [ 462,   15,    3,    3,    3,    3,    3,    3,    3,    3,    3,    3,\n",
       "            3,    3,    3,    3,    3,    3,    3,    3,    3,    3,    3,    3,\n",
       "            3,    3,    3,    3,    3,    3,    3,    3,    3,    3,    3,    3,\n",
       "            3,    3,    3,    3,    3,    3,    3,    3,    3,    3,    3,    3,\n",
       "            3,    3,    3]])"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "labels"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let us decode the labels."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['yëf',\n",
       " 'coppalin',\n",
       " 'Bii néeg ban õga wax?',\n",
       " 'Bule xale laa wax?',\n",
       " 'Noona góor gi dugg, xale yi gis ka, mu toog, ñépp toog.',\n",
       " 'xor',\n",
       " 'ñaanaatu',\n",
       " 'Gis naa jeeg bi.',\n",
       " 'ci biir',\n",
       " 'noonuu']"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset.decode(labels)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pytorch1-HleOW5am-py3.10",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
