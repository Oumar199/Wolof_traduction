{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Custom Transformer Training\n",
    "-------------------------------\n",
    "\n",
    "In this notebook we will train the custom transformer on multiple GPUs if they are available. Each GPU is inside an aws instance. We will use the functions that we create at [single](_custom_transformer_train_single.ipynb) to distribute the training over multiple aws instances with the PyTorch's sagemaker framework. \n",
    "\n",
    "The following steps will be pursued to achieve the work:\n",
    "\n",
    "- Parametrize the S3 bucket and recuperate the role\n",
    "- Split the data from a local csv file and place each split inside the S3 bucket\n",
    "- Place the tokenizer inside the S3 bucket\n",
    "- Place the best model inside the S3 bucket\n",
    "- Specify the arguments to pass to a python file used for compiling and training the model on multiple g4dn machines\n",
    "- Configure the PyTorch's sagemaker framework with necessary parameters and call the fit method to begin the training.\n",
    "- Download the checkpoints and the logs from S3 bucket"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from wolof_translate import *"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "➡️ Parametrize the S3 bucket"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import sagemaker\n",
    "import sagemaker\n",
    "\n",
    "# initialize a session and a region\n",
    "sagemaker_session = sagemaker.Session()\n",
    "region = sagemaker_session.boto_region_name\n",
    "\n",
    "# recuperate the default bucket and specify a prefix\n",
    "bucket = sagemaker_session.default_bucket()\n",
    "prefix = \"sagemaker/wf_translation\"\n",
    "\n",
    "# get the role\n",
    "role = sagemaker.get_execution_role()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "➡️ Split the data and add the splits into the bucket"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train is stored at: s3://sagemaker-us-east-1-634397825065/sagemaker/wf_translation/train_set.csv\n",
      "Test is stored at: s3://sagemaker-us-east-1-634397825065/sagemaker/wf_translation/test_set.csv    \n",
      "Valid is stored at: s3://sagemaker-us-east-1-634397825065/sagemaker/wf_translation/valid_set.csv\n"
     ]
    }
   ],
   "source": [
    "# specify the data directory and the data file\n",
    "data_directory = 'data/extractions/new_data/'\n",
    "data_file = 'corpora_v6.csv'\n",
    "\n",
    "# split the data\n",
    "split_data(random_state=0, data_directory=data_directory, csv_file=data_file)\n",
    "\n",
    "# upload the splits to the S3 bucket for the current session\n",
    "train_split = sagemaker_session.upload_data(\n",
    "    path=os.path.join(data_directory, 'train_set.csv'),\n",
    "    bucket=bucket,\n",
    "    key_prefix=prefix\n",
    ")\n",
    "\n",
    "valid_split = sagemaker_session.upload_data(\n",
    "    path=os.path.join(data_directory, 'valid_set.csv'),\n",
    "    bucket=bucket,\n",
    "    key_prefix=prefix\n",
    ")\n",
    "\n",
    "test_split = sagemaker_session.upload_data(\n",
    "    path=os.path.join(data_directory, 'test_set.csv'),\n",
    "    bucket=bucket,\n",
    "    key_prefix=prefix\n",
    ")\n",
    "\n",
    "# print the path where the splits are stored\n",
    "print(f'Train is stored at: {train_split}\\nTest is stored at: {test_split}\\\n",
    "    \\nValid is stored at: {valid_split}')\n",
    "\n",
    "# specify a dictionary containing the inputs\n",
    "inputs = {\n",
    "    'training': train_split,\n",
    "    'testing': test_split,\n",
    "    'validation': valid_split\n",
    "}"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "➡️ Place the tokenizer inside a bucket"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokenizer is stored at: s3://sagemaker-us-east-1-634397825065/sagemaker/wf_translation/tokenizer_v5.model\n"
     ]
    }
   ],
   "source": [
    "# path of the tokenizer\n",
    "tokenizer_path = 'wolof-translate/wolof_translate/tokenizers/t5_tokenizers/tokenizer_v5.model'\n",
    "\n",
    "# place the tokenizer inside the S3 bucket\n",
    "tokenizer = sagemaker_session.upload_data(\n",
    "    path=tokenizer_path,\n",
    "    bucket=bucket,\n",
    "    key_prefix=prefix\n",
    ")\n",
    "\n",
    "# print the path where the tokenizer is stored\n",
    "print(f'Tokenizer is stored at: {tokenizer}')\n",
    "\n",
    "# add the tokenizer to the inputs dictionary\n",
    "inputs['tokenizer'] = tokenizer"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "➡️ Place the best checkpoints' directory inside a bucket **"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import boto3\n",
    "import os\n",
    "\n",
    "# path of the model\n",
    "model_path = 'custom_transformer_v6_fw_best' # --------------------------> Must be changed when continuing training\n",
    "\n",
    "s3_client = boto3.client('s3')\n",
    "\n",
    "for root, _, files in os.walk(model_path):\n",
    "    for file in files:\n",
    "        local_path = os.path.join(root, file)\n",
    "        s3_key = os.path.relpath(local_path, model_path)\n",
    "        s3_object_key = os.path.join(prefix, s3_key)\n",
    "\n",
    "        s3_client.upload_file(local_path, bucket, s3_object_key)\n",
    "\n",
    "\n",
    "# add the S3 URI of the directory to the inputs dictionary\n",
    "inputs['model'] = os.path.join(f's3://{bucket}/{prefix}/{os.path.basename(model_path)}')\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "➡️ Specify the arguments to pass to the framework as hyperparameter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# specify the output path\n",
    "output_path = f's3://{bucket}/{prefix}/output'\n",
    "\n",
    "# specify the instance type and the instance count\n",
    "instance_type = 'ml.g4dn.2xlarge'\n",
    "instance_count = 4\n",
    "\n",
    "# specify the hyperparameters\n",
    "hyperparameters = {\n",
    "    'epochs': 1000,\n",
    "    'log_step': 10,\n",
    "    'metric_for_best_model': 'bleu',\n",
    "    'metric_objective': 'maximize',\n",
    "    'corpus_1': 'french',\n",
    "    'corpus_2': 'wolof',\n",
    "    'drop_out_rate': 0.291121690756753,\n",
    "    'd_model': 512,\n",
    "    'n_head': 8,\n",
    "    'dim_ff': 2024,\n",
    "    'n_encoders': 6,\n",
    "    'n_decoders': 6,\n",
    "    'learning_rate': None,\n",
    "    'weight_decay': 0.0,\n",
    "    'char_p': 0.082269346292589,\n",
    "    'word_p': 0.005292549318241768,\n",
    "    'end_mark': 3,\n",
    "    'label_smoothing': 0.1,\n",
    "    'max_len': 20,\n",
    "    'random_state': 0,\n",
    "    'boundaries': '2,31,59,87,115,143,171',\n",
    "    'batch_sizes': '256,128,64,32,16,8,4,2',\n",
    "    'batch_size': 256, \n",
    "    'warmup_init': True,\n",
    "    'relative_step': True,\n",
    "    'num_workers': 1,\n",
    "    'pin_memory': True,\n",
    "    'new_model_dir': 'custom_transformer_v6_fw', \n",
    "    'continue': False, # --------------------------> Must be changed when continuing training\n",
    "    'logging_dir': 'custom_transformer_fw',\n",
    "    'save_best': True,\n",
    "    'version': 6,\n",
    "    'backend': 'gloo'\n",
    "}\n",
    "\n",
    "    "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "➡️ Configuration and training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using provided s3_resource\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:sagemaker.image_uris:image_uri is not presented, retrieving image_uri based on instance_type, framework etc.\n",
      "INFO:sagemaker:Creating training-job with name: pytorch-training-2023-07-08-20-06-34-061\n"
     ]
    },
    {
     "ename": "ClientError",
     "evalue": "An error occurred (AccessDeniedException) when calling the CreateTrainingJob operation: User: arn:aws:sts::634397825065:assumed-role/LabRole/SageMaker is not authorized to perform: sagemaker:CreateTrainingJob on resource: arn:aws:sagemaker:us-east-1:634397825065:training-job/pytorch-training-2023-07-08-20-06-34-061 with an explicit deny in an identity-based policy",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mClientError\u001b[0m                               Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[13], line 16\u001b[0m\n\u001b[1;32m      4\u001b[0m estimator \u001b[38;5;241m=\u001b[39m PyTorch(\n\u001b[1;32m      5\u001b[0m     entry_point\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtrain.py\u001b[39m\u001b[38;5;124m'\u001b[39m,\n\u001b[1;32m      6\u001b[0m     role\u001b[38;5;241m=\u001b[39mrole,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     12\u001b[0m     hyperparameters\u001b[38;5;241m=\u001b[39mhyperparameters,\n\u001b[1;32m     13\u001b[0m )\n\u001b[1;32m     15\u001b[0m \u001b[38;5;66;03m# fit the estimator\u001b[39;00m\n\u001b[0;32m---> 16\u001b[0m \u001b[43mestimator\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43minputs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages/sagemaker/workflow/pipeline_context.py:300\u001b[0m, in \u001b[0;36mrunnable_by_pipeline.<locals>.wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    296\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m context\n\u001b[1;32m    298\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m _StepArguments(retrieve_caller_name(self_instance), run_func, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m--> 300\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mrun_func\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages/sagemaker/estimator.py:1249\u001b[0m, in \u001b[0;36mEstimatorBase.fit\u001b[0;34m(self, inputs, wait, logs, job_name, experiment_config)\u001b[0m\n\u001b[1;32m   1246\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_prepare_for_training(job_name\u001b[38;5;241m=\u001b[39mjob_name)\n\u001b[1;32m   1248\u001b[0m experiment_config \u001b[38;5;241m=\u001b[39m check_and_get_run_experiment_config(experiment_config)\n\u001b[0;32m-> 1249\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlatest_training_job \u001b[38;5;241m=\u001b[39m \u001b[43m_TrainingJob\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstart_new\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mexperiment_config\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1250\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mjobs\u001b[38;5;241m.\u001b[39mappend(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlatest_training_job)\n\u001b[1;32m   1251\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m wait:\n",
      "File \u001b[0;32m~/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages/sagemaker/estimator.py:2185\u001b[0m, in \u001b[0;36m_TrainingJob.start_new\u001b[0;34m(cls, estimator, inputs, experiment_config)\u001b[0m\n\u001b[1;32m   2160\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Create a new Amazon SageMaker training job from the estimator.\u001b[39;00m\n\u001b[1;32m   2161\u001b[0m \n\u001b[1;32m   2162\u001b[0m \u001b[38;5;124;03mArgs:\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   2181\u001b[0m \u001b[38;5;124;03m    all information about the started training job.\u001b[39;00m\n\u001b[1;32m   2182\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m   2183\u001b[0m train_args \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mcls\u001b[39m\u001b[38;5;241m.\u001b[39m_get_train_args(estimator, inputs, experiment_config)\n\u001b[0;32m-> 2185\u001b[0m \u001b[43mestimator\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msagemaker_session\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mtrain_args\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   2187\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mcls\u001b[39m(estimator\u001b[38;5;241m.\u001b[39msagemaker_session, estimator\u001b[38;5;241m.\u001b[39m_current_job_name)\n",
      "File \u001b[0;32m~/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages/sagemaker/session.py:877\u001b[0m, in \u001b[0;36mSession.train\u001b[0;34m(self, input_mode, input_config, role, job_name, output_config, resource_config, vpc_config, hyperparameters, stop_condition, tags, metric_definitions, enable_network_isolation, image_uri, training_image_config, algorithm_arn, encrypt_inter_container_traffic, use_spot_instances, checkpoint_s3_uri, checkpoint_local_path, experiment_config, debugger_rule_configs, debugger_hook_config, tensorboard_output_config, enable_sagemaker_metrics, profiler_rule_configs, profiler_config, environment, retry_strategy)\u001b[0m\n\u001b[1;32m    874\u001b[0m     LOGGER\u001b[38;5;241m.\u001b[39mdebug(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtrain request: \u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m\"\u001b[39m, json\u001b[38;5;241m.\u001b[39mdumps(request, indent\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m4\u001b[39m))\n\u001b[1;32m    875\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msagemaker_client\u001b[38;5;241m.\u001b[39mcreate_training_job(\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mrequest)\n\u001b[0;32m--> 877\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_intercept_create_request\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrain_request\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msubmit\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[38;5;18;43m__name__\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages/sagemaker/session.py:5370\u001b[0m, in \u001b[0;36mSession._intercept_create_request\u001b[0;34m(self, request, create, func_name)\u001b[0m\n\u001b[1;32m   5353\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_intercept_create_request\u001b[39m(\n\u001b[1;32m   5354\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m   5355\u001b[0m     request: typing\u001b[38;5;241m.\u001b[39mDict,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   5358\u001b[0m     \u001b[38;5;66;03m# pylint: disable=unused-argument\u001b[39;00m\n\u001b[1;32m   5359\u001b[0m ):\n\u001b[1;32m   5360\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"This function intercepts the create job request.\u001b[39;00m\n\u001b[1;32m   5361\u001b[0m \n\u001b[1;32m   5362\u001b[0m \u001b[38;5;124;03m    PipelineSession inherits this Session class and will override\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   5368\u001b[0m \u001b[38;5;124;03m        func_name (str): the name of the function needed intercepting\u001b[39;00m\n\u001b[1;32m   5369\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m-> 5370\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mcreate\u001b[49m\u001b[43m(\u001b[49m\u001b[43mrequest\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages/sagemaker/session.py:875\u001b[0m, in \u001b[0;36mSession.train.<locals>.submit\u001b[0;34m(request)\u001b[0m\n\u001b[1;32m    873\u001b[0m LOGGER\u001b[38;5;241m.\u001b[39minfo(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCreating training-job with name: \u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m\"\u001b[39m, job_name)\n\u001b[1;32m    874\u001b[0m LOGGER\u001b[38;5;241m.\u001b[39mdebug(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtrain request: \u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m\"\u001b[39m, json\u001b[38;5;241m.\u001b[39mdumps(request, indent\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m4\u001b[39m))\n\u001b[0;32m--> 875\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msagemaker_client\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcreate_training_job\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mrequest\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages/botocore/client.py:530\u001b[0m, in \u001b[0;36mClientCreator._create_api_method.<locals>._api_call\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    526\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m(\n\u001b[1;32m    527\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mpy_operation_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m() only accepts keyword arguments.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    528\u001b[0m     )\n\u001b[1;32m    529\u001b[0m \u001b[38;5;66;03m# The \"self\" in this scope is referring to the BaseClient.\u001b[39;00m\n\u001b[0;32m--> 530\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_make_api_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43moperation_name\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages/botocore/client.py:964\u001b[0m, in \u001b[0;36mBaseClient._make_api_call\u001b[0;34m(self, operation_name, api_params)\u001b[0m\n\u001b[1;32m    962\u001b[0m     error_code \u001b[38;5;241m=\u001b[39m parsed_response\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mError\u001b[39m\u001b[38;5;124m\"\u001b[39m, {})\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCode\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    963\u001b[0m     error_class \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mexceptions\u001b[38;5;241m.\u001b[39mfrom_code(error_code)\n\u001b[0;32m--> 964\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m error_class(parsed_response, operation_name)\n\u001b[1;32m    965\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    966\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m parsed_response\n",
      "\u001b[0;31mClientError\u001b[0m: An error occurred (AccessDeniedException) when calling the CreateTrainingJob operation: User: arn:aws:sts::634397825065:assumed-role/LabRole/SageMaker is not authorized to perform: sagemaker:CreateTrainingJob on resource: arn:aws:sagemaker:us-east-1:634397825065:training-job/pytorch-training-2023-07-08-20-06-34-061 with an explicit deny in an identity-based policy"
     ]
    }
   ],
   "source": [
    "from sagemaker.pytorch import PyTorch\n",
    "\n",
    "# specify the estimator\n",
    "estimator = PyTorch(\n",
    "    entry_point='train.py',\n",
    "    role=role,\n",
    "    py_version='py38',\n",
    "    framework_version='1.11.0',\n",
    "    instance_count=instance_count,\n",
    "    instance_type=instance_type,\n",
    "    output_path=output_path,\n",
    "    hyperparameters=hyperparameters,\n",
    ")\n",
    "\n",
    "# fit the estimator\n",
    "estimator.fit(inputs)\n",
    "    "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "➡️ Download logs and model from S3 bucket"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import boto3\n",
    "\n",
    "s3_client = boto3.client('s3')\n",
    "\n",
    "# Recuperate the current directory\n",
    "current_dir = os.getcwd()\n",
    "\n",
    "# Récupérer les emplacements de modèle et de sortie de données\n",
    "model_dir = os.environ['SM_MODEL_DIR']\n",
    "output_data_dir = os.environ['SM_OUTPUT_DATA_DIR']\n",
    "\n",
    "# Télécharger le contenu de SM_MODEL_DIR\n",
    "s3_client.download_file(model_dir, '', current_dir, recursive=True)\n",
    "\n",
    "# Télécharger le contenu de SM_OUTPUT_DATA_DIR\n",
    "s3_client.download_file(output_data_dir, '', current_dir, recursive=True)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pytorch1-HleOW5am-py3.10",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
