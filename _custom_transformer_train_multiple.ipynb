{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Custom Transformer Training\n",
    "-------------------------------\n",
    "\n",
    "In this notebook we will train the custom transformer on multiple GPUs if they are available. Each GPU is inside an aws instance. We will use the functions that we create at [single](_custom_transformer_train_single.ipynb) to distribute the training over multiple aws instances with the PyTorch's sagemaker framework. \n",
    "\n",
    "The following steps will be pursued to achieve the work:\n",
    "\n",
    "- Parametrize the S3 bucket and recuperate the role\n",
    "- Split the data from a local csv file and place each split inside the S3 bucket\n",
    "- Place the tokenizer inside the S3 bucket\n",
    "- Place the best model inside the S3 bucket\n",
    "- Specify the arguments to pass to a python file used for compiling and training the model on multiple g4dn machines\n",
    "- Configure the PyTorch's sagemaker framework with necessary parameters and call the fit method to begin the training.\n",
    "- Download the checkpoints and the logs from S3 bucket"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Oumar Kane\\AppData\\Local\\pypoetry\\Cache\\virtualenvs\\pytorch1-HleOW5am-py3.10\\lib\\site-packages\\tqdm\\auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from libraries import *"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "➡️ Parametrize the S3 bucket"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import sagemaker\n",
    "import sagemaker\n",
    "\n",
    "# initialize a session and a region\n",
    "sagemaker_session = sagemaker.Session()\n",
    "region = sagemaker_session.boto_region_name\n",
    "\n",
    "# recuperate the default bucket and specify a prefix\n",
    "bucket = sagemaker_session.default_bucket()\n",
    "prefix = \"sagemaker/wf_translation\"\n",
    "\n",
    "# get the role\n",
    "role = sagemaker.get_execution_role()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "➡️ Split the data and add the splits into the bucket"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# specify the data directory and the data file\n",
    "data_directory = 'data/extractions/new_data/'\n",
    "data_file = 'corpora_v6'\n",
    "\n",
    "# split the data\n",
    "split_data(random_state=0, data_directory=data_directory, csv_file=data_file)\n",
    "\n",
    "# upload the splits to the S3 bucket for the current session\n",
    "train_split = sagemaker_session.upload_data(\n",
    "    path=os.path.join(data_directory, 'train_set.csv'),\n",
    "    bucket=bucket,\n",
    "    key_prefix=prefix\n",
    ")\n",
    "\n",
    "valid_split = sagemaker_session.upload_data(\n",
    "    path=os.path.join(data_directory, 'valid_set.csv'),\n",
    "    bucket=bucket,\n",
    "    key_prefix=prefix\n",
    ")\n",
    "\n",
    "test_split = sagemaker_session.upload_data(\n",
    "    path=os.path.join(data_directory, 'test_set.csv'),\n",
    "    bucket=bucket,\n",
    "    key_prefix=prefix\n",
    ")\n",
    "\n",
    "# print the path where the splits are stored\n",
    "print(f'Train is stored at: {train_split}\\nTest is stored at: {test_split}\\\n",
    "    \\nValid is stored at: {valid_split}')\n",
    "\n",
    "# specify a dictionary containing the inputs\n",
    "inputs = {\n",
    "    'training': train_split,\n",
    "    'testing': test_split,\n",
    "    'validation': valid_split\n",
    "}"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "➡️ Place the tokenizer inside a bucket"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# path of the tokenizer\n",
    "tokenizer_path = 'wolof-translate/wolof_translate/tokenizers/t5_tokenizers/tokenizer_v5.model'\n",
    "\n",
    "# place the tokenizer inside the S3 bucket\n",
    "tokenizer = sagemaker_session.upload_data(\n",
    "    path=tokenizer_path,\n",
    "    bucket=bucket,\n",
    "    key_prefix=prefix\n",
    ")\n",
    "\n",
    "# print the path where the tokenizer is stored\n",
    "print(f'Tokenizer is stored at: {tokenizer}')\n",
    "\n",
    "# add the tokenizer to the inputs dictionary\n",
    "inputs['tokenizer'] = tokenizer"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "➡️ Place the best checkpoints inside a bucket **"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# path of the last checkpoint\n",
    "model_path = 'custom_transformer_v6_fw_best' # --------------------------> Must be changed when continuing training\n",
    "\n",
    "# place the last checkpoint inside the S3 bucket\n",
    "model = sagemaker_session.upload_data(\n",
    "    path=model_path,\n",
    "    bucket=bucket,\n",
    "    key_prefix=prefix\n",
    ")\n",
    "\n",
    "# print the path where the last checkpoint is stored\n",
    "print(f'Model is stored at: {model}')\n",
    "\n",
    "# add the last checkpoint to the inputs dictionary\n",
    "inputs['model'] = model"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "➡️ Specify the arguments to pass to the framework as hyperparameter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# specify the output path\n",
    "output_path = f's3://{bucket}/{prefix}/output'\n",
    "\n",
    "# specify the instance type and the instance count\n",
    "instance_type = 'ml.g4dn.2xlarge'\n",
    "instance_count = 4\n",
    "\n",
    "# specify the hyperparameters\n",
    "hyperparameters = {\n",
    "    'epochs': 100,\n",
    "    'log_step': 10,\n",
    "    'corpus_1': 'french',\n",
    "    'corpus_2': 'wolof',\n",
    "    'drop_out_rate': 0.291121690756753,\n",
    "    'd_model': 512,\n",
    "    'n_head': 8,\n",
    "    'dim_ff': 2024,\n",
    "    'n_encoders': 6,\n",
    "    'n_decoders': 6,\n",
    "    'learning_rate': None,\n",
    "    'weight_decay': 0.0,\n",
    "    'char_p': 0.082269346292589,\n",
    "    'word_p': 0.005292549318241768,\n",
    "    'end_mark': 3,\n",
    "    'label_smoothing': 0.1,\n",
    "    'max_len': 20,\n",
    "    'random_state': 0,\n",
    "    'boundaries': '2,31,59,87,115,143,171',\n",
    "    'batch_sizes': '256,128,64,32,16,8,4,2',\n",
    "    'batch_size': 256, \n",
    "    'warmup_init': True,\n",
    "    'relative_step': True,\n",
    "    'num_workers': 1,\n",
    "    'pin_memory': True,\n",
    "    'new_model_dir': 'custom_transformer_v6_fw', \n",
    "    'continue': False, # --------------------------> Must be changed when continuing training\n",
    "    'logging_dir': 'custom_transformer_fw',\n",
    "    'save_best': True,\n",
    "    'version': 6,\n",
    "    'backend': 'gloo'\n",
    "}\n",
    "\n",
    "    "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "➡️ Configuration and training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sagemaker.pytorch import PyTorch\n",
    "\n",
    "# specify the estimator\n",
    "estimator = PyTorch(\n",
    "    entry_point='train.py',\n",
    "    role=role,\n",
    "    py_version='py38',\n",
    "    framework_version='1.11.0',\n",
    "    instance_count=instance_count,\n",
    "    instance_type=instance_type,\n",
    "    output_path=output_path,\n",
    "    hyperparameters=hyperparameters,\n",
    ")\n",
    "\n",
    "# fit the estimator\n",
    "estimator.fit(inputs)\n",
    "    "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "➡️ Download logs and model from S3 bucket"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import boto3\n",
    "\n",
    "s3_client = boto3.client('s3')\n",
    "\n",
    "# Recuperate the current directory\n",
    "current_dir = os.getcwd()\n",
    "\n",
    "# Récupérer les emplacements de modèle et de sortie de données\n",
    "model_dir = os.environ['SM_MODEL_DIR']\n",
    "output_data_dir = os.environ['SM_OUTPUT_DATA_DIR']\n",
    "\n",
    "# Télécharger le contenu de SM_MODEL_DIR\n",
    "s3_client.download_file(model_dir, '', current_dir, recursive=True)\n",
    "\n",
    "# Télécharger le contenu de SM_OUTPUT_DATA_DIR\n",
    "s3_client.download_file(output_data_dir, '', current_dir, recursive=True)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pytorch1-HleOW5am-py3.10",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
