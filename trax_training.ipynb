{
  "cells": [
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "Qajks4HgDTiY"
      },
      "source": [
        "Training a transformer on the Wolof-French parallel corpus: Test 1\n",
        "--------------------------------\n",
        "\n",
        "Tutorials:\n",
        "\n",
        "- [google_colab_tutorial](https://colab.research.google.com/github/google/trax/blob/master/trax/intro.ipynb)\n",
        "\n",
        "- [Trax_documentation](https://trax-ml.readthedocs.io/en/latest/index.html)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YDRPMcKRNOzu",
        "outputId": "ebc2db25-f46e-4c55-80b6-3b1731836e21"
      },
      "outputs": [],
      "source": [
        "# # some installations\n",
        "# !pip install evaluate -q\n",
        "# !pip install sacrebleu -q\n",
        "# !pip install transformers -q\n",
        "# !pip install tokenizers -q\n",
        "# !pip install nlpaug -q\n",
        "# !pip install -q wandb --upgrade\n",
        "# !pip install -q sentencepiece\n",
        "# !pip install -q -U trax"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OVq7sbZWNb_v",
        "outputId": "c883095c-332a-44d9-aede-d70263452280"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "env: WANDB_LOG_MODEL=true\n",
            "env: WANDB_API_KEY=53c099408fab02d1e4fff7386e8dfc1e759689a1\n"
          ]
        }
      ],
      "source": [
        "# define the wandb environment without notebook\n",
        "%env WANDB_LOG_MODEL=true\n",
        "%env WANDB_API_KEY=53c099408fab02d1e4fff7386e8dfc1e759689a1"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "p8XtzmnQDTid"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "c:\\Users\\Oumar Kane\\AppData\\Local\\pypoetry\\Cache\\virtualenvs\\pytorch1-HleOW5am-py3.10\\lib\\site-packages\\tqdm\\auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
            "  from .autonotebook import tqdm as notebook_tqdm\n"
          ]
        }
      ],
      "source": [
        "import sys\n",
        "\n",
        "# add the main directory path\n",
        "# path = \"/content/drive/MyDrive/Memoire/subject2/\"\n",
        "\n",
        "# sys.path.extend([path])\n",
        "\n",
        "import re\n",
        "import trax\n",
        "import evaluate\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from typing import *\n",
        "import trax.fastmath as fsnp\n",
        "from trax.supervised import training\n",
        "from wolof_translate.utils.sent_corrections import *\n",
        "from wolof_translate.utils.split_with_valid import split_data\n",
        "from transformers import T5TokenizerFast, PreTrainedTokenizerFast\n",
        "from wolof_translate.utils.improvements.end_marks import add_end_mark\n",
        "from wolof_translate.utils.sent_transformers import TransformerSequences"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "ftYXoTCMDTig"
      },
      "source": [
        "The following steps are necessary:\n",
        "\n",
        "1. Loading data with Trax (see [tensorflow_dataset](https://www.tensorflow.org/guide/data?hl=fr))\n",
        "2. Initializing the model\n",
        "3. Initializing and beginning the training on some steps: identifying the optimizer, the loss function\n",
        "4. Evaluating the model"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "30oYf0FfDTih"
      },
      "source": [
        "## Load the data"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "ZpatwjuwMIm-"
      },
      "source": [
        "Let us add the sentencepiece tokenizer that we will use."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "0SP9fZAtDTii"
      },
      "outputs": [],
      "source": [
        "tk_path = f'wolof-translate/wolof_translate/tokenizers/t5_tokenizers/tokenizer_v5.model' # the path\n",
        "\n",
        "tokenizer = T5TokenizerFast(vocab_file = tk_path) # the tokenizer"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "nldrXtGrQJcx"
      },
      "source": [
        "Let us create bellow a generator which will load the tokenized sentences."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "LUEmbCLFMUZ7"
      },
      "outputs": [],
      "source": [
        "def load_tokens(path: str, tokenizer: PreTrainedTokenizerFast, input_column: str = 'french',\n",
        "                target_column: str = 'wolof', max_length: int = 21,\n",
        "                transformations: dict = {'french': None, 'wolof': None}):\n",
        "  \"\"\"Load the tokens\n",
        "\n",
        "  Args:\n",
        "    path (str): Path of the dataset. A csv file.\n",
        "    input_column (str): The input column. Defaults to 'french'.\n",
        "    target_column (str): The target column. Defaults to 'wolof'.\n",
        "    max_length (int): The max length. Defaults to 21.\n",
        "    transformations (dict): The transformations to make. Defaults to {'french': None, 'wolof': None}\n",
        "  \"\"\"\n",
        "\n",
        "  # load the data set\n",
        "  data_set = pd.read_csv(path)\n",
        "\n",
        "  # recuperate the inputs\n",
        "  inputs = data_set[input_column]\n",
        "\n",
        "  # recuperate the targets\n",
        "  targets = data_set[target_column]\n",
        "\n",
        "  # load the sentences\n",
        "  for i in range(len(inputs)):\n",
        "\n",
        "    # recuperate the sentences\n",
        "    input = inputs[i]\n",
        "\n",
        "    target = targets[i]\n",
        "\n",
        "    # transform the sentences\n",
        "    if transformations[input_column]:\n",
        "\n",
        "      input = transformations[input_column](input)[0]\n",
        "\n",
        "    if transformations[target_column]:\n",
        "\n",
        "      target = transformations[target_column](target)[0]\n",
        "\n",
        "    # tokenize the sentences\n",
        "    input_tokens = tokenizer(input, truncation = True, max_length = max_length,\n",
        "                             padding = 'max_length')['input_ids']\n",
        "\n",
        "    target_tokens = tokenizer(target, truncation = True, max_length = max_length,\n",
        "                              padding = 'max_length')['input_ids']\n",
        "\n",
        "    # return the tokens\n",
        "    yield (np.array(input_tokens), np.array(target_tokens), np.ones(len(target_tokens)))\n",
        "\n",
        "\n"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "Hof2VMYyWT23"
      },
      "source": [
        "We can create a data pipeline for pre processing the sentences."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "e9klmog-UmiK"
      },
      "outputs": [],
      "source": [
        "# split the datas between train, test and validation sets\n",
        "split_data(random_state = 0, data_directory = f'{path}new_data/', csv_file = 'ad_sentences.csv')\n",
        "\n",
        "# initialize the datasets' paths\n",
        "train_path = f'data/extractions/new_data/train_set.csv'\n",
        "\n",
        "valid_path = f'data/extractions/new_data/valid_set.csv'\n",
        "\n",
        "# initialize the batch size (default 16)\n",
        "batch_size = 16\n",
        "\n",
        "# initialize the generator\n",
        "train_generator = iter(list(load_tokens(train_path, tokenizer)))\n",
        "\n",
        "valid_generator = iter(list(load_tokens(valid_path, tokenizer)))\n",
        "\n",
        "# the shuffler\n",
        "shuffler = trax.data.Shuffle(100)\n",
        "\n",
        "# the batch sampler\n",
        "batch = trax.data.Batch(batch_size)\n",
        "\n",
        "# initialize the data pipelines\n",
        "train_pipeline = trax.data.Serial(\n",
        "    shuffler,\n",
        "    batch,\n",
        "    trax.data.AddLossWeights(tokenizer.pad_token_id)\n",
        ")\n",
        "\n",
        "# initialize the data pipelines\n",
        "valid_pipeline = trax.data.Serial(\n",
        "    batch,\n",
        "    trax.data.AddLossWeights(tokenizer.pad_token_id)\n",
        ")"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "PonkOgvYfIh2"
      },
      "source": [
        "## The model"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "h2TVaZuhfLBg"
      },
      "source": [
        "We will use the transformer with the following parameters:\n",
        "\n",
        "- vocab_size: that's of the tokenizer\n",
        "- d model: default\n",
        "- d ff: default\n",
        "- n heads: default\n",
        "- n encoders: default\n",
        "- n decoders: default\n",
        "- max len: default\n",
        "- drop out: default\n",
        "- mode: default"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "Zoz0-k6WcuHi"
      },
      "outputs": [],
      "source": [
        "# initialize the model\n",
        "model = trax.models.Transformer(len(tokenizer))"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "7yWh4Qx_gfjH"
      },
      "source": [
        "## Training task"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "DBIUP3eDgqY_"
      },
      "source": [
        "The Adafactor will be used as optimizer and the cross entropy loss function. For the evaluation we will use the Cross entropy loss, the BLEU score and the Accuracy score."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 59,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KjduKHthGvah",
        "outputId": "1cb2130a-895c-4d54-a348-209fa17572da"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/ipykernel/ipkernel.py:283: DeprecationWarning: `should_run_async` will not call `transform_cell` automatically in the future. Please pass the result to `transformed_cell` argument and any exception that happen during thetransform in `preprocessing_exc_tuple` in IPython 7.17 and above.\n",
            "  and should_run_async(code)\n"
          ]
        }
      ],
      "source": [
        "# creating the BLEU score layer\n",
        "\n",
        "class BLEU(trax.layers.Layer):\n",
        "\n",
        "  def __init__(self, tokenizer):\n",
        "\n",
        "    super().__init__(n_in = 2, name = 'Bleu')\n",
        "\n",
        "    self._tokenizer = tokenizer\n",
        "\n",
        "    self._special_tokens = self._tokenizer.convert_ids_to_tokens(self._tokenizer.all_special_ids)\n",
        "\n",
        "    self._name = 'Bleu'\n",
        "\n",
        "    self._metric = evaluate.load('sacrebleu')\n",
        "\n",
        "    # self._n_in = 2\n",
        "\n",
        "    # self._n_out = 1\n",
        "\n",
        "  def postprocess_text(self, preds, labels):\n",
        "\n",
        "    preds = [pred.strip() for pred in preds]\n",
        "\n",
        "    labels = [[label.strip()] for label in labels]\n",
        "\n",
        "    return preds, labels\n",
        "\n",
        "  def forward(self, preds, labels):\n",
        "\n",
        "    preds = np.argmax(preds, axis = -1)\n",
        "\n",
        "    # labels = inputs[1]\n",
        "\n",
        "    decoded_preds = self._tokenizer.batch_decode(preds, skip_special_tokens=True) if not self.decoder else self.decoder(preds)\n",
        "\n",
        "    labels = np.where(labels != -100, labels, self._tokenizer.pad_token_id)\n",
        "\n",
        "    decoded_labels = self._tokenizer.batch_decode(labels, skip_special_tokens=True) if not self.decoder else self.decoder(labels)\n",
        "\n",
        "    decoded_preds, decoded_labels = self.postprocess_text(decoded_preds, decoded_labels)\n",
        "    \n",
        "    result = self._metric.compute(predictions=decoded_preds, references=decoded_labels)\n",
        "\n",
        "    result = np.round(result[\"score\"], 4)\n",
        "\n",
        "    return np.array(result)\n",
        "  \n",
        "  def decoder(self, labels):\n",
        "\n",
        "    if labels.ndim < 2:\n",
        "            \n",
        "        labels = labels[None, :]\n",
        "\n",
        "    sentences = self._tokenizer.batch_decode(labels, skip_special_tokens=True)\n",
        "\n",
        "    return [re.sub('|'.join(self._special_tokens), '', sentence) for sentence in sentences]\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 63,
      "metadata": {
        "id": "1otK5zv5gIzy"
      },
      "outputs": [],
      "source": [
        "# optimizer\n",
        "optimizer = trax.optimizers.Adafactor(1e-4)\n",
        "\n",
        "# loss\n",
        "loss_fn = trax.layers.CrossEntropyLossWithLogSoftmax()\n",
        "accuracy = trax.layers.Accuracy()\n",
        "bleu = BLEU(tokenizer)\n",
        "\n",
        "# initialize the training task\n",
        "training_task = training.TrainTask(\n",
        "    labeled_data=train_pipeline(train_generator),\n",
        "    loss_layer = loss_fn,\n",
        "    optimizer = optimizer,\n",
        "    n_steps_per_checkpoint=50\n",
        ")\n",
        "\n",
        "# initialize the validation task\n",
        "validation_task = training.EvalTask(\n",
        "    labeled_data = valid_pipeline(valid_generator),\n",
        "    metrics = [loss_fn, accuracy]\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 64,
      "metadata": {
        "id": "bC4JKOh22_53"
      },
      "outputs": [],
      "source": [
        "# metric((np.array([[0.3, 0.7], [0.1, 0.9], [0.5, 0.5]]), np.array([[3], [1], [9]]), np.array([[1], [1], [1]])))"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "F_7FZzx0khOu"
      },
      "source": [
        "## Training"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 67,
      "metadata": {
        "id": "Ls5Xbs4Fh4Cw"
      },
      "outputs": [],
      "source": [
        "# initialize the output directory\n",
        "output_dir = f'{path}training/outputs'\n",
        "\n",
        "!rm -rf {output_dir}\n",
        "\n",
        "# initialize the training loop\n",
        "training_loop = training.Loop(model,\n",
        "                              training_task,\n",
        "                              eval_tasks = [validation_task],\n",
        "                              checkpoint_high_metric = 'Accuracy',\n",
        "                              output_dir = output_dir)"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "FsDtQhK4n4fg"
      },
      "source": [
        "Run the training task."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CgoKq9X6k2rT",
        "outputId": "d62eb91d-65d3-45f7-c870-b93e765f492e"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/trax/layers/base.py:851: FutureWarning: GzipFile was opened for writing, but this will change in future Python releases.  Specify the mode argument for opening it for writing.\n",
            "  with gzip.GzipFile(fileobj=f, compresslevel=compresslevel) as gzipf:\n"
          ]
        }
      ],
      "source": [
        "training_loop.run(2000)"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "pytorch1-HleOW5am-py3.10",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.8"
    },
    "orig_nbformat": 4
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
