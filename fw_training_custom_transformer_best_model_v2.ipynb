{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Training the best custom Transformer ðŸ¤–\n",
    "-----------------------------------\n",
    "\n",
    "In this notebook, we will continue the training of the best custom transformer on the new extracted sentences from the bool **Grammaire de Wolof Moderne**. We obtained, after a hyperparameter tuning with `wandb`, a best bleu score of **?** for french to wolof translation model. We provide, bellow, the main evaluation figures, obtained from the hyperparameter search step.\n",
    "\n",
    "- Parallel coordinates:\n",
    "\n",
    "- Parameter importance (from [panel]()):\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let us add some libraries bellow:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Oumar Kane\\AppData\\Local\\pypoetry\\Cache\\virtualenvs\\pytorch1-HleOW5am-py3.10\\lib\\site-packages\\tqdm\\auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "# let us import all necessary libraries\n",
    "from transformers import AutoModelForSeq2SeqLM, Seq2SeqTrainingArguments, Seq2SeqTrainer, T5TokenizerFast, set_seed, AdamW\n",
    "from wolof_translate.utils.sent_transformers import TransformerSequences\n",
    "from torch.nn import TransformerEncoderLayer, TransformerDecoderLayer\n",
    "from torch.utils.data import Dataset, DataLoader, random_split\n",
    "from wolof_translate.data.dataset_v2 import SentenceDataset\n",
    "from wolof_translate.utils.sent_corrections import *\n",
    "from sklearn.model_selection import train_test_split\n",
    "from torch.optim.lr_scheduler import _LRScheduler\n",
    "# from custom_rnn.utils.kwargs import Kwargs\n",
    "from torch.nn.utils.rnn import pad_sequence\n",
    "from plotly.subplots import make_subplots\n",
    "from nlpaug.augmenter import char as nac\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.nn import functional as F\n",
    "import plotly.graph_objects as go\n",
    "from tokenizers import Tokenizer\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm import tqdm, trange\n",
    "from functools import partial\n",
    "from torch.nn import utils\n",
    "from copy import deepcopy\n",
    "from torch import optim\n",
    "from typing import *\n",
    "from torch import nn\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import itertools\n",
    "import evaluate\n",
    "import random\n",
    "import string\n",
    "import shutil\n",
    "import wandb\n",
    "import torch\n",
    "import json\n",
    "import copy\n",
    "import os\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Steps"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We must add some classes that we implemented when making the hyperparameter search including:\n",
    "- The custom Sinusoidal-based encoder\n",
    "- The custom Size prediction module\n",
    "- The custom Transformer requiring the `pytorch encoder and decoder stacked layers`\n",
    "- The custom Transformer' learning rate scheduler\n",
    "- The custom Trainer\n",
    "\n",
    "And include them in our `wolof-translate` package.\n",
    "\n",
    "-------------------\n",
    "\n",
    "After that we will continue the training of the custom Transformer, for which we will resume its parameters from the saved checkpoints.\n",
    "\n",
    "-------------------\n",
    "\n",
    "The last part is to evaluate the model on the test set."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let us go into our pipeline ðŸ‘Œ"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Add custom modules"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Custom Positional Encoder"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let us add bellow the positional encoder module which will permit us to put the positions of the sequence elements on the embedding vector."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing wolof-translate/wolof_translate/models/transformers/position.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile wolof-translate/wolof_translate/models/transformers/position.py\n",
    "\n",
    "from torch import nn\n",
    "import numpy as np\n",
    "import torch\n",
    "\n",
    "class PositionalEncoding(nn.Module):\n",
    "\n",
    "    def __init__(self, n_poses_max: int = 500, d_model: int = 512):\n",
    "        super(PositionalEncoding, self).__init__()    \n",
    "        \n",
    "        self.n_poses = n_poses_max\n",
    "        \n",
    "        self.n_dims = d_model\n",
    "        \n",
    "        # the angle is calculated as following\n",
    "        angle = lambda pos, i: pos / 10000 ** (i / self.n_dims)\n",
    "\n",
    "        # let's initialize the different token positions\n",
    "        poses = np.arange(0, self.n_poses)\n",
    "\n",
    "        # let's initialize also the different dimension indexes\n",
    "        dims = np.arange(0, self.n_dims)\n",
    "\n",
    "        # let's initialize the index of the different positional vector values\n",
    "        circle_index = np.arange(0, self.n_dims / 2)\n",
    "\n",
    "        # let's create the possible combinations between a position and a dimension index\n",
    "        xv, yv = np.meshgrid(poses, circle_index)\n",
    "\n",
    "        # let's create a matrix which will contain all the different points initialized\n",
    "        points = np.zeros((self.n_poses, self.n_dims))\n",
    "\n",
    "        # let's calculate the circle y axis coordinates\n",
    "        points[:, ::2] = np.sin(angle(xv.T, yv.T))\n",
    "\n",
    "        # let's calculate the circle x axis coordinates\n",
    "        points[:, 1::2] = np.cos(angle(xv.T, yv.T))\n",
    "        \n",
    "        self.register_buffer('pe', torch.from_numpy(points).unsqueeze(0))\n",
    "    \n",
    "    def forward(self, input_: torch.Tensor):\n",
    "        \n",
    "        # let's scale the input\n",
    "        input_ = input_ * torch.sqrt(torch.tensor(self.n_dims))\n",
    "        \n",
    "        # let's recuperate the result of the sum between the input and the positional encoding vectors\n",
    "        return input_ + self.pe[:, :input_.size(1), :].type_as(input_)\n",
    "    "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let us define bellow the Size Prediction's module. It is a multi layer perceptron with multiple layers of linear + relu activation + drop out + layer normalization. The number of features and the number of layers, the layer normalization activation and the drop out rate are given as parameters to the module.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SizePredict(nn.Module):\n",
    "    \n",
    "    def __init__(self, input_size: int, target_size: int = 1, n_features: int = 100, n_layers: int = 1, normalization: bool = True, drop_out: float = 0.1):\n",
    "        super(SizePredict, self).__init__()\n",
    "        \n",
    "        self.layers = nn.ModuleList([])\n",
    "        \n",
    "        for l in range(n_layers):\n",
    "            \n",
    "            # we have to add batch normalization and drop_out if their are specified\n",
    "            self.layers.append(\n",
    "                nn.Sequential(\n",
    "                    nn.Linear(input_size if l == 0 else n_features, n_features),\n",
    "                    nn.ReLU(),\n",
    "                    nn.Dropout(drop_out),\n",
    "                    nn.LayerNorm(n_features) if normalization else nn.Identity(),\n",
    "                )\n",
    "            )\n",
    "        \n",
    "        # Initiate the last linear layer\n",
    "        self.output_layer = nn.Linear(n_features, target_size)\n",
    "    \n",
    "    def forward(self, input_: torch.Tensor):\n",
    "        \n",
    "        # let's pass the input into the different sequences\n",
    "        out = input_\n",
    "        \n",
    "        for layer in self.layers:\n",
    "            \n",
    "            out = layer(out)\n",
    "        \n",
    "        # return the final result (you have to take the absolute value of the result to make the number positive)\n",
    "        return self.output_layer(out)\n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pytorch1-HleOW5am-py3.10",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
