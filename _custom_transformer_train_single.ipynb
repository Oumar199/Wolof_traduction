{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Custom Transformer Training\n",
    "-------------------------------\n",
    "\n",
    "In this notebook we will train the custom transformer on multiple GPUs if they are available. The GPUs are in a single machine. In [multiple](_custom_transformer_train_multiple.ipynb), we will use sagemaker to distribute the training of the model over multiple instances. \n",
    "\n",
    "We will pursue the following steps:\n",
    "\n",
    "- Load the libraries\n",
    "- Creating function to recuperate datasets (arguments: char_p, word_p, max_len, end_mark, corpus_1, corpus_2, data_directory)\n",
    "- Training (The model is automatically saved)(arguments: config dictionary initialized before)\n",
    "- Predictions"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "➡️ Import the libraries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Oumar Kane\\AppData\\Local\\pypoetry\\Cache\\virtualenvs\\pytorch1-HleOW5am-py3.10\\lib\\site-packages\\tqdm\\auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "Global seed set to 0\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from wolof_translate import *\n",
    "\n",
    "# specify a seed for everything\n",
    "lt.seed_everything(0)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "➡️ Function to recuperate datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting wolof-translate/wolof_translate/utils/recuperate_datasets.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile wolof-translate/wolof_translate/utils/recuperate_datasets.py\n",
    "from wolof_translate import *\n",
    "\n",
    "def recuperate_datasets(char_p: float, word_p: float, max_len: int, end_mark: int, tokenizer: T5TokenizerFast,\n",
    "                        corpus_1: str = 'french', corpus_2: str = 'wolof', \n",
    "                        train_file: str = 'data/extractions/new_data/train_set.csv', \n",
    "                        test_file: str = 'data/extractions/new_data/test_file.csv'):\n",
    "\n",
    "  # Let us recuperate the end_mark adding option\n",
    "  if end_mark == 1:\n",
    "    # Create augmentation to add on French sentences\n",
    "    fr_augmentation_1 = TransformerSequences(nac.KeyboardAug(aug_char_p=char_p, aug_word_p=word_p,\n",
    "                                                             aug_word_max = max_len),\n",
    "                                          remove_mark_space, delete_guillemet_space, add_mark_space)\n",
    "\n",
    "    fr_augmentation_2 = TransformerSequences(remove_mark_space, delete_guillemet_space, add_mark_space)\n",
    "    \n",
    "  else:\n",
    "    \n",
    "    if end_mark == 2:\n",
    "\n",
    "      end_mark_fn = partial(add_end_mark, end_mark_to_remove = '!', replace = True)\n",
    "    \n",
    "    elif end_mark == 3:\n",
    "\n",
    "      end_mark_fn = partial(add_end_mark)\n",
    "    \n",
    "    elif end_mark == 4:\n",
    "\n",
    "      end_mark_fn = partial(add_end_mark, end_mark_to_remove = '!')\n",
    "    \n",
    "    else:  \n",
    "        \n",
    "        raise ValueError(f'No end mark number {end_mark}')\n",
    "\n",
    "    # Create augmentation to add on French sentences\n",
    "    fr_augmentation_1 = TransformerSequences(nac.KeyboardAug(aug_char_p=char_p, aug_word_p=word_p,\n",
    "                                                             aug_word_max = max_len),\n",
    "                                          remove_mark_space, delete_guillemet_space, add_mark_space, end_mark_fn)\n",
    "    \n",
    "    fr_augmentation_2 = TransformerSequences(remove_mark_space, delete_guillemet_space, add_mark_space, end_mark_fn)\n",
    "    \n",
    "  # Recuperate the train dataset\n",
    "  train_dataset_aug = SentenceDataset(train_file,\n",
    "                                        tokenizer,\n",
    "                                        truncation = False,\n",
    "                                        cp1_transformer = fr_augmentation_1,\n",
    "                                        cp2_transformer = fr_augmentation_2,\n",
    "                                        corpus_1=corpus_1,\n",
    "                                        corpus_2=corpus_2\n",
    "                                        )\n",
    "\n",
    "  # Recuperate the valid dataset\n",
    "  valid_dataset = SentenceDataset(test_file,\n",
    "                                        tokenizer,\n",
    "                                        cp1_transformer = fr_augmentation_2,\n",
    "                                        cp2_transformer = fr_augmentation_2,\n",
    "                                        corpus_1=corpus_1,\n",
    "                                        corpus_2=corpus_2,\n",
    "                                        truncation = False)\n",
    "  \n",
    "  # Return the datasets\n",
    "  return train_dataset_aug, valid_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "%run wolof-translate/wolof_translate/utils/recuperate_datasets.py"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "➡️ Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# initialize the configurations\n",
    "config = {\n",
    "    'epochs': 15,\n",
    "    'max_epoch': None,\n",
    "    'log_step': 5,\n",
    "    'metric_for_best_model': 'bleu',\n",
    "    'metric_objective': 'maximize',\n",
    "    'corpus_1': 'french',\n",
    "    'corpus_2': 'wolof',\n",
    "    'train_file': 'data/extractions/new_data/train_set.csv',\n",
    "    'test_file': 'data/extractions/new_data/valid_set.csv',\n",
    "    'drop_out_rate': 0.291121690756753,\n",
    "    'd_model': 512,\n",
    "    'n_head': 8,\n",
    "    'dim_ff': 2024,\n",
    "    'n_encoders': 6,\n",
    "    'n_decoders': 6,\n",
    "    'learning_rate': None,\n",
    "    'weight_decay': 0.0,\n",
    "    'char_p': 0.082269346292589,\n",
    "    'word_p': 0.005292549318241768,\n",
    "    'end_mark': 3,\n",
    "    'label_smoothing': 0.1,\n",
    "    'max_len': 20,\n",
    "    'random_state': 0,\n",
    "    'boundaries': [2, 31, 59, 87, 115, 143, 171],\n",
    "    'batch_sizes': [256, 128, 64, 32, 16, 8, 4, 2],\n",
    "    'batch_size': None, \n",
    "    'warmup_init': True,\n",
    "    'relative_step': True,\n",
    "    'num_workers': 0,\n",
    "    'pin_memory': False,\n",
    "    # --------------------> Must be changed when continuing a training\n",
    "    'model_dir': 'data/checkpoints/custom_transformer_v6_fw_best',\n",
    "    'new_model_dir': 'data/checkpoints/custom_transformer_v6_fw',\n",
    "    'continue': False, # --------------------------> Must be changed when continuing training\n",
    "    'logging_dir': 'data/logs/custom_transformer_fw',\n",
    "    'save_best': True,\n",
    "    'tokenizer_path': 'wolof-translate/wolof_translate/tokenizers/t5_tokenizers/tokenizer_v5.model',\n",
    "    'data_directory': 'data/extractions/new_data/',\n",
    "    'data_file': 'corpora_v6.csv',\n",
    "    'version': 6,\n",
    "    # in the case of a distributed training\n",
    "    'backend': None,\n",
    "    'hosts': [],\n",
    "    'current_host': None,\n",
    "    'num_gpus': 5,\n",
    "    'logger': None,\n",
    "    'return_trainer': True,\n",
    "    'include_split': True,\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %%writefile wolof-translate/wolof_translate/utils/training.py\n",
    "from wolof_translate import *\n",
    "import warnings\n",
    "\n",
    "def train(config: dict):\n",
    "    \n",
    "    # ---------------------------------------\n",
    "    # add distribution if necessary (https://github.com/aws/amazon-sagemaker-examples/blob/main/sagemaker-python-sdk/pytorch_mnist/mnist.py)\n",
    "    \n",
    "    logger = config['logger']\n",
    "    \n",
    "    is_distributed = len(config['hosts']) > 1 and config['backend'] is not None\n",
    "    \n",
    "    use_cuda = config['num_gpus'] > 0\n",
    "    \n",
    "    config.update({\"num_workers\": 1, \"pin_memory\": True} if use_cuda else {})\n",
    "\n",
    "    if not logger is None:\n",
    "        \n",
    "        logger.debug(\"Distributed training - {}\".format(is_distributed))\n",
    "        \n",
    "        logger.debug(\"Number of gpus available - {}\".format(config['num_gpus']))\n",
    "        \n",
    "    if is_distributed:\n",
    "        # Initialize the distributed environment.\n",
    "        world_size = len(config['hosts'])\n",
    "        \n",
    "        os.environ[\"WORLD_SIZE\"] = str(world_size)\n",
    "        \n",
    "        host_rank = config['hosts'].index(config['current_host'])\n",
    "        \n",
    "        os.environ[\"RANK\"] = str(host_rank)\n",
    "        \n",
    "        dist.init_process_group(backend=config['backend'], rank=host_rank, world_size=world_size)\n",
    "        \n",
    "        if not logger is None: logger.info(\n",
    "            \"Initialized the distributed environment: '{}' backend on {} nodes. \".format(\n",
    "                config['backend'], dist.get_world_size()\n",
    "            )\n",
    "            + \"Current host rank is {}. Number of gpus: {}\".format(dist.get_rank(), config['num_gpus'])\n",
    "        )\n",
    "    # ---------------------------------------\n",
    "    \n",
    "    # split the data\n",
    "    if config['include_split']: split_data(config['random_state'], config['data_directory'], config['data_file'])\n",
    "\n",
    "    # recuperate the tokenizer\n",
    "    tokenizer = T5TokenizerFast(config['tokenizer_path'])\n",
    "    \n",
    "    # recuperate train and test set\n",
    "    train_dataset, test_dataset = recuperate_datasets(config['char_p'],\n",
    "                                                        config['word_p'], config['max_len'],\n",
    "                                                        config['end_mark'], tokenizer, config['corpus_1'],\n",
    "                                                        config['corpus_2'],\n",
    "                                                        config['train_file'], config['test_file'])\n",
    "    \n",
    "    # initialize the evaluation object\n",
    "    evaluation = TranslationEvaluation(tokenizer, train_dataset.decode)\n",
    "\n",
    "    # let us initialize the trainer\n",
    "    trainer = ModelRunner(model = Transformer, version=config['version'], seed = 0, evaluation = evaluation, optimizer = Adafactor)\n",
    "\n",
    "    # initialize the encoder and the decoder layers\n",
    "    encoder_layer = nn.TransformerEncoderLayer(config['d_model'],\n",
    "                                                config['n_head'],\n",
    "                                                config['dim_ff'],\n",
    "                                                config['drop_out_rate'], batch_first = True)\n",
    "\n",
    "    decoder_layer = nn.TransformerDecoderLayer(config['d_model'],\n",
    "                                                config['n_head'],\n",
    "                                                config['dim_ff'],\n",
    "                                                config['drop_out_rate'], batch_first = True)\n",
    "\n",
    "    # let us initialize the encoder and the decoder\n",
    "    encoder = nn.TransformerEncoder(encoder_layer, config['n_encoders'])\n",
    "\n",
    "    decoder = nn.TransformerDecoder(decoder_layer, config['n_decoders'])\n",
    "\n",
    "    #-------------------------------------\n",
    "    # in the case when the linear learning rate scheduler with warmup is used\n",
    "    \n",
    "    # let us calculate the appropriate warmup steps (let us take a max epoch of 100)\n",
    "    # length = len(train_dataset)\n",
    "\n",
    "    # n_steps = length // config['batch_size']\n",
    "\n",
    "    # num_steps = config['max_epoch'] * n_steps\n",
    "\n",
    "    # warmup_steps = (config['max_epoch'] * n_steps) * config['warmup_ratio']\n",
    "\n",
    "    # Initialize the scheduler parameters\n",
    "    # scheduler_args = {'num_warmup_steps': warmup_steps, 'num_training_steps': num_steps}\n",
    "    #-------------------------------------\n",
    "\n",
    "    # Initialize the transformer parameters\n",
    "    model_args = {\n",
    "        'vocab_size': len(tokenizer),\n",
    "        'encoder': encoder,\n",
    "        'decoder': decoder,\n",
    "        'class_criterion': nn.CrossEntropyLoss(label_smoothing = config['label_smoothing']),\n",
    "        'max_len': config['max_len']\n",
    "    }\n",
    "\n",
    "    # Initialize the optimizer parameters\n",
    "    optimizer_args = {\n",
    "        'lr': config['learning_rate'],\n",
    "        'weight_decay': config['weight_decay'],\n",
    "        # 'betas': (0.9, 0.98),\n",
    "        'warmup_init': config['warmup_init'],\n",
    "        'relative_step': config['relative_step']\n",
    "    }\n",
    "\n",
    "    # ----------------------------\n",
    "    # initialize the bucket samplers for distributed environment\n",
    "    boundaries = config['boundaries']\n",
    "    batch_sizes = config['batch_sizes']\n",
    "\n",
    "    train_sampler = SequenceLengthBatchSampler(train_dataset,\n",
    "                                                boundaries = boundaries,\n",
    "                                                batch_sizes = batch_sizes)\n",
    "\n",
    "    test_sampler = SequenceLengthBatchSampler(test_dataset,\n",
    "                                                boundaries = boundaries,\n",
    "                                                batch_sizes = batch_sizes)\n",
    "\n",
    "    # ------------------------------\n",
    "    # initialize a bucket sampler with fixed batch size in the case of single machine\n",
    "    # with parallelization on multiple gpus\n",
    "    # train_sampler = BucketSampler(train_dataset, config['batch_size'])\n",
    "\n",
    "    # test_sampler = BucketSampler(test_dataset, config['batch_size'])\n",
    "    \n",
    "    # ------------------------------\n",
    "\n",
    "    # Initialize the loaders parameters\n",
    "    train_loader_args = {'batch_sampler': train_sampler, 'collate_fn': collate_fn,\n",
    "                        'num_workers': config['num_workers'], 'pin_memory': config['pin_memory']}\n",
    "\n",
    "    test_loader_args = {'batch_sampler': test_sampler, 'collate_fn': collate_fn,\n",
    "                        'num_workers': config['num_workers'], 'pin_memory': config['pin_memory']}\n",
    "\n",
    "    # Add the datasets and hyperparameters to trainer\n",
    "    trainer.compile(train_dataset, test_dataset, tokenizer, train_loader_args,\n",
    "                    test_loader_args, optimizer_kwargs = optimizer_args,\n",
    "                    model_kwargs = model_args,\n",
    "                    # lr_scheduler=get_linear_schedule_with_warmup,\n",
    "                    # lr_scheduler_kwargs=scheduler_args,\n",
    "                    predict_with_generate = True,\n",
    "                    is_distributed=is_distributed,\n",
    "                    logging_dir=config['logging_dir'],\n",
    "                    dist=dist\n",
    "                    )\n",
    "\n",
    "    # load the model\n",
    "    trainer.load(config['model_dir'], load_best = not config['continue'])\n",
    "    \n",
    "    # Train the model\n",
    "    trainer.train(config['epochs'] - trainer.current_epoch, auto_save = True, log_step = config['log_step'], saving_directory=config['new_model_dir'], save_best = config['save_best'],\n",
    "                  metric_for_best_model = config['metric_for_best_model'], metric_objective = config['metric_objective'])\n",
    "    \n",
    "    if config['return_trainer']:\n",
    "        \n",
    "        return trainer\n",
    "    \n",
    "    return None\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Below train and save if we want."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from wolof_translate.utils.training import train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/10 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "For epoch 6: \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train batch number 41: 100%|██████████| 40/40 [00:33<00:00,  1.20batches/s]\n",
      " 10%|█         | 1/10 [00:33<05:01, 33.46s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=============================\n",
      "\n",
      "For epoch 7: \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train batch number 41: 100%|██████████| 40/40 [00:32<00:00,  1.24batches/s]\n",
      " 20%|██        | 2/10 [01:05<04:21, 32.75s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=============================\n",
      "\n",
      "For epoch 8: \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train batch number 41: 100%|██████████| 40/40 [00:31<00:00,  1.26batches/s]\n",
      " 30%|███       | 3/10 [01:37<03:45, 32.24s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=============================\n",
      "\n",
      "For epoch 9: \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train batch number 41: 100%|██████████| 40/40 [00:31<00:00,  1.28batches/s]\n",
      " 40%|████      | 4/10 [02:08<03:11, 31.85s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=============================\n",
      "\n",
      "For epoch 10: \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train batch number 41: 100%|██████████| 40/40 [00:34<00:00,  1.14batches/s]\n",
      "Test batch number 2:   0%|          | 0/9 [00:14<?, ?batches/s]c:\\Users\\Oumar Kane\\AppData\\Local\\pypoetry\\Cache\\virtualenvs\\pytorch1-HleOW5am-py3.10\\lib\\site-packages\\torch\\nn\\modules\\transformer.py:296: UserWarning: The PyTorch API of nested tensors is in prototype stage and will change in the near future. (Triggered internally at ..\\aten\\src\\ATen\\NestedTensorImpl.cpp:179.)\n",
      "  output = torch._nested_tensor_from_mask(output, src_key_padding_mask.logical_not(), mask_check=False)\n",
      "c:\\Users\\Oumar Kane\\AppData\\Local\\pypoetry\\Cache\\virtualenvs\\pytorch1-HleOW5am-py3.10\\lib\\site-packages\\torch\\nn\\modules\\activation.py:1160: UserWarning: Converting mask without torch.bool dtype to bool; this will negatively affect performance. Prefer to use a boolean mask directly. (Triggered internally at ..\\aten\\src\\ATen\\native\\transformers\\attention.cpp:152.)\n",
      "  return torch._native_multi_head_attention(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "embedding\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Test batch number 3:  11%|█         | 1/9 [00:16<02:13, 16.69s/batches]c:\\Users\\Oumar Kane\\AppData\\Local\\pypoetry\\Cache\\virtualenvs\\pytorch1-HleOW5am-py3.10\\lib\\site-packages\\torch\\nn\\modules\\activation.py:1160: UserWarning: Converting mask without torch.bool dtype to bool; this will negatively affect performance. Prefer to use a boolean mask directly. (Triggered internally at ..\\aten\\src\\ATen\\native\\transformers\\attention.cpp:152.)\n",
      "  return torch._native_multi_head_attention(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "embedding\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Test batch number 4:  22%|██▏       | 2/9 [00:19<01:00,  8.68s/batches]c:\\Users\\Oumar Kane\\AppData\\Local\\pypoetry\\Cache\\virtualenvs\\pytorch1-HleOW5am-py3.10\\lib\\site-packages\\torch\\nn\\modules\\activation.py:1160: UserWarning: Converting mask without torch.bool dtype to bool; this will negatively affect performance. Prefer to use a boolean mask directly. (Triggered internally at ..\\aten\\src\\ATen\\native\\transformers\\attention.cpp:152.)\n",
      "  return torch._native_multi_head_attention(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "embedding\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Test batch number 5:  33%|███▎      | 3/9 [00:23<00:39,  6.57s/batches]c:\\Users\\Oumar Kane\\AppData\\Local\\pypoetry\\Cache\\virtualenvs\\pytorch1-HleOW5am-py3.10\\lib\\site-packages\\torch\\nn\\modules\\activation.py:1160: UserWarning: Converting mask without torch.bool dtype to bool; this will negatively affect performance. Prefer to use a boolean mask directly. (Triggered internally at ..\\aten\\src\\ATen\\native\\transformers\\attention.cpp:152.)\n",
      "  return torch._native_multi_head_attention(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "embedding\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Test batch number 6:  44%|████▍     | 4/9 [00:27<00:27,  5.56s/batches]c:\\Users\\Oumar Kane\\AppData\\Local\\pypoetry\\Cache\\virtualenvs\\pytorch1-HleOW5am-py3.10\\lib\\site-packages\\torch\\nn\\modules\\activation.py:1160: UserWarning: Converting mask without torch.bool dtype to bool; this will negatively affect performance. Prefer to use a boolean mask directly. (Triggered internally at ..\\aten\\src\\ATen\\native\\transformers\\attention.cpp:152.)\n",
      "  return torch._native_multi_head_attention(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "embedding\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Test batch number 7:  56%|█████▌    | 5/9 [00:30<00:18,  4.62s/batches]c:\\Users\\Oumar Kane\\AppData\\Local\\pypoetry\\Cache\\virtualenvs\\pytorch1-HleOW5am-py3.10\\lib\\site-packages\\torch\\nn\\modules\\activation.py:1160: UserWarning: Converting mask without torch.bool dtype to bool; this will negatively affect performance. Prefer to use a boolean mask directly. (Triggered internally at ..\\aten\\src\\ATen\\native\\transformers\\attention.cpp:152.)\n",
      "  return torch._native_multi_head_attention(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "embedding\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Test batch number 8:  67%|██████▋   | 6/9 [00:36<00:14,  4.82s/batches]c:\\Users\\Oumar Kane\\AppData\\Local\\pypoetry\\Cache\\virtualenvs\\pytorch1-HleOW5am-py3.10\\lib\\site-packages\\torch\\nn\\modules\\activation.py:1160: UserWarning: Converting mask without torch.bool dtype to bool; this will negatively affect performance. Prefer to use a boolean mask directly. (Triggered internally at ..\\aten\\src\\ATen\\native\\transformers\\attention.cpp:152.)\n",
      "  return torch._native_multi_head_attention(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "embedding\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Test batch number 9:  78%|███████▊  | 7/9 [00:41<00:10,  5.06s/batches]c:\\Users\\Oumar Kane\\AppData\\Local\\pypoetry\\Cache\\virtualenvs\\pytorch1-HleOW5am-py3.10\\lib\\site-packages\\torch\\nn\\modules\\activation.py:1160: UserWarning: Converting mask without torch.bool dtype to bool; this will negatively affect performance. Prefer to use a boolean mask directly. (Triggered internally at ..\\aten\\src\\ATen\\native\\transformers\\attention.cpp:152.)\n",
      "  return torch._native_multi_head_attention(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "embedding\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Test batch number 10:  89%|████████▉ | 8/9 [00:47<00:05,  5.21s/batches]c:\\Users\\Oumar Kane\\AppData\\Local\\pypoetry\\Cache\\virtualenvs\\pytorch1-HleOW5am-py3.10\\lib\\site-packages\\torch\\nn\\modules\\activation.py:1160: UserWarning: Converting mask without torch.bool dtype to bool; this will negatively affect performance. Prefer to use a boolean mask directly. (Triggered internally at ..\\aten\\src\\ATen\\native\\transformers\\attention.cpp:152.)\n",
      "  return torch._native_multi_head_attention(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "embedding\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Test batch number 10: 100%|██████████| 9/9 [00:53<00:00,  5.92s/batches]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Metrics: {'train_loss': 33.21320210562812, 'test_loss': 7.546239852905273, 'accuracy': 0.06587777777777776, 'bleu': 0.16345555555555558, 'gen_len': 108.0}\n",
      "\n",
      "=============================\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 50%|█████     | 5/10 [03:41<04:28, 53.72s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "For epoch 11: \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train batch number 41: 100%|██████████| 40/40 [00:32<00:00,  1.23batches/s]\n",
      " 60%|██████    | 6/10 [04:13<03:06, 46.52s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=============================\n",
      "\n",
      "For epoch 12: \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train batch number 41: 100%|██████████| 40/40 [00:31<00:00,  1.25batches/s]\n",
      " 70%|███████   | 7/10 [04:45<02:05, 41.74s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=============================\n",
      "\n",
      "For epoch 13: \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train batch number 41: 100%|██████████| 40/40 [00:35<00:00,  1.12batches/s]\n",
      " 80%|████████  | 8/10 [05:21<01:19, 39.86s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=============================\n",
      "\n",
      "For epoch 14: \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train batch number 41: 100%|██████████| 40/40 [00:34<00:00,  1.15batches/s]\n",
      " 90%|█████████ | 9/10 [05:56<00:38, 38.25s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=============================\n",
      "\n",
      "For epoch 15: \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train batch number 41: 100%|██████████| 40/40 [00:33<00:00,  1.20batches/s]\n",
      "Test batch number 2:   0%|          | 0/9 [00:14<?, ?batches/s]c:\\Users\\Oumar Kane\\AppData\\Local\\pypoetry\\Cache\\virtualenvs\\pytorch1-HleOW5am-py3.10\\lib\\site-packages\\torch\\nn\\modules\\activation.py:1160: UserWarning: Converting mask without torch.bool dtype to bool; this will negatively affect performance. Prefer to use a boolean mask directly. (Triggered internally at ..\\aten\\src\\ATen\\native\\transformers\\attention.cpp:152.)\n",
      "  return torch._native_multi_head_attention(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "embedding\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Test batch number 3:  11%|█         | 1/9 [00:16<02:12, 16.51s/batches]c:\\Users\\Oumar Kane\\AppData\\Local\\pypoetry\\Cache\\virtualenvs\\pytorch1-HleOW5am-py3.10\\lib\\site-packages\\torch\\nn\\modules\\activation.py:1160: UserWarning: Converting mask without torch.bool dtype to bool; this will negatively affect performance. Prefer to use a boolean mask directly. (Triggered internally at ..\\aten\\src\\ATen\\native\\transformers\\attention.cpp:152.)\n",
      "  return torch._native_multi_head_attention(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "embedding\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Test batch number 4:  22%|██▏       | 2/9 [00:19<01:00,  8.70s/batches]c:\\Users\\Oumar Kane\\AppData\\Local\\pypoetry\\Cache\\virtualenvs\\pytorch1-HleOW5am-py3.10\\lib\\site-packages\\torch\\nn\\modules\\activation.py:1160: UserWarning: Converting mask without torch.bool dtype to bool; this will negatively affect performance. Prefer to use a boolean mask directly. (Triggered internally at ..\\aten\\src\\ATen\\native\\transformers\\attention.cpp:152.)\n",
      "  return torch._native_multi_head_attention(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "embedding\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Test batch number 5:  33%|███▎      | 3/9 [00:23<00:39,  6.53s/batches]c:\\Users\\Oumar Kane\\AppData\\Local\\pypoetry\\Cache\\virtualenvs\\pytorch1-HleOW5am-py3.10\\lib\\site-packages\\torch\\nn\\modules\\activation.py:1160: UserWarning: Converting mask without torch.bool dtype to bool; this will negatively affect performance. Prefer to use a boolean mask directly. (Triggered internally at ..\\aten\\src\\ATen\\native\\transformers\\attention.cpp:152.)\n",
      "  return torch._native_multi_head_attention(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "embedding\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Test batch number 6:  44%|████▍     | 4/9 [00:27<00:27,  5.49s/batches]c:\\Users\\Oumar Kane\\AppData\\Local\\pypoetry\\Cache\\virtualenvs\\pytorch1-HleOW5am-py3.10\\lib\\site-packages\\torch\\nn\\modules\\activation.py:1160: UserWarning: Converting mask without torch.bool dtype to bool; this will negatively affect performance. Prefer to use a boolean mask directly. (Triggered internally at ..\\aten\\src\\ATen\\native\\transformers\\attention.cpp:152.)\n",
      "  return torch._native_multi_head_attention(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "embedding\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Test batch number 7:  56%|█████▌    | 5/9 [00:30<00:17,  4.47s/batches]c:\\Users\\Oumar Kane\\AppData\\Local\\pypoetry\\Cache\\virtualenvs\\pytorch1-HleOW5am-py3.10\\lib\\site-packages\\torch\\nn\\modules\\activation.py:1160: UserWarning: Converting mask without torch.bool dtype to bool; this will negatively affect performance. Prefer to use a boolean mask directly. (Triggered internally at ..\\aten\\src\\ATen\\native\\transformers\\attention.cpp:152.)\n",
      "  return torch._native_multi_head_attention(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "embedding\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Test batch number 8:  67%|██████▋   | 6/9 [00:34<00:12,  4.24s/batches]c:\\Users\\Oumar Kane\\AppData\\Local\\pypoetry\\Cache\\virtualenvs\\pytorch1-HleOW5am-py3.10\\lib\\site-packages\\torch\\nn\\modules\\activation.py:1160: UserWarning: Converting mask without torch.bool dtype to bool; this will negatively affect performance. Prefer to use a boolean mask directly. (Triggered internally at ..\\aten\\src\\ATen\\native\\transformers\\attention.cpp:152.)\n",
      "  return torch._native_multi_head_attention(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "embedding\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Test batch number 9:  78%|███████▊  | 7/9 [00:38<00:08,  4.17s/batches]c:\\Users\\Oumar Kane\\AppData\\Local\\pypoetry\\Cache\\virtualenvs\\pytorch1-HleOW5am-py3.10\\lib\\site-packages\\torch\\nn\\modules\\activation.py:1160: UserWarning: Converting mask without torch.bool dtype to bool; this will negatively affect performance. Prefer to use a boolean mask directly. (Triggered internally at ..\\aten\\src\\ATen\\native\\transformers\\attention.cpp:152.)\n",
      "  return torch._native_multi_head_attention(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "embedding\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Test batch number 10:  89%|████████▉ | 8/9 [00:43<00:04,  4.64s/batches]c:\\Users\\Oumar Kane\\AppData\\Local\\pypoetry\\Cache\\virtualenvs\\pytorch1-HleOW5am-py3.10\\lib\\site-packages\\torch\\nn\\modules\\activation.py:1160: UserWarning: Converting mask without torch.bool dtype to bool; this will negatively affect performance. Prefer to use a boolean mask directly. (Triggered internally at ..\\aten\\src\\ATen\\native\\transformers\\attention.cpp:152.)\n",
      "  return torch._native_multi_head_attention(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "embedding\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Test batch number 10: 100%|██████████| 9/9 [00:50<00:00,  5.56s/batches]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Metrics: {'train_loss': 29.828085475497776, 'test_loss': 7.098637474907769, 'accuracy': 0.07026666666666666, 'bleu': 0.1617, 'gen_len': 108.0}\n",
      "\n",
      "=============================\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 10/10 [07:21<00:00, 44.17s/it]\n"
     ]
    }
   ],
   "source": [
    "# with warnings.catch_warnings():\n",
    "    # warnings.simplefilter(\"ignore\")\n",
    "trainer = train(config)\n",
    "\n",
    "# save if necessary"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "➡️ Predictions\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluation batch number 2:   0%|          | 0/7 [00:12<?, ?batches/s]c:\\Users\\Oumar Kane\\AppData\\Local\\pypoetry\\Cache\\virtualenvs\\pytorch1-HleOW5am-py3.10\\lib\\site-packages\\torch\\nn\\modules\\activation.py:1160: UserWarning: Converting mask without torch.bool dtype to bool; this will negatively affect performance. Prefer to use a boolean mask directly. (Triggered internally at ..\\aten\\src\\ATen\\native\\transformers\\attention.cpp:152.)\n",
      "  return torch._native_multi_head_attention(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "embedding\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluation batch number 3:  14%|█▍        | 1/7 [00:15<01:32, 15.42s/batches]c:\\Users\\Oumar Kane\\AppData\\Local\\pypoetry\\Cache\\virtualenvs\\pytorch1-HleOW5am-py3.10\\lib\\site-packages\\torch\\nn\\modules\\activation.py:1160: UserWarning: Converting mask without torch.bool dtype to bool; this will negatively affect performance. Prefer to use a boolean mask directly. (Triggered internally at ..\\aten\\src\\ATen\\native\\transformers\\attention.cpp:152.)\n",
      "  return torch._native_multi_head_attention(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "embedding\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluation batch number 4:  29%|██▊       | 2/7 [00:18<00:42,  8.45s/batches]c:\\Users\\Oumar Kane\\AppData\\Local\\pypoetry\\Cache\\virtualenvs\\pytorch1-HleOW5am-py3.10\\lib\\site-packages\\torch\\nn\\modules\\activation.py:1160: UserWarning: Converting mask without torch.bool dtype to bool; this will negatively affect performance. Prefer to use a boolean mask directly. (Triggered internally at ..\\aten\\src\\ATen\\native\\transformers\\attention.cpp:152.)\n",
      "  return torch._native_multi_head_attention(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "embedding\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluation batch number 5:  43%|████▎     | 3/7 [00:21<00:23,  5.81s/batches]c:\\Users\\Oumar Kane\\AppData\\Local\\pypoetry\\Cache\\virtualenvs\\pytorch1-HleOW5am-py3.10\\lib\\site-packages\\torch\\nn\\modules\\activation.py:1160: UserWarning: Converting mask without torch.bool dtype to bool; this will negatively affect performance. Prefer to use a boolean mask directly. (Triggered internally at ..\\aten\\src\\ATen\\native\\transformers\\attention.cpp:152.)\n",
      "  return torch._native_multi_head_attention(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "embedding\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluation batch number 6:  57%|█████▋    | 4/7 [00:25<00:15,  5.20s/batches]c:\\Users\\Oumar Kane\\AppData\\Local\\pypoetry\\Cache\\virtualenvs\\pytorch1-HleOW5am-py3.10\\lib\\site-packages\\torch\\nn\\modules\\activation.py:1160: UserWarning: Converting mask without torch.bool dtype to bool; this will negatively affect performance. Prefer to use a boolean mask directly. (Triggered internally at ..\\aten\\src\\ATen\\native\\transformers\\attention.cpp:152.)\n",
      "  return torch._native_multi_head_attention(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "embedding\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluation batch number 7:  71%|███████▏  | 5/7 [00:28<00:08,  4.12s/batches]c:\\Users\\Oumar Kane\\AppData\\Local\\pypoetry\\Cache\\virtualenvs\\pytorch1-HleOW5am-py3.10\\lib\\site-packages\\torch\\nn\\modules\\activation.py:1160: UserWarning: Converting mask without torch.bool dtype to bool; this will negatively affect performance. Prefer to use a boolean mask directly. (Triggered internally at ..\\aten\\src\\ATen\\native\\transformers\\attention.cpp:152.)\n",
      "  return torch._native_multi_head_attention(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "embedding\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluation batch number 8:  86%|████████▌ | 6/7 [00:31<00:03,  3.87s/batches]c:\\Users\\Oumar Kane\\AppData\\Local\\pypoetry\\Cache\\virtualenvs\\pytorch1-HleOW5am-py3.10\\lib\\site-packages\\torch\\nn\\modules\\activation.py:1160: UserWarning: Converting mask without torch.bool dtype to bool; this will negatively affect performance. Prefer to use a boolean mask directly. (Triggered internally at ..\\aten\\src\\ATen\\native\\transformers\\attention.cpp:152.)\n",
      "  return torch._native_multi_head_attention(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "embedding\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluation batch number 8: 100%|██████████| 7/7 [00:36<00:00,  5.23s/batches]\n"
     ]
    }
   ],
   "source": [
    "if not trainer is None:\n",
    "    \n",
    "    # recuperate the tokenizer\n",
    "    tokenizer = T5TokenizerFast(config['tokenizer_path'])\n",
    "    \n",
    "    # recuperate the test dataset\n",
    "    # initialize the transformation sequence\n",
    "    end_mark_fn = partial(add_end_mark)\n",
    "    augmentation = TransformerSequences(remove_mark_space, delete_guillemet_space, add_mark_space, end_mark_fn)\n",
    "\n",
    "\n",
    "    # let us get the test set\n",
    "    test_dataset = SentenceDataset(f\"{config['data_directory']}test_set.csv\",\n",
    "                                            tokenizer = tokenizer,\n",
    "                                            cp1_transformer = augmentation,\n",
    "                                            cp2_transformer = augmentation,\n",
    "                                            corpus_1=config['corpus_1'],\n",
    "                                            corpus_2=config['corpus_2'],\n",
    "                                            truncation = False)\n",
    "\n",
    "    # initialize the bucket samplers for distributed environment\n",
    "    boundaries = config['boundaries']\n",
    "    batch_sizes = config['batch_sizes']\n",
    "\n",
    "    test_sampler = SequenceLengthBatchSampler(test_dataset,\n",
    "                                                boundaries = boundaries,\n",
    "                                                batch_sizes = batch_sizes)\n",
    "\n",
    "    test_loader_args = {'batch_sampler': test_sampler, 'collate_fn': collate_fn,\n",
    "                            'num_workers': config['num_workers'], 'pin_memory': config['pin_memory']}\n",
    "\n",
    "    metrics, prediction = trainer.evaluate(test_dataset, test_loader_args)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'test_loss': 7.070769105638776,\n",
       " 'accuracy': 0.08175714285714286,\n",
       " 'bleu': 0.22375714285714285,\n",
       " 'gen_len': 82.14285714285714}"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>original_sentences</th>\n",
       "      <th>translations</th>\n",
       "      <th>predictions</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Vous parlez de quelle dame?</td>\n",
       "      <td>Jile jigéen jan ŋgeen wax?</td>\n",
       "      <td>??????????</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>C'est notre ami!</td>\n",
       "      <td>Suñu sarit la!</td>\n",
       "      <td>!!!!!!!!!!</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>L'homme t'avait vu.</td>\n",
       "      <td>Góor gi gisóon na la.</td>\n",
       "      <td>..........</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Celui qui est en haut!</td>\n",
       "      <td>Kenn ki ci kaw!</td>\n",
       "      <td>!!!!!!!!!!</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>À l'intérieur.</td>\n",
       "      <td>Ci biir.</td>\n",
       "      <td>..........</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>281</th>\n",
       "      <td>Ce n'est que longtemps après, quand l'égoïsme ...</td>\n",
       "      <td>Teg nañ ciy ati-at ma door a jëli ni jigéen, n...</td>\n",
       "      <td>,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>282</th>\n",
       "      <td>J'ai ressenti de l'étonnement, et même de l'in...</td>\n",
       "      <td>Li wóor te wér moo di ne bi loolu lépp weesoo,...</td>\n",
       "      <td>,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>283</th>\n",
       "      <td>À quel point les arbres aux troncs rectilignes...</td>\n",
       "      <td>Dàtti garab yaa ngi lunk, sànneeku jëm ca kow,...</td>\n",
       "      <td>,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>284</th>\n",
       "      <td>Je peux ressentir l'émotion qu'il éprouve à tr...</td>\n",
       "      <td>Li koy yëngal noonu, xam naa ko. Lan moo ko dà...</td>\n",
       "      <td>,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>285</th>\n",
       "      <td>J'étais vraiment sur le pont d'un bateau. Le b...</td>\n",
       "      <td>Ku ma laajoon fan laa nekk, ma ni la : « Man? ...</td>\n",
       "      <td>,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>286 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                    original_sentences  \\\n",
       "0                          Vous parlez de quelle dame?   \n",
       "1                                     C'est notre ami!   \n",
       "2                                  L'homme t'avait vu.   \n",
       "3                               Celui qui est en haut!   \n",
       "4                                       À l'intérieur.   \n",
       "..                                                 ...   \n",
       "281  Ce n'est que longtemps après, quand l'égoïsme ...   \n",
       "282  J'ai ressenti de l'étonnement, et même de l'in...   \n",
       "283  À quel point les arbres aux troncs rectilignes...   \n",
       "284  Je peux ressentir l'émotion qu'il éprouve à tr...   \n",
       "285  J'étais vraiment sur le pont d'un bateau. Le b...   \n",
       "\n",
       "                                          translations  \\\n",
       "0                           Jile jigéen jan ŋgeen wax?   \n",
       "1                                       Suñu sarit la!   \n",
       "2                                Góor gi gisóon na la.   \n",
       "3                                      Kenn ki ci kaw!   \n",
       "4                                             Ci biir.   \n",
       "..                                                 ...   \n",
       "281  Teg nañ ciy ati-at ma door a jëli ni jigéen, n...   \n",
       "282  Li wóor te wér moo di ne bi loolu lépp weesoo,...   \n",
       "283  Dàtti garab yaa ngi lunk, sànneeku jëm ca kow,...   \n",
       "284  Li koy yëngal noonu, xam naa ko. Lan moo ko dà...   \n",
       "285  Ku ma laajoon fan laa nekk, ma ni la : « Man? ...   \n",
       "\n",
       "                                           predictions  \n",
       "0                                           ??????????  \n",
       "1                                           !!!!!!!!!!  \n",
       "2                                           ..........  \n",
       "3                                           !!!!!!!!!!  \n",
       "4                                           ..........  \n",
       "..                                                 ...  \n",
       "281  ,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,...  \n",
       "282  ,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,...  \n",
       "283  ,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,...  \n",
       "284  ,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,...  \n",
       "285  ,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,...  \n",
       "\n",
       "[286 rows x 3 columns]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prediction"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pytorch1-HleOW5am-py3.10",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
