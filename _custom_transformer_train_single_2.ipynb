{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Custom Transformer Training\n",
    "-------------------------------\n",
    "\n",
    "In this notebook we will train the custom transformer on multiple GPUs if they are available. The GPUs are in a single machine. In [multiple](_custom_transformer_train_multiple.ipynb), we will use sagemaker to distribute the training of the model over multiple instances. \n",
    "\n",
    "We will pursue the following steps:\n",
    "\n",
    "- Load the libraries\n",
    "- Creating function to recuperate datasets (arguments: char_p, word_p, max_len, end_mark, corpus_1, corpus_2, data_directory)\n",
    "- Training (The model is automatically saved)(arguments: config dictionary initialized before)\n",
    "- Predictions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "➡️ Import the libraries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Global seed set to 0\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from wolof_translate import *\n",
    "\n",
    "# specify a seed for everything\n",
    "lt.seed_everything(0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "➡️ Function to recuperate datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting wolof-translate/wolof_translate/utils/recuperate_datasets.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile wolof-translate/wolof_translate/utils/recuperate_datasets.py\n",
    "from wolof_translate import *\n",
    "\n",
    "def recuperate_datasets(char_p: float, word_p: float, max_len: int, end_mark: int, tokenizer: T5TokenizerFast,\n",
    "                        corpus_1: str = 'french', corpus_2: str = 'wolof', \n",
    "                        train_file: str = 'data/extractions/new_data/train_set.csv', \n",
    "                        test_file: str = 'data/extractions/new_data/test_file.csv'):\n",
    "\n",
    "  # Let us recuperate the end_mark adding option\n",
    "  if end_mark == 1:\n",
    "    # Create augmentation to add on French sentences\n",
    "    fr_augmentation_1 = TransformerSequences(nac.KeyboardAug(aug_char_p=char_p, aug_word_p=word_p,\n",
    "                                                             aug_word_max = max_len),\n",
    "                                          remove_mark_space, delete_guillemet_space, add_mark_space)\n",
    "\n",
    "    fr_augmentation_2 = TransformerSequences(remove_mark_space, delete_guillemet_space, add_mark_space)\n",
    "    \n",
    "  else:\n",
    "    \n",
    "    if end_mark == 2:\n",
    "\n",
    "      end_mark_fn = partial(add_end_mark, end_mark_to_remove = '!', replace = True)\n",
    "    \n",
    "    elif end_mark == 3:\n",
    "\n",
    "      end_mark_fn = partial(add_end_mark)\n",
    "    \n",
    "    elif end_mark == 4:\n",
    "\n",
    "      end_mark_fn = partial(add_end_mark, end_mark_to_remove = '!')\n",
    "    \n",
    "    else:  \n",
    "        \n",
    "        raise ValueError(f'No end mark number {end_mark}')\n",
    "\n",
    "    # Create augmentation to add on French sentences\n",
    "    fr_augmentation_1 = TransformerSequences(nac.KeyboardAug(aug_char_p=char_p, aug_word_p=word_p,\n",
    "                                                             aug_word_max = max_len),\n",
    "                                          remove_mark_space, delete_guillemet_space, add_mark_space, end_mark_fn)\n",
    "    \n",
    "    fr_augmentation_2 = TransformerSequences(remove_mark_space, delete_guillemet_space, add_mark_space, end_mark_fn)\n",
    "    \n",
    "  # Recuperate the train dataset\n",
    "  train_dataset_aug = SentenceDataset(train_file,\n",
    "                                        tokenizer,\n",
    "                                        truncation = False,\n",
    "                                        cp1_transformer = fr_augmentation_1,\n",
    "                                        cp2_transformer = fr_augmentation_2,\n",
    "                                        corpus_1=corpus_1,\n",
    "                                        corpus_2=corpus_2\n",
    "                                        )\n",
    "\n",
    "  # Recuperate the valid dataset\n",
    "  valid_dataset = SentenceDataset(test_file,\n",
    "                                        tokenizer,\n",
    "                                        cp1_transformer = fr_augmentation_2,\n",
    "                                        cp2_transformer = fr_augmentation_2,\n",
    "                                        corpus_1=corpus_1,\n",
    "                                        corpus_2=corpus_2,\n",
    "                                        truncation = False)\n",
    "  \n",
    "  # Return the datasets\n",
    "  return train_dataset_aug, valid_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "%run wolof-translate/wolof_translate/utils/recuperate_datasets.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "➡️ Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# initialize the configurations\n",
    "config = {\n",
    "    'epochs': 100,\n",
    "    'max_epoch': None,\n",
    "    'log_step': 5,\n",
    "    'metric_for_best_model': 'test_loss',\n",
    "    'metric_objective': 'minimize',\n",
    "    'corpus_1': 'french',\n",
    "    'corpus_2': 'wolof',\n",
    "    'train_file': 'data/extractions/new_data/train_set.csv',\n",
    "    'test_file': 'data/extractions/new_data/valid_set.csv',\n",
    "    'drop_out_rate': 0.09208880734618193,\n",
    "    'd_model': 256,\n",
    "    'n_head': 4,\n",
    "    'dim_ff': 1830,\n",
    "    'n_encoders': 3,\n",
    "    'n_decoders': 3,\n",
    "    'learning_rate': None,\n",
    "    'weight_decay': 0.0,\n",
    "    'char_p': 0.4241110694728468,\n",
    "    'word_p': 0.013007053766023914,\n",
    "    'end_mark': 3,\n",
    "    'label_smoothing': 0.1,\n",
    "    'max_len': 20,\n",
    "    'random_state': 0,\n",
    "    'boundaries': [2, 31, 59, 87, 115, 143, 171],\n",
    "    'batch_sizes': [256, 128, 64, 32, 16, 8, 4, 2],\n",
    "    'batch_size': None, \n",
    "    'warmup_init': True,\n",
    "    'relative_step': True,\n",
    "    'num_workers': 0,\n",
    "    'pin_memory': False,\n",
    "    # --------------------> Must be changed when continuing a training\n",
    "    'model_dir': 'custom_transformer_v6_fw_best',\n",
    "    'new_model_dir': 'custom_transformer_v6_fw_2',\n",
    "    'continue': False, # --------------------------> Must be changed when continuing training\n",
    "    'logging_dir': 'data/logs/custom_transformer_fw_2',\n",
    "    'save_best': True,\n",
    "    'tokenizer_path': 'wolof-translate/wolof_translate/tokenizers/t5_tokenizers/tokenizer_v5.model',\n",
    "    'data_directory': 'data/extractions/new_data/',\n",
    "    'data_file': 'corpora_v6.csv',\n",
    "    'version': 6,\n",
    "    # in the case of a distributed training\n",
    "    'backend': None,\n",
    "    'hosts': [],\n",
    "    'current_host': None,\n",
    "    'num_gpus': 5,\n",
    "    'logger': None,\n",
    "    'return_trainer': True,\n",
    "    'include_split': True,\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting wolof-translate/wolof_translate/utils/training.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile wolof-translate/wolof_translate/utils/training.py\n",
    "from wolof_translate import *\n",
    "import warnings\n",
    "\n",
    "def train(config: dict):\n",
    "    \n",
    "    # ---------------------------------------\n",
    "    # add distribution if necessary (https://github.com/aws/amazon-sagemaker-examples/blob/main/sagemaker-python-sdk/pytorch_mnist/mnist.py)\n",
    "    \n",
    "    logger = config['logger']\n",
    "    \n",
    "    is_distributed = len(config['hosts']) > 1 and config['backend'] is not None\n",
    "    \n",
    "    use_cuda = config['num_gpus'] > 0\n",
    "    \n",
    "    config.update({\"num_workers\": 1, \"pin_memory\": True} if use_cuda else {})\n",
    "\n",
    "    if not logger is None:\n",
    "        \n",
    "        logger.debug(\"Distributed training - {}\".format(is_distributed))\n",
    "        \n",
    "        logger.debug(\"Number of gpus available - {}\".format(config['num_gpus']))\n",
    "        \n",
    "    if is_distributed:\n",
    "        # Initialize the distributed environment.\n",
    "        world_size = len(config['hosts'])\n",
    "        \n",
    "        os.environ[\"WORLD_SIZE\"] = str(world_size)\n",
    "        \n",
    "        host_rank = config['hosts'].index(config['current_host'])\n",
    "        \n",
    "        os.environ[\"RANK\"] = str(host_rank)\n",
    "        \n",
    "        dist.init_process_group(backend=config['backend'], rank=host_rank, world_size=world_size)\n",
    "        \n",
    "        if not logger is None: logger.info(\n",
    "            \"Initialized the distributed environment: '{}' backend on {} nodes. \".format(\n",
    "                config['backend'], dist.get_world_size()\n",
    "            )\n",
    "            + \"Current host rank is {}. Number of gpus: {}\".format(dist.get_rank(), config['num_gpus'])\n",
    "        )\n",
    "    # ---------------------------------------\n",
    "    \n",
    "    # split the data\n",
    "    if config['include_split']: split_data(config['random_state'], config['data_directory'], config['data_file'])\n",
    "\n",
    "    # recuperate the tokenizer\n",
    "    tokenizer = T5TokenizerFast(config['tokenizer_path'])\n",
    "    \n",
    "    # recuperate train and test set\n",
    "    train_dataset, test_dataset = recuperate_datasets(config['char_p'],\n",
    "                                                        config['word_p'], config['max_len'],\n",
    "                                                        config['end_mark'], tokenizer, config['corpus_1'],\n",
    "                                                        config['corpus_2'],\n",
    "                                                        config['train_file'], config['test_file'])\n",
    "    \n",
    "    # initialize the evaluation object\n",
    "    evaluation = TranslationEvaluation(tokenizer, train_dataset.decode)\n",
    "\n",
    "    # let us initialize the trainer\n",
    "    trainer = ModelRunner(model = Transformer, version=config['version'], seed = 0, evaluation = evaluation, optimizer = Adafactor)\n",
    "\n",
    "    # initialize the encoder and the decoder layers\n",
    "    encoder_layer = nn.TransformerEncoderLayer(config['d_model'],\n",
    "                                                config['n_head'],\n",
    "                                                config['dim_ff'],\n",
    "                                                config['drop_out_rate'], batch_first = True)\n",
    "\n",
    "    decoder_layer = nn.TransformerDecoderLayer(config['d_model'],\n",
    "                                                config['n_head'],\n",
    "                                                config['dim_ff'],\n",
    "                                                config['drop_out_rate'], batch_first = True)\n",
    "\n",
    "    # let us initialize the encoder and the decoder\n",
    "    encoder = nn.TransformerEncoder(encoder_layer, config['n_encoders'])\n",
    "\n",
    "    decoder = nn.TransformerDecoder(decoder_layer, config['n_decoders'])\n",
    "\n",
    "    #-------------------------------------\n",
    "    # in the case when the linear learning rate scheduler with warmup is used\n",
    "    \n",
    "    # let us calculate the appropriate warmup steps (let us take a max epoch of 100)\n",
    "    # length = len(train_dataset)\n",
    "\n",
    "    # n_steps = length // config['batch_size']\n",
    "\n",
    "    # num_steps = config['max_epoch'] * n_steps\n",
    "\n",
    "    # warmup_steps = (config['max_epoch'] * n_steps) * config['warmup_ratio']\n",
    "\n",
    "    # Initialize the scheduler parameters\n",
    "    # scheduler_args = {'num_warmup_steps': warmup_steps, 'num_training_steps': num_steps}\n",
    "    #-------------------------------------\n",
    "\n",
    "    # Initialize the transformer parameters\n",
    "    model_args = {\n",
    "        'vocab_size': len(tokenizer),\n",
    "        'encoder': encoder,\n",
    "        'decoder': decoder,\n",
    "        'class_criterion': nn.CrossEntropyLoss(label_smoothing = config['label_smoothing']),\n",
    "        'max_len': config['max_len']\n",
    "    }\n",
    "\n",
    "    # Initialize the optimizer parameters\n",
    "    optimizer_args = {\n",
    "        'lr': config['learning_rate'],\n",
    "        'weight_decay': config['weight_decay'],\n",
    "        # 'betas': (0.9, 0.98),\n",
    "        'warmup_init': config['warmup_init'],\n",
    "        'relative_step': config['relative_step']\n",
    "    }\n",
    "\n",
    "    # ----------------------------\n",
    "    # initialize the bucket samplers for distributed environment\n",
    "    boundaries = config['boundaries']\n",
    "    batch_sizes = config['batch_sizes']\n",
    "\n",
    "    train_sampler = SequenceLengthBatchSampler(train_dataset,\n",
    "                                                boundaries = boundaries,\n",
    "                                                batch_sizes = batch_sizes)\n",
    "\n",
    "    test_sampler = SequenceLengthBatchSampler(test_dataset,\n",
    "                                                boundaries = boundaries,\n",
    "                                                batch_sizes = batch_sizes)\n",
    "\n",
    "    # ------------------------------\n",
    "    # initialize a bucket sampler with fixed batch size in the case of single machine\n",
    "    # with parallelization on multiple gpus\n",
    "    # train_sampler = BucketSampler(train_dataset, config['batch_size'])\n",
    "\n",
    "    # test_sampler = BucketSampler(test_dataset, config['batch_size'])\n",
    "    \n",
    "    # ------------------------------\n",
    "\n",
    "    # Initialize the loaders parameters\n",
    "    train_loader_args = {'batch_sampler': train_sampler, 'collate_fn': collate_fn,\n",
    "                        'num_workers': config['num_workers'], 'pin_memory': config['pin_memory']}\n",
    "\n",
    "    test_loader_args = {'batch_sampler': test_sampler, 'collate_fn': collate_fn,\n",
    "                        'num_workers': config['num_workers'], 'pin_memory': config['pin_memory']}\n",
    "\n",
    "    # Add the datasets and hyperparameters to trainer\n",
    "    trainer.compile(train_dataset, test_dataset, tokenizer, train_loader_args,\n",
    "                    test_loader_args, optimizer_kwargs = optimizer_args,\n",
    "                    model_kwargs = model_args,\n",
    "                    # lr_scheduler=get_linear_schedule_with_warmup,\n",
    "                    # lr_scheduler_kwargs=scheduler_args,\n",
    "                    predict_with_generate = True,\n",
    "                    is_distributed=is_distributed,\n",
    "                    logging_dir=config['logging_dir'],\n",
    "                    dist=dist\n",
    "                    )\n",
    "\n",
    "    # load the model\n",
    "    trainer.load(config['model_dir'], load_best = not config['continue'])\n",
    "    \n",
    "    # Train the model\n",
    "    trainer.train(config['epochs'] - trainer.current_epoch, auto_save = True, log_step = config['log_step'], saving_directory=config['new_model_dir'], save_best = config['save_best'],\n",
    "                  metric_for_best_model = config['metric_for_best_model'], metric_objective = config['metric_objective'])\n",
    "    \n",
    "    if config['return_trainer']:\n",
    "        \n",
    "        return trainer\n",
    "    \n",
    "    return None\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Below train and save if we want."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from wolof_translate.utils.training import train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/95 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "For epoch 6: \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train batch number 2:   0%|          | 0/41 [00:00<?, ?batches/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages/torch/nn/functional.py:4999: UserWarning: Support for mismatched key_padding_mask and attn_mask is deprecated. Use same type for both instead.\n",
      "  warnings.warn(\n",
      "Train batch number 42: 100%|██████████| 41/41 [00:10<00:00,  3.90batches/s]\n",
      "  1%|          | 1/95 [00:10<16:27, 10.50s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=============================\n",
      "\n",
      "For epoch 7: \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train batch number 2:   0%|          | 0/41 [00:00<?, ?batches/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train batch number 42: 100%|██████████| 41/41 [00:09<00:00,  4.11batches/s]\n",
      "  2%|▏         | 2/95 [00:20<15:48, 10.20s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=============================\n",
      "\n",
      "For epoch 8: \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train batch number 2:   0%|          | 0/41 [00:00<?, ?batches/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train batch number 42: 100%|██████████| 41/41 [00:09<00:00,  4.23batches/s]\n",
      "  3%|▎         | 3/95 [00:30<15:17,  9.97s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=============================\n",
      "\n",
      "For epoch 9: \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train batch number 2:   0%|          | 0/41 [00:00<?, ?batches/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train batch number 42: 100%|██████████| 41/41 [00:09<00:00,  4.15batches/s]\n",
      "  4%|▍         | 4/95 [00:40<15:04,  9.94s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=============================\n",
      "\n",
      "For epoch 10: \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train batch number 2:   0%|          | 0/41 [00:00<?, ?batches/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train batch number 42: 100%|██████████| 41/41 [00:09<00:00,  4.10batches/s]\n",
      "Test batch number 2:   0%|          | 0/9 [00:00<?, ?batches/s]/home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages/torch/nn/modules/transformer.py:287: UserWarning: The PyTorch API of nested tensors is in prototype stage and will change in the near future. (Triggered internally at /opt/conda/conda-bld/pytorch_1679586020379/work/aten/src/ATen/NestedTensorImpl.cpp:177.)\n",
      "  output = torch._nested_tensor_from_mask(output, src_key_padding_mask.logical_not(), mask_check=False)\n",
      "/home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages/torch/nn/modules/activation.py:1144: UserWarning: Converting mask without torch.bool dtype to bool; this will negatively affect performance. Prefer to use a boolean mask directly. (Triggered internally at /opt/conda/conda-bld/pytorch_1679586020379/work/aten/src/ATen/native/transformers/attention.cpp:150.)\n",
      "  return torch._native_multi_head_attention(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Test batch number 3:  11%|█         | 1/9 [00:00<00:07,  1.11batches/s]/home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages/torch/nn/modules/activation.py:1144: UserWarning: Converting mask without torch.bool dtype to bool; this will negatively affect performance. Prefer to use a boolean mask directly. (Triggered internally at /opt/conda/conda-bld/pytorch_1679586020379/work/aten/src/ATen/native/transformers/attention.cpp:150.)\n",
      "  return torch._native_multi_head_attention(\n",
      "Test batch number 4:  22%|██▏       | 2/9 [00:02<00:07,  1.03s/batches]/home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages/torch/nn/modules/activation.py:1144: UserWarning: Converting mask without torch.bool dtype to bool; this will negatively affect performance. Prefer to use a boolean mask directly. (Triggered internally at /opt/conda/conda-bld/pytorch_1679586020379/work/aten/src/ATen/native/transformers/attention.cpp:150.)\n",
      "  return torch._native_multi_head_attention(\n",
      "Test batch number 5:  33%|███▎      | 3/9 [00:03<00:06,  1.13s/batches]/home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages/torch/nn/modules/activation.py:1144: UserWarning: Converting mask without torch.bool dtype to bool; this will negatively affect performance. Prefer to use a boolean mask directly. (Triggered internally at /opt/conda/conda-bld/pytorch_1679586020379/work/aten/src/ATen/native/transformers/attention.cpp:150.)\n",
      "  return torch._native_multi_head_attention(\n",
      "Test batch number 6:  44%|████▍     | 4/9 [00:04<00:05,  1.15s/batches]/home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages/torch/nn/modules/activation.py:1144: UserWarning: Converting mask without torch.bool dtype to bool; this will negatively affect performance. Prefer to use a boolean mask directly. (Triggered internally at /opt/conda/conda-bld/pytorch_1679586020379/work/aten/src/ATen/native/transformers/attention.cpp:150.)\n",
      "  return torch._native_multi_head_attention(\n",
      "Test batch number 7:  56%|█████▌    | 5/9 [00:05<00:04,  1.06s/batches]/home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages/torch/nn/modules/activation.py:1144: UserWarning: Converting mask without torch.bool dtype to bool; this will negatively affect performance. Prefer to use a boolean mask directly. (Triggered internally at /opt/conda/conda-bld/pytorch_1679586020379/work/aten/src/ATen/native/transformers/attention.cpp:150.)\n",
      "  return torch._native_multi_head_attention(\n",
      "Test batch number 8:  67%|██████▋   | 6/9 [00:06<00:03,  1.11s/batches]/home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages/torch/nn/modules/activation.py:1144: UserWarning: Converting mask without torch.bool dtype to bool; this will negatively affect performance. Prefer to use a boolean mask directly. (Triggered internally at /opt/conda/conda-bld/pytorch_1679586020379/work/aten/src/ATen/native/transformers/attention.cpp:150.)\n",
      "  return torch._native_multi_head_attention(\n",
      "Test batch number 9:  78%|███████▊  | 7/9 [00:07<00:02,  1.15s/batches]/home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages/torch/nn/modules/activation.py:1144: UserWarning: Converting mask without torch.bool dtype to bool; this will negatively affect performance. Prefer to use a boolean mask directly. (Triggered internally at /opt/conda/conda-bld/pytorch_1679586020379/work/aten/src/ATen/native/transformers/attention.cpp:150.)\n",
      "  return torch._native_multi_head_attention(\n",
      "Test batch number 10:  89%|████████▉ | 8/9 [00:09<00:01,  1.30s/batches]/home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages/torch/nn/modules/activation.py:1144: UserWarning: Converting mask without torch.bool dtype to bool; this will negatively affect performance. Prefer to use a boolean mask directly. (Triggered internally at /opt/conda/conda-bld/pytorch_1679586020379/work/aten/src/ATen/native/transformers/attention.cpp:150.)\n",
      "  return torch._native_multi_head_attention(\n",
      "Test batch number 10: 100%|██████████| 9/9 [00:10<00:00,  1.22s/batches]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Metrics: {'train_loss': 7.678882628483806, 'test_loss': 7.75705405548736, 'accuracy': 0.07233671328671328, 'bleu': 0.1418608391608392, 'gen_len': 34.04895104895105}\n",
      "\n",
      "=============================\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  5%|▌         | 5/95 [01:01<21:07, 14.08s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "For epoch 11: \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train batch number 2:   0%|          | 0/41 [00:00<?, ?batches/s]/home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages/torch/nn/functional.py:4999: UserWarning: Support for mismatched key_padding_mask and attn_mask is deprecated. Use same type for both instead.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train batch number 42: 100%|██████████| 41/41 [00:09<00:00,  4.21batches/s]\n",
      "  6%|▋         | 6/95 [01:11<18:42, 12.61s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=============================\n",
      "\n",
      "For epoch 12: \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train batch number 2:   0%|          | 0/41 [00:00<?, ?batches/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train batch number 42: 100%|██████████| 41/41 [00:10<00:00,  3.82batches/s]\n",
      "  7%|▋         | 7/95 [01:21<17:35, 12.00s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=============================\n",
      "\n",
      "For epoch 13: \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train batch number 2:   0%|          | 0/41 [00:00<?, ?batches/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train batch number 42: 100%|██████████| 41/41 [00:09<00:00,  4.14batches/s]\n",
      "  8%|▊         | 8/95 [01:31<16:26, 11.33s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=============================\n",
      "\n",
      "For epoch 14: \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train batch number 2:   0%|          | 0/41 [00:00<?, ?batches/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train batch number 42: 100%|██████████| 41/41 [00:09<00:00,  4.12batches/s]\n",
      "  9%|▉         | 9/95 [01:41<15:37, 10.90s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=============================\n",
      "\n",
      "For epoch 15: \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train batch number 2:   0%|          | 0/41 [00:00<?, ?batches/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train batch number 42: 100%|██████████| 41/41 [00:09<00:00,  4.27batches/s]\n",
      "Test batch number 2:   0%|          | 0/9 [00:00<?, ?batches/s]/home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages/torch/nn/modules/activation.py:1144: UserWarning: Converting mask without torch.bool dtype to bool; this will negatively affect performance. Prefer to use a boolean mask directly. (Triggered internally at /opt/conda/conda-bld/pytorch_1679586020379/work/aten/src/ATen/native/transformers/attention.cpp:150.)\n",
      "  return torch._native_multi_head_attention(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Test batch number 3:  11%|█         | 1/9 [00:00<00:06,  1.16batches/s]/home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages/torch/nn/modules/activation.py:1144: UserWarning: Converting mask without torch.bool dtype to bool; this will negatively affect performance. Prefer to use a boolean mask directly. (Triggered internally at /opt/conda/conda-bld/pytorch_1679586020379/work/aten/src/ATen/native/transformers/attention.cpp:150.)\n",
      "  return torch._native_multi_head_attention(\n",
      "Test batch number 4:  22%|██▏       | 2/9 [00:02<00:07,  1.04s/batches]/home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages/torch/nn/modules/activation.py:1144: UserWarning: Converting mask without torch.bool dtype to bool; this will negatively affect performance. Prefer to use a boolean mask directly. (Triggered internally at /opt/conda/conda-bld/pytorch_1679586020379/work/aten/src/ATen/native/transformers/attention.cpp:150.)\n",
      "  return torch._native_multi_head_attention(\n",
      "Test batch number 5:  33%|███▎      | 3/9 [00:03<00:06,  1.11s/batches]/home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages/torch/nn/modules/activation.py:1144: UserWarning: Converting mask without torch.bool dtype to bool; this will negatively affect performance. Prefer to use a boolean mask directly. (Triggered internally at /opt/conda/conda-bld/pytorch_1679586020379/work/aten/src/ATen/native/transformers/attention.cpp:150.)\n",
      "  return torch._native_multi_head_attention(\n",
      "Test batch number 6:  44%|████▍     | 4/9 [00:04<00:05,  1.07s/batches]/home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages/torch/nn/modules/activation.py:1144: UserWarning: Converting mask without torch.bool dtype to bool; this will negatively affect performance. Prefer to use a boolean mask directly. (Triggered internally at /opt/conda/conda-bld/pytorch_1679586020379/work/aten/src/ATen/native/transformers/attention.cpp:150.)\n",
      "  return torch._native_multi_head_attention(\n",
      "Test batch number 7:  56%|█████▌    | 5/9 [00:04<00:03,  1.04batches/s]/home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages/torch/nn/modules/activation.py:1144: UserWarning: Converting mask without torch.bool dtype to bool; this will negatively affect performance. Prefer to use a boolean mask directly. (Triggered internally at /opt/conda/conda-bld/pytorch_1679586020379/work/aten/src/ATen/native/transformers/attention.cpp:150.)\n",
      "  return torch._native_multi_head_attention(\n",
      "Test batch number 8:  67%|██████▋   | 6/9 [00:06<00:03,  1.01s/batches]/home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages/torch/nn/modules/activation.py:1144: UserWarning: Converting mask without torch.bool dtype to bool; this will negatively affect performance. Prefer to use a boolean mask directly. (Triggered internally at /opt/conda/conda-bld/pytorch_1679586020379/work/aten/src/ATen/native/transformers/attention.cpp:150.)\n",
      "  return torch._native_multi_head_attention(\n",
      "Test batch number 9:  78%|███████▊  | 7/9 [00:07<00:02,  1.09s/batches]/home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages/torch/nn/modules/activation.py:1144: UserWarning: Converting mask without torch.bool dtype to bool; this will negatively affect performance. Prefer to use a boolean mask directly. (Triggered internally at /opt/conda/conda-bld/pytorch_1679586020379/work/aten/src/ATen/native/transformers/attention.cpp:150.)\n",
      "  return torch._native_multi_head_attention(\n",
      "Test batch number 10:  89%|████████▉ | 8/9 [00:09<00:01,  1.31s/batches]/home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages/torch/nn/modules/activation.py:1144: UserWarning: Converting mask without torch.bool dtype to bool; this will negatively affect performance. Prefer to use a boolean mask directly. (Triggered internally at /opt/conda/conda-bld/pytorch_1679586020379/work/aten/src/ATen/native/transformers/attention.cpp:150.)\n",
      "  return torch._native_multi_head_attention(\n",
      "Test batch number 10: 100%|██████████| 9/9 [00:10<00:00,  1.19s/batches]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Metrics: {'train_loss': 6.723200789486209, 'test_loss': 7.204742138202373, 'accuracy': 0.07606468531468531, 'bleu': 0.15678181818181822, 'gen_len': 34.04895104895105}\n",
      "\n",
      "=============================\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 11%|█         | 10/95 [02:02<19:47, 13.97s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "For epoch 16: \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train batch number 2:   0%|          | 0/41 [00:00<?, ?batches/s]/home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages/torch/nn/functional.py:4999: UserWarning: Support for mismatched key_padding_mask and attn_mask is deprecated. Use same type for both instead.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train batch number 42: 100%|██████████| 41/41 [00:10<00:00,  3.96batches/s]\n",
      " 12%|█▏        | 11/95 [02:13<18:00, 12.86s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=============================\n",
      "\n",
      "For epoch 17: \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train batch number 2:   0%|          | 0/41 [00:00<?, ?batches/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train batch number 42: 100%|██████████| 41/41 [00:09<00:00,  4.28batches/s]\n",
      " 13%|█▎        | 12/95 [02:22<16:24, 11.86s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=============================\n",
      "\n",
      "For epoch 18: \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train batch number 2:   0%|          | 0/41 [00:00<?, ?batches/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train batch number 42: 100%|██████████| 41/41 [00:09<00:00,  4.18batches/s]\n",
      " 14%|█▎        | 13/95 [02:32<15:22, 11.25s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=============================\n",
      "\n",
      "For epoch 19: \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train batch number 2:   0%|          | 0/41 [00:00<?, ?batches/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train batch number 42: 100%|██████████| 41/41 [00:09<00:00,  4.28batches/s]\n",
      " 15%|█▍        | 14/95 [02:42<14:30, 10.74s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=============================\n",
      "\n",
      "For epoch 20: \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train batch number 2:   0%|          | 0/41 [00:00<?, ?batches/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train batch number 42: 100%|██████████| 41/41 [00:09<00:00,  4.31batches/s]\n",
      "Test batch number 2:   0%|          | 0/9 [00:00<?, ?batches/s]/home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages/torch/nn/modules/activation.py:1144: UserWarning: Converting mask without torch.bool dtype to bool; this will negatively affect performance. Prefer to use a boolean mask directly. (Triggered internally at /opt/conda/conda-bld/pytorch_1679586020379/work/aten/src/ATen/native/transformers/attention.cpp:150.)\n",
      "  return torch._native_multi_head_attention(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Test batch number 3:  11%|█         | 1/9 [00:00<00:07,  1.06batches/s]/home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages/torch/nn/modules/activation.py:1144: UserWarning: Converting mask without torch.bool dtype to bool; this will negatively affect performance. Prefer to use a boolean mask directly. (Triggered internally at /opt/conda/conda-bld/pytorch_1679586020379/work/aten/src/ATen/native/transformers/attention.cpp:150.)\n",
      "  return torch._native_multi_head_attention(\n",
      "Test batch number 4:  22%|██▏       | 2/9 [00:02<00:08,  1.16s/batches]/home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages/torch/nn/modules/activation.py:1144: UserWarning: Converting mask without torch.bool dtype to bool; this will negatively affect performance. Prefer to use a boolean mask directly. (Triggered internally at /opt/conda/conda-bld/pytorch_1679586020379/work/aten/src/ATen/native/transformers/attention.cpp:150.)\n",
      "  return torch._native_multi_head_attention(\n",
      "Test batch number 5:  33%|███▎      | 3/9 [00:03<00:06,  1.14s/batches]/home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages/torch/nn/modules/activation.py:1144: UserWarning: Converting mask without torch.bool dtype to bool; this will negatively affect performance. Prefer to use a boolean mask directly. (Triggered internally at /opt/conda/conda-bld/pytorch_1679586020379/work/aten/src/ATen/native/transformers/attention.cpp:150.)\n",
      "  return torch._native_multi_head_attention(\n",
      "Test batch number 6:  44%|████▍     | 4/9 [00:04<00:05,  1.12s/batches]/home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages/torch/nn/modules/activation.py:1144: UserWarning: Converting mask without torch.bool dtype to bool; this will negatively affect performance. Prefer to use a boolean mask directly. (Triggered internally at /opt/conda/conda-bld/pytorch_1679586020379/work/aten/src/ATen/native/transformers/attention.cpp:150.)\n",
      "  return torch._native_multi_head_attention(\n",
      "Test batch number 7:  56%|█████▌    | 5/9 [00:05<00:04,  1.01s/batches]/home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages/torch/nn/modules/activation.py:1144: UserWarning: Converting mask without torch.bool dtype to bool; this will negatively affect performance. Prefer to use a boolean mask directly. (Triggered internally at /opt/conda/conda-bld/pytorch_1679586020379/work/aten/src/ATen/native/transformers/attention.cpp:150.)\n",
      "  return torch._native_multi_head_attention(\n",
      "Test batch number 8:  67%|██████▋   | 6/9 [00:06<00:03,  1.07s/batches]/home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages/torch/nn/modules/activation.py:1144: UserWarning: Converting mask without torch.bool dtype to bool; this will negatively affect performance. Prefer to use a boolean mask directly. (Triggered internally at /opt/conda/conda-bld/pytorch_1679586020379/work/aten/src/ATen/native/transformers/attention.cpp:150.)\n",
      "  return torch._native_multi_head_attention(\n",
      "Test batch number 9:  78%|███████▊  | 7/9 [00:07<00:02,  1.11s/batches]/home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages/torch/nn/modules/activation.py:1144: UserWarning: Converting mask without torch.bool dtype to bool; this will negatively affect performance. Prefer to use a boolean mask directly. (Triggered internally at /opt/conda/conda-bld/pytorch_1679586020379/work/aten/src/ATen/native/transformers/attention.cpp:150.)\n",
      "  return torch._native_multi_head_attention(\n",
      "Test batch number 10:  89%|████████▉ | 8/9 [00:09<00:01,  1.31s/batches]/home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages/torch/nn/modules/activation.py:1144: UserWarning: Converting mask without torch.bool dtype to bool; this will negatively affect performance. Prefer to use a boolean mask directly. (Triggered internally at /opt/conda/conda-bld/pytorch_1679586020379/work/aten/src/ATen/native/transformers/attention.cpp:150.)\n",
      "  return torch._native_multi_head_attention(\n",
      "Test batch number 10: 100%|██████████| 9/9 [00:10<00:00,  1.21s/batches]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Metrics: {'train_loss': 6.123343567791912, 'test_loss': 6.983398509192301, 'accuracy': 0.10837412587412587, 'bleu': 0.14396888111888112, 'gen_len': 34.04895104895105}\n",
      "\n",
      "=============================\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 16%|█▌        | 15/95 [03:02<18:22, 13.78s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "For epoch 21: \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train batch number 2:   0%|          | 0/41 [00:00<?, ?batches/s]/home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages/torch/nn/functional.py:4999: UserWarning: Support for mismatched key_padding_mask and attn_mask is deprecated. Use same type for both instead.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train batch number 42: 100%|██████████| 41/41 [00:10<00:00,  3.89batches/s]\n",
      " 17%|█▋        | 16/95 [03:13<16:51, 12.80s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=============================\n",
      "\n",
      "For epoch 22: \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train batch number 2:   0%|          | 0/41 [00:00<?, ?batches/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train batch number 42: 100%|██████████| 41/41 [00:09<00:00,  4.10batches/s]\n",
      " 18%|█▊        | 17/95 [03:23<15:32, 11.96s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=============================\n",
      "\n",
      "For epoch 23: \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train batch number 2:   0%|          | 0/41 [00:00<?, ?batches/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train batch number 42: 100%|██████████| 41/41 [00:09<00:00,  4.32batches/s]\n",
      " 19%|█▉        | 18/95 [03:32<14:23, 11.22s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=============================\n",
      "\n",
      "For epoch 24: \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train batch number 2:   0%|          | 0/41 [00:00<?, ?batches/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train batch number 42: 100%|██████████| 41/41 [00:09<00:00,  4.18batches/s]\n",
      " 20%|██        | 19/95 [03:42<13:40, 10.80s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=============================\n",
      "\n",
      "For epoch 25: \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train batch number 2:   0%|          | 0/41 [00:00<?, ?batches/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train batch number 42: 100%|██████████| 41/41 [00:10<00:00,  3.73batches/s]\n",
      "Test batch number 2:   0%|          | 0/9 [00:00<?, ?batches/s]/home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages/torch/nn/modules/activation.py:1144: UserWarning: Converting mask without torch.bool dtype to bool; this will negatively affect performance. Prefer to use a boolean mask directly. (Triggered internally at /opt/conda/conda-bld/pytorch_1679586020379/work/aten/src/ATen/native/transformers/attention.cpp:150.)\n",
      "  return torch._native_multi_head_attention(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Test batch number 3:  11%|█         | 1/9 [00:00<00:07,  1.12batches/s]/home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages/torch/nn/modules/activation.py:1144: UserWarning: Converting mask without torch.bool dtype to bool; this will negatively affect performance. Prefer to use a boolean mask directly. (Triggered internally at /opt/conda/conda-bld/pytorch_1679586020379/work/aten/src/ATen/native/transformers/attention.cpp:150.)\n",
      "  return torch._native_multi_head_attention(\n",
      "Test batch number 4:  22%|██▏       | 2/9 [00:02<00:07,  1.03s/batches]/home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages/torch/nn/modules/activation.py:1144: UserWarning: Converting mask without torch.bool dtype to bool; this will negatively affect performance. Prefer to use a boolean mask directly. (Triggered internally at /opt/conda/conda-bld/pytorch_1679586020379/work/aten/src/ATen/native/transformers/attention.cpp:150.)\n",
      "  return torch._native_multi_head_attention(\n",
      "Test batch number 5:  33%|███▎      | 3/9 [00:03<00:06,  1.08s/batches]/home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages/torch/nn/modules/activation.py:1144: UserWarning: Converting mask without torch.bool dtype to bool; this will negatively affect performance. Prefer to use a boolean mask directly. (Triggered internally at /opt/conda/conda-bld/pytorch_1679586020379/work/aten/src/ATen/native/transformers/attention.cpp:150.)\n",
      "  return torch._native_multi_head_attention(\n",
      "Test batch number 6:  44%|████▍     | 4/9 [00:04<00:05,  1.05s/batches]/home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages/torch/nn/modules/activation.py:1144: UserWarning: Converting mask without torch.bool dtype to bool; this will negatively affect performance. Prefer to use a boolean mask directly. (Triggered internally at /opt/conda/conda-bld/pytorch_1679586020379/work/aten/src/ATen/native/transformers/attention.cpp:150.)\n",
      "  return torch._native_multi_head_attention(\n",
      "Test batch number 7:  56%|█████▌    | 5/9 [00:04<00:03,  1.03batches/s]/home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages/torch/nn/modules/activation.py:1144: UserWarning: Converting mask without torch.bool dtype to bool; this will negatively affect performance. Prefer to use a boolean mask directly. (Triggered internally at /opt/conda/conda-bld/pytorch_1679586020379/work/aten/src/ATen/native/transformers/attention.cpp:150.)\n",
      "  return torch._native_multi_head_attention(\n",
      "Test batch number 8:  67%|██████▋   | 6/9 [00:06<00:03,  1.08s/batches]/home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages/torch/nn/modules/activation.py:1144: UserWarning: Converting mask without torch.bool dtype to bool; this will negatively affect performance. Prefer to use a boolean mask directly. (Triggered internally at /opt/conda/conda-bld/pytorch_1679586020379/work/aten/src/ATen/native/transformers/attention.cpp:150.)\n",
      "  return torch._native_multi_head_attention(\n",
      "Test batch number 9:  78%|███████▊  | 7/9 [00:07<00:02,  1.17s/batches]/home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages/torch/nn/modules/activation.py:1144: UserWarning: Converting mask without torch.bool dtype to bool; this will negatively affect performance. Prefer to use a boolean mask directly. (Triggered internally at /opt/conda/conda-bld/pytorch_1679586020379/work/aten/src/ATen/native/transformers/attention.cpp:150.)\n",
      "  return torch._native_multi_head_attention(\n",
      "Test batch number 10:  89%|████████▉ | 8/9 [00:09<00:01,  1.34s/batches]/home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages/torch/nn/modules/activation.py:1144: UserWarning: Converting mask without torch.bool dtype to bool; this will negatively affect performance. Prefer to use a boolean mask directly. (Triggered internally at /opt/conda/conda-bld/pytorch_1679586020379/work/aten/src/ATen/native/transformers/attention.cpp:150.)\n",
      "  return torch._native_multi_head_attention(\n",
      "Test batch number 10: 100%|██████████| 9/9 [00:10<00:00,  1.20s/batches]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Metrics: {'train_loss': 5.769862362845373, 'test_loss': 6.949723965638167, 'accuracy': 0.08115314685314687, 'bleu': 0.18209160839160834, 'gen_len': 34.04895104895105}\n",
      "\n",
      "=============================\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 21%|██        | 20/95 [04:05<17:49, 14.26s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "For epoch 26: \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train batch number 2:   0%|          | 0/41 [00:00<?, ?batches/s]/home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages/torch/nn/functional.py:4999: UserWarning: Support for mismatched key_padding_mask and attn_mask is deprecated. Use same type for both instead.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train batch number 42: 100%|██████████| 41/41 [00:09<00:00,  4.18batches/s]\n",
      " 22%|██▏       | 21/95 [04:14<15:56, 12.92s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=============================\n",
      "\n",
      "For epoch 27: \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train batch number 2:   0%|          | 0/41 [00:00<?, ?batches/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train batch number 42: 100%|██████████| 41/41 [00:09<00:00,  4.21batches/s]\n",
      " 23%|██▎       | 22/95 [04:24<14:34, 11.97s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=============================\n",
      "\n",
      "For epoch 28: \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train batch number 2:   0%|          | 0/41 [00:00<?, ?batches/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train batch number 42: 100%|██████████| 41/41 [00:10<00:00,  4.10batches/s]\n",
      " 24%|██▍       | 23/95 [04:34<13:39, 11.38s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=============================\n",
      "\n",
      "For epoch 29: \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train batch number 2:   0%|          | 0/41 [00:00<?, ?batches/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train batch number 42: 100%|██████████| 41/41 [00:10<00:00,  3.74batches/s]\n",
      " 25%|██▌       | 24/95 [04:45<13:19, 11.26s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=============================\n",
      "\n",
      "For epoch 30: \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train batch number 2:   0%|          | 0/41 [00:00<?, ?batches/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train batch number 42: 100%|██████████| 41/41 [00:10<00:00,  4.03batches/s]\n",
      "Test batch number 2:   0%|          | 0/9 [00:00<?, ?batches/s]/home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages/torch/nn/modules/activation.py:1144: UserWarning: Converting mask without torch.bool dtype to bool; this will negatively affect performance. Prefer to use a boolean mask directly. (Triggered internally at /opt/conda/conda-bld/pytorch_1679586020379/work/aten/src/ATen/native/transformers/attention.cpp:150.)\n",
      "  return torch._native_multi_head_attention(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Test batch number 3:  11%|█         | 1/9 [00:00<00:07,  1.13batches/s]/home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages/torch/nn/modules/activation.py:1144: UserWarning: Converting mask without torch.bool dtype to bool; this will negatively affect performance. Prefer to use a boolean mask directly. (Triggered internally at /opt/conda/conda-bld/pytorch_1679586020379/work/aten/src/ATen/native/transformers/attention.cpp:150.)\n",
      "  return torch._native_multi_head_attention(\n",
      "Test batch number 4:  22%|██▏       | 2/9 [00:01<00:06,  1.00batches/s]/home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages/torch/nn/modules/activation.py:1144: UserWarning: Converting mask without torch.bool dtype to bool; this will negatively affect performance. Prefer to use a boolean mask directly. (Triggered internally at /opt/conda/conda-bld/pytorch_1679586020379/work/aten/src/ATen/native/transformers/attention.cpp:150.)\n",
      "  return torch._native_multi_head_attention(\n",
      "Test batch number 5:  33%|███▎      | 3/9 [00:03<00:06,  1.11s/batches]/home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages/torch/nn/modules/activation.py:1144: UserWarning: Converting mask without torch.bool dtype to bool; this will negatively affect performance. Prefer to use a boolean mask directly. (Triggered internally at /opt/conda/conda-bld/pytorch_1679586020379/work/aten/src/ATen/native/transformers/attention.cpp:150.)\n",
      "  return torch._native_multi_head_attention(\n",
      "Test batch number 6:  44%|████▍     | 4/9 [00:04<00:05,  1.12s/batches]/home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages/torch/nn/modules/activation.py:1144: UserWarning: Converting mask without torch.bool dtype to bool; this will negatively affect performance. Prefer to use a boolean mask directly. (Triggered internally at /opt/conda/conda-bld/pytorch_1679586020379/work/aten/src/ATen/native/transformers/attention.cpp:150.)\n",
      "  return torch._native_multi_head_attention(\n",
      "Test batch number 7:  56%|█████▌    | 5/9 [00:05<00:04,  1.03s/batches]/home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages/torch/nn/modules/activation.py:1144: UserWarning: Converting mask without torch.bool dtype to bool; this will negatively affect performance. Prefer to use a boolean mask directly. (Triggered internally at /opt/conda/conda-bld/pytorch_1679586020379/work/aten/src/ATen/native/transformers/attention.cpp:150.)\n",
      "  return torch._native_multi_head_attention(\n",
      "Test batch number 8:  67%|██████▋   | 6/9 [00:06<00:03,  1.07s/batches]/home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages/torch/nn/modules/activation.py:1144: UserWarning: Converting mask without torch.bool dtype to bool; this will negatively affect performance. Prefer to use a boolean mask directly. (Triggered internally at /opt/conda/conda-bld/pytorch_1679586020379/work/aten/src/ATen/native/transformers/attention.cpp:150.)\n",
      "  return torch._native_multi_head_attention(\n",
      "Test batch number 9:  78%|███████▊  | 7/9 [00:07<00:02,  1.15s/batches]/home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages/torch/nn/modules/activation.py:1144: UserWarning: Converting mask without torch.bool dtype to bool; this will negatively affect performance. Prefer to use a boolean mask directly. (Triggered internally at /opt/conda/conda-bld/pytorch_1679586020379/work/aten/src/ATen/native/transformers/attention.cpp:150.)\n",
      "  return torch._native_multi_head_attention(\n",
      "Test batch number 10:  89%|████████▉ | 8/9 [00:09<00:01,  1.30s/batches]/home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages/torch/nn/modules/activation.py:1144: UserWarning: Converting mask without torch.bool dtype to bool; this will negatively affect performance. Prefer to use a boolean mask directly. (Triggered internally at /opt/conda/conda-bld/pytorch_1679586020379/work/aten/src/ATen/native/transformers/attention.cpp:150.)\n",
      "  return torch._native_multi_head_attention(\n",
      "Test batch number 10: 100%|██████████| 9/9 [00:10<00:00,  1.21s/batches]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Metrics: {'train_loss': 5.499035483528341, 'test_loss': 7.266024682905291, 'accuracy': 0.05013006993006993, 'bleu': 0.14280244755244756, 'gen_len': 34.04895104895105}\n",
      "\n",
      "=============================\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 26%|██▋       | 25/95 [05:06<16:39, 14.27s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "For epoch 31: \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train batch number 2:   0%|          | 0/41 [00:00<?, ?batches/s]/home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages/torch/nn/functional.py:4999: UserWarning: Support for mismatched key_padding_mask and attn_mask is deprecated. Use same type for both instead.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train batch number 40:  93%|█████████▎| 38/41 [00:09<00:00,  7.78batches/s]"
     ]
    }
   ],
   "source": [
    "# with warnings.catch_warnings():\n",
    "    # warnings.simplefilter(\"ignore\")\n",
    "trainer = train(config)\n",
    "\n",
    "# save if necessary"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "➡️ Predictions\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluation batch number 2:   0%|          | 0/7 [00:00<?, ?batches/s]/home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages/torch/nn/modules/activation.py:1144: UserWarning: Converting mask without torch.bool dtype to bool; this will negatively affect performance. Prefer to use a boolean mask directly. (Triggered internally at /opt/conda/conda-bld/pytorch_1679586020379/work/aten/src/ATen/native/transformers/attention.cpp:150.)\n",
      "  return torch._native_multi_head_attention(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluation batch number 3:  14%|█▍        | 1/7 [00:01<00:07,  1.23s/batches]/home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages/torch/nn/modules/activation.py:1144: UserWarning: Converting mask without torch.bool dtype to bool; this will negatively affect performance. Prefer to use a boolean mask directly. (Triggered internally at /opt/conda/conda-bld/pytorch_1679586020379/work/aten/src/ATen/native/transformers/attention.cpp:150.)\n",
      "  return torch._native_multi_head_attention(\n",
      "Evaluation batch number 4:  29%|██▊       | 2/7 [00:04<00:11,  2.23s/batches]/home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages/torch/nn/modules/activation.py:1144: UserWarning: Converting mask without torch.bool dtype to bool; this will negatively affect performance. Prefer to use a boolean mask directly. (Triggered internally at /opt/conda/conda-bld/pytorch_1679586020379/work/aten/src/ATen/native/transformers/attention.cpp:150.)\n",
      "  return torch._native_multi_head_attention(\n",
      "Evaluation batch number 5:  43%|████▎     | 3/7 [00:06<00:09,  2.40s/batches]/home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages/torch/nn/modules/activation.py:1144: UserWarning: Converting mask without torch.bool dtype to bool; this will negatively affect performance. Prefer to use a boolean mask directly. (Triggered internally at /opt/conda/conda-bld/pytorch_1679586020379/work/aten/src/ATen/native/transformers/attention.cpp:150.)\n",
      "  return torch._native_multi_head_attention(\n",
      "Evaluation batch number 6:  57%|█████▋    | 4/7 [00:11<00:10,  3.42s/batches]/home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages/torch/nn/modules/activation.py:1144: UserWarning: Converting mask without torch.bool dtype to bool; this will negatively affect performance. Prefer to use a boolean mask directly. (Triggered internally at /opt/conda/conda-bld/pytorch_1679586020379/work/aten/src/ATen/native/transformers/attention.cpp:150.)\n",
      "  return torch._native_multi_head_attention(\n",
      "Evaluation batch number 7:  71%|███████▏  | 5/7 [00:13<00:05,  2.77s/batches]/home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages/torch/nn/modules/activation.py:1144: UserWarning: Converting mask without torch.bool dtype to bool; this will negatively affect performance. Prefer to use a boolean mask directly. (Triggered internally at /opt/conda/conda-bld/pytorch_1679586020379/work/aten/src/ATen/native/transformers/attention.cpp:150.)\n",
      "  return torch._native_multi_head_attention(\n",
      "Evaluation batch number 8:  86%|████████▌ | 6/7 [00:15<00:02,  2.62s/batches]/home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages/torch/nn/modules/activation.py:1144: UserWarning: Converting mask without torch.bool dtype to bool; this will negatively affect performance. Prefer to use a boolean mask directly. (Triggered internally at /opt/conda/conda-bld/pytorch_1679586020379/work/aten/src/ATen/native/transformers/attention.cpp:150.)\n",
      "  return torch._native_multi_head_attention(\n",
      "Evaluation batch number 8: 100%|██████████| 7/7 [00:18<00:00,  2.67s/batches]\n"
     ]
    }
   ],
   "source": [
    "if not trainer is None:\n",
    "    \n",
    "    # recuperate the tokenizer\n",
    "    tokenizer = T5TokenizerFast(config['tokenizer_path'])\n",
    "    \n",
    "    # recuperate the test dataset\n",
    "    # initialize the transformation sequence\n",
    "    end_mark_fn = partial(add_end_mark)\n",
    "    augmentation = TransformerSequences(remove_mark_space, delete_guillemet_space, add_mark_space, end_mark_fn)\n",
    "\n",
    "\n",
    "    # let us get the test set\n",
    "    test_dataset = SentenceDataset(f\"{config['data_directory']}test_set.csv\",\n",
    "                                            tokenizer = tokenizer,\n",
    "                                            cp1_transformer = augmentation,\n",
    "                                            cp2_transformer = augmentation,\n",
    "                                            corpus_1=config['corpus_1'],\n",
    "                                            corpus_2=config['corpus_2'],\n",
    "                                            truncation = False)\n",
    "\n",
    "    # initialize the bucket samplers for distributed environment\n",
    "    boundaries = config['boundaries']\n",
    "    batch_sizes = config['batch_sizes']\n",
    "\n",
    "    test_sampler = SequenceLengthBatchSampler(test_dataset,\n",
    "                                                boundaries = boundaries,\n",
    "                                                batch_sizes = batch_sizes)\n",
    "\n",
    "    test_loader_args = {'batch_sampler': test_sampler, 'collate_fn': collate_fn,\n",
    "                            'num_workers': config['num_workers'], 'pin_memory': config['pin_memory']}\n",
    "\n",
    "    metrics, prediction = trainer.evaluate(test_dataset, test_loader_args)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'test_loss': 8.648195539202009,\n",
       " 'accuracy': 0.012357142857142858,\n",
       " 'bleu': 0.12092857142857141,\n",
       " 'gen_len': 82.14285714285714}"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>original_sentences</th>\n",
       "      <th>translations</th>\n",
       "      <th>predictions</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Es-tu en paix?</td>\n",
       "      <td>Jaam nga am?</td>\n",
       "      <td>Naka Yan ŋga ŋga ŋga ŋga ŋga ŋga ŋgaest</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Il a vu tes autres amis!</td>\n",
       "      <td>Gis na sa xarit yeneen yi!</td>\n",
       "      <td>Gis Gis ngi ngia am am am am am</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Donne deux fois rien que là!</td>\n",
       "      <td>Joxeel foofu doŋŋ ñari yoon!</td>\n",
       "      <td>Ci nataal bi waay dem dem dem moo moo moo</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Pourquoi?</td>\n",
       "      <td>Lu waral?</td>\n",
       "      <td>A Yan naa yiy yiyestestest</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Cela simplement!</td>\n",
       "      <td>Loolu doŋŋ!</td>\n",
       "      <td>Ci Seet bépp waay dem dem moo moo moo moo</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>281</th>\n",
       "      <td>Ce n'est que longtemps après, quand l'égoïsme ...</td>\n",
       "      <td>Teg nañ ciy ati-at ma door a jëli ni jigéen, n...</td>\n",
       "      <td>Ci Baay ko ñu ñu ñu ñu ñu ñu ñu ñu ñu ñu ñu ñu...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>282</th>\n",
       "      <td>J'ai ressenti de l'étonnement, et même de l'in...</td>\n",
       "      <td>Li wóor te wér moo di ne bi loolu lépp weesoo,...</td>\n",
       "      <td>A A ko ñu ñi ñi ñi ñi ñi ñi ñi ñi ñi ñi ñi ñi ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>283</th>\n",
       "      <td>À quel point les arbres aux troncs rectilignes...</td>\n",
       "      <td>Dàtti garab yaa ngi lunk, sànneeku jëm ca kow,...</td>\n",
       "      <td>Ciyyy waay ñi ñi ñi ñi ñi ñi ñi ñi ñi ñi ñi ñi...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>284</th>\n",
       "      <td>Je peux ressentir l'émotion qu'il éprouve à tr...</td>\n",
       "      <td>Li koy yëngal noonu, xam naa ko. Lan moo ko dà...</td>\n",
       "      <td>Ay ko ko waay ñi ñi ñi ñi ñi ñi ñi ñi ñi ñi ñi...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>285</th>\n",
       "      <td>J'étais vraiment sur le pont d'un bateau. Le b...</td>\n",
       "      <td>Ku ma laajoon fan laa nekk, ma ni la : « Man? ...</td>\n",
       "      <td>Li B ko ko ko ko ak ak ak ak ak ak ak ak aku ñ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>286 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                    original_sentences  \\\n",
       "0                                       Es-tu en paix?   \n",
       "1                             Il a vu tes autres amis!   \n",
       "2                         Donne deux fois rien que là!   \n",
       "3                                            Pourquoi?   \n",
       "4                                     Cela simplement!   \n",
       "..                                                 ...   \n",
       "281  Ce n'est que longtemps après, quand l'égoïsme ...   \n",
       "282  J'ai ressenti de l'étonnement, et même de l'in...   \n",
       "283  À quel point les arbres aux troncs rectilignes...   \n",
       "284  Je peux ressentir l'émotion qu'il éprouve à tr...   \n",
       "285  J'étais vraiment sur le pont d'un bateau. Le b...   \n",
       "\n",
       "                                          translations  \\\n",
       "0                                         Jaam nga am?   \n",
       "1                           Gis na sa xarit yeneen yi!   \n",
       "2                         Joxeel foofu doŋŋ ñari yoon!   \n",
       "3                                            Lu waral?   \n",
       "4                                          Loolu doŋŋ!   \n",
       "..                                                 ...   \n",
       "281  Teg nañ ciy ati-at ma door a jëli ni jigéen, n...   \n",
       "282  Li wóor te wér moo di ne bi loolu lépp weesoo,...   \n",
       "283  Dàtti garab yaa ngi lunk, sànneeku jëm ca kow,...   \n",
       "284  Li koy yëngal noonu, xam naa ko. Lan moo ko dà...   \n",
       "285  Ku ma laajoon fan laa nekk, ma ni la : « Man? ...   \n",
       "\n",
       "                                           predictions  \n",
       "0              Naka Yan ŋga ŋga ŋga ŋga ŋga ŋga ŋgaest  \n",
       "1                      Gis Gis ngi ngia am am am am am  \n",
       "2            Ci nataal bi waay dem dem dem moo moo moo  \n",
       "3                           A Yan naa yiy yiyestestest  \n",
       "4            Ci Seet bépp waay dem dem moo moo moo moo  \n",
       "..                                                 ...  \n",
       "281  Ci Baay ko ñu ñu ñu ñu ñu ñu ñu ñu ñu ñu ñu ñu...  \n",
       "282  A A ko ñu ñi ñi ñi ñi ñi ñi ñi ñi ñi ñi ñi ñi ...  \n",
       "283  Ciyyy waay ñi ñi ñi ñi ñi ñi ñi ñi ñi ñi ñi ñi...  \n",
       "284  Ay ko ko waay ñi ñi ñi ñi ñi ñi ñi ñi ñi ñi ñi...  \n",
       "285  Li B ko ko ko ko ak ak ak ak ak ak ak ak aku ñ...  \n",
       "\n",
       "[286 rows x 3 columns]"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_pytorch_p310",
   "language": "python",
   "name": "conda_pytorch_p310"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
