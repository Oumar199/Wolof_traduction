{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pytorch create buckets with same lengths\n",
    "---------------------------------------\n",
    "Let us use a class create by chat gpt to generate batches of sequences of same lengths."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Load the data with a custom dataloader"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following custom dataset is very similar to that we created in [tokenizing_sentences](creating_tokenizer_for_all_sentences_3.ipynb)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting wolof-translate/wolof_translate/utils/bucket_iterator.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile wolof-translate/wolof_translate/utils/bucket_iterator.py\n",
    "\n",
    "import torch\n",
    "import numpy as np\n",
    "from torch.utils.data import Sampler\n",
    "from torch.nn.utils.rnn import pad_sequence\n",
    "from math import ceil\n",
    "\n",
    "class SequenceLengthBatchSampler(Sampler):\n",
    "    def __init__(self, dataset, boundaries, batch_sizes):\n",
    "        self.dataset = dataset\n",
    "        self.boundaries = boundaries\n",
    "        self.batch_sizes = batch_sizes\n",
    "\n",
    "    def __iter__(self):\n",
    "        indices = list(range(len(self.dataset)))  # Get indices of the dataset\n",
    "        sorted_indices = sorted(indices, key=lambda i: max(len(self.dataset[i][0]), len(self.dataset[i][1])))  # Sort indices based on sequence length\n",
    "        self.batches = []\n",
    "\n",
    "        # Group indices into batches of sequences with the same length\n",
    "        for boundary in self.boundaries:\n",
    "            batch = [i for i in sorted_indices if len(self.dataset[i][0]) <= boundary]  # Filter indices based on length boundary\n",
    "            self.batches.append(batch)\n",
    "            sorted_indices = [i for i in sorted_indices if i not in batch]  # Remove processed indices\n",
    "\n",
    "        # Add remaining indices to the last batch\n",
    "        self.batches.append(sorted_indices)\n",
    "\n",
    "        # Yield batches with the corresponding batch sizes\n",
    "        for batch_indices, batch_size in zip(self.batches, self.batch_sizes):\n",
    "            num_batches = len(batch_indices) // batch_size\n",
    "            for i in range(num_batches):\n",
    "                yield batch_indices[i * batch_size: (i + 1) * batch_size]\n",
    "\n",
    "            remaining_indices = len(batch_indices) % batch_size\n",
    "            if remaining_indices > 0:\n",
    "                yield batch_indices[-remaining_indices:]\n",
    "\n",
    "    def __len__(self):\n",
    "        return sum(len(batch) // batch_size + 1 for batch, batch_size in zip(self.batches, self.batch_sizes))\n",
    "\n",
    "class BucketSampler(Sampler):\n",
    "    def __init__(self, dataset, batch_size, sort_key=lambda x: max(len(x[0]), len(x[1]))):\n",
    "        self.dataset = dataset\n",
    "        self.batch_size = batch_size\n",
    "        self.sort_key = sort_key\n",
    "\n",
    "    def __iter__(self):\n",
    "        indices = np.argsort([self.sort_key(self.dataset[i]) for i in range(len(self.dataset))])\n",
    "        batches = [indices[i:i + self.batch_size] for i in range(0, len(indices), self.batch_size)]\n",
    "        if self.batch_size > 1:\n",
    "            np.random.shuffle(batches)\n",
    "        for batch in batches:\n",
    "            yield batch.tolist()\n",
    "\n",
    "    def __len__(self):\n",
    "        return ceil(len(self.dataset) / self.batch_size)\n",
    "\n",
    "\n",
    "def collate_fn(batch):\n",
    "    # Separate the input sequences, target sequences, and attention masks\n",
    "    input_seqs, input_masks, target_seqs, target_masks = zip(*batch)\n",
    "\n",
    "    # Pad the input sequences to have the same length\n",
    "    padded_input_seqs = pad_sequence(input_seqs, batch_first=True)\n",
    "\n",
    "    # Pad the target sequences to have the same length\n",
    "    padded_target_seqs = pad_sequence(target_seqs, batch_first=True)\n",
    "\n",
    "    # Pad the input masks to have the same length\n",
    "    padded_input_masks = pad_sequence(input_masks, batch_first=True)\n",
    "\n",
    "    # Pad the labels masks to have the same length\n",
    "    padded_target_masks = pad_sequence(target_masks, batch_first=True)\n",
    "\n",
    "    return padded_input_seqs, padded_input_masks, padded_target_seqs, padded_target_masks\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "%run wolof-translate/wolof_translate/data/dataset_v4.py\n",
    "%run wolof-translate/wolof_translate/utils/bucket_iterator.py"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-----------------"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let us create two datasets. One for the training and another for the validation. We need to upload and split the sentences before."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "from wolof_translate.utils.split_with_valid import split_data\n",
    "from wolof_translate.data.dataset_v4 import SentenceDataset\n",
    "# from wolof_translate.utils.bucket_iterator import SameLengthBatchSampler, collate_fn\n",
    "from transformers import T5TokenizerFast\n",
    "\n",
    "# split the data\n",
    "split_data(random_state=0, csv_file='corpora_v6.csv')\n",
    "\n",
    "# tokenizer\n",
    "tokenizer = T5TokenizerFast('wolof-translate/wolof_translate/tokenizers/t5_tokenizers/tokenizer_v5.model')\n",
    "\n",
    "# load the train data\n",
    "valid_dataset = SentenceDataset('data/extractions/new_data/valid_set.csv', tokenizer)\n",
    "\n",
    "sampler = SequenceLengthBatchSampler(valid_dataset, [2, 31, 59, 87, 115, 143, 171], [256, 128, 64, 32, 16, 8, 4, 2])\n",
    "dataloader = torch.utils.data.DataLoader(valid_dataset, batch_sampler=sampler, collate_fn=collate_fn)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 2])\n",
      "torch.Size([128, 11])\n",
      "torch.Size([85, 31])\n",
      "torch.Size([42, 59])\n",
      "torch.Size([16, 83])\n",
      "torch.Size([6, 115])\n",
      "torch.Size([4, 139])\n",
      "torch.Size([1, 153])\n",
      "torch.Size([2, 224])\n",
      "torch.Size([1, 248])\n"
     ]
    }
   ],
   "source": [
    "i = 0\n",
    "for input_, mask_, labels, _ in dataloader:\n",
    "    i+=1\n",
    "    print(input_.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Batch 10: 100%|██████████| 10/10 [00:01<00:00,  8.64it/s]\n"
     ]
    }
   ],
   "source": [
    "progress = tqdm(dataloader)\n",
    "\n",
    "i = 0\n",
    "\n",
    "for batch in progress:\n",
    "    \n",
    "    i += 1\n",
    "    \n",
    "    progress.set_description(f\"Batch {i}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "10"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(dataloader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "10"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "i"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "--------------------------"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let us create two datasets. One for the training and another for the validation. We need to upload and split the sentences before."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "from wolof_translate.utils.split_with_valid import split_data\n",
    "from wolof_translate.data.dataset_v4 import SentenceDataset\n",
    "# from wolof_translate.utils.bucket_iterator import SameLengthBatchSampler, collate_fn\n",
    "from transformers import T5TokenizerFast\n",
    "\n",
    "# split the data\n",
    "split_data(random_state=0, csv_file='corpora_v6.csv')\n",
    "\n",
    "# tokenizer\n",
    "tokenizer = T5TokenizerFast('wolof-translate/wolof_translate/tokenizers/t5_tokenizers/tokenizer_v5.model')\n",
    "\n",
    "# load the train data\n",
    "valid_dataset = SentenceDataset('data/extractions/new_data/valid_set.csv', tokenizer)\n",
    "\n",
    "sampler = BucketSampler(valid_dataset, 16)\n",
    "dataloader = torch.utils.data.DataLoader(valid_dataset, batch_sampler=sampler, collate_fn=collate_fn)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([16, 8])\n",
      "torch.Size([16, 10])\n",
      "torch.Size([16, 14])\n",
      "torch.Size([16, 59])\n",
      "torch.Size([14, 248])\n",
      "torch.Size([16, 39])\n",
      "torch.Size([16, 23])\n",
      "torch.Size([16, 6])\n",
      "torch.Size([16, 7])\n",
      "torch.Size([16, 12])\n",
      "torch.Size([16, 16])\n",
      "torch.Size([16, 8])\n",
      "torch.Size([16, 6])\n",
      "torch.Size([16, 10])\n",
      "torch.Size([16, 47])\n",
      "torch.Size([16, 83])\n",
      "torch.Size([16, 9])\n",
      "torch.Size([16, 30])\n"
     ]
    }
   ],
   "source": [
    "i = 0\n",
    "for input_, mask_, labels, _ in dataloader:\n",
    "    i+=1\n",
    "    print(input_.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Batch 18: 100%|██████████| 18/18 [00:00<00:00, 58.15it/s]\n"
     ]
    }
   ],
   "source": [
    "progress = tqdm(dataloader)\n",
    "\n",
    "i = 0\n",
    "\n",
    "for batch in progress:\n",
    "    \n",
    "    i += 1\n",
    "    \n",
    "    progress.set_description(f\"Batch {i}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "18"
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(dataloader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "18"
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "i"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pytorch1-HleOW5am-py3.10",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
