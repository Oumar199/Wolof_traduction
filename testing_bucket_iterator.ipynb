{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pytorch create buckets with same lengths\n",
    "---------------------------------------\n",
    "Let us use a class create by chat gpt to generate batches of sequences of same lengths."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Load the data with a custom dataloader"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following custom dataset is very similar to that we created in [tokenizing_sentences](creating_tokenizer_for_all_sentences_3.ipynb)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = torch.nn.Linear(1, 1)\n",
    "\n",
    "model = torch.nn.parallel.DataParallel(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.module.weight.data = torch.ones(1, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting wolof-translate/wolof_translate/utils/bucket_iterator.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile wolof-translate/wolof_translate/utils/bucket_iterator.py\n",
    "\n",
    "import torch\n",
    "import numpy as np\n",
    "from torch.utils.data import Sampler\n",
    "from torch.nn.utils.rnn import pad_sequence\n",
    "from math import ceil\n",
    "\n",
    "class SequenceLengthBatchSampler(Sampler):\n",
    "    def __init__(self, dataset, boundaries, batch_sizes):\n",
    "        self.dataset = dataset\n",
    "        self.boundaries = boundaries\n",
    "        self.batch_sizes = batch_sizes\n",
    "        self.data_info = {}\n",
    "\n",
    "        # Initialize dictionary with indices and element lengths\n",
    "        for i, data in enumerate(dataset):\n",
    "            length = max(len(data[0]), len(data[2]))\n",
    "            self.data_info[i] = {\"index\": i, \"length\": length}\n",
    "\n",
    "    def __iter__(self):\n",
    "        indices = list(self.data_info.keys())  # Get indices from the data_info dictionary\n",
    "        np.random.shuffle(indices)  # Shuffle the indices\n",
    "        sorted_indices = sorted(indices, key=lambda i: self.data_info[i][\"length\"])  # Sort indices based on element length\n",
    "        self.batches = []\n",
    "\n",
    "        # Group indices into batches of sequences with the same length\n",
    "        for boundary in self.boundaries:\n",
    "            batch = [i for i in sorted_indices if self.data_info[i][\"length\"] <= boundary]  # Filter indices based on length boundary\n",
    "            self.batches.append(batch)\n",
    "            sorted_indices = [i for i in sorted_indices if i not in batch]  # Remove processed indices\n",
    "\n",
    "        # Add remaining indices to the last batch\n",
    "        self.batches.append(sorted_indices)\n",
    "\n",
    "        # Yield batches with the corresponding batch sizes\n",
    "        for batch_indices, batch_size in zip(self.batches, self.batch_sizes):\n",
    "            num_batches = len(batch_indices) // batch_size\n",
    "            \n",
    "            for i in range(num_batches):\n",
    "                \n",
    "                # recuperate the current bucket\n",
    "                current_bucket = batch_indices[i * batch_size: (i + 1) * batch_size]\n",
    "                \n",
    "                # shuffle the current bucket\n",
    "                np.random.shuffle(current_bucket)\n",
    "                \n",
    "                # yield the current bucket\n",
    "                yield [self.data_info[i][\"index\"] for i in current_bucket]\n",
    "\n",
    "            remaining_indices = len(batch_indices) % batch_size\n",
    "            if remaining_indices > 0:\n",
    "                # recuperate the current bucket\n",
    "                current_bucket = batch_indices[-remaining_indices:]\n",
    "                \n",
    "                # shuffle the current bucket\n",
    "                np.random.shuffle(current_bucket)\n",
    "                \n",
    "                # yield the current bucket\n",
    "                yield [self.data_info[i][\"index\"] for i in batch_indices[-remaining_indices:]]\n",
    "\n",
    "    def __len__(self):\n",
    "        return sum(ceil(len(batch) / batch_size) for batch, batch_size in zip(self.batches, self.batch_sizes))\n",
    "\n",
    "class BucketSampler(Sampler):\n",
    "    def __init__(self, dataset, batch_size, sort_key=lambda x: max(len(x[0]), len(x[1]))):\n",
    "        self.dataset = dataset\n",
    "        self.batch_size = batch_size\n",
    "        self.sort_key = sort_key\n",
    "\n",
    "    def __iter__(self):\n",
    "        indices = np.argsort([self.sort_key(self.dataset[i]) for i in range(len(self.dataset))])\n",
    "        batches = [indices[i:i + self.batch_size] for i in range(0, len(indices), self.batch_size)]\n",
    "        if self.batch_size > 1:\n",
    "            np.random.shuffle(batches)\n",
    "        for batch in batches:\n",
    "            yield batch.tolist()\n",
    "\n",
    "    def __len__(self):\n",
    "        return ceil(len(self.dataset) / self.batch_size)\n",
    "\n",
    "\n",
    "def collate_fn(batch):\n",
    "    from torch.nn.utils.rnn import pad_sequence\n",
    "    # Separate the input sequences, target sequences, and attention masks\n",
    "    input_seqs, input_masks, target_seqs, target_masks = zip(*batch)\n",
    "\n",
    "    # Pad the input sequences to have the same length\n",
    "    padded_input_seqs = pad_sequence(input_seqs, batch_first=True)\n",
    "\n",
    "    # Pad the target sequences to have the same length\n",
    "    padded_target_seqs = pad_sequence(target_seqs, batch_first=True)\n",
    "\n",
    "    # Pad the input masks to have the same length\n",
    "    padded_input_masks = pad_sequence(input_masks, batch_first=True)\n",
    "\n",
    "    # Pad the labels masks to have the same length\n",
    "    padded_target_masks = pad_sequence(target_masks, batch_first=True)\n",
    "\n",
    "    return padded_input_seqs, padded_input_masks, padded_target_seqs, padded_target_masks\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "%run wolof-translate/wolof_translate/data/dataset_v4.py\n",
    "%run wolof-translate/wolof_translate/utils/bucket_iterator.py"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-----------------"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let us create two datasets. One for the training and another for the validation. We need to upload and split the sentences before."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "from wolof_translate.utils.split_with_valid import split_data\n",
    "from wolof_translate.data.dataset_v4 import SentenceDataset\n",
    "# from wolof_translate.utils.bucket_iterator import SameLengthBatchSampler, collate_fn\n",
    "from transformers import T5TokenizerFast\n",
    "\n",
    "# split the data\n",
    "split_data(random_state=0, csv_file='corpora_v6.csv')\n",
    "\n",
    "# tokenizer\n",
    "tokenizer = T5TokenizerFast('wolof-translate/wolof_translate/tokenizers/t5_tokenizers/tokenizer_v5.model')\n",
    "\n",
    "# load the train data\n",
    "train_dataset = SentenceDataset('data/extractions/new_data/train_set.csv', tokenizer)\n",
    "\n",
    "sampler = SequenceLengthBatchSampler(train_dataset, [2, 31, 59, 87, 115, 143, 171], [256, 128, 64, 32, 16, 8, 4, 2])\n",
    "dataloader = torch.utils.data.DataLoader(train_dataset, batch_sampler=sampler, collate_fn=collate_fn)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 2])\n",
      "torch.Size([128, 5])\n",
      "torch.Size([128, 6])\n",
      "torch.Size([128, 7])\n",
      "torch.Size([128, 8])\n",
      "torch.Size([128, 9])\n",
      "torch.Size([128, 9])\n",
      "torch.Size([128, 10])\n",
      "torch.Size([128, 11])\n",
      "torch.Size([128, 12])\n",
      "torch.Size([128, 13])\n",
      "torch.Size([128, 15])\n",
      "torch.Size([128, 18])\n",
      "torch.Size([128, 25])\n",
      "torch.Size([95, 31])\n",
      "torch.Size([64, 36])\n",
      "torch.Size([64, 40])\n",
      "torch.Size([64, 46])\n",
      "torch.Size([64, 53])\n",
      "torch.Size([45, 59])\n",
      "torch.Size([32, 66])\n",
      "torch.Size([32, 72])\n",
      "torch.Size([32, 78])\n",
      "torch.Size([32, 86])\n",
      "torch.Size([5, 87])\n",
      "torch.Size([16, 92])\n",
      "torch.Size([16, 102])\n",
      "torch.Size([16, 112])\n",
      "torch.Size([3, 115])\n",
      "torch.Size([8, 119])\n",
      "torch.Size([8, 133])\n",
      "torch.Size([5, 142])\n",
      "torch.Size([4, 148])\n",
      "torch.Size([4, 162])\n",
      "torch.Size([2, 170])\n",
      "torch.Size([2, 185])\n",
      "torch.Size([2, 191])\n",
      "torch.Size([2, 213])\n",
      "torch.Size([2, 244])\n",
      "torch.Size([2, 210])\n",
      "torch.Size([1, 152])\n"
     ]
    }
   ],
   "source": [
    "i = 0\n",
    "for input_, mask_, labels, _ in dataloader:\n",
    "    i+=1\n",
    "    print(input_.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm, trange"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Batch 41: 100%|██████████| 41/41 [00:01<00:00, 24.44it/s]\n"
     ]
    }
   ],
   "source": [
    "progress = tqdm(dataloader)\n",
    "\n",
    "i = 0\n",
    "\n",
    "for batch in progress:\n",
    "    \n",
    "    i += 1\n",
    "    \n",
    "    progress.set_description(f\"Batch {i}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "41"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(dataloader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "41"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "i"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "batch number 41: 100%|██████████| 41/41 [00:00<00:00, 1136.76batches/s]\n"
     ]
    }
   ],
   "source": [
    "loader = list(iter(dataloader))\n",
    "                    \n",
    "with trange(len(loader), unit = \"batches\", position = 0, leave = True) as pbar:\n",
    "# for i in tqdm(range(len(loader))):\n",
    "    for i in pbar:\n",
    "    \n",
    "        pbar.set_description(f\"batch number {i + 1}\")\n",
    "        \n",
    "        data = loader[i]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "--------------------------"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let us create two datasets. One for the training and another for the validation. We need to upload and split the sentences before."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 251,
   "metadata": {},
   "outputs": [],
   "source": [
    "from wolof_translate.utils.split_with_valid import split_data\n",
    "from wolof_translate.data.dataset_v4 import SentenceDataset\n",
    "# from wolof_translate.utils.bucket_iterator import SameLengthBatchSampler, collate_fn\n",
    "from transformers import T5TokenizerFast\n",
    "\n",
    "# split the data\n",
    "split_data(random_state=0, csv_file='corpora_v6.csv')\n",
    "\n",
    "# tokenizer\n",
    "tokenizer = T5TokenizerFast('wolof-translate/wolof_translate/tokenizers/t5_tokenizers/tokenizer_v5.model')\n",
    "\n",
    "# load the train data\n",
    "valid_dataset = SentenceDataset('data/extractions/new_data/valid_set.csv', tokenizer)\n",
    "\n",
    "sampler = BucketSampler(valid_dataset, 16)\n",
    "dataloader = torch.utils.data.DataLoader(valid_dataset, batch_sampler=sampler, collate_fn=collate_fn)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 252,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([16, 47])\n",
      "torch.Size([14, 248])\n",
      "torch.Size([16, 8])\n",
      "torch.Size([16, 14])\n",
      "torch.Size([16, 10])\n",
      "torch.Size([16, 6])\n",
      "torch.Size([16, 30])\n",
      "torch.Size([16, 6])\n",
      "torch.Size([16, 8])\n",
      "torch.Size([16, 39])\n",
      "torch.Size([16, 23])\n",
      "torch.Size([16, 59])\n",
      "torch.Size([16, 10])\n",
      "torch.Size([16, 12])\n",
      "torch.Size([16, 7])\n",
      "torch.Size([16, 16])\n",
      "torch.Size([16, 9])\n",
      "torch.Size([16, 83])\n"
     ]
    }
   ],
   "source": [
    "i = 0\n",
    "for input_, mask_, labels, _ in dataloader:\n",
    "    i+=1\n",
    "    print(input_.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 253,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 254,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Batch 18: 100%|██████████| 18/18 [00:00<00:00, 64.00it/s]\n"
     ]
    }
   ],
   "source": [
    "progress = tqdm(dataloader)\n",
    "\n",
    "i = 0\n",
    "\n",
    "for batch in progress:\n",
    "    \n",
    "    i += 1\n",
    "    \n",
    "    progress.set_description(f\"Batch {i}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 255,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "18"
      ]
     },
     "execution_count": 255,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(dataloader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 256,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "18"
      ]
     },
     "execution_count": 256,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "i"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "------------------------------"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pytorch1-HleOW5am-py3.10",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
