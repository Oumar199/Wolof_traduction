{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Corpus Tokenizer\n",
    "--------------------------------\n",
    "\n",
    "In this part we will create a tokenizer from scratch. We already identified the min-frequencies but taking them in account will remove the most important words in the sentences. We will create a custom BPE (Byte-pair Encoding) tokenizer which don't require to normalize the tokens. That tokenizer will be trained and saved in order to use it as the tokenizer of the GPT-2 model that we will use latter on the training step. \n",
    "\n",
    "To understand how is working the BPE tokenizer, see the following tutorial [BPE_tokenizer](https://huggingface.co/learn/nlp-course/chapter6/5?fw=pt#:~:text=Byte%2DPair%20Encoding%20(BPE),HuggingFace)."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For the tokenizer training the following steps will be required:\n",
    "\n",
    "- Creating a batch generator to generate the batches of sentences\n",
    "- Load the BPE tokenizer\n",
    "- Configure the pre-tokenizer\n",
    "- Initialize the trainer: vocabulary size of `20000` at max and special tokens = `'<|endoftext|>'` (To identify the beginning and the ending of a text), `'<|translateto|>'` (To separate the French sentences from the Wolof sentences).\n",
    "- Train the tokenizer\n",
    "- Initialize the decoder method: `ByteLevel Decoder`.\n",
    "- Initialize the post-processor for the GPT-2 tokenizer: `ByteLevel post-processing` for the GPT-2 tokenizer.\n",
    "- Save the tokenizer locally"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let us import the necessary libraries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for creating the tokenizer\n",
    "from tokenizers import (\n",
    "    decoders,\n",
    "    models,\n",
    "    pre_tokenizers,\n",
    "    processors,\n",
    "    trainers,\n",
    "    Tokenizer,\n",
    ")\n",
    "\n",
    "# for importing and manipulating the sentences\n",
    "import pandas as pd"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Load dataset and create generator"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will create one tokenizer for both of the French and Wolof corpora. So we will stack the french and wolof sentences at the same lines."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load sentences\n",
    "sentences = pd.read_csv(\"data/extractions/new_data/sent_extraction.csv\")\n",
    "\n",
    "# initialize a batch size\n",
    "BATCH_SIZE = 50\n",
    "\n",
    "# create generators (for the corpora)\n",
    "def generate_sentences():\n",
    "    \n",
    "    # stacking the sentences\n",
    "    concat_sentences = lambda line_index: sentences.loc[line_index, \"french_corpus\"] + \" \" + sentences.loc[line_index, \"wolof_corpus\"]  \n",
    "    \n",
    "    sentences[\"corpora\"] = sentences.index.map(concat_sentences)\n",
    "    \n",
    "    sents = sentences[\"corpora\"].to_list()\n",
    "    \n",
    "    for i in range(1, len(sents), BATCH_SIZE):\n",
    "        \n",
    "        yield sents[i:i+BATCH_SIZE]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Load the BPE Tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = Tokenizer(models.BPE())"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Configure the pre-tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer.pre_tokenizer = pre_tokenizers.ByteLevel(False)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Initialize the BPE Trainer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer = trainers.BpeTrainer(vocab_size = 20000, special_tokens = [\"<|endoftext|>\", \"<|translateto|>\", \"<|pad|>\"])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Train the tokenizer from the iterator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer.train_from_iterator(generate_sentences(), trainer)\n",
    "\n",
    "tokenizer.enable_padding()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let us print the vocab size."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "15688"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.get_vocab_size()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Initialize the decoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer.decoder = decoders.ByteLevel()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Initialize the post-processor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer.post_process = processors.ByteLevel(False)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Save the tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer.save(\"wolof-translate/wolof_translate/tokenizers/tokenizer_v1.json\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let us make an example with the sentence \"Je suis ici.\" translate in wolof by \"Magui fi.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokens:\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['<|endoftext|>',\n",
       " 'Je',\n",
       " 'Ġsuis',\n",
       " 'Ġici',\n",
       " '.',\n",
       " '<|translateto|>',\n",
       " 'M',\n",
       " 'ag',\n",
       " 'ui',\n",
       " 'Ġfi',\n",
       " '.',\n",
       " '<|endoftext|>']"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "encoding = tokenizer.encode(\"<|endoftext|>Je suis ici.<|translateto|>Magui fi.<|endoftext|>\")\n",
    "\n",
    "print(\"Tokens:\")\n",
    "encoding.tokens"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If we want to use the tokenizer when training the GPT-2 Model we will provide the tokenizer to the `PreTrainedTokenizerFast` class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import PreTrainedTokenizerFast\n",
    "\n",
    "wrapped_tokenizer = PreTrainedTokenizerFast(\n",
    "    tokenizer_object=tokenizer,\n",
    "    bos_token=\"<|endoftext|>\",\n",
    "    eos_token=\"<|endoftext|>\",\n",
    "    pad_token=\"<|pad|>\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "15688"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(wrapped_tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input_ids': [0, 26, 117, 5418, 10, 1321, 1146, 4706, 10, 1, 25, 69, 3675, 71, 9, 9919, 61, 271, 10, 3777, 712, 657, 10, 0, 2, 2, 2, 2, 2, 2], 'token_type_ids': [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0]}"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wrapped_tokenizer(\"<|endoftext|>Bonjour. Je suis ici.<|translateto|>Asalamu-aleykum. Magui fi.<|endoftext|>\", max_length=30, padding='max_length')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pytorch1-HleOW5am-py3.10",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
