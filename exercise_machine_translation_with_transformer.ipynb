{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Machine Translation with the Transformer [Denis Rothman book]\n",
    "--------------------------"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preprocessing a WMT dataset"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We downloaded the French-English dataset from the <i style = \"color: violet\">European Parliament Proceedings Parallel Corpus 1996-2011</i>. The link is [dataset_french_english](https://www.statmt.org/europarl/v7/fr-en.tgz)."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We extracted two files from the dataset:\n",
    "\n",
    "- europarl-v7.fr-en.en\n",
    "- europarl-v7.fr-en.fr\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Preprocessing the raw data"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will preprocess europarl-v7.fr-en.en and europarl-v7.fr-en.fr.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "from pickle import dump\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We define the function to load the file into memory:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_doc(filename):\n",
    "    # open the file as a read only\n",
    "    with open(filename, mode=\"rt\", encoding=\"utf-8\") as f:\n",
    "    \n",
    "        # read all text\n",
    "        text = f.read()\n",
    "    \n",
    "    return text"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The loaded document is then split into sentences:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def to_sentences(doc):\n",
    "    \n",
    "    return doc.strip().split('\\n')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The shortest and the longest lengths are retrieved:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sentence_lengths(sentences):\n",
    "    \n",
    "    lengths = [len(s.split()) for s in sentences]\n",
    "    \n",
    "    return min(lengths), max(lengths)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The imported sentence lines now need to be cleaned to avoid training useless and noisy tokens. The lines are normalized, tokenized on white spaces, and converted to lower case. The punctuation is removed from each token, non printable characters are removed, and tokens containing numbers are excluded. The cleaned line is stored as a string. The program runs the cleaning function and returns clean appended strings:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# clean lines\n",
    "import re\n",
    "import string\n",
    "import unicodedata\n",
    "def clean_lines(lines):\n",
    "    \n",
    "    cleaned = list()\n",
    "    \n",
    "    # prepare regex for char filtering\n",
    "    re_print = re.compile('[^%s]' % re.escape(string.printable))\n",
    "    \n",
    "    # prepare translation table for removing punctuation\n",
    "    table  = str.maketrans('', '', string.punctuation)\n",
    "    \n",
    "    for line in lines:\n",
    "        \n",
    "        # normalize unicode characters\n",
    "        line = unicodedata.normalize('NFD', line).encode('ascii', 'ignore')\n",
    "        line = line.decode('UTF-8')\n",
    "        \n",
    "        # tokenize on white space\n",
    "        line = line.split()\n",
    "        \n",
    "        # convert to lower case\n",
    "        line = [word.lower() for word in line]\n",
    "        \n",
    "        # remove punctuation from each token\n",
    "        line = [word.translate(table) for word in line]\n",
    "        \n",
    "        # remove non-printable chars from each token\n",
    "        line = [re_print.sub('', w) for w in line]\n",
    "        \n",
    "        # remove tokens with numbers in them\n",
    "        line = [word for word in line if word.isalpha()]\n",
    "        \n",
    "        # store as string\n",
    "        cleaned.append(' '.join(line))\n",
    "    \n",
    "    return cleaned"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The English data is loaded and cleaned first."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "English data: sentences=2007723, min=0, max=668\n"
     ]
    }
   ],
   "source": [
    "# load English data\n",
    "filename = 'data/europarl-v7.fr-en.en'\n",
    "\n",
    "doc = load_doc(filename)\n",
    "\n",
    "sentences = to_sentences(doc)\n",
    "\n",
    "minlen, maxlen = sentence_lengths(sentences)\n",
    "\n",
    "print('English data: sentences=%d, min=%d, max=%d' % (len(sentences), minlen, maxlen))\n",
    "\n",
    "cleanf = clean_lines(sentences)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The dataset is now clean, and pickle dumps it into a serialized file named English.pkl:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "data/English.pkl  saved\n"
     ]
    }
   ],
   "source": [
    "filename = 'data/English.pkl'\n",
    "outfile = open(filename, 'wb')\n",
    "pickle.dump(cleanf, outfile)\n",
    "outfile.close()\n",
    "print(filename, \" saved\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We now repeat the same process with the French data and dump it into a serialized file named French.pkl:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "French data: sentences=2007723, min=0, max=693\n",
      "data/French.pkl  saved\n"
     ]
    }
   ],
   "source": [
    "# load French data\n",
    "filename = 'data/europarl-v7.fr-en.fr'\n",
    "\n",
    "doc = load_doc(filename)\n",
    "\n",
    "sentences = to_sentences(doc)\n",
    "\n",
    "minlen, maxlen = sentence_lengths(sentences)\n",
    "\n",
    "print('French data: sentences=%d, min=%d, max=%d' % (len(sentences), minlen, maxlen))\n",
    "\n",
    "cleanf = clean_lines(sentences)\n",
    "\n",
    "filename = 'data/French.pkl'\n",
    "\n",
    "outfile = open(filename, 'wb')\n",
    "\n",
    "pickle.dump(cleanf, outfile)\n",
    "\n",
    "outfile.close()\n",
    "\n",
    "print(filename, \" saved\")\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The main preprocessing is done. But we still need to make sure the datasets do not contain noisy and confused tokens."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Finalizing the preprocessing of the datasets"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let us now define a function that will load the datasets that were cleaned up in the previous section and then save them once the preprocessing is finalized:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pickle import load\n",
    "from pickle import dump\n",
    "from collections import Counter\n",
    "\n",
    "# load a clean dataset\n",
    "def load_clean_sentences(filename):\n",
    "    \n",
    "    return load(open(filename, 'rb'))\n",
    "\n",
    "# save a list of clean sentences to file\n",
    "def save_clean_sentences(sentences, filename):\n",
    "    \n",
    "    dump(sentences, open(filename, 'wb'))\n",
    "    \n",
    "    print('Saved: %s' % filename)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We now define a function that will create a vocabulary counter. It is important to know how many times a word is used in the sequences we will parse. For example, if a word is only used once in a dataset containing two million lines, we will waste our energy if we use precious GPU resources to learn it. Let's define the counter:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a frequency table for all words\n",
    "def to_vocab(lines):\n",
    "    vocab = Counter()\n",
    "    for line in lines:\n",
    "        tokens = line.split()\n",
    "        vocab.update(tokens)\n",
    "    return vocab"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The vocabulary counter will detect words with a frequency that is below min_occurance:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# remove all words with a frequency below a threshold\n",
    "def trim_vocab(vocab, min_occurance):\n",
    "    tokens = [k for k, c in vocab.items() if c >= min_occurance]\n",
    "    return set(tokens)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this case, min_occurance = 5 and the words that are below or equal to this threshold have been removed to avoid wasting the training model's time analyzing them."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We now have to deal with **Out-Of-Vocabulary (OOV)** words. OOV words can be misspelled words, abbreviations, or any word that does not fit standard vocabulary representations. We could use automatic spelling, but it would not solve all of the problems. For this example, we will simply replace OOV words with the unk (unknown) token:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# mark all OOV with \"unk\" for all lines\n",
    "def update_dataset(lines, vocab):\n",
    "    \n",
    "    new_lines = list()\n",
    "    \n",
    "    for line in lines:\n",
    "        \n",
    "        new_tokens = list()\n",
    "        \n",
    "        for token in line.split():\n",
    "            \n",
    "            if token in vocab:\n",
    "                \n",
    "                new_tokens.append(token)\n",
    "            \n",
    "            else:\n",
    "                \n",
    "                new_tokens.append('unk')\n",
    "        \n",
    "        new_line = ' '.join(new_tokens)\n",
    "        \n",
    "        new_lines.append(new_line)\n",
    "    \n",
    "    return new_lines"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will now run the functions for the English dataset, then save the output and display 20 lines:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "English Vocabulary: 105357\n",
      "New English Vocabulary: 41746\n",
      "Saved: data/english_vocab.pkl\n",
      "line 0 : resumption of the session\n",
      "line 1 : i declare resumed the session of the european parliament adjourned on friday december and i would like once again to wish you a happy new year in the hope that you enjoyed a pleasant festive period\n",
      "line 2 : although as you will have seen the dreaded millennium bug failed to materialise still the people in a number of countries suffered a series of natural disasters that truly were dreadful\n",
      "line 3 : you have requested a debate on this subject in the course of the next few days during this partsession\n",
      "line 4 : in the meantime i should like to observe a minute s silence as a number of members have requested on behalf of all the victims concerned particularly those of the terrible storms in the various countries of the european union\n",
      "line 5 : please rise then for this minute s silence\n",
      "line 6 : the house rose and observed a minute s silence\n",
      "line 7 : madam president on a point of order\n",
      "line 8 : you will be aware from the press and television that there have been a number of bomb explosions and killings in sri lanka\n",
      "line 9 : one of the people assassinated very recently in sri lanka was mr unk unk who had visited the european parliament just a few months ago\n",
      "line 10 : would it be appropriate for you madam president to write a letter to the sri lankan president expressing parliaments regret at his and the other violent deaths in sri lanka and urging her to do everything she possibly can to seek a peaceful reconciliation to a very difficult situation\n",
      "line 11 : yes mr evans i feel an initiative of the type you have just suggested would be entirely appropriate\n",
      "line 12 : if the house agrees i shall do as mr evans has suggested\n",
      "line 13 : madam president on a point of order\n",
      "line 14 : i would like your advice about rule concerning inadmissibility\n",
      "line 15 : my question relates to something that will come up on thursday and which i will then raise again\n",
      "line 16 : the cunha report on multiannual guidance programmes comes before parliament on thursday and contains a proposal in paragraph that a form of quota penalties should be introduced for countries which fail to meet their fleet reduction targets annually\n",
      "line 17 : it says that this should be done despite the principle of relative stability\n",
      "line 18 : i believe that the principle of relative stability is a fundamental legal principle of the common fisheries policy and a proposal to subvert it would be legally inadmissible\n",
      "line 19 : i want to know whether one can raise an objection of that kind to what is merely a report not a legislative proposal and whether that is something i can competently do on thursday\n"
     ]
    }
   ],
   "source": [
    "# load English dataset\n",
    "filename = 'data/English.pkl'\n",
    "\n",
    "lines = load_clean_sentences(filename)\n",
    "\n",
    "# calculate vocabulary\n",
    "vocab = to_vocab(lines)\n",
    "\n",
    "print('English Vocabulary: %d' % len(vocab))\n",
    "\n",
    "# reduce vocabulary\n",
    "vocab = trim_vocab(vocab, 5)\n",
    "\n",
    "print('New English Vocabulary: %d' % len(vocab))\n",
    "\n",
    "# mark out of vocabulary words\n",
    "lines = update_dataset(lines, vocab)\n",
    "\n",
    "# save updated dataset\n",
    "filename = 'data/english_vocab.pkl'\n",
    "\n",
    "save_clean_sentences(lines, filename)\n",
    "\n",
    "# spot check\n",
    "for i in range(20):\n",
    "    \n",
    "    print(\"line\", i, \":\", lines[i])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let us now run the functions for the French dataset, then save the output and display 20 lines:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "French Vocabulary: 141642\n",
      "New French Vocabulary: 58800\n",
      "Saved: data/french_vocab.pkl\n",
      "line 0 : reprise de la session\n",
      "line 1 : je declare reprise la session du parlement europeen qui avait ete interrompue le vendredi decembre dernier et je vous renouvelle tous mes vux en esperant que vous avez passe de bonnes vacances\n",
      "line 2 : comme vous avez pu le constater le grand bogue de lan ne sest pas produit en revanche les citoyens dun certain nombre de nos pays ont ete victimes de catastrophes naturelles qui ont vraiment ete terribles\n",
      "line 3 : vous avez souhaite un debat a ce sujet dans les prochains jours au cours de cette periode de session\n",
      "line 4 : en attendant je souhaiterais comme un certain nombre de collegues me lont demande que nous observions une minute de silence pour toutes les victimes des tempetes notamment dans les differents pays de lunion europeenne qui ont ete touches\n",
      "line 5 : je vous invite a vous lever pour cette minute de silence\n",
      "line 6 : le parlement debout observe une minute de silence\n",
      "line 7 : madame la presidente cest une motion de procedure\n",
      "line 8 : vous avez probablement appris par la presse et par la television que plusieurs attentats a la bombe et crimes ont ete perpetres au sri lanka\n",
      "line 9 : lune des personnes qui vient detre assassinee au sri lanka est m unk unk qui avait rendu visite au parlement europeen il y a quelques mois a peine\n",
      "line 10 : ne pensezvous pas madame la presidente quil conviendrait decrire une lettre au president du sri lanka pour lui communiquer que le parlement deplore les morts violentes dont celle de m unk et pour linviter instamment a faire tout ce qui est en son pouvoir pour chercher une reconciliation pacifique et mettre un terme a cette situation particulierement difficile\n",
      "line 11 : oui monsieur evans je pense quune initiative dans le sens que vous venez de suggerer serait tout a fait appropriee\n",
      "line 12 : si lassemblee en est daccord je ferai comme m evans la suggere\n",
      "line 13 : madame la presidente cest une motion de procedure\n",
      "line 14 : je voudrais vous demander un conseil au sujet de larticle qui concerne lirrecevabilite\n",
      "line 15 : ma question porte sur un sujet qui est a lordre du jour du jeudi et que je souleverai donc une nouvelle fois\n",
      "line 16 : le paragraphe du rapport cunha sur les programmes dorientation pluriannuels qui sera soumis au parlement ce jeudi propose dintroduire des sanctions applicables aux pays qui ne respectent pas les objectifs annuels de reduction de leur flotte\n",
      "line 17 : il precise que cela devrait etre fait malgre le principe de stabilite relative\n",
      "line 18 : a mon sens le principe de stabilite relative est un principe juridique fondamental de la politique commune de la peche et toute proposition le bouleversant serait juridiquement irrecevable\n",
      "line 19 : je voudrais savoir si lon peut avancer une objection de ce type a ce qui nest quun rapport pas une proposition legislative et si je suis habilite a le faire ce jeudi\n"
     ]
    }
   ],
   "source": [
    "# load French dataset\n",
    "filename = 'data/French.pkl'\n",
    "\n",
    "lines = load_clean_sentences(filename)\n",
    "\n",
    "# calculate vocabulary\n",
    "vocab = to_vocab(lines)\n",
    "\n",
    "print('French Vocabulary: %d' % len(vocab))\n",
    "\n",
    "# reduce vocabulary\n",
    "vocab = trim_vocab(vocab, 5)\n",
    "\n",
    "print('New French Vocabulary: %d' % len(vocab))\n",
    "\n",
    "# mark out of vocabulary words\n",
    "lines = update_dataset(lines, vocab)\n",
    "\n",
    "# save updated dataset\n",
    "filename = 'data/french_vocab.pkl'\n",
    "\n",
    "save_clean_sentences(lines, filename)\n",
    "\n",
    "# spot check\n",
    "for i in range(20):\n",
    "    \n",
    "    print(\"line\", i, \":\", lines[i])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluating machine translation with BLEU"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let us begin with the Geometric evaluations."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Geometric evaluations"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The BLEU method compares the parts of a candidate sentence to a reference sentence or several reference sentences."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import the nltk module\n",
    "from nltk.translate.bleu_score import sentence_bleu\n",
    "from nltk.translate.bleu_score import SmoothingFunction"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It then simulates a comparison between a candidate translation produced by the \n",
    "machine translation model and the actual translation(s) references in the dataset. \n",
    "Bear in mind that a sentence could have been repeated several times and translated \n",
    "by different translators in different ways, making it challenging to find efficient \n",
    "evaluation strategies."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The program can evaluate one or more references:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Example 1 1.0\n",
      "Example 2 1.0\n"
     ]
    }
   ],
   "source": [
    "#Example 1\n",
    "reference = [['the', 'cat', 'likes', 'milk'], ['cat', 'likes' 'milk']] \n",
    "candidate = ['the', 'cat', 'likes', 'milk']\n",
    "score = sentence_bleu(reference, candidate) \n",
    "print('Example 1', score)\n",
    "#Example 2\n",
    "reference = [['the', 'cat', 'likes', 'milk']] \n",
    "candidate = ['the', 'cat', 'likes', 'milk'] \n",
    "score = sentence_bleu(reference, candidate) \n",
    "print('Example 2', score)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The output of both examples is 1."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A straightforward evaluation P of the candidate (C), the reference (R), and the \n",
    "number of correct tokens found in C (N) can be represented as a geometric function:\n",
    "$$\n",
    "P(N, C, R) = \\prod_{n = 1}^N p_n\n",
    "$$"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This geometric approach is rigid if you are looking for 3-gram overlap, for example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Example 3 1.0547686614863434e-154\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Oumar Kane\\AppData\\Local\\pypoetry\\Cache\\virtualenvs\\pytorch1-HleOW5am-py3.10\\lib\\site-packages\\nltk\\translate\\bleu_score.py:552: UserWarning: \n",
      "The hypothesis contains 0 counts of 3-gram overlaps.\n",
      "Therefore the BLEU score evaluates to 0, independently of\n",
      "how many N-gram overlaps of lower order it contains.\n",
      "Consider using lower n-gram order or use SmoothingFunction()\n",
      "  warnings.warn(_msg)\n",
      "c:\\Users\\Oumar Kane\\AppData\\Local\\pypoetry\\Cache\\virtualenvs\\pytorch1-HleOW5am-py3.10\\lib\\site-packages\\nltk\\translate\\bleu_score.py:552: UserWarning: \n",
      "The hypothesis contains 0 counts of 4-gram overlaps.\n",
      "Therefore the BLEU score evaluates to 0, independently of\n",
      "how many N-gram overlaps of lower order it contains.\n",
      "Consider using lower n-gram order or use SmoothingFunction()\n",
      "  warnings.warn(_msg)\n"
     ]
    }
   ],
   "source": [
    "#Example 3\n",
    "reference = [['the', 'cat', 'likes', 'milk']] \n",
    "candidate = ['the', 'cat', 'enjoys','milk'] \n",
    "score = sentence_bleu(reference, candidate) \n",
    "print('Example 3', score)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A human can see that the score should be 1 and not 0.7. The hyperparameters can be changed, but the approach remains rigid."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Applying a smoothing technique"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Smoothing is a very efficient method. BLEU smoothing can be traced back to label smoothing, applied to softmax outputs in the Transformer."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Chencherry smoothing"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's first evaluate a French-English example with smoothing:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "without soothing score 0.37188004246466494\n"
     ]
    }
   ],
   "source": [
    "#Example 4\n",
    "reference = [['je','vous','invite', 'a', 'vous', 'lever','pour', \n",
    "'cette', 'minute', 'de', 'silence']]\n",
    "candidate = ['levez','vous','svp','pour', 'cette', 'minute', 'de', \n",
    "'silence']\n",
    "score = sentence_bleu(reference, candidate) \n",
    "print(\"without soothing score\", score)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, let us add some open minded smoothing to the evaluation:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "with smoothing score 0.6194291765462159\n"
     ]
    }
   ],
   "source": [
    "chencherry = SmoothingFunction()\n",
    "\n",
    "r1 = list('je vous invite a vous lever pour cette minute de silence')\n",
    "candidate = list('levez vous svp pour cette minute de silence')\n",
    "\n",
    "print(\"with smoothing score\", sentence_bleu([r1], candidate, smoothing_function=chencherry.method1))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Translations with Trax"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Installing Trax"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "\n",
    "try:\n",
    "    import trax\n",
    "except ImportError:\n",
    "    !pip install -U trax"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Creating a Transformer model"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our Trax function will retrieve a pretrained model configuration in a few lines of code:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = trax.models.Transformer(\n",
    "    input_vocab_size = 33300,\n",
    "    d_model = 512, d_ff = 2048,\n",
    "    n_heads = 8, n_encoder_layers = 6, n_decoder_layers = 6,\n",
    "    max_len = 2048, mode = 'predict'\n",
    ")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Initializing the model using pretrained weights"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let us give life to the model by initializing the weights:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.init_from_file('gs://trax-ml/models/translation/ende_wmt32k.pkl.gz', \n",
    "                     weights_only=True)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Tokenizing a sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentence = 'I am only a machine but I have machine intelligence.'\n",
    "tokenized = list(trax.data.tokenize(iter([sentence]), # Operates on streams. \n",
    "                                   vocab_dir='gs://trax-ml/vocabs/', \n",
    "                                   vocab_file='ende_32k.subword'))[0]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Decoding from the Transformer"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The Transformer encodes the sentence in English and will decode it in German. The \n",
    "model and its weights constitute its set of abilities.\n",
    "Trax has made the decoding function intuitive to use:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenized = tokenized[None, :] # Add batch dimension.\n",
    "tokenized_translation = trax.supervised.decoding.autoregressive_sample( \n",
    "    model, tokenized, temperature=0.0)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### De-tokenizing and displaying the translation"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Google Brain has produced a mainstream, disruptive, and intuitive implementation \n",
    "of the Transformer with Trax.\n",
    "The program now de-tokenizes and displays the translation in a few lines:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenized_translation = tokenized_translation[0][:-1] # Remove batch \n",
    "and EOS.\n",
    "translation = trax.data.detokenize(tokenized_translation, \n",
    "                                   vocab_dir='gs://trax-ml/vocabs/', \n",
    "                                   vocab_file='ende_32k.subword') \n",
    "print(\"The sentence:\",sentence)\n",
    "print(\"The translation:\",translation)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pytorch1-HleOW5am-py3.10",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
