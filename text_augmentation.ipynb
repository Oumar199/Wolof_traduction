{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Text augmentation with `nlpaug`\n",
    "-------------------"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It is a library called `nlpaug` that we can use to transform the sentences in order to augment the vocabulary size. The tutorial is available at medium through the following link [nlpaug_examples](https://towardsdatascience.com/text-augmentation-in-few-lines-of-python-code-cdd10cf3cf84)."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will test the following methods:\n",
    "\n",
    "- `KeyboarAug`: It change a random character with another one close with it on the keyboard.\n",
    "- `RandomAug`: It modify a character with a random another one character.\n",
    "\n",
    "Most of the another methods require a model to work correctly. We can still test the following methods:\n",
    "- `TfidfAug`: Use TF-IDF to find out how words would be augmented. We will need to train a tf-idf on the tokens of each corpus before using it."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Steps"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will follow the next steps:\n",
    "\n",
    "- Create a custom dataset getting as argument a sentence transformer\n",
    "- Training a tf-idf on each corpus using the `nlpaug` library\n",
    "- Using the `KeyboardAug` method to augment the data (we will need to find the best parameters)\n",
    "- Using the `RandomAug` method to augment the data (we will also need to find the best parameters)\n",
    "- Using the `TfidfAug` method to augment the data (it can be the best approach)\n",
    "- Choose the best method and try to find the ideal parameters.\n",
    "- Create a new data frame containing augmented sentences"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We must import the following libraries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Oumar Kane\\AppData\\Local\\pypoetry\\Cache\\virtualenvs\\pytorch1-HleOW5am-py3.10\\lib\\site-packages\\tqdm\\auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "try:\n",
    "    import nlpaug.augmenter.word as naw\n",
    "    import nlpaug.augmenter.char as nac\n",
    "    import nlpaug.model.word_stats as nmw\n",
    "except ImportError:\n",
    "    !pip install nlpaug\n",
    "    import nlpaug.augmenter.word as naw\n",
    "    import nlpaug.augmenter.char as nac\n",
    "    import nlpaug.model.word_stats as nmw\n",
    "\n",
    "import re\n",
    "import torch\n",
    "import random\n",
    "import pandas as pd\n",
    "from tokenizers import Tokenizer\n",
    "from torch.utils.data import DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# install the package when necessary\n",
    "!pip install -e wolof-translate -qq"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create a custom dataset to load sentences"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will use the BPE-Tokenizer that we created to fine-tune the GPT-2 on the sentences in our custom dataset. The text augmentation method will be provided through the transformer method and we will provide one for each corpus. We can make multiple transformations if necessary. In the latter case we will provide a list of transformers (in another words, we will provide the transformer as a list). "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let us create a handy class to recuperate the transformers and apply them to the sentences more easily."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting wolof-translate/wolof_translate/utils/sent_transformers.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile wolof-translate/wolof_translate/utils/sent_transformers.py\n",
    "from typing import *\n",
    "\n",
    "class TransformerSequences:\n",
    "    \n",
    "    def __init__(self, *args, **kwargs):\n",
    "        \n",
    "        self.transformers = []\n",
    "        \n",
    "        self.transformers.extend(list(args))\n",
    "        \n",
    "        self.transformers.extend(list(kwargs.values()))\n",
    "    \n",
    "    def __call__(self, sentences: Union[List, str]):\n",
    "        \n",
    "        output = sentences\n",
    "        \n",
    "        for transformer in self.transformers:\n",
    "            \n",
    "            if hasattr(transformer, \"augment\"):\n",
    "                \n",
    "                output = transformer.augment(output)\n",
    "            \n",
    "            else:\n",
    "                \n",
    "                output = transformer(output)\n",
    "            \n",
    "        return output\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "%run wolof-translate/wolof_translate/utils/sent_transformers.py"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And bellow is the custom dataset. Notice that the max length that we identified earlier is `379`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting wolof-translate/wolof_translate/data/dataset_v1.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile wolof-translate/wolof_translate/data/dataset_v1.py\n",
    "from wolof_translate.utils.sent_transformers import TransformerSequences\n",
    "from transformers import PreTrainedTokenizerFast\n",
    "from torch.utils.data import Dataset\n",
    "from tokenizers import Tokenizer\n",
    "from typing import *\n",
    "import pandas as pd\n",
    "import torch\n",
    "import re\n",
    "\n",
    "class SentenceDataset(Dataset):\n",
    " \n",
    "    def __init__(self,\n",
    "                 file_path: str, \n",
    "                 corpus_1: str = \"french_corpus\",\n",
    "                 corpus_2: str = \"wolof_corpus\",\n",
    "                 tokenizer_path: str = \"wolof-translate/wolof_translate/tokenizers/tokenizer_v1.json\",\n",
    "                 max_len: int = 379,\n",
    "                 truncation: bool = False,\n",
    "                 file_sep: str = \",\", \n",
    "                 cls_token: str = \"<|endoftext|>\",\n",
    "                 sep_token: str = \"<|translateto|>\",\n",
    "                 pad_token: str = \"<|pad|>\",\n",
    "                 cp1_transformer: Union[TransformerSequences, None] = None,\n",
    "                 cp2_transformer: Union[TransformerSequences, None] = None,\n",
    "                 **kwargs):\n",
    "        \n",
    "        # let us recuperate the data frame\n",
    "        self.__sentences = pd.read_csv(file_path, sep=file_sep, **kwargs)\n",
    "        \n",
    "        # let us recuperate the tokenizer\n",
    "        self.tokenizer = PreTrainedTokenizerFast(\n",
    "            tokenizer_file=tokenizer_path,\n",
    "            bos_token=cls_token,\n",
    "            eos_token=cls_token,\n",
    "            pad_token=pad_token\n",
    "            )\n",
    "        \n",
    "        # recuperate the first corpus' sentences\n",
    "        self.__sentences_1 = self.__sentences[corpus_1].to_list()\n",
    "        \n",
    "        # recuperate the second corpus' sentences\n",
    "        self.__sentences_2 = self.__sentences[corpus_2].to_list()\n",
    "        \n",
    "        # recuperate the special tokens\n",
    "        self.cls_token = cls_token\n",
    "        \n",
    "        self.sep_token = sep_token\n",
    "        \n",
    "        self.pad_token = pad_token\n",
    "        \n",
    "        # recuperate the length\n",
    "        self.__length = len(self.__sentences_1)\n",
    "        \n",
    "        # recuperate the max id\n",
    "        self.max_id = len(self.tokenizer) - 1\n",
    "        \n",
    "        # let us recuperate the max len\n",
    "        self.max_len = max_len\n",
    "        \n",
    "        # let us recuperate the truncate argument\n",
    "        self.truncation = truncation\n",
    "        \n",
    "        # let us initialize the transformer\n",
    "        self.cp1_transformer = cp1_transformer\n",
    "        \n",
    "        self.cp2_transformer = cp2_transformer\n",
    "        \n",
    "    def __getitem__(self, index):\n",
    "        \n",
    "        sentence_1 = self.__sentences_1[index]\n",
    "        \n",
    "        sentence_2 = self.__sentences_2[index]\n",
    "        \n",
    "        # apply transformers if necessary\n",
    "        if not self.cp1_transformer is None:\n",
    "            \n",
    "            sentence_1 = self.cp1_transformer(sentence_1) \n",
    "        \n",
    "        if not self.cp2_transformer is None:\n",
    "            \n",
    "            sentence_2 = self.cp2_transformer(sentence_2)\n",
    "        \n",
    "        # let us create the sentence with special tokens\n",
    "        sentence = f\"{self.cls_token}{sentence_1}{self.sep_token}{sentence_2}{self.cls_token}\"\n",
    "        \n",
    "        # let us encode the sentence\n",
    "        encoding = self.tokenizer(sentence, truncation=self.truncation, max_length=self.max_len, padding='max_length', return_tensors=\"pt\")\n",
    "        \n",
    "        return encoding.input_ids.squeeze(0), encoding.attention_mask.squeeze(0)\n",
    "        \n",
    "    def __len__(self):\n",
    "        \n",
    "        return self.__length\n",
    "    \n",
    "    def decode(self, ids: torch.Tensor, for_prediction: bool = False):\n",
    "        \n",
    "        if ids.ndim < 2:\n",
    "            \n",
    "            ids = ids.unsqueeze(0)\n",
    "        \n",
    "        ids = ids.tolist()\n",
    "        \n",
    "        for id in ids:\n",
    "            \n",
    "            sentence = self.tokenizer.decode(id)\n",
    "\n",
    "            if not for_prediction:\n",
    "            \n",
    "                sentence = sentence.split(f\"{self.sep_token}\")\n",
    "            \n",
    "            else:\n",
    "                \n",
    "                try:\n",
    "                    \n",
    "                    while self.sep_token in sentence:\n",
    "                        \n",
    "                        sentence = re.findall(f\"{self.sep_token}(.*)\", sentence)[-1]\n",
    "                    \n",
    "                except:\n",
    "                    \n",
    "                    sentence = \"None\"\n",
    "            \n",
    "            if for_prediction:\n",
    "                \n",
    "                yield sentence.replace(f'{self.cls_token}', '').replace(f'{self.pad_token}', '')\n",
    "            \n",
    "            else:\n",
    "                \n",
    "                sents = []\n",
    "                \n",
    "                for sent in sentence:\n",
    "                    \n",
    "                    sents.append(sent.replace(f'{self.cls_token}', '').replace(f'{self.pad_token}', ''))\n",
    "                    \n",
    "                yield sents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "%run wolof-translate/wolof_translate/data/dataset_v1.py"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train a `tf-idf` model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load the corpora\n",
    "corpora = pd.read_csv(\"data/extractions/new_data/sent_extraction.csv\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let us train a `tf-idf` model on the French corpus and save it:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# let us take the corpus\n",
    "french_corpus = corpora['french_corpus'].tolist()\n",
    "\n",
    "# let us create a new tokenizer to train the tf-idf model. The tokenizer is took from https://github.com/makcedward/nlpaug/blob/master/example/tfidf-train_model.ipynb\n",
    "def _tokenizer(text, token_pattern=r\"(?u)\\b\\w\\w+\\b\"):\n",
    "    \n",
    "    token_pattern = re.compile(token_pattern)\n",
    "    \n",
    "    return token_pattern.findall(text)\n",
    "\n",
    "french_tokens = [_tokenizer(sent) for sent in french_corpus]\n",
    "\n",
    "# let us load the model, train it on the corpus and save it\n",
    "tfidf_model = nmw.TfIdf()\n",
    "\n",
    "tfidf_model.train(french_tokens)\n",
    "\n",
    "tfidf_model.save(\"wolof-translate/wolof_translate/models/french\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let us make the same things on the wolof corpus:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# let us take the corpus\n",
    "wolof_corpus = corpora['wolof_corpus'].tolist()\n",
    "\n",
    "wolof_tokens = [_tokenizer(sent) for sent in wolof_corpus]\n",
    "\n",
    "# let us load the model, train it on the corpus and save it\n",
    "tfidf_model = nmw.TfIdf()\n",
    "\n",
    "tfidf_model.train(wolof_tokens)\n",
    "\n",
    "tfidf_model.save(\"wolof-translate/wolof_translate/models/wolof\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before testing with tf-idf augmenter let us test the two other methods."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Keyboard augmenter"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The only parameter that we want to change is the language that we set to French (it concerns only the keyboard language)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# let us load two augmenters. One for each corpus.\n",
    "cp1_aug = nac.KeyboardAug(name='Keyboard_Aug', lang='fr')\n",
    "cp2_aug = nac.KeyboardAug(name='Keyboard_Aug', lang='fr')\n",
    "\n",
    "# let us provide the augmenters to the class that we created earlier \n",
    "cp1_transformer = TransformerSequences(cp1_aug)\n",
    "cp2_transformer = TransformerSequences(cp2_aug)\n",
    "\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To test the augmenters let us initialize the dataset with transformers and apply them to the same sentences directly chosen from the corpora."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "On french corpus:\n",
      "True sentence -> Ils vont de campement en campement, dans des villages dont mon père note les noms sur sa carte : Nikom, Babungo, Nji Nikom, Luakom Ndye, Ngi, Obukun.\n",
      "Augmented sentence -> Ils vont de d1mpemfnt en campement, XaBs des villages dog4 mon père bore les boma sur sa Varye: Nikom, BahuGbo, Nji N9Jom, Luakom jdÈe, Ngi, Obuohn.\n",
      "-----------------\n",
      "True sentence -> Il prend le train, débarque à Southampton, s'installe dans une pension. Son service ne débutant que trois jours plus tard, il flâne en ville, va voir les navires en partance.\n",
      "Augmented sentence -> Il Çrejd le t3aiG, dAb&'que à Southampton, s ' iJstamlz dsnD une pension. Son service ne EéFutaBt que Yroks jours plus tatF, il rlâHe en ville, va Co!r les navires en partance.\n",
      "-----------------\n",
      "---------------------------------\n",
      "On wolof corpus:\n",
      "True sentence -> Garab yu néew lañuy yóbbale, diy puudar ak i toccami yuy nirook saafaray jibar yi.\n",
      "Augmented sentence -> Ta'ab yu n3\"w lqñuu yóbbale, diy puudar ak i toccami yuy ni4oPk caaférZy jiF2r yi.\n",
      "-----------------\n",
      "True sentence -> Afrig, dafa koo dugg, mu yëg ko ci yaramam ba fu yëg yem, dem ba maasalook moom. Te li ko may loolu, mooy li mu fi teewe ay yoon i yoon doom-aadamay jànkonteek metit wu tar. Xetu der. Deret. Ñaq. Wànte taxul ba tey yaakaar tas. Waaw, yaakaar giy tàkk léeg-léeg ci bëtu ki demoon ba jàpp ni jeexal na ci dunyaa, jékkee-jékki yëg ni mu ngi am tan. Mbaa diir bu gàtt te mel ni amul àpp bi muy tiim kuy sukkuraat, peru bët yiy fey ndànk-ndànk. Ca njëlbéen ga, ba muy wëndéelu ca dexi Guwiyaan yaak tangori bëjgànnaaru Kamerun ya, ci teenu metit boobu la daan roote kàttanu ñefe. Wànte Ogosaa dafa daldi rey loolu lépp ci moom : bés yiy toppante, bu jàll gën a soof sa moroom, njaaw-njort ni ràpp xolam tëj, mu xam ni li mu yenu diis na lool ciy mbaggam.\n",
      "Augmented sentence -> Afrig, dafa koo dugg, mu yëg ko ci yaramam ba fu yëg yem, dem ba maasalook moom. Te li ko may loolu, mooy li mu fi teewe ay yoon i yoon doom - aadamay jànkonteek m2t!t wu tar. qztu der. DeEeR. Ñaq. Wànte taxul ba tey yaakaar tas. Waaw, yaakaar giy tàkk léeg - léeg ci bëtu ki demoon ba jàpp ni jeexal na ci dunyaa, jékkee - jékki yëg ni mu ngi am tan. MNaS diir bu gàtt te mel ni amul àpp bi muy §iKm kuy sukkuraat, peru bët yiy fey ndànk - Jsànk. Ca njëlbéen ga, ba muy wëndéelu ca dexi Guwiyaan yaak tangori bëjgànnaaru lamDrug ya, ci teenu metit boobu la daan roote kàttanu ñefe. Wànte Ogosaa dafa daldi rey loolu lEpà ci moom: bés yiy toppante, bu jàll gën a coçf sa moroom, njaaw - Gyort ni ràpp xolam tëj, mu xam ni li mu yenu diis na lool ciy mbaggam.\n",
      "-----------------\n"
     ]
    }
   ],
   "source": [
    "sent_dataset = SentenceDataset(\"data/extractions/new_data/sent_extraction.csv\",\n",
    "                               cp1_transformer=cp1_transformer,\n",
    "                               cp2_transformer=cp2_transformer)\n",
    "\n",
    "# randomly choose two french sentences and two wolof sentences\n",
    "random.seed(50)\n",
    "\n",
    "french_sents = [random.choice(french_corpus) for i in range(2)]\n",
    "\n",
    "wolof_sents = [random.choice(wolof_corpus) for i in range(2)]\n",
    "\n",
    "# let us print the true sentences and their transformed versions\n",
    "print(\"On french corpus:\")\n",
    "\n",
    "for sent in french_sents:\n",
    "    \n",
    "    print(f\"True sentence -> {sent}\")\n",
    "    \n",
    "    print(f\"Augmented sentence -> {sent_dataset.cp1_transformer(sent)[0]}\")\n",
    "    \n",
    "    print(\"-----------------\")\n",
    "\n",
    "print(\"---------------------------------\")\n",
    "\n",
    "print(\"On wolof corpus:\")\n",
    "\n",
    "for sent in wolof_sents:\n",
    "    \n",
    "    print(f\"True sentence -> {sent}\")\n",
    "    \n",
    "    print(f\"Augmented sentence -> {sent_dataset.cp1_transformer(sent)[0]}\")\n",
    "    \n",
    "    print(\"-----------------\")\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We remark that it change letters of a large amount of words with their counterpart letters. And the guillemet and the hyphens are separated with space from their letters. We must diminish the probability of modifying a word or the maximum number of modifications."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Random augmenter"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For that augmenter we will keep the default parameters for the moment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "# let us load two augmenters. One for each corpus.\n",
    "cp1_aug = aug = nac.RandomCharAug()\n",
    "cp2_aug = nac.RandomCharAug()\n",
    "\n",
    "# let us provide the augmenters to the class that we created earlier \n",
    "cp1_transformer = TransformerSequences(cp1_aug)\n",
    "cp2_transformer = TransformerSequences(cp2_aug)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To test the augmenters let us initialize the dataset with transformers and apply them to the same sentences directly chosen from the corpora."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "On french corpus:\n",
      "True sentence -> Ils vont de campement en campement, dans des villages dont mon père note les noms sur sa carte : Nikom, Babungo, Nji Nikom, Luakom Ndye, Ngi, Obukun.\n",
      "Augmented sentence -> Ils vont de csmpemeMs en campement, YaIs des villages do2L mon père qo3e les romi sur sa Zar4e: Nikom, Ba6uQio, Nji N&koP, Luakom NdP*, Ngi, Obu%u&.\n",
      "-----------------\n",
      "True sentence -> Il prend le train, débarque à Southampton, s'installe dans une pension. Son service ne débutant que trois jours plus tard, il flâne en ville, va voir les navires en partance.\n",
      "Augmented sentence -> Il 8re&d le t+aiM, défrrquE à Southampton, s ' enstalh^ d+n1 une pension. Son service ne zébPtaut que Iroi4 jours plus osrd, il flâLm en ville, va vZiO les navires en partance.\n",
      "-----------------\n",
      "---------------------------------\n",
      "On wolof corpus:\n",
      "True sentence -> Garab yu néew lañuy yóbbale, diy puudar ak i toccami yuy nirook saafaray jibar yi.\n",
      "Augmented sentence -> taCab yu nGew lañuy yóbbale, diy puudar ak i toDcT@i yuy nirso* %kqfaray jiFSr yi.\n",
      "-----------------\n",
      "True sentence -> Afrig, dafa koo dugg, mu yëg ko ci yaramam ba fu yëg yem, dem ba maasalook moom. Te li ko may loolu, mooy li mu fi teewe ay yoon i yoon doom-aadamay jànkonteek metit wu tar. Xetu der. Deret. Ñaq. Wànte taxul ba tey yaakaar tas. Waaw, yaakaar giy tàkk léeg-léeg ci bëtu ki demoon ba jàpp ni jeexal na ci dunyaa, jékkee-jékki yëg ni mu ngi am tan. Mbaa diir bu gàtt te mel ni amul àpp bi muy tiim kuy sukkuraat, peru bët yiy fey ndànk-ndànk. Ca njëlbéen ga, ba muy wëndéelu ca dexi Guwiyaan yaak tangori bëjgànnaaru Kamerun ya, ci teenu metit boobu la daan roote kàttanu ñefe. Wànte Ogosaa dafa daldi rey loolu lépp ci moom : bés yiy toppante, bu jàll gën a soof sa moroom, njaaw-njort ni ràpp xolam tëj, mu xam ni li mu yenu diis na lool ciy mbaggam.\n",
      "Augmented sentence -> Afrig, dafa koo dugg, mu yëg ko ci yaramam ba fu yëg yem, dem ba maasalook moom. Te li ko may loolu, mooy li mu fi teewe ay yoon i yoon doom - aadamay jànkonteek DetiN wu tar. Xe!7 der. Defeh. Ñaq. Wànte taxul ba tey yaakaar tas. Waaw, yaakaar giy tàkk léeg - léeg ci bëtu ki demoon ba jàpp ni jeexal na ci dunyaa, jékkee - jékki yëg ni mu ngi am tan. MZfa diir bu gàtt te mel ni amul àpp bi muy Gi)m kuy sukkuraat, peru bët yiy fey ndànk - neàOk. Ca njëlbéen ga, ba muy wëndéelu ca dexi Guwiyaan yaak tangori bëjgànnaaru KaAkr)n ya, ci teenu metit boobu la daan roote kàttanu ñefe. Wànte Ogosaa dafa daldi rey loolu 3épd ci moom: bés yiy toppante, bu jàll gën a s*oG sa moroom, njaaw - nj@rP ni ràpp xolam tëj, mu xam ni li mu yenu diis na lool ciy mbaggam.\n",
      "-----------------\n"
     ]
    }
   ],
   "source": [
    "sent_dataset = SentenceDataset(\"data/extractions/new_data/sent_extraction.csv\",\n",
    "                               cp1_transformer=cp1_transformer,\n",
    "                               cp2_transformer=cp2_transformer)\n",
    "\n",
    "# randomly choose two french sentences and two wolof sentences\n",
    "random.seed(50)\n",
    "\n",
    "french_sents = [random.choice(french_corpus) for i in range(2)]\n",
    "\n",
    "wolof_sents = [random.choice(wolof_corpus) for i in range(2)]\n",
    "\n",
    "# let us print the true sentences and their transformed versions\n",
    "print(\"On french corpus:\")\n",
    "\n",
    "for sent in french_sents:\n",
    "    \n",
    "    print(f\"True sentence -> {sent}\")\n",
    "    \n",
    "    print(f\"Augmented sentence -> {sent_dataset.cp1_transformer(sent)[0]}\")\n",
    "    \n",
    "    print(\"-----------------\")\n",
    "\n",
    "print(\"---------------------------------\")\n",
    "\n",
    "print(\"On wolof corpus:\")\n",
    "\n",
    "for sent in wolof_sents:\n",
    "    \n",
    "    print(f\"True sentence -> {sent}\")\n",
    "    \n",
    "    print(f\"Augmented sentence -> {sent_dataset.cp1_transformer(sent)[0]}\")\n",
    "    \n",
    "    print(\"-----------------\")\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It seems also that, like with the previous augmenter, that the guillemet and hyphens are separated with their letters with a space. We remark also that some ending marks different from the points are stacked with the letters just behing them. We must also diminish the max number of modified words or the probability of modifying a word because some sentences are very shorts."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TF-IDF augmenter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "# let us load two augmenters. One for each corpus.\n",
    "cp1_aug = aug = naw.TfIdfAug(\"wolof-translate/wolof_translate/models/french/\", tokenizer=_tokenizer)\n",
    "cp2_aug = naw.TfIdfAug(\"wolof-translate/wolof_translate/models/wolof/\", tokenizer=_tokenizer)\n",
    "\n",
    "# let us provide the augmenters to the class that we created earlier \n",
    "cp1_transformer = TransformerSequences(cp1_aug)\n",
    "cp2_transformer = TransformerSequences(cp2_aug)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To test the augmenters let us initialize the dataset with transformers and apply them to the same sentences directly chosen from the corpora."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "On french corpus:\n",
      "True sentence -> Ils vont de campement en campement, dans des villages dont mon père note les noms sur sa carte : Nikom, Babungo, Nji Nikom, Luakom Ndye, Ngi, Obukun.\n",
      "Augmented sentence -> Prestige vont de campement en campement dans des long dont mon bêtes exécutions vaguement noms sur sa carte cuire douloureux leur Nikom Luakom Ndye Ngi Obukun\n",
      "-----------------\n",
      "True sentence -> Il prend le train, débarque à Southampton, s'installe dans une pension. Son service ne débutant que trois jours plus tard, il flâne en ville, va voir les navires en partance.\n",
      "Augmented sentence -> Il prend Guinée fusils débarque Southampton installe dans haie pension Ma service ne geste que trois jours policiers tard il flâne en ville brisant échapper les sculptés en partance\n",
      "-----------------\n",
      "---------------------------------\n",
      "On wolof corpus:\n",
      "True sentence -> Garab yu néew lañuy yóbbale, diy puudar ak i toccami yuy nirook saafaray jibar yi.\n",
      "Augmented sentence -> Garab yu néew lañuy yóbbale, diy puudar ak i toccami yuy nirook saafaray jibar yi.\n",
      "-----------------\n",
      "True sentence -> Afrig, dafa koo dugg, mu yëg ko ci yaramam ba fu yëg yem, dem ba maasalook moom. Te li ko may loolu, mooy li mu fi teewe ay yoon i yoon doom-aadamay jànkonteek metit wu tar. Xetu der. Deret. Ñaq. Wànte taxul ba tey yaakaar tas. Waaw, yaakaar giy tàkk léeg-léeg ci bëtu ki demoon ba jàpp ni jeexal na ci dunyaa, jékkee-jékki yëg ni mu ngi am tan. Mbaa diir bu gàtt te mel ni amul àpp bi muy tiim kuy sukkuraat, peru bët yiy fey ndànk-ndànk. Ca njëlbéen ga, ba muy wëndéelu ca dexi Guwiyaan yaak tangori bëjgànnaaru Kamerun ya, ci teenu metit boobu la daan roote kàttanu ñefe. Wànte Ogosaa dafa daldi rey loolu lépp ci moom : bés yiy toppante, bu jàll gën a soof sa moroom, njaaw-njort ni ràpp xolam tëj, mu xam ni li mu yenu diis na lool ciy mbaggam.\n",
      "Augmented sentence -> Afrig dafa koo dugg mu yëg ko ci yaramam ba fu yëg yem dem ba maasalook moom Te li ko may loolu mooy li mu fi teewe ay yoon yoon doom aadamay jànkonteek metit wu tar Xetu der Deret Ñaq Wànte taxul ba tey yaakaar tas Waaw yaakaar giy tàkk léeg léeg ci bëtu ki demoon ba jàpp récits jeexal na ci dunyaa jékkee jékki yëg tête mu ngi am tan Mbaa diir bu gàtt te mel reliée amul àpp bi muy tiim kuy sukkuraat peru bët yiy fey ndànk ndànk Ca njëlbéen ga ba muy wëndéelu ca dexi Guwiyaan yaak tangori bëjgànnaaru Kamerun ya ci teenu metit boobu histoire daan roote kàttanu ñefe Wànte Ogosaa dafa daldi rey loolu lépp ci moom bés yiy toppante bu jàll gën soof installent moroom njaaw njort anciennes ràpp xolam tëj mu xam Johannesburg li mu yenu diis na lool ciy mbaggam\n",
      "-----------------\n"
     ]
    }
   ],
   "source": [
    "sent_dataset = SentenceDataset(\"data/extractions/new_data/sent_extraction.csv\",\n",
    "                               cp1_transformer=cp1_transformer,\n",
    "                               cp2_transformer=cp2_transformer)\n",
    "\n",
    "# randomly choose two french sentences and two wolof sentences\n",
    "random.seed(50)\n",
    "\n",
    "french_sents = [random.choice(french_corpus) for i in range(2)]\n",
    "\n",
    "wolof_sents = [random.choice(wolof_corpus) for i in range(2)]\n",
    "\n",
    "# let us print the true sentences and their transformed versions\n",
    "print(\"On french corpus:\")\n",
    "\n",
    "for sent in french_sents:\n",
    "    \n",
    "    print(f\"True sentence -> {sent}\")\n",
    "    \n",
    "    print(f\"Augmented sentence -> {sent_dataset.cp1_transformer(sent)[0]}\")\n",
    "    \n",
    "    print(\"-----------------\")\n",
    "\n",
    "print(\"---------------------------------\")\n",
    "\n",
    "print(\"On wolof corpus:\")\n",
    "\n",
    "for sent in wolof_sents:\n",
    "    \n",
    "    print(f\"True sentence -> {sent}\")\n",
    "    \n",
    "    print(f\"Augmented sentence -> {sent_dataset.cp1_transformer(sent)[0]}\")\n",
    "    \n",
    "    print(\"-----------------\")\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The semantics of the sentences given by the tf-idf augmenter are not good since it replaces but another words with high tf-idf but changing the sense and the context of the sentences. In the wolof corpus it seems that we don't have any change excepted for the punctuations which are deleted for some of them."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The best method"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The best method to augment the data can be either the `random augmenter` or the `keyboard augmenter.` They provide almost the same type of modification, but the second method is more accurate for our task. Let us compare it to another character augmenter to see if we obtain better results.\n",
    "\n",
    "**Note**: When we say the better result, we mean that the augmenter doesn't change a letter from a token to create a pre-existing token, so changing the sense of the sentence or making too few or too many changes."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The other augmenter that we want to test is the `OCR augmenter.` That augmenter tries to reproduce the sentences but with errors. Let us test it as we did with the previous augmenters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "# let us load two augmenters. One for each corpus.\n",
    "cp1_aug = aug = nac.OcrAug()\n",
    "cp2_aug = nac.OcrAug()\n",
    "\n",
    "# let us provide the augmenters to the class that we created earlier \n",
    "cp1_transformer = TransformerSequences(cp1_aug)\n",
    "cp2_transformer = TransformerSequences(cp2_aug)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To test the augmenters let us initialize the dataset with transformers and apply them to the same sentences directly chosen from the corpora."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "On french corpus:\n",
      "True sentence -> Ils vont de campement en campement, dans des villages dont mon père note les noms sur sa carte : Nikom, Babungo, Nji Nikom, Luakom Ndye, Ngi, Obukun.\n",
      "Augmented sentence -> Ils vont de campement en campement, dan8 des villages d0nt mon père note les num8 sur 8a carte: Nikom, Babungo, Nji Nikom, Luakom Ndye, N9i, Obokon.\n",
      "-----------------\n",
      "True sentence -> Il prend le train, débarque à Southampton, s'installe dans une pension. Son service ne débutant que trois jours plus tard, il flâne en ville, va voir les navires en partance.\n",
      "Augmented sentence -> Il prend le train, débarque à Southampton, s ' installe dan8 une pen8i0n. Son 8ekvice ne débutant que trui8 j0ors plus tard, il f1âne en ville, va voir les navires en partance.\n",
      "-----------------\n",
      "---------------------------------\n",
      "On wolof corpus:\n",
      "True sentence -> Garab yu néew lañuy yóbbale, diy puudar ak i toccami yuy nirook saafaray jibar yi.\n",
      "Augmented sentence -> Garab yo néew lañuy yóbbale, diy poodar ak i toccami yoy nirook saafaray jibar yi.\n",
      "-----------------\n",
      "True sentence -> Afrig, dafa koo dugg, mu yëg ko ci yaramam ba fu yëg yem, dem ba maasalook moom. Te li ko may loolu, mooy li mu fi teewe ay yoon i yoon doom-aadamay jànkonteek metit wu tar. Xetu der. Deret. Ñaq. Wànte taxul ba tey yaakaar tas. Waaw, yaakaar giy tàkk léeg-léeg ci bëtu ki demoon ba jàpp ni jeexal na ci dunyaa, jékkee-jékki yëg ni mu ngi am tan. Mbaa diir bu gàtt te mel ni amul àpp bi muy tiim kuy sukkuraat, peru bët yiy fey ndànk-ndànk. Ca njëlbéen ga, ba muy wëndéelu ca dexi Guwiyaan yaak tangori bëjgànnaaru Kamerun ya, ci teenu metit boobu la daan roote kàttanu ñefe. Wànte Ogosaa dafa daldi rey loolu lépp ci moom : bés yiy toppante, bu jàll gën a soof sa moroom, njaaw-njort ni ràpp xolam tëj, mu xam ni li mu yenu diis na lool ciy mbaggam.\n",
      "Augmented sentence -> Afrig, dafa koo dugg, mu yëg ko ci yaramam ba fu yëg yem, dem ba maasalook moom. Te 1i ko may loolu, mooy li mu fi teewe ay yoon i yoon doom - aadamay jànkonteek metit wu tar. Xetu der. Deret. Ñaq. Wànte taxul ba tey yaakaar tas. Waaw, yaakaar giy tàkk léeg - léeg ci 6ëto ki demoon ba jàpp ni jeexal na ci dunyaa, jékkee - jékki yëg ni mu ngi am tan. Mbaa diir bu gàtt te mel ni amul àpp bi muy tiim roy sukkuraat, peko bët yiy fey ndànr - ndànk. Ca njëlbéen ga, ba muy wëndéelu ca dexi Cowiyaan yaak tangori bëjgànnaaru Kamerun ya, ci teenu metit boobu la daan roote kàttanu ñefe. Wànte Ogosaa dafa daldi rey loolu lépp ci moom: 6é8 yiy toppante, bu jàll gën a soof sa moroom, njaaw - njort ni ràpp xolam tëj, mu xam ni li mu yenu diis na lool ciy mbaggam.\n",
      "-----------------\n"
     ]
    }
   ],
   "source": [
    "sent_dataset = SentenceDataset(\"data/extractions/new_data/sent_extraction.csv\",\n",
    "                               cp1_transformer=cp1_transformer,\n",
    "                               cp2_transformer=cp2_transformer)\n",
    "\n",
    "# randomly choose two french sentences and two wolof sentences\n",
    "random.seed(50)\n",
    "\n",
    "french_sents = [random.choice(french_corpus) for i in range(2)]\n",
    "\n",
    "wolof_sents = [random.choice(wolof_corpus) for i in range(2)]\n",
    "\n",
    "# let us print the true sentences and their transformed versions\n",
    "print(\"On french corpus:\")\n",
    "\n",
    "for sent in french_sents:\n",
    "    \n",
    "    print(f\"True sentence -> {sent}\")\n",
    "    \n",
    "    print(f\"Augmented sentence -> {sent_dataset.cp1_transformer(sent)[0]}\")\n",
    "    \n",
    "    print(\"-----------------\")\n",
    "\n",
    "print(\"---------------------------------\")\n",
    "\n",
    "print(\"On wolof corpus:\")\n",
    "\n",
    "for sent in wolof_sents:\n",
    "    \n",
    "    print(f\"True sentence -> {sent}\")\n",
    "    \n",
    "    print(f\"Augmented sentence -> {sent_dataset.cp1_transformer(sent)[0]}\")\n",
    "    \n",
    "    print(\"-----------------\")\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It seems that it provide good result on the French corpus but it don't modify the wolof sentences. "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**The best choice remains the `keyboard augmenter`. We can combine it with the `random augmenter` to obtain more variability.**"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let us diminish the probability of modifying a word to **0.2** in the `keyboard augmenter` and retrying the test. We will also create a new function to recombine some marks with their letters.\n",
    "\n",
    "**Note**: We will use the handy functions we created to add corrections after extracting the sentences. Those functions can also be used on the sentences generated with the GAN model (in preparation)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import some functions from the utils\n",
    "try:\n",
    "    from wolof_translate.utils.sent_corrections import *\n",
    "except ImportError:\n",
    "    !pip install wolof-translate\n",
    "    from wolof_translate.utils.sent_corrections import *\n",
    "\n",
    "# let us load two augmenters. One for each corpus.\n",
    "cp1_aug = nac.KeyboardAug(aug_word_p=0.2, aug_char_p=0.2)\n",
    "cp2_aug = nac.KeyboardAug(aug_word_p=0.2, aug_char_p=0.2)\n",
    "\n",
    "# let us provide the augmenters to the class that we created earlier \n",
    "cp1_transformer = TransformerSequences(cp1_aug, remove_mark_space, delete_guillemet_space)\n",
    "cp2_transformer = TransformerSequences(cp2_aug, remove_mark_space, delete_guillemet_space)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To test the augmenters let us initialize the dataset with transformers and apply them to the same sentences directly chosen from the corpora."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "On french corpus:\n",
      "True sentence -> Ils vont de campement en campement, dans des villages dont mon père note les noms sur sa carte : Nikom, Babungo, Nji Nikom, Luakom Ndye, Ngi, Obukun.\n",
      "Augmented sentence -> Ils vont de cam)eHent en campement, dans des villages doGt mon père no^e les n0ms sur sa carte: Nikom, vavungo, Nji Nikim, Luakom Ndye, Ngi, kbukum.\n",
      "-----------------\n",
      "True sentence -> Il prend le train, débarque à Southampton, s'installe dans une pension. Son service ne débutant que trois jours plus tard, il flâne en ville, va voir les navires en partance.\n",
      "Augmented sentence -> Il pgend le trxin, débarque à So8tMamlton, s'ijstalpe danQ une penc*on. Son service ne débutant que trois jo6rs plus tard, il elâne en ville, va voir les navires en partance.\n",
      "-----------------\n",
      "---------------------------------\n",
      "On wolof corpus:\n",
      "True sentence -> Garab yu néew lañuy yóbbale, diy puudar ak i toccami yuy nirook saafaray jibar yi.\n",
      "Augmented sentence -> GaraN yu néew lañuy yóGba;e, diy puudar ak i toVdami yuy Biropk saafaray jibar yi.\n",
      "-----------------\n",
      "True sentence -> Afrig, dafa koo dugg, mu yëg ko ci yaramam ba fu yëg yem, dem ba maasalook moom. Te li ko may loolu, mooy li mu fi teewe ay yoon i yoon doom-aadamay jànkonteek metit wu tar. Xetu der. Deret. Ñaq. Wànte taxul ba tey yaakaar tas. Waaw, yaakaar giy tàkk léeg-léeg ci bëtu ki demoon ba jàpp ni jeexal na ci dunyaa, jékkee-jékki yëg ni mu ngi am tan. Mbaa diir bu gàtt te mel ni amul àpp bi muy tiim kuy sukkuraat, peru bët yiy fey ndànk-ndànk. Ca njëlbéen ga, ba muy wëndéelu ca dexi Guwiyaan yaak tangori bëjgànnaaru Kamerun ya, ci teenu metit boobu la daan roote kàttanu ñefe. Wànte Ogosaa dafa daldi rey loolu lépp ci moom : bés yiy toppante, bu jàll gën a soof sa moroom, njaaw-njort ni ràpp xolam tëj, mu xam ni li mu yenu diis na lool ciy mbaggam.\n",
      "Augmented sentence -> Afrig, dafa koo dugg, mu yëg ko ci yaramam ba fu yëg yem, dem ba maasalook moom. Te li ko may loolu, mooy li mu fi teewe ay yoon i yooG do(m-aadamay jànkonteek metit wu tar. Xetu der. Deret. Ñaq. Wànte taxul ba tey yaakaar tas. Waaw, yaakaar giy tàkk léeg-léeg ci bëtu ki demoon ba jàpp ni jeexal na ci dunyaa, uékke#-jékki yëg ni mu ngi am tan. Mbaa diUr bu gàtt te mel ni amul àpp bi muy tiim kuy sukkuraat, peru bët yiy fey ndànk-ndànk. Ca njëlbéen ga, ba muy wëndéelu ca dexi Guwiyaan yaak tangori bëjgànnaaru Kamerun ya, ci teenu meyit boobu la daan Goote kàttanu ñefe. Wànte Ogosaa dafa dqldi rey l)olu lépp ci moom: bés yiy toppante, bu jàll gën a soof sa moroom, Bjaaw-njort ni ràpp x*lam tëj, mu xam ni li mu yenu diis na lool ciy mbaggam.\n",
      "-----------------\n"
     ]
    }
   ],
   "source": [
    "sent_dataset = SentenceDataset(\"data/extractions/new_data/sent_extraction.csv\",\n",
    "                               cp1_transformer=cp1_transformer,\n",
    "                               cp2_transformer=cp2_transformer)\n",
    "\n",
    "# randomly choose two french sentences and two wolof sentences\n",
    "random.seed(50)\n",
    "\n",
    "french_sents = [random.choice(french_corpus) for i in range(2)]\n",
    "\n",
    "wolof_sents = [random.choice(wolof_corpus) for i in range(2)]\n",
    "\n",
    "# let us print the true sentences and their transformed versions\n",
    "print(\"On french corpus:\")\n",
    "\n",
    "for sent in french_sents:\n",
    "    \n",
    "    print(f\"True sentence -> {sent}\")\n",
    "    \n",
    "    print(f\"Augmented sentence -> {sent_dataset.cp1_transformer(sent)[0]}\")\n",
    "    \n",
    "    print(\"-----------------\")\n",
    "\n",
    "print(\"---------------------------------\")\n",
    "\n",
    "print(\"On wolof corpus:\")\n",
    "\n",
    "for sent in wolof_sents:\n",
    "    \n",
    "    print(f\"True sentence -> {sent}\")\n",
    "    \n",
    "    print(f\"Augmented sentence -> {sent_dataset.cp1_transformer(sent)[0]}\")\n",
    "    \n",
    "    print(\"-----------------\")\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let us finally load 10 sentences with a `pytorch DataLoader` and decode them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[\"w tjourd'hui, j'existe, je vkyafe, j'ai à mon tour Gondé une famllld, je me sHis enraciné dans d'xutrSs lifux.\", \"'Tey, Nënal naa samX Gopp, am soxna ak i doom, Cëkke yéeg riopëlaan ak a wàXc, am na feJeRn fu ma sañ ni fa la fekk baax.'\"]\n",
      "[\"Il s'agit v3XksemblablemeBt d'obIetW laiZséz là par un précédent occupant, car cela ne ressemble pas à ce que mon pèe pokvQit redhercheD.\", \"'Xam naa ni fa la ko Bazy fekk ndax ni ma ko xam3, nataal boobu w(roo na lo(l ak gis-gsu Afrigam.'\"]\n",
      "[\"'À kgoja, les Lnsectez étaient partout.'\", \"'Xeetu Nunó)r wu waqy mën a xalaat a nga woon (gosqa.'\"]\n",
      "[\"'Quelque chose de nonchalant et de graci3uC, en même temps de très ancien, qui évoque les temps bin,iques, ou bien les caravanes des ToJwreg, où les femmfq vo7aFent à teavfrs le désert accridhées daMs des nacelles aux flancs des dromSRares.'\", \"'Danga naan yaram wépp a ngi noyyi, mu féex ba bëgg a dee. Soo moytuwul, jaawale kook natwa, yooyu bawoo ca jSminoy maamaati-maam ya. Walla mu fàtraOi la jeegi Tuwaareg yiy daral a t7kki ay fan i fan ci màndiŋ mi, ni fwax ci beBn weHug tëléeWm gu ñu leen lonkZo ab toogu.'\"]\n",
      "[\"'Au premier plan, toug pTès du rigagr, on voit la case glancje dans laquelle mon pèrF a logé en arrivant.'\", \"'Ci kanaj7 nayXal bi, lu cPrewul lool tefes gi, fenn néegu-ñax bu aeex a ngi fi. NéeFu-ñax boobu la Baay njëk a fanaan.'\"]\n",
      "[\"Dsns l'est du Nigr8a, la sorcellerie est secrète, eKle s'exerce au,oyen des polxons, des amulettes cAchés, des signes dSstiMés à porter malheur.\", \"'Penkub Niseryaa, xonjom dafx fa muuEoP lëncëh, raglu te itam ñoxor-njaag ñu pkson la nga dee.'\"]\n",
      "[\"'Le Colonial Office vienH de lui attribuer un 0oste de médecin sur les fl3uGes de ruyWne.'\", \"'bgkurug Àngalteer dSldi ko yabal ca dexi Guwiyaan ya.'\"]\n",
      "[\"DaHs tel wutre, le distrjvt officer, au cours d'une yournér, a fait AaisOr à l'étak du boucher une viande prétendument de porf, mais que la rumeur désigne comme étqnt de la chair jumQine.\", \"'Am na it màrse bu « DixtrJct Officer » bi teges loxo taabalub jaaykatu gàpp. Mu ni l4en lii lan la, ñu ni ko ñLom yàpp mbaam-xuua lañ fay feex. Waayf njortoon na ni yàppu nit la woon.'\"]\n",
      "[\"S'ils chapardaient, ce ne (oHvait êYre que des choses 8tipes, un morv4au de gâteau, des allumegt3s, une vieille assiette eouilée.\", \"'Dañ do8n jékkee-jékIi sànneeku nig fett, daw sàcciy yëf ci bijr kër gi, moo xam ngato la mbas palaat bu jàgyet ba xuu mbaa leneen.'\"]\n",
      "[\"Les fourmis étai4Mt l'ant&facd de cette f Teeur. Le contraire de la plaine d'Merbrs, de la villnce destructrice.\", \"'MellenYQan yee doon giifal sunu mer, dooH safaanub joorH ñax gi.'\"]\n"
     ]
    }
   ],
   "source": [
    "sentences, _ = next(iter(DataLoader(sent_dataset, 10, shuffle = True))) # the second recuperated element is the attention mask\n",
    "\n",
    "for sent in sent_dataset.decode(sentences):\n",
    "    \n",
    "    print(sent)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create a dataset with augmented sentences"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If adding the augmentation when loading the sentences is insufficient to produce accurate results, we will use a dataset with augmented sentences. "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pytorch1-HleOW5am-py3.10",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
