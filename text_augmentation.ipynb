{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Text augmentation with `nlpaug`\n",
    "-------------------"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It is a library called `nlpaug` that we can use to transform the sentences in order to augment the vocabulary size. The tutorial is available at medium through the following link [nlpaug_examples](https://towardsdatascience.com/text-augmentation-in-few-lines-of-python-code-cdd10cf3cf84)."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will test the following methods:\n",
    "\n",
    "- `KeyboarAug`: It change a random character with another one close with it on the keyboard.\n",
    "- `RandomAug`: It modify a character with a random another one character.\n",
    "\n",
    "Most of the another methods require a model to work correctly. We can still test the following methods:\n",
    "- `TfidfAug`: Use TF-IDF to find out how words would be augmented. We will need to train a tf-idf on the tokens of each corpus before using it."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Steps"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will follow the next steps:\n",
    "\n",
    "- Create a custom dataset getting as argument a sentence transformer\n",
    "- Training a tf-idf on each corpus using the `nlpaug` library\n",
    "- Using the `KeyboardAug` method to augment the data (we will need to find the best parameters)\n",
    "- Using the `RandomAug` method to augment the data (we will also need to find the best parameters)\n",
    "- Using the `TfidfAug` method to augment the data (it can be the best approach)\n",
    "- Choose the best method and try to find the ideal parameters.\n",
    "- Create a new data frame containing augmented sentences"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We must import the following libraries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    import nlpaug.augmenter.word as naw\n",
    "    import nlpaug.augmenter.char as nac\n",
    "    import nlpaug.model.word_stats as nmw\n",
    "except ImportError:\n",
    "    !pip install nlpaug\n",
    "    import nlpaug.augmenter.word as naw\n",
    "    import nlpaug.augmenter.char as nac\n",
    "    import nlpaug.model.word_stats as nmw\n",
    "\n",
    "import re\n",
    "import torch\n",
    "import random\n",
    "import pandas as pd\n",
    "from tokenizers import Tokenizer\n",
    "from torch.utils.data import DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# install the package when necessary\n",
    "!pip install -e wolof-translate -qq"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create a custom dataset to load sentences"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will use the BPE-Tokenizer that we created to fine-tune the GPT-2 on the sentences in our custom dataset. The text augmentation method will be provided through the transformer method and we will provide one for each corpus. We can make multiple transformations if necessary. In the latter case we will provide a list of transformers (in another words, we will provide the transformer as a list). "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let us create a handy class to recuperate the transformers and apply them to the sentences more easily."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting wolof-translate/wolof_translate/utils/sent_transformers.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile wolof-translate/wolof_translate/utils/sent_transformers.py\n",
    "from typing import *\n",
    "\n",
    "class TransformerSequences:\n",
    "    \n",
    "    def __init__(self, *args, **kwargs):\n",
    "        \n",
    "        self.transformers = []\n",
    "        \n",
    "        self.transformers.extend(list(args))\n",
    "        \n",
    "        self.transformers.extend(list(kwargs.values()))\n",
    "    \n",
    "    def __call__(self, sentences: Union[List, str]):\n",
    "        \n",
    "        output = sentences\n",
    "        \n",
    "        for transformer in self.transformers:\n",
    "            \n",
    "            if hasattr(transformer, \"augment\"):\n",
    "                \n",
    "                output = transformer.augment(output)\n",
    "            \n",
    "            else:\n",
    "                \n",
    "                output = transformer(output)\n",
    "            \n",
    "        return output\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "%run wolof-translate/wolof_translate/utils/sent_transformers.py"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And bellow is the custom dataset. Notice that the max length that we identified earlier is `379`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting wolof-translate/wolof_translate/data/dataset_v1.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile wolof-translate/wolof_translate/data/dataset_v1.py\n",
    "from wolof_translate.utils.sent_transformers import TransformerSequences\n",
    "from transformers import PreTrainedTokenizerFast\n",
    "from torch.utils.data import Dataset\n",
    "from tokenizers import Tokenizer\n",
    "from typing import *\n",
    "import pandas as pd\n",
    "import torch\n",
    "import re\n",
    "\n",
    "class SentenceDataset(Dataset):\n",
    " \n",
    "    def __init__(self,\n",
    "                 file_path: str, \n",
    "                 corpus_1: str = \"french_corpus\",\n",
    "                 corpus_2: str = \"wolof_corpus\",\n",
    "                 tokenizer_path: str = \"wolof-translate/wolof_translate/tokenizers/tokenizer_v1.json\",\n",
    "                 max_len: int = 379,\n",
    "                 truncation: bool = False,\n",
    "                 file_sep: str = \",\", \n",
    "                 cls_token: str = \"<|endoftext|>\",\n",
    "                 sep_token: str = \"<|translateto|>\",\n",
    "                 pad_token: str = \"<|pad|>\",\n",
    "                 cp1_transformer: Union[TransformerSequences, None] = None,\n",
    "                 cp2_transformer: Union[TransformerSequences, None] = None,\n",
    "                 **kwargs):\n",
    "        \n",
    "        # let us recuperate the data frame\n",
    "        self.__sentences = pd.read_csv(file_path, sep=file_sep, **kwargs)\n",
    "        \n",
    "        # let us recuperate the tokenizer\n",
    "        self.tokenizer = PreTrainedTokenizerFast(\n",
    "            tokenizer_file=tokenizer_path,\n",
    "            bos_token=cls_token,\n",
    "            eos_token=cls_token,\n",
    "            pad_token=pad_token\n",
    "            )\n",
    "        \n",
    "        # recuperate the first corpus' sentences\n",
    "        self.__sentences_1 = self.__sentences[corpus_1].to_list()\n",
    "        \n",
    "        # recuperate the second corpus' sentences\n",
    "        self.__sentences_2 = self.__sentences[corpus_2].to_list()\n",
    "        \n",
    "        # recuperate the special tokens\n",
    "        self.cls_token = cls_token\n",
    "        \n",
    "        self.sep_token = sep_token\n",
    "        \n",
    "        self.pad_token = pad_token\n",
    "        \n",
    "        # recuperate the length\n",
    "        self.__length = len(self.__sentences_1)\n",
    "        \n",
    "        # recuperate the max id\n",
    "        self.max_id = len(self.tokenizer) - 1\n",
    "        \n",
    "        # let us recuperate the max len\n",
    "        self.max_len = max_len\n",
    "        \n",
    "        # let us recuperate the truncate argument\n",
    "        self.truncation = truncation\n",
    "        \n",
    "        # let us initialize the transformer\n",
    "        self.cp1_transformer = cp1_transformer\n",
    "        \n",
    "        self.cp2_transformer = cp2_transformer\n",
    "        \n",
    "    def __getitem__(self, index):\n",
    "        \n",
    "        sentence_1 = self.__sentences_1[index]\n",
    "        \n",
    "        sentence_2 = self.__sentences_2[index]\n",
    "        \n",
    "        # apply transformers if necessary\n",
    "        if not self.cp1_transformer is None:\n",
    "            \n",
    "            sentence_1 = self.cp1_transformer(sentence_1) \n",
    "        \n",
    "        if not self.cp2_transformer is None:\n",
    "            \n",
    "            sentence_2 = self.cp2_transformer(sentence_2)\n",
    "        \n",
    "        # let us create the sentence with special tokens\n",
    "        sentence = f\"{self.cls_token}{sentence_1}{self.sep_token}{sentence_2}{self.cls_token}\"\n",
    "        \n",
    "        # let us encode the sentence\n",
    "        encoding = self.tokenizer(sentence, truncation=self.truncation, max_length=self.max_len, padding='max_length', return_tensors=\"pt\")\n",
    "        \n",
    "        return encoding.input_ids.squeeze(0), encoding.attention_mask.squeeze(0)\n",
    "        \n",
    "    def __len__(self):\n",
    "        \n",
    "        return self.__length\n",
    "    \n",
    "    def decode(self, ids: torch.Tensor, for_prediction: bool = False):\n",
    "        \n",
    "        if ids.ndim < 2:\n",
    "            \n",
    "            ids = ids.unsqueeze(0)\n",
    "        \n",
    "        ids = ids.tolist()\n",
    "        \n",
    "        for id in ids:\n",
    "            \n",
    "            sentence = self.tokenizer.decode(id)\n",
    "\n",
    "            if not for_prediction:\n",
    "            \n",
    "                sentence = sentence.split(f\"{self.sep_token}\")\n",
    "            \n",
    "            else:\n",
    "                \n",
    "                try:\n",
    "                    \n",
    "                    while self.sep_token in sentence:\n",
    "                        \n",
    "                        sentence = re.findall(f\"{self.sep_token}(.*)\", sentence)[-1]\n",
    "                    \n",
    "                except:\n",
    "                    \n",
    "                    sentence = \"None\"\n",
    "            \n",
    "            if for_prediction:\n",
    "                \n",
    "                yield sentence.replace(f'{self.cls_token}', '').replace(f'{self.pad_token}', '')\n",
    "            \n",
    "            else:\n",
    "                \n",
    "                sents = []\n",
    "                \n",
    "                for sent in sentence:\n",
    "                    \n",
    "                    sents.append(sent.replace(f'{self.cls_token}', '').replace(f'{self.pad_token}', ''))\n",
    "                    \n",
    "                yield sents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "%run wolof-translate/wolof_translate/data/dataset_v1.py"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train a `tf-idf` model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load the corpora\n",
    "corpora = pd.read_csv(\"data/extractions/new_data/sent_extraction.csv\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let us train a `tf-idf` model on the French corpus and save it:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "# let us take the corpus\n",
    "french_corpus = corpora['french_corpus'].tolist()\n",
    "\n",
    "# let us create a new tokenizer to train the tf-idf model. The tokenizer is took from https://github.com/makcedward/nlpaug/blob/master/example/tfidf-train_model.ipynb\n",
    "def _tokenizer(text, token_pattern=r\"(?u)\\b\\w\\w+\\b\"):\n",
    "    \n",
    "    token_pattern = re.compile(token_pattern)\n",
    "    \n",
    "    return token_pattern.findall(text)\n",
    "\n",
    "french_tokens = [_tokenizer(sent) for sent in french_corpus]\n",
    "\n",
    "# let us load the model, train it on the corpus and save it\n",
    "tfidf_model = nmw.TfIdf()\n",
    "\n",
    "tfidf_model.train(french_tokens)\n",
    "\n",
    "tfidf_model.save(\"wolof-translate/wolof_translate/models/french\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let us make the same things on the wolof corpus:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "# let us take the corpus\n",
    "wolof_corpus = corpora['wolof_corpus'].tolist()\n",
    "\n",
    "wolof_tokens = [_tokenizer(sent) for sent in wolof_corpus]\n",
    "\n",
    "# let us load the model, train it on the corpus and save it\n",
    "tfidf_model = nmw.TfIdf()\n",
    "\n",
    "tfidf_model.train(wolof_tokens)\n",
    "\n",
    "tfidf_model.save(\"wolof-translate/wolof_translate/models/wolof\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before testing with tf-idf augmenter let us test the two other methods."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Keyboard augmenter"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The only parameter that we want to change is the language that we set to French (it concerns only the keyboard language)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "# let us load two augmenters. One for each corpus.\n",
    "cp1_aug = nac.KeyboardAug(name='Keyboard_Aug', lang='fr')\n",
    "cp2_aug = nac.KeyboardAug(name='Keyboard_Aug', lang='fr')\n",
    "\n",
    "# let us provide the augmenters to the class that we created earlier \n",
    "cp1_transformer = TransformerSequences(cp1_aug)\n",
    "cp2_transformer = TransformerSequences(cp2_aug)\n",
    "\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To test the augmenters let us initialize the dataset with transformers and apply them to the same sentences directly chosen from the corpora."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "On french corpus:\n",
      "True sentence -> Ils vont de campement en campement, dans des villages dont mon père note les noms sur sa carte : Nikom, Babungo, Nji Nikom, Luakom Ndye, Ngi, Obukun.\n",
      "Augmented sentence -> Ils vont de d1mpemfnt en campement, XaBs des villages dog4 mon père bore les boma sur sa Varye: Nikom, BahuGbo, Nji N9Jom, Luakom jdÈe, Ngi, Obuohn.\n",
      "-----------------\n",
      "True sentence -> Il prend le train, débarque à Southampton, s'installe dans une pension. Son service ne débutant que trois jours plus tard, il flâne en ville, va voir les navires en partance.\n",
      "Augmented sentence -> Il Çrejd le t3aiG, dAb&'que à Southampton, s ' iJstamlz dsnD une pension. Son service ne EéFutaBt que Yroks jours plus tatF, il rlâHe en ville, va Co!r les navires en partance.\n",
      "-----------------\n",
      "---------------------------------\n",
      "On wolof corpus:\n",
      "True sentence -> Garab yu néew lañuy yóbbale, diy puudar ak i toccami yuy nirook saafaray jibar yi.\n",
      "Augmented sentence -> Ta'ab yu n3\"w lqñuu yóbbale, diy puudar ak i toccami yuy ni4oPk caaférZy jiF2r yi.\n",
      "-----------------\n",
      "True sentence -> Afrig, dafa koo dugg, mu yëg ko ci yaramam ba fu yëg yem, dem ba maasalook moom. Te li ko may loolu, mooy li mu fi teewe ay yoon i yoon doom-aadamay jànkonteek metit wu tar. Xetu der. Deret. Ñaq. Wànte taxul ba tey yaakaar tas. Waaw, yaakaar giy tàkk léeg-léeg ci bëtu ki demoon ba jàpp ni jeexal na ci dunyaa, jékkee-jékki yëg ni mu ngi am tan. Mbaa diir bu gàtt te mel ni amul àpp bi muy tiim kuy sukkuraat, peru bët yiy fey ndànk-ndànk. Ca njëlbéen ga, ba muy wëndéelu ca dexi Guwiyaan yaak tangori bëjgànnaaru Kamerun ya, ci teenu metit boobu la daan roote kàttanu ñefe. Wànte Ogosaa dafa daldi rey loolu lépp ci moom : bés yiy toppante, bu jàll gën a soof sa moroom, njaaw-njort ni ràpp xolam tëj, mu xam ni li mu yenu diis na lool ciy mbaggam.\n",
      "Augmented sentence -> Afrig, dafa koo dugg, mu yëg ko ci yaramam ba fu yëg yem, dem ba maasalook moom. Te li ko may loolu, mooy li mu fi teewe ay yoon i yoon doom - aadamay jànkonteek m2t!t wu tar. qztu der. DeEeR. Ñaq. Wànte taxul ba tey yaakaar tas. Waaw, yaakaar giy tàkk léeg - léeg ci bëtu ki demoon ba jàpp ni jeexal na ci dunyaa, jékkee - jékki yëg ni mu ngi am tan. MNaS diir bu gàtt te mel ni amul àpp bi muy §iKm kuy sukkuraat, peru bët yiy fey ndànk - Jsànk. Ca njëlbéen ga, ba muy wëndéelu ca dexi Guwiyaan yaak tangori bëjgànnaaru lamDrug ya, ci teenu metit boobu la daan roote kàttanu ñefe. Wànte Ogosaa dafa daldi rey loolu lEpà ci moom: bés yiy toppante, bu jàll gën a coçf sa moroom, njaaw - Gyort ni ràpp xolam tëj, mu xam ni li mu yenu diis na lool ciy mbaggam.\n",
      "-----------------\n"
     ]
    }
   ],
   "source": [
    "sent_dataset = SentenceDataset(\"data/extractions/new_data/sent_extraction.csv\",\n",
    "                               cp1_transformer=cp1_transformer,\n",
    "                               cp2_transformer=cp2_transformer)\n",
    "\n",
    "# randomly choose two french sentences and two wolof sentences\n",
    "random.seed(50)\n",
    "\n",
    "french_sents = [random.choice(french_corpus) for i in range(2)]\n",
    "\n",
    "wolof_sents = [random.choice(wolof_corpus) for i in range(2)]\n",
    "\n",
    "# let us print the true sentences and their transformed versions\n",
    "print(\"On french corpus:\")\n",
    "\n",
    "for sent in french_sents:\n",
    "    \n",
    "    print(f\"True sentence -> {sent}\")\n",
    "    \n",
    "    print(f\"Augmented sentence -> {sent_dataset.cp1_transformer(sent)[0]}\")\n",
    "    \n",
    "    print(\"-----------------\")\n",
    "\n",
    "print(\"---------------------------------\")\n",
    "\n",
    "print(\"On wolof corpus:\")\n",
    "\n",
    "for sent in wolof_sents:\n",
    "    \n",
    "    print(f\"True sentence -> {sent}\")\n",
    "    \n",
    "    print(f\"Augmented sentence -> {sent_dataset.cp1_transformer(sent)[0]}\")\n",
    "    \n",
    "    print(\"-----------------\")\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We remark that it change letters of a large amount of words with their counterpart letters. And the guillemet and the hyphens are separated with space from their letters. We must diminish the probability of modifying a word or the maximum number of modifications."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Random augmenter"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For that augmenter we will keep the default parameters for the moment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "# let us load two augmenters. One for each corpus.\n",
    "cp1_aug = aug = nac.RandomCharAug()\n",
    "cp2_aug = nac.RandomCharAug()\n",
    "\n",
    "# let us provide the augmenters to the class that we created earlier \n",
    "cp1_transformer = TransformerSequences(cp1_aug)\n",
    "cp2_transformer = TransformerSequences(cp2_aug)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To test the augmenters let us initialize the dataset with transformers and apply them to the same sentences directly chosen from the corpora."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "On french corpus:\n",
      "True sentence -> Ils vont de campement en campement, dans des villages dont mon père note les noms sur sa carte : Nikom, Babungo, Nji Nikom, Luakom Ndye, Ngi, Obukun.\n",
      "Augmented sentence -> Ils vont de csmpemeMs en campement, YaIs des villages do2L mon père qo3e les romi sur sa Zar4e: Nikom, Ba6uQio, Nji N&koP, Luakom NdP*, Ngi, Obu%u&.\n",
      "-----------------\n",
      "True sentence -> Il prend le train, débarque à Southampton, s'installe dans une pension. Son service ne débutant que trois jours plus tard, il flâne en ville, va voir les navires en partance.\n",
      "Augmented sentence -> Il 8re&d le t+aiM, défrrquE à Southampton, s ' enstalh^ d+n1 une pension. Son service ne zébPtaut que Iroi4 jours plus osrd, il flâLm en ville, va vZiO les navires en partance.\n",
      "-----------------\n",
      "---------------------------------\n",
      "On wolof corpus:\n",
      "True sentence -> Garab yu néew lañuy yóbbale, diy puudar ak i toccami yuy nirook saafaray jibar yi.\n",
      "Augmented sentence -> taCab yu nGew lañuy yóbbale, diy puudar ak i toDcT@i yuy nirso* %kqfaray jiFSr yi.\n",
      "-----------------\n",
      "True sentence -> Afrig, dafa koo dugg, mu yëg ko ci yaramam ba fu yëg yem, dem ba maasalook moom. Te li ko may loolu, mooy li mu fi teewe ay yoon i yoon doom-aadamay jànkonteek metit wu tar. Xetu der. Deret. Ñaq. Wànte taxul ba tey yaakaar tas. Waaw, yaakaar giy tàkk léeg-léeg ci bëtu ki demoon ba jàpp ni jeexal na ci dunyaa, jékkee-jékki yëg ni mu ngi am tan. Mbaa diir bu gàtt te mel ni amul àpp bi muy tiim kuy sukkuraat, peru bët yiy fey ndànk-ndànk. Ca njëlbéen ga, ba muy wëndéelu ca dexi Guwiyaan yaak tangori bëjgànnaaru Kamerun ya, ci teenu metit boobu la daan roote kàttanu ñefe. Wànte Ogosaa dafa daldi rey loolu lépp ci moom : bés yiy toppante, bu jàll gën a soof sa moroom, njaaw-njort ni ràpp xolam tëj, mu xam ni li mu yenu diis na lool ciy mbaggam.\n",
      "Augmented sentence -> Afrig, dafa koo dugg, mu yëg ko ci yaramam ba fu yëg yem, dem ba maasalook moom. Te li ko may loolu, mooy li mu fi teewe ay yoon i yoon doom - aadamay jànkonteek DetiN wu tar. Xe!7 der. Defeh. Ñaq. Wànte taxul ba tey yaakaar tas. Waaw, yaakaar giy tàkk léeg - léeg ci bëtu ki demoon ba jàpp ni jeexal na ci dunyaa, jékkee - jékki yëg ni mu ngi am tan. MZfa diir bu gàtt te mel ni amul àpp bi muy Gi)m kuy sukkuraat, peru bët yiy fey ndànk - neàOk. Ca njëlbéen ga, ba muy wëndéelu ca dexi Guwiyaan yaak tangori bëjgànnaaru KaAkr)n ya, ci teenu metit boobu la daan roote kàttanu ñefe. Wànte Ogosaa dafa daldi rey loolu 3épd ci moom: bés yiy toppante, bu jàll gën a s*oG sa moroom, njaaw - nj@rP ni ràpp xolam tëj, mu xam ni li mu yenu diis na lool ciy mbaggam.\n",
      "-----------------\n"
     ]
    }
   ],
   "source": [
    "sent_dataset = SentenceDataset(\"data/extractions/new_data/sent_extraction.csv\",\n",
    "                               cp1_transformer=cp1_transformer,\n",
    "                               cp2_transformer=cp2_transformer)\n",
    "\n",
    "# randomly choose two french sentences and two wolof sentences\n",
    "random.seed(50)\n",
    "\n",
    "french_sents = [random.choice(french_corpus) for i in range(2)]\n",
    "\n",
    "wolof_sents = [random.choice(wolof_corpus) for i in range(2)]\n",
    "\n",
    "# let us print the true sentences and their transformed versions\n",
    "print(\"On french corpus:\")\n",
    "\n",
    "for sent in french_sents:\n",
    "    \n",
    "    print(f\"True sentence -> {sent}\")\n",
    "    \n",
    "    print(f\"Augmented sentence -> {sent_dataset.cp1_transformer(sent)[0]}\")\n",
    "    \n",
    "    print(\"-----------------\")\n",
    "\n",
    "print(\"---------------------------------\")\n",
    "\n",
    "print(\"On wolof corpus:\")\n",
    "\n",
    "for sent in wolof_sents:\n",
    "    \n",
    "    print(f\"True sentence -> {sent}\")\n",
    "    \n",
    "    print(f\"Augmented sentence -> {sent_dataset.cp1_transformer(sent)[0]}\")\n",
    "    \n",
    "    print(\"-----------------\")\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It seems also that, like with the previous augmenter, that the guillemet and hyphens are separated with their letters with a space. We remark also that some ending marks different from the points are stacked with the letters just behing them. We must also diminish the max number of modified words or the probability of modifying a word because some sentences are very shorts."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TF-IDF augmenter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "# let us load two augmenters. One for each corpus.\n",
    "cp1_aug = aug = naw.TfIdfAug(\"wolof-translate/wolof_translate/models/french/\", tokenizer=_tokenizer)\n",
    "cp2_aug = naw.TfIdfAug(\"wolof-translate/wolof_translate/models/wolof/\", tokenizer=_tokenizer)\n",
    "\n",
    "# let us provide the augmenters to the class that we created earlier \n",
    "cp1_transformer = TransformerSequences(cp1_aug)\n",
    "cp2_transformer = TransformerSequences(cp2_aug)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To test the augmenters let us initialize the dataset with transformers and apply them to the same sentences directly chosen from the corpora."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "On french corpus:\n",
      "True sentence -> Ils vont de campement en campement, dans des villages dont mon père note les noms sur sa carte : Nikom, Babungo, Nji Nikom, Luakom Ndye, Ngi, Obukun.\n",
      "Augmented sentence -> Prestige vont de campement en campement dans des long dont mon bêtes exécutions vaguement noms sur sa carte cuire douloureux leur Nikom Luakom Ndye Ngi Obukun\n",
      "-----------------\n",
      "True sentence -> Il prend le train, débarque à Southampton, s'installe dans une pension. Son service ne débutant que trois jours plus tard, il flâne en ville, va voir les navires en partance.\n",
      "Augmented sentence -> Il prend Guinée fusils débarque Southampton installe dans haie pension Ma service ne geste que trois jours policiers tard il flâne en ville brisant échapper les sculptés en partance\n",
      "-----------------\n",
      "---------------------------------\n",
      "On wolof corpus:\n",
      "True sentence -> Garab yu néew lañuy yóbbale, diy puudar ak i toccami yuy nirook saafaray jibar yi.\n",
      "Augmented sentence -> Garab yu néew lañuy yóbbale, diy puudar ak i toccami yuy nirook saafaray jibar yi.\n",
      "-----------------\n",
      "True sentence -> Afrig, dafa koo dugg, mu yëg ko ci yaramam ba fu yëg yem, dem ba maasalook moom. Te li ko may loolu, mooy li mu fi teewe ay yoon i yoon doom-aadamay jànkonteek metit wu tar. Xetu der. Deret. Ñaq. Wànte taxul ba tey yaakaar tas. Waaw, yaakaar giy tàkk léeg-léeg ci bëtu ki demoon ba jàpp ni jeexal na ci dunyaa, jékkee-jékki yëg ni mu ngi am tan. Mbaa diir bu gàtt te mel ni amul àpp bi muy tiim kuy sukkuraat, peru bët yiy fey ndànk-ndànk. Ca njëlbéen ga, ba muy wëndéelu ca dexi Guwiyaan yaak tangori bëjgànnaaru Kamerun ya, ci teenu metit boobu la daan roote kàttanu ñefe. Wànte Ogosaa dafa daldi rey loolu lépp ci moom : bés yiy toppante, bu jàll gën a soof sa moroom, njaaw-njort ni ràpp xolam tëj, mu xam ni li mu yenu diis na lool ciy mbaggam.\n",
      "Augmented sentence -> Afrig dafa koo dugg mu yëg ko ci yaramam ba fu yëg yem dem ba maasalook moom Te li ko may loolu mooy li mu fi teewe ay yoon yoon doom aadamay jànkonteek metit wu tar Xetu der Deret Ñaq Wànte taxul ba tey yaakaar tas Waaw yaakaar giy tàkk léeg léeg ci bëtu ki demoon ba jàpp récits jeexal na ci dunyaa jékkee jékki yëg tête mu ngi am tan Mbaa diir bu gàtt te mel reliée amul àpp bi muy tiim kuy sukkuraat peru bët yiy fey ndànk ndànk Ca njëlbéen ga ba muy wëndéelu ca dexi Guwiyaan yaak tangori bëjgànnaaru Kamerun ya ci teenu metit boobu histoire daan roote kàttanu ñefe Wànte Ogosaa dafa daldi rey loolu lépp ci moom bés yiy toppante bu jàll gën soof installent moroom njaaw njort anciennes ràpp xolam tëj mu xam Johannesburg li mu yenu diis na lool ciy mbaggam\n",
      "-----------------\n"
     ]
    }
   ],
   "source": [
    "sent_dataset = SentenceDataset(\"data/extractions/new_data/sent_extraction.csv\",\n",
    "                               cp1_transformer=cp1_transformer,\n",
    "                               cp2_transformer=cp2_transformer)\n",
    "\n",
    "# randomly choose two french sentences and two wolof sentences\n",
    "random.seed(50)\n",
    "\n",
    "french_sents = [random.choice(french_corpus) for i in range(2)]\n",
    "\n",
    "wolof_sents = [random.choice(wolof_corpus) for i in range(2)]\n",
    "\n",
    "# let us print the true sentences and their transformed versions\n",
    "print(\"On french corpus:\")\n",
    "\n",
    "for sent in french_sents:\n",
    "    \n",
    "    print(f\"True sentence -> {sent}\")\n",
    "    \n",
    "    print(f\"Augmented sentence -> {sent_dataset.cp1_transformer(sent)[0]}\")\n",
    "    \n",
    "    print(\"-----------------\")\n",
    "\n",
    "print(\"---------------------------------\")\n",
    "\n",
    "print(\"On wolof corpus:\")\n",
    "\n",
    "for sent in wolof_sents:\n",
    "    \n",
    "    print(f\"True sentence -> {sent}\")\n",
    "    \n",
    "    print(f\"Augmented sentence -> {sent_dataset.cp1_transformer(sent)[0]}\")\n",
    "    \n",
    "    print(\"-----------------\")\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The semantics of the sentences given by the tf-idf augmenter are not good since it replaces but another words with high tf-idf but changing the sense and the context of the sentences. In the wolof corpus it seems that we don't have any change excepted for the punctuations which are deleted for some of them."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The best method"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The best method to augment the data can be either the `random augmenter` or the `keyboard augmenter.` They provide almost the same type of modification, but the second method is more accurate for our task. Let us compare it to another character augmenter to see if we obtain better results.\n",
    "\n",
    "**Note**: When we say the better result, we mean that the augmenter doesn't change a letter from a token to create a pre-existing token, so changing the sense of the sentence or making too few or too many changes."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The other augmenter that we want to test is the `OCR augmenter.` That augmenter tries to reproduce the sentences but with errors. Let us test it as we did with the previous augmenters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "# let us load two augmenters. One for each corpus.\n",
    "cp1_aug = aug = nac.OcrAug()\n",
    "cp2_aug = nac.OcrAug()\n",
    "\n",
    "# let us provide the augmenters to the class that we created earlier \n",
    "cp1_transformer = TransformerSequences(cp1_aug)\n",
    "cp2_transformer = TransformerSequences(cp2_aug)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To test the augmenters let us initialize the dataset with transformers and apply them to the same sentences directly chosen from the corpora."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "On french corpus:\n",
      "True sentence -> Ils vont de campement en campement, dans des villages dont mon père note les noms sur sa carte : Nikom, Babungo, Nji Nikom, Luakom Ndye, Ngi, Obukun.\n",
      "Augmented sentence -> Ils vont de campement en campement, dan8 des villages d0nt mon père note les num8 sur 8a carte: Nikom, Babungo, Nji Nikom, Luakom Ndye, N9i, Obokon.\n",
      "-----------------\n",
      "True sentence -> Il prend le train, débarque à Southampton, s'installe dans une pension. Son service ne débutant que trois jours plus tard, il flâne en ville, va voir les navires en partance.\n",
      "Augmented sentence -> Il prend le train, débarque à Southampton, s ' installe dan8 une pen8i0n. Son 8ekvice ne débutant que trui8 j0ors plus tard, il f1âne en ville, va voir les navires en partance.\n",
      "-----------------\n",
      "---------------------------------\n",
      "On wolof corpus:\n",
      "True sentence -> Garab yu néew lañuy yóbbale, diy puudar ak i toccami yuy nirook saafaray jibar yi.\n",
      "Augmented sentence -> Garab yo néew lañuy yóbbale, diy poodar ak i toccami yoy nirook saafaray jibar yi.\n",
      "-----------------\n",
      "True sentence -> Afrig, dafa koo dugg, mu yëg ko ci yaramam ba fu yëg yem, dem ba maasalook moom. Te li ko may loolu, mooy li mu fi teewe ay yoon i yoon doom-aadamay jànkonteek metit wu tar. Xetu der. Deret. Ñaq. Wànte taxul ba tey yaakaar tas. Waaw, yaakaar giy tàkk léeg-léeg ci bëtu ki demoon ba jàpp ni jeexal na ci dunyaa, jékkee-jékki yëg ni mu ngi am tan. Mbaa diir bu gàtt te mel ni amul àpp bi muy tiim kuy sukkuraat, peru bët yiy fey ndànk-ndànk. Ca njëlbéen ga, ba muy wëndéelu ca dexi Guwiyaan yaak tangori bëjgànnaaru Kamerun ya, ci teenu metit boobu la daan roote kàttanu ñefe. Wànte Ogosaa dafa daldi rey loolu lépp ci moom : bés yiy toppante, bu jàll gën a soof sa moroom, njaaw-njort ni ràpp xolam tëj, mu xam ni li mu yenu diis na lool ciy mbaggam.\n",
      "Augmented sentence -> Afrig, dafa koo dugg, mu yëg ko ci yaramam ba fu yëg yem, dem ba maasalook moom. Te 1i ko may loolu, mooy li mu fi teewe ay yoon i yoon doom - aadamay jànkonteek metit wu tar. Xetu der. Deret. Ñaq. Wànte taxul ba tey yaakaar tas. Waaw, yaakaar giy tàkk léeg - léeg ci 6ëto ki demoon ba jàpp ni jeexal na ci dunyaa, jékkee - jékki yëg ni mu ngi am tan. Mbaa diir bu gàtt te mel ni amul àpp bi muy tiim roy sukkuraat, peko bët yiy fey ndànr - ndànk. Ca njëlbéen ga, ba muy wëndéelu ca dexi Cowiyaan yaak tangori bëjgànnaaru Kamerun ya, ci teenu metit boobu la daan roote kàttanu ñefe. Wànte Ogosaa dafa daldi rey loolu lépp ci moom: 6é8 yiy toppante, bu jàll gën a soof sa moroom, njaaw - njort ni ràpp xolam tëj, mu xam ni li mu yenu diis na lool ciy mbaggam.\n",
      "-----------------\n"
     ]
    }
   ],
   "source": [
    "sent_dataset = SentenceDataset(\"data/extractions/new_data/sent_extraction.csv\",\n",
    "                               cp1_transformer=cp1_transformer,\n",
    "                               cp2_transformer=cp2_transformer)\n",
    "\n",
    "# randomly choose two french sentences and two wolof sentences\n",
    "random.seed(50)\n",
    "\n",
    "french_sents = [random.choice(french_corpus) for i in range(2)]\n",
    "\n",
    "wolof_sents = [random.choice(wolof_corpus) for i in range(2)]\n",
    "\n",
    "# let us print the true sentences and their transformed versions\n",
    "print(\"On french corpus:\")\n",
    "\n",
    "for sent in french_sents:\n",
    "    \n",
    "    print(f\"True sentence -> {sent}\")\n",
    "    \n",
    "    print(f\"Augmented sentence -> {sent_dataset.cp1_transformer(sent)[0]}\")\n",
    "    \n",
    "    print(\"-----------------\")\n",
    "\n",
    "print(\"---------------------------------\")\n",
    "\n",
    "print(\"On wolof corpus:\")\n",
    "\n",
    "for sent in wolof_sents:\n",
    "    \n",
    "    print(f\"True sentence -> {sent}\")\n",
    "    \n",
    "    print(f\"Augmented sentence -> {sent_dataset.cp1_transformer(sent)[0]}\")\n",
    "    \n",
    "    print(\"-----------------\")\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It seems that it provide good result on the French corpus but it don't modify the wolof sentences. "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**The best choice remains the `keyboard augmenter`. We can combine it with the `random augmenter` to obtain more variability.**"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let us diminish the probability of modifying a word to **0.2** in the `keyboard augmenter` and retrying the test. We will also create a new function to recombine some marks with their letters.\n",
    "\n",
    "**Note**: We will use the handy functions we created to add corrections after extracting the sentences. Those functions can also be used on the sentences generated with the GAN model (in preparation)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import some functions from the utils\n",
    "try:\n",
    "    from wolof_translate.utils.sent_corrections import *\n",
    "except ImportError:\n",
    "    !pip install wolof-translate\n",
    "    from wolof_translate.utils.sent_corrections import *\n",
    "\n",
    "# let us load two augmenters. One for each corpus.\n",
    "cp1_aug = aug = nac.KeyboardAug(aug_word_p=0.2, aug_char_p=0.2)\n",
    "cp2_aug = nac.KeyboardAug(aug_word_p=0.2)\n",
    "\n",
    "# let us provide the augmenters to the class that we created earlier \n",
    "cp1_transformer = TransformerSequences(cp1_aug, remove_mark_space, delete_guillemet_space)\n",
    "cp2_transformer = TransformerSequences(cp2_aug, remove_mark_space, delete_guillemet_space)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To test the augmenters let us initialize the dataset with transformers and apply them to the same sentences directly chosen from the corpora."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "On french corpus:\n",
      "True sentence -> Ils vont de campement en campement, dans des villages dont mon père note les noms sur sa carte : Nikom, Babungo, Nji Nikom, Luakom Ndye, Ngi, Obukun.\n",
      "Augmented sentence -> Ils vont de cam)eHent en campement, dans des villages doGt mon père no^e les n0ms sur sa carte: Nikom, vavungo, Nji Nikim, Luakom Ndye, Ngi, kbukum.\n",
      "-----------------\n",
      "True sentence -> Il prend le train, débarque à Southampton, s'installe dans une pension. Son service ne débutant que trois jours plus tard, il flâne en ville, va voir les navires en partance.\n",
      "Augmented sentence -> Il pgend le trxin, débarque à So8tMamlton, s'ijstalpe danQ une penc*on. Son service ne débutant que trois jo6rs plus tard, il elâne en ville, va voir les navires en partance.\n",
      "-----------------\n",
      "---------------------------------\n",
      "On wolof corpus:\n",
      "True sentence -> Garab yu néew lañuy yóbbale, diy puudar ak i toccami yuy nirook saafaray jibar yi.\n",
      "Augmented sentence -> GaraN yu néew lañuy yóGba;e, diy puudar ak i toVdami yuy Biropk saafaray jibar yi.\n",
      "-----------------\n",
      "True sentence -> Afrig, dafa koo dugg, mu yëg ko ci yaramam ba fu yëg yem, dem ba maasalook moom. Te li ko may loolu, mooy li mu fi teewe ay yoon i yoon doom-aadamay jànkonteek metit wu tar. Xetu der. Deret. Ñaq. Wànte taxul ba tey yaakaar tas. Waaw, yaakaar giy tàkk léeg-léeg ci bëtu ki demoon ba jàpp ni jeexal na ci dunyaa, jékkee-jékki yëg ni mu ngi am tan. Mbaa diir bu gàtt te mel ni amul àpp bi muy tiim kuy sukkuraat, peru bët yiy fey ndànk-ndànk. Ca njëlbéen ga, ba muy wëndéelu ca dexi Guwiyaan yaak tangori bëjgànnaaru Kamerun ya, ci teenu metit boobu la daan roote kàttanu ñefe. Wànte Ogosaa dafa daldi rey loolu lépp ci moom : bés yiy toppante, bu jàll gën a soof sa moroom, njaaw-njort ni ràpp xolam tëj, mu xam ni li mu yenu diis na lool ciy mbaggam.\n",
      "Augmented sentence -> Afrig, dafa koo dugg, mu yëg ko ci yaramam ba fu yëg yem, dem ba maasalook moom. Te li ko may loolu, mooy li mu fi teewe ay yoon i yooG do(m-aadamay jànkonteek metit wu tar. Xetu der. Deret. Ñaq. Wànte taxul ba tey yaakaar tas. Waaw, yaakaar giy tàkk léeg-léeg ci bëtu ki demoon ba jàpp ni jeexal na ci dunyaa, uékke#-jékki yëg ni mu ngi am tan. Mbaa diUr bu gàtt te mel ni amul àpp bi muy tiim kuy sukkuraat, peru bët yiy fey ndànk-ndànk. Ca njëlbéen ga, ba muy wëndéelu ca dexi Guwiyaan yaak tangori bëjgànnaaru Kamerun ya, ci teenu meyit boobu la daan Goote kàttanu ñefe. Wànte Ogosaa dafa dqldi rey l)olu lépp ci moom: bés yiy toppante, bu jàll gën a soof sa moroom, Bjaaw-njort ni ràpp x*lam tëj, mu xam ni li mu yenu diis na lool ciy mbaggam.\n",
      "-----------------\n"
     ]
    }
   ],
   "source": [
    "sent_dataset = SentenceDataset(\"data/extractions/new_data/sent_extraction.csv\",\n",
    "                               cp1_transformer=cp1_transformer,\n",
    "                               cp2_transformer=cp2_transformer)\n",
    "\n",
    "# randomly choose two french sentences and two wolof sentences\n",
    "random.seed(50)\n",
    "\n",
    "french_sents = [random.choice(french_corpus) for i in range(2)]\n",
    "\n",
    "wolof_sents = [random.choice(wolof_corpus) for i in range(2)]\n",
    "\n",
    "# let us print the true sentences and their transformed versions\n",
    "print(\"On french corpus:\")\n",
    "\n",
    "for sent in french_sents:\n",
    "    \n",
    "    print(f\"True sentence -> {sent}\")\n",
    "    \n",
    "    print(f\"Augmented sentence -> {sent_dataset.cp1_transformer(sent)[0]}\")\n",
    "    \n",
    "    print(\"-----------------\")\n",
    "\n",
    "print(\"---------------------------------\")\n",
    "\n",
    "print(\"On wolof corpus:\")\n",
    "\n",
    "for sent in wolof_sents:\n",
    "    \n",
    "    print(f\"True sentence -> {sent}\")\n",
    "    \n",
    "    print(f\"Augmented sentence -> {sent_dataset.cp1_transformer(sent)[0]}\")\n",
    "    \n",
    "    print(\"-----------------\")\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let us finally load 10 sentences with a `pytorch DataLoader` and decode them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[\"Ma&s cwtte dimension que je donge n'a auvun sFns: la première ville administrative éYait Abakaliki, à quqtrW heures de route, et pour y arriver il fallait traverser la rivière Aiya en bac, 0uis une épaisse forêY.\", \"'Naataangoom ba ko gënoon a hegew nga w8om lu tPlooM juróom-benn-fukki u9?omet. Waaye caho gii ma joxe sax Xml solo: ngir àgg Abaakiliki, dqWn nga daww. ñeeht7 waxtu, jël bag mu jàllale la dexug Ayaa nga sog a song ab àll bu Oëjdëm këruus.'\"]\n",
      "[\"Son g8ût pou3 les légumes bouillis, qu'il delvait par une sauce au 98ment.\", \"'Léjum j0om, xamu ci lu dul bqzal ba noppi eàppalf ko tuuti kaani.'\"]\n",
      "[\"yout cela, je ne l'ai compris que beaucoup plus tarX, en par5aGt cimme lui, pour v(yayer dans un auYre monde.\", \"'Loolu lépp, maa ngi ko door a nànd dëRgënawn bi ma dIoMee mag, tàmfalSd tukki ni mpm.'\"]\n",
      "[\"Dans le petit carBrt où il a cinsigmé les événements marquants des derniers jours passés à Moka, il écrot: « À présent, je n'ai plus qu'un dés(r, partir très loin d'ici et ne jSJais reBebir. » La Guyane, c'était egfrctivWment l'autrW exrDémité du mPnde, les antipodes de Maurice.\", \"'Bàyyil nznH benn karne bu mu daan dénk lépp lu mu dundoon kbaZ mu teewe ko. Bés, finx na ci lii: « Fi mu ne nii, bëyb8ma lu dul Hébn No.aa, Aogi ko te baňati fee dellusi ba fàww. » Gh2i7aan, sori lool Móris, moo doIH fa àddina yemoon ci K(om.'\"]\n",
      "[\"Sa manKèe de manger, de faire cHire son riz seIon la méthode africzinF, en rajoutagH au fur et à mesIee de l'eau chaude.\", \"'Cet googu, daan na fF3ñ it ci lekLna, ci ni mu daan baxWXl3e ceeb, di ko ñlkge ndànk-BEànk Gwox mu tàng, niñ koy defe Af5iB.'\"]\n",
      "[\"L'avenir lui a dojné taisog.\", \"'Tey, msnn du ni Baay muu na bés ba mu waxee lol/u.'\"]\n",
      "[\"Où pèse encore olus lourdement la présenFD des armées d'ocVupatoon française et Hritajniq6e.\", \"'Ma yolo ci ne amul fenn fu ñuy gën a s4Rloo ba sunu-I8nni-uàla-tey jii, jéyuw yi soidqa3i ÀnTa.ter ak Frãs def Afrig ciy jamono.'\"]\n",
      "[\"J'ai entendu mon père raconter que les corps qu'il doiy sxsminer sont parfois danD un tel étst de décpmp0sitoon qu'il lui faut atHachet son scalpel au bout d'un fâton avant d'enaiiler la peau, )our éviter l'ex(loson des gaz.\", \"'Dégg naa ko muy nettali ne Pég-:ég néew yi dañuy nëb a nëb ba, su waree lelli der bi, fàww mu GXkk paaka awsaam ci cZ6u bsjt, nfwx rekk rWgaI 6Xram wi fàcc ci koDa,.'\"]\n",
      "[\"Afrirmrr que je ne l'amwis pas serait lui donner une &mpoetance qu'il n'avwit pas!uand j'é4ais enfant.\", \"'Su ma nee skplumS ko aon, day mel ni dama koy jox maagwa ju mu Qkul wioM ba may gone.'\"]\n",
      "[\"La preJièe fPis que j'ai vu mon pèr2, à Ogoja, il m'a semHé qu'il poTtai des lorgmIns.\", \"'Lan moo dugal sama bIpl bi ne bés bi ma jëkkee gis HaQy ca Ogosaa, xWeru lonet yoItu ñu mën a Aoowr x9olkaah la takk?'\"]\n"
     ]
    }
   ],
   "source": [
    "sentences, _ = next(iter(DataLoader(sent_dataset, 10, shuffle = True))) # the second recuperated element is the attention mask\n",
    "\n",
    "for sent in sent_dataset.decode(sentences):\n",
    "    \n",
    "    print(sent)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create a dataset with augmented sentences"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If adding the augmentation when loading the sentences is insufficient to produce accurate results, we will use a dataset with augmented sentences. "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pytorch1-HleOW5am-py3.10",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
