{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Custom Transformer Training\n",
    "-------------------------------\n",
    "\n",
    "In this notebook we will train the custom transformer on multiple GPUs if they are available. The GPUs are in a single machine. In [multiple](_custom_transformer_train_multiple.ipynb), we will use sagemaker to distribute the training of the model over multiple instances. \n",
    "\n",
    "We will pursue the following steps:\n",
    "\n",
    "- Load the libraries\n",
    "- Creating function to recuperate datasets (arguments: char_p, word_p, max_len, end_mark, corpus_1, corpus_2, data_directory)\n",
    "- Training (The model is automatically saved)(arguments: config dictionary initialized before)\n",
    "- Predictions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-------------------------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### French-Wolof v5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "➡️ Import the libraries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Global seed set to 0\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from wolof_translate import *\n",
    "\n",
    "# specify a seed for everything\n",
    "lt.seed_everything(0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "➡️ Function to recuperate datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting wolof-translate/wolof_translate/utils/recuperate_datasets.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile wolof-translate/wolof_translate/utils/recuperate_datasets.py\n",
    "from wolof_translate import *\n",
    "\n",
    "def recuperate_datasets(char_p: float, word_p: float, max_len: int, end_mark: int, tokenizer: T5TokenizerFast,\n",
    "                        corpus_1: str = 'french', corpus_2: str = 'wolof', \n",
    "                        train_file: str = 'data/extractions/new_data/train_set.csv', \n",
    "                        test_file: str = 'data/extractions/new_data/test_file.csv'):\n",
    "\n",
    "  # Let us recuperate the end_mark adding option\n",
    "  if end_mark == 1:\n",
    "    # Create augmentation to add on French sentences\n",
    "    fr_augmentation_1 = TransformerSequences(nac.KeyboardAug(aug_char_p=char_p, aug_word_p=word_p,\n",
    "                                                             aug_word_max = max_len),\n",
    "                                          remove_mark_space, delete_guillemet_space, add_mark_space)\n",
    "\n",
    "    fr_augmentation_2 = TransformerSequences(remove_mark_space, delete_guillemet_space, add_mark_space)\n",
    "    \n",
    "  else:\n",
    "    \n",
    "    if end_mark == 2:\n",
    "\n",
    "      end_mark_fn = partial(add_end_mark, end_mark_to_remove = '!', replace = True)\n",
    "    \n",
    "    elif end_mark == 3:\n",
    "\n",
    "      end_mark_fn = partial(add_end_mark)\n",
    "    \n",
    "    elif end_mark == 4:\n",
    "\n",
    "      end_mark_fn = partial(add_end_mark, end_mark_to_remove = '!')\n",
    "    \n",
    "    else:  \n",
    "        \n",
    "        raise ValueError(f'No end mark number {end_mark}')\n",
    "\n",
    "    # Create augmentation to add on French sentences\n",
    "    fr_augmentation_1 = TransformerSequences(nac.KeyboardAug(aug_char_p=char_p, aug_word_p=word_p,\n",
    "                                                             aug_word_max = max_len),\n",
    "                                          remove_mark_space, delete_guillemet_space, add_mark_space, end_mark_fn)\n",
    "    \n",
    "    fr_augmentation_2 = TransformerSequences(remove_mark_space, delete_guillemet_space, add_mark_space, end_mark_fn)\n",
    "    \n",
    "  # Recuperate the train dataset\n",
    "  train_dataset_aug = SentenceDataset(train_file,\n",
    "                                        tokenizer,\n",
    "                                        truncation = False,\n",
    "                                        cp1_transformer = fr_augmentation_1,\n",
    "                                        cp2_transformer = fr_augmentation_2,\n",
    "                                        corpus_1=corpus_1,\n",
    "                                        corpus_2=corpus_2\n",
    "                                        )\n",
    "\n",
    "  # Recuperate the valid dataset\n",
    "  valid_dataset = SentenceDataset(test_file,\n",
    "                                        tokenizer,\n",
    "                                        cp1_transformer = fr_augmentation_2,\n",
    "                                        cp2_transformer = fr_augmentation_2,\n",
    "                                        corpus_1=corpus_1,\n",
    "                                        corpus_2=corpus_2,\n",
    "                                        truncation = False)\n",
    "  \n",
    "  # Return the datasets\n",
    "  return train_dataset_aug, valid_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "%run wolof-translate/wolof_translate/utils/recuperate_datasets.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "➡️ Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# initialize the configurations\n",
    "config = {\n",
    "    'epochs': 21,\n",
    "    'max_epoch': None,\n",
    "    'log_step': 1,\n",
    "    'metric_for_best_model': 'test_loss',\n",
    "    'metric_objective': 'minimize',\n",
    "    'corpus_1': 'french',\n",
    "    'corpus_2': 'wolof',\n",
    "    'train_file': 'data/extractions/new_data/train_set.csv',\n",
    "    'test_file': 'data/extractions/new_data/valid_set.csv',\n",
    "    'drop_out_rate': 0.291121690756753,\n",
    "    'd_model': 512,\n",
    "    'n_head': 8,\n",
    "    'dim_ff': 2024,\n",
    "    'n_encoders': 6,\n",
    "    'n_decoders': 6,\n",
    "    'learning_rate': 1e-3,\n",
    "    'weight_decay': 0.0,\n",
    "    'char_p': 0.8986208054599546,\n",
    "    'word_p': 0.7876712525708085,\n",
    "    'end_mark': 3,\n",
    "    'label_smoothing': 0.1,\n",
    "    'max_len': 20,\n",
    "    'random_state': 0,\n",
    "    'boundaries': [2, 23, 43, 64, 84, 104],\n",
    "    'batch_sizes': [256, 128, 64, 32, 16, 8, 4],\n",
    "    'batch_size': None, \n",
    "    'warmup_init': False,\n",
    "    'relative_step': False,\n",
    "    'num_workers': 0,\n",
    "    'pin_memory': False,\n",
    "    # --------------------> Must be changed when continuing a training\n",
    "    'model_dir': 't5_small_v5_fw',\n",
    "    'new_model_dir': 't5_small_v5_fw',\n",
    "    'continue': False, # --------------------------> Must be changed when continuing training\n",
    "    'logging_dir': 'data/logs/t5_small_fw',\n",
    "    'save_best': True,\n",
    "    'tokenizer_path': 'wolof-translate/wolof_translate/tokenizers/t5_tokenizers/tokenizer_v4.model',\n",
    "    'data_directory': 'data/extractions/new_data/',\n",
    "    'data_file': 'ad_sentences.csv',\n",
    "    'version': 5,\n",
    "    # in the case of a distributed training\n",
    "    'backend': None,\n",
    "    'hosts': [],\n",
    "    'current_host': None,\n",
    "    'num_gpus': 5,\n",
    "    'logger': None,\n",
    "    'return_trainer': True,\n",
    "    'include_split': True,\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting wolof-translate/wolof_translate/utils/hg_training.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile wolof-translate/wolof_translate/utils/hg_training.py\n",
    "from wolof_translate import *\n",
    "import warnings\n",
    "\n",
    "def train(config: dict):\n",
    "    \n",
    "    # ---------------------------------------\n",
    "    # add distribution if necessary (https://github.com/aws/amazon-sagemaker-examples/blob/main/sagemaker-python-sdk/pytorch_mnist/mnist.py)\n",
    "    \n",
    "    logger = config['logger']\n",
    "    \n",
    "    is_distributed = len(config['hosts']) > 1 and config['backend'] is not None\n",
    "    \n",
    "    use_cuda = config['num_gpus'] > 0\n",
    "    \n",
    "    config.update({\"num_workers\": 1, \"pin_memory\": True} if use_cuda else {})\n",
    "\n",
    "    if not logger is None:\n",
    "        \n",
    "        logger.debug(\"Distributed training - {}\".format(is_distributed))\n",
    "        \n",
    "        logger.debug(\"Number of gpus available - {}\".format(config['num_gpus']))\n",
    "        \n",
    "    if is_distributed:\n",
    "        # Initialize the distributed environment.\n",
    "        world_size = len(config['hosts'])\n",
    "        \n",
    "        os.environ[\"WORLD_SIZE\"] = str(world_size)\n",
    "        \n",
    "        host_rank = config['hosts'].index(config['current_host'])\n",
    "        \n",
    "        os.environ[\"RANK\"] = str(host_rank)\n",
    "        \n",
    "        dist.init_process_group(backend=config['backend'], rank=host_rank, world_size=world_size)\n",
    "        \n",
    "        if not logger is None: logger.info(\n",
    "            \"Initialized the distributed environment: '{}' backend on {} nodes. \".format(\n",
    "                config['backend'], dist.get_world_size()\n",
    "            )\n",
    "            + \"Current host rank is {}. Number of gpus: {}\".format(dist.get_rank(), config['num_gpus'])\n",
    "        )\n",
    "    # ---------------------------------------\n",
    "    \n",
    "    # split the data\n",
    "    if config['include_split']: split_data(config['random_state'], config['data_directory'], config['data_file'])\n",
    "\n",
    "    # recuperate the tokenizer\n",
    "    tokenizer = T5TokenizerFast(config['tokenizer_path'])\n",
    "    \n",
    "    # Initialize the model name\n",
    "    model_name = 't5-small'\n",
    "\n",
    "    # import the model with its pre-trained weights\n",
    "    model = T5ForConditionalGeneration.from_pretrained(model_name)\n",
    "\n",
    "    # resize the token embeddings\n",
    "    model.resize_token_embeddings(len(tokenizer))\n",
    "    \n",
    "    # recuperate train and test set\n",
    "    train_dataset, test_dataset = recuperate_datasets(config['char_p'],\n",
    "                                                        config['word_p'], config['max_len'],\n",
    "                                                        config['end_mark'], tokenizer, config['corpus_1'],\n",
    "                                                        config['corpus_2'],\n",
    "                                                        config['train_file'], config['test_file'])\n",
    "    \n",
    "    # initialize the evaluation object\n",
    "    evaluation = TranslationEvaluation(tokenizer, train_dataset.decode)\n",
    "\n",
    "    # let us initialize the trainer\n",
    "    trainer = ModelRunner(model = model, version=config['version'], seed = 0, evaluation = evaluation, optimizer = Adafactor)\n",
    "\n",
    "    #-------------------------------------\n",
    "    # in the case when the linear learning rate scheduler with warmup is used\n",
    "    \n",
    "    # let us calculate the appropriate warmup steps (let us take a max epoch of 100)\n",
    "    # length = len(train_dataset)\n",
    "\n",
    "    # n_steps = length // config['batch_size']\n",
    "\n",
    "    # num_steps = config['max_epoch'] * n_steps\n",
    "\n",
    "    # warmup_steps = (config['max_epoch'] * n_steps) * config['warmup_ratio']\n",
    "\n",
    "    # Initialize the scheduler parameters\n",
    "    # scheduler_args = {'num_warmup_steps': warmup_steps, 'num_training_steps': num_steps}\n",
    "    #-------------------------------------\n",
    "\n",
    "    # Initialize the optimizer parameters\n",
    "    optimizer_args = {\n",
    "        'lr': config['learning_rate'],\n",
    "        'weight_decay': config['weight_decay'],\n",
    "        # 'betas': (0.9, 0.98),\n",
    "        'warmup_init': config['warmup_init'],\n",
    "        'relative_step': config['relative_step']\n",
    "    }\n",
    "\n",
    "    # ----------------------------\n",
    "    # initialize the bucket samplers for distributed environment\n",
    "    boundaries = config['boundaries']\n",
    "    batch_sizes = config['batch_sizes']\n",
    "\n",
    "    train_sampler = SequenceLengthBatchSampler(train_dataset,\n",
    "                                                boundaries = boundaries,\n",
    "                                                batch_sizes = batch_sizes)\n",
    "\n",
    "    test_sampler = SequenceLengthBatchSampler(test_dataset,\n",
    "                                                boundaries = boundaries,\n",
    "                                                batch_sizes = batch_sizes)\n",
    "\n",
    "    # ------------------------------\n",
    "    # initialize a bucket sampler with fixed batch size in the case of single machine\n",
    "    # with parallelization on multiple gpus\n",
    "    # train_sampler = BucketSampler(train_dataset, config['batch_size'])\n",
    "\n",
    "    # test_sampler = BucketSampler(test_dataset, config['batch_size'])\n",
    "    \n",
    "    # ------------------------------\n",
    "\n",
    "    # Initialize the loaders parameters\n",
    "    train_loader_args = {'batch_sampler': train_sampler, 'collate_fn': collate_fn,\n",
    "                        'num_workers': config['num_workers'], 'pin_memory': config['pin_memory']}\n",
    "\n",
    "    test_loader_args = {'batch_sampler': test_sampler, 'collate_fn': collate_fn,\n",
    "                        'num_workers': config['num_workers'], 'pin_memory': config['pin_memory']}\n",
    "\n",
    "    # Add the datasets and hyperparameters to trainer\n",
    "    trainer.compile(train_dataset, test_dataset, tokenizer, train_loader_args,\n",
    "                    test_loader_args, optimizer_kwargs = optimizer_args,\n",
    "                    # lr_scheduler=get_linear_schedule_with_warmup,\n",
    "                    # lr_scheduler_kwargs=scheduler_args,\n",
    "                    predict_with_generate = True,\n",
    "                    hugging_face = True,\n",
    "                    is_distributed=is_distributed,\n",
    "                    logging_dir=config['logging_dir'],\n",
    "                    dist=dist\n",
    "                    )\n",
    "\n",
    "    # load the model\n",
    "    trainer.load(config['model_dir'], load_best = not config['continue'])\n",
    "    \n",
    "    # Train the model\n",
    "    trainer.train(config['epochs'] - trainer.current_epoch, auto_save = True, log_step = config['log_step'], saving_directory=config['new_model_dir'], save_best = config['save_best'],\n",
    "                  metric_for_best_model = config['metric_for_best_model'], metric_objective = config['metric_objective'])\n",
    "    \n",
    "    if config['return_trainer']:\n",
    "        \n",
    "        return trainer\n",
    "    \n",
    "    return None\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Below train and save if we want."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "from wolof_translate.utils.hg_training import train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/25 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "For epoch 6: \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train batch number 2:   0%|          | 0/43 [00:00<?, ?batches/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train batch number 44: 100%|██████████| 43/43 [00:05<00:00,  7.41batches/s]\n",
      "Test batch number 2:   0%|          | 0/7 [00:00<?, ?batches/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Test batch number 8: 100%|██████████| 7/7 [00:03<00:00,  2.23batches/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Metrics: {'train_loss': 2.5114653375916345, 'test_loss': 2.7761967109911367, 'bleu': 2.266399494949495, 'gen_len': 13.217162121212121}\n",
      "\n",
      "=============================\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  4%|▍         | 1/25 [00:10<04:22, 10.92s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "For epoch 7: \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train batch number 2:   0%|          | 0/43 [00:00<?, ?batches/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train batch number 44: 100%|██████████| 43/43 [00:05<00:00,  7.40batches/s]\n",
      "Test batch number 2:   0%|          | 0/7 [00:00<?, ?batches/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Test batch number 8: 100%|██████████| 7/7 [00:03<00:00,  1.98batches/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Metrics: {'train_loss': 2.42468051816903, 'test_loss': 2.6943533950381813, 'bleu': 2.4339045454545456, 'gen_len': 12.560631313131314}\n",
      "\n",
      "=============================\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  8%|▊         | 2/25 [00:22<04:17, 11.18s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "For epoch 8: \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train batch number 2:   0%|          | 0/43 [00:00<?, ?batches/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train batch number 44: 100%|██████████| 43/43 [00:05<00:00,  7.49batches/s]\n",
      "Test batch number 2:   0%|          | 0/7 [00:00<?, ?batches/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Test batch number 8: 100%|██████████| 7/7 [00:03<00:00,  1.98batches/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Metrics: {'train_loss': 2.3517192236461786, 'test_loss': 2.634918566906091, 'bleu': 2.3541358585858587, 'gen_len': 13.671723737373739}\n",
      "\n",
      "=============================\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 12%|█▏        | 3/25 [00:33<04:06, 11.22s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "For epoch 9: \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train batch number 2:   0%|          | 0/43 [00:00<?, ?batches/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train batch number 44: 100%|██████████| 43/43 [00:05<00:00,  7.39batches/s]\n",
      "Test batch number 2:   0%|          | 0/7 [00:00<?, ?batches/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Test batch number 8: 100%|██████████| 7/7 [00:03<00:00,  1.99batches/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Metrics: {'train_loss': 2.2777167031615697, 'test_loss': 2.590723254463889, 'bleu': 2.4163686868686867, 'gen_len': 14.858569696969699}\n",
      "\n",
      "=============================\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 16%|█▌        | 4/25 [00:44<03:56, 11.26s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "For epoch 10: \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train batch number 2:   0%|          | 0/43 [00:00<?, ?batches/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train batch number 44: 100%|██████████| 43/43 [00:05<00:00,  7.29batches/s]\n",
      "Test batch number 2:   0%|          | 0/7 [00:00<?, ?batches/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Test batch number 8: 100%|██████████| 7/7 [00:03<00:00,  2.13batches/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Metrics: {'train_loss': 2.223224872882271, 'test_loss': 2.6014229601079766, 'bleu': 2.3500434343434353, 'gen_len': 11.525243434343436}\n",
      "\n",
      "=============================\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 20%|██        | 5/25 [00:54<03:35, 10.80s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "For epoch 11: \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train batch number 2:   0%|          | 0/43 [00:00<?, ?batches/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train batch number 44: 100%|██████████| 43/43 [00:06<00:00,  7.09batches/s]\n",
      "Test batch number 2:   0%|          | 0/7 [00:00<?, ?batches/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Test batch number 8: 100%|██████████| 7/7 [00:03<00:00,  2.07batches/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Metrics: {'train_loss': 2.1764694935607425, 'test_loss': 2.5320193478555386, 'bleu': 2.618009595959596, 'gen_len': 12.217173737373738}\n",
      "\n",
      "=============================\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 24%|██▍       | 6/25 [01:06<03:29, 11.01s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "For epoch 12: \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train batch number 2:   0%|          | 0/43 [00:00<?, ?batches/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train batch number 44: 100%|██████████| 43/43 [00:05<00:00,  7.34batches/s]\n",
      "Test batch number 2:   0%|          | 0/7 [00:00<?, ?batches/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Test batch number 8: 100%|██████████| 7/7 [00:03<00:00,  2.32batches/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Metrics: {'train_loss': 2.1303964158841753, 'test_loss': 2.5434020721551147, 'bleu': 2.744057070707071, 'gen_len': 11.03030404040404}\n",
      "\n",
      "=============================\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 28%|██▊       | 7/25 [01:15<03:09, 10.55s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "For epoch 13: \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train batch number 2:   0%|          | 0/43 [00:00<?, ?batches/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train batch number 44: 100%|██████████| 43/43 [00:05<00:00,  7.40batches/s]\n",
      "Test batch number 2:   0%|          | 0/7 [00:00<?, ?batches/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Test batch number 8: 100%|██████████| 7/7 [00:03<00:00,  2.02batches/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Metrics: {'train_loss': 2.0804740817089926, 'test_loss': 2.5099077537806354, 'bleu': 2.563536868686869, 'gen_len': 12.616167676767677}\n",
      "\n",
      "=============================\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 32%|███▏      | 8/25 [01:27<03:03, 10.78s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "For epoch 14: \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train batch number 2:   0%|          | 0/43 [00:00<?, ?batches/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train batch number 44: 100%|██████████| 43/43 [00:05<00:00,  7.33batches/s]\n",
      "Test batch number 2:   0%|          | 0/7 [00:00<?, ?batches/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Test batch number 8: 100%|██████████| 7/7 [00:03<00:00,  2.17batches/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Metrics: {'train_loss': 2.0455848709229487, 'test_loss': 2.52968150919134, 'bleu': 3.1110070707070716, 'gen_len': 11.015133333333333}\n",
      "\n",
      "=============================\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 36%|███▌      | 9/25 [01:36<02:47, 10.47s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "For epoch 15: \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train batch number 2:   0%|          | 0/43 [00:00<?, ?batches/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train batch number 44: 100%|██████████| 43/43 [00:05<00:00,  7.37batches/s]\n",
      "Test batch number 3:  14%|█▍        | 1/7 [00:00<00:01,  4.58batches/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Test batch number 8: 100%|██████████| 7/7 [00:03<00:00,  2.09batches/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Metrics: {'train_loss': 1.9926729948742343, 'test_loss': 2.5232395668222445, 'bleu': 2.864400505050505, 'gen_len': 11.368671212121212}\n",
      "\n",
      "=============================\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 40%|████      | 10/25 [01:46<02:34, 10.30s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "For epoch 16: \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train batch number 2:   0%|          | 0/43 [00:00<?, ?batches/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train batch number 44: 100%|██████████| 43/43 [00:05<00:00,  7.33batches/s]\n",
      "Test batch number 2:   0%|          | 0/7 [00:00<?, ?batches/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Test batch number 8: 100%|██████████| 7/7 [00:03<00:00,  2.00batches/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Metrics: {'train_loss': 1.9518446629745607, 'test_loss': 2.5120888189835986, 'bleu': 2.8665191919191924, 'gen_len': 11.176735858585861}\n",
      "\n",
      "=============================\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 44%|████▍     | 11/25 [01:56<02:23, 10.23s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "For epoch 17: \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train batch number 2:   0%|          | 0/43 [00:00<?, ?batches/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train batch number 44: 100%|██████████| 43/43 [00:05<00:00,  7.32batches/s]\n",
      "Test batch number 2:   0%|          | 0/7 [00:00<?, ?batches/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Test batch number 8: 100%|██████████| 7/7 [00:03<00:00,  2.07batches/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Metrics: {'train_loss': 1.9194935183974475, 'test_loss': 2.5144295403451626, 'bleu': 3.2974808080808082, 'gen_len': 11.974762626262626}\n",
      "\n",
      "=============================\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 48%|████▊     | 12/25 [02:06<02:11, 10.13s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "For epoch 18: \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train batch number 2:   0%|          | 0/43 [00:00<?, ?batches/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train batch number 44: 100%|██████████| 43/43 [00:06<00:00,  7.16batches/s]\n",
      "Test batch number 2:   0%|          | 0/7 [00:00<?, ?batches/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Test batch number 8: 100%|██████████| 7/7 [00:03<00:00,  2.09batches/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Metrics: {'train_loss': 1.8835548197144882, 'test_loss': 2.5088922808868714, 'bleu': 2.9407616161616166, 'gen_len': 11.671747474747475}\n",
      "\n",
      "=============================\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 52%|█████▏    | 13/25 [02:18<02:05, 10.50s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "For epoch 19: \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train batch number 2:   0%|          | 0/43 [00:00<?, ?batches/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train batch number 44: 100%|██████████| 43/43 [00:05<00:00,  7.24batches/s]\n",
      "Test batch number 2:   0%|          | 0/7 [00:00<?, ?batches/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Test batch number 8: 100%|██████████| 7/7 [00:03<00:00,  2.14batches/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Metrics: {'train_loss': 1.842434627055217, 'test_loss': 2.5319321829863273, 'bleu': 2.9120015151515157, 'gen_len': 10.03538484848485}\n",
      "\n",
      "=============================\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 56%|█████▌    | 14/25 [02:28<01:53, 10.31s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "For epoch 20: \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train batch number 2:   0%|          | 0/43 [00:00<?, ?batches/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train batch number 44: 100%|██████████| 43/43 [00:05<00:00,  7.34batches/s]\n",
      "Test batch number 2:   0%|          | 0/7 [00:00<?, ?batches/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Test batch number 8: 100%|██████████| 7/7 [00:03<00:00,  2.05batches/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Metrics: {'train_loss': 1.8105094993061694, 'test_loss': 2.497430601505318, 'bleu': 2.875379292929293, 'gen_len': 11.111095454545456}\n",
      "\n",
      "=============================\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 60%|██████    | 15/25 [02:39<01:45, 10.59s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "For epoch 21: \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train batch number 2:   0%|          | 0/43 [00:00<?, ?batches/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train batch number 44: 100%|██████████| 43/43 [00:05<00:00,  7.26batches/s]\n",
      "Test batch number 2:   0%|          | 0/7 [00:00<?, ?batches/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Test batch number 8: 100%|██████████| 7/7 [00:03<00:00,  2.26batches/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Metrics: {'train_loss': 1.7762581537525384, 'test_loss': 2.4893684290876292, 'bleu': 3.0885227272727276, 'gen_len': 11.181794444444446}\n",
      "\n",
      "=============================\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 64%|██████▍   | 16/25 [02:50<01:36, 10.72s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "For epoch 22: \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train batch number 2:   0%|          | 0/43 [00:00<?, ?batches/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train batch number 44: 100%|██████████| 43/43 [00:05<00:00,  7.25batches/s]\n",
      "Test batch number 2:   0%|          | 0/7 [00:00<?, ?batches/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Test batch number 8: 100%|██████████| 7/7 [00:02<00:00,  2.47batches/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Metrics: {'train_loss': 1.7403693729228116, 'test_loss': 2.5324128372500643, 'bleu': 2.810114141414142, 'gen_len': 9.777791919191921}\n",
      "\n",
      "=============================\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 68%|██████▊   | 17/25 [02:59<01:22, 10.36s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "For epoch 23: \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train batch number 2:   0%|          | 0/43 [00:00<?, ?batches/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train batch number 44: 100%|██████████| 43/43 [00:05<00:00,  7.26batches/s]\n",
      "Test batch number 2:   0%|          | 0/7 [00:00<?, ?batches/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Test batch number 8: 100%|██████████| 7/7 [00:03<00:00,  2.31batches/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Metrics: {'train_loss': 1.6994967128113057, 'test_loss': 2.506803387343282, 'bleu': 2.749485858585859, 'gen_len': 10.61617676767677}\n",
      "\n",
      "=============================\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 72%|███████▏  | 18/25 [03:09<01:11, 10.17s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "For epoch 24: \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train batch number 2:   0%|          | 0/43 [00:00<?, ?batches/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train batch number 44: 100%|██████████| 43/43 [00:05<00:00,  7.27batches/s]\n",
      "Test batch number 2:   0%|          | 0/7 [00:00<?, ?batches/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Test batch number 8: 100%|██████████| 7/7 [00:03<00:00,  2.06batches/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Metrics: {'train_loss': 1.6744661027309942, 'test_loss': 2.52493722992714, 'bleu': 2.8192727272727276, 'gen_len': 12.17172676767677}\n",
      "\n",
      "=============================\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 76%|███████▌  | 19/25 [03:19<01:00, 10.13s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "For epoch 25: \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train batch number 2:   0%|          | 0/43 [00:00<?, ?batches/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train batch number 44: 100%|██████████| 43/43 [00:05<00:00,  7.19batches/s]\n",
      "Test batch number 2:   0%|          | 0/7 [00:00<?, ?batches/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Test batch number 8: 100%|██████████| 7/7 [00:03<00:00,  2.08batches/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Metrics: {'train_loss': 1.6336687108069412, 'test_loss': 2.5186826291710447, 'bleu': 3.126379797979798, 'gen_len': 11.545429292929294}\n",
      "\n",
      "=============================\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 80%|████████  | 20/25 [03:29<00:50, 10.12s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "For epoch 26: \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train batch number 2:   0%|          | 0/43 [00:00<?, ?batches/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train batch number 44: 100%|██████████| 43/43 [00:05<00:00,  7.23batches/s]\n",
      "Test batch number 2:   0%|          | 0/7 [00:00<?, ?batches/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Test batch number 8: 100%|██████████| 7/7 [00:03<00:00,  2.04batches/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Metrics: {'train_loss': 1.595658942463276, 'test_loss': 2.5293197246512986, 'bleu': 3.3226782828282833, 'gen_len': 11.797979292929293}\n",
      "\n",
      "=============================\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 84%|████████▍ | 21/25 [03:39<00:40, 10.10s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "For epoch 27: \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train batch number 2:   0%|          | 0/43 [00:00<?, ?batches/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train batch number 44: 100%|██████████| 43/43 [00:05<00:00,  7.20batches/s]\n",
      "Test batch number 2:   0%|          | 0/7 [00:00<?, ?batches/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Test batch number 8: 100%|██████████| 7/7 [00:03<00:00,  2.08batches/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Metrics: {'train_loss': 1.5806173942723958, 'test_loss': 2.5458529356754185, 'bleu': 3.031581818181818, 'gen_len': 12.040430303030304}\n",
      "\n",
      "=============================\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 88%|████████▊ | 22/25 [03:49<00:30, 10.07s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "For epoch 28: \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train batch number 2:   0%|          | 0/43 [00:00<?, ?batches/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train batch number 44: 100%|██████████| 43/43 [00:05<00:00,  7.21batches/s]\n",
      "Test batch number 2:   0%|          | 0/7 [00:00<?, ?batches/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Test batch number 8: 100%|██████████| 7/7 [00:03<00:00,  1.96batches/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Metrics: {'train_loss': 1.5406483437802352, 'test_loss': 2.5303414036529235, 'bleu': 3.12880101010101, 'gen_len': 12.808074747474748}\n",
      "\n",
      "=============================\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 92%|█████████▏| 23/25 [03:59<00:20, 10.11s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "For epoch 29: \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train batch number 2:   0%|          | 0/43 [00:00<?, ?batches/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train batch number 44: 100%|██████████| 43/43 [00:05<00:00,  7.20batches/s]\n",
      "Test batch number 3:  14%|█▍        | 1/7 [00:00<00:01,  4.60batches/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Test batch number 8: 100%|██████████| 7/7 [00:03<00:00,  2.09batches/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Metrics: {'train_loss': 1.5093537220692503, 'test_loss': 2.5414568535005206, 'bleu': 3.5049414141414146, 'gen_len': 12.07069797979798}\n",
      "\n",
      "=============================\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 96%|█████████▌| 24/25 [04:09<00:10, 10.09s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "For epoch 30: \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train batch number 2:   0%|          | 0/43 [00:00<?, ?batches/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train batch number 44: 100%|██████████| 43/43 [00:06<00:00,  7.15batches/s]\n",
      "Test batch number 2:   0%|          | 0/7 [00:00<?, ?batches/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Test batch number 8: 100%|██████████| 7/7 [00:03<00:00,  2.11batches/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Metrics: {'train_loss': 1.4772157685655651, 'test_loss': 2.5400167571173777, 'bleu': 3.1790439393939396, 'gen_len': 12.126261111111113}\n",
      "\n",
      "=============================\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 25/25 [04:20<00:00, 10.40s/it]\n"
     ]
    }
   ],
   "source": [
    "# with warnings.catch_warnings():\n",
    "    # warnings.simplefilter(\"ignore\")\n",
    "trainer = train(config)\n",
    "\n",
    "# save if necessary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0it [00:00, ?it/s]\n"
     ]
    }
   ],
   "source": [
    "# with warnings.catch_warnings():\n",
    "    # warnings.simplefilter(\"ignore\")\n",
    "trainer = train(config)\n",
    "\n",
    "# save if necessary"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "➡️ Predictions\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluation batch number 2:   0%|          | 0/6 [00:00<?, ?batches/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluation batch number 7: 100%|██████████| 6/6 [00:03<00:00,  1.64batches/s]\n"
     ]
    }
   ],
   "source": [
    "if not trainer is None:\n",
    "    \n",
    "    # recuperate the tokenizer\n",
    "    tokenizer = T5TokenizerFast(config['tokenizer_path'])\n",
    "    \n",
    "    # recuperate the test dataset\n",
    "    # initialize the transformation sequence\n",
    "    end_mark_fn = partial(add_end_mark)\n",
    "    augmentation = TransformerSequences(remove_mark_space, delete_guillemet_space, add_mark_space, end_mark_fn)\n",
    "\n",
    "\n",
    "    # let us get the test set\n",
    "    test_dataset = SentenceDataset(f\"{config['data_directory']}test_set.csv\",\n",
    "                                            tokenizer = tokenizer,\n",
    "                                            cp1_transformer = augmentation,\n",
    "                                            cp2_transformer = augmentation,\n",
    "                                            corpus_1=config['corpus_1'],\n",
    "                                            corpus_2=config['corpus_2'],\n",
    "                                            truncation = False)\n",
    "\n",
    "    # initialize the bucket samplers for distributed environment\n",
    "    boundaries = config['boundaries']\n",
    "    batch_sizes = config['batch_sizes']\n",
    "\n",
    "    test_sampler = SequenceLengthBatchSampler(test_dataset,\n",
    "                                                boundaries = boundaries,\n",
    "                                                batch_sizes = batch_sizes)\n",
    "\n",
    "    test_loader_args = {'batch_sampler': test_sampler, 'collate_fn': collate_fn,\n",
    "                            'num_workers': config['num_workers'], 'pin_memory': config['pin_memory']}\n",
    "\n",
    "    metrics, prediction = trainer.evaluate(test_dataset, test_loader_args)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'test_loss': 2.249727627243659,\n",
       " 'bleu': 2.7937212121212123,\n",
       " 'gen_len': 7.303034343434343}"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>original_sentences</th>\n",
       "      <th>translations</th>\n",
       "      <th>predictions</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>C'est des femmes.</td>\n",
       "      <td>Jigéen lanu.</td>\n",
       "      <td>Nit la.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Cet homme qui avait voulu.</td>\n",
       "      <td>Góor gii bëggóon.</td>\n",
       "      <td>Góor gii ŋga dem.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Par ici?</td>\n",
       "      <td>Ci fii?</td>\n",
       "      <td>Ci wax?</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Qu'il entre!</td>\n",
       "      <td>Na dugg ci biir su bëggée!</td>\n",
       "      <td>Koo dem!</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>À l'intérieur si tu ne veux pas!</td>\n",
       "      <td>Ci biir soo bëggul!</td>\n",
       "      <td>Ci biir!</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>193</th>\n",
       "      <td>Ceux-là, cependant, sont des cases. Celle qui ...</td>\n",
       "      <td>Waaw lii nag ay néegi ñax la, néegi ñax bi ci ...</td>\n",
       "      <td>Lii ab néeg la, néeg bi dañ kooale, néeg bi d...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>194</th>\n",
       "      <td>On voit sur la photo beaucoup de personnes sor...</td>\n",
       "      <td>Ñu gis ci nataal bi ay nit ñu bari ñu génn ci ...</td>\n",
       "      <td>Nataal bii de gis naa ci benn bool bu weex ak...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>195</th>\n",
       "      <td>Ceux-là, aussi, sont des gendarmes. Ils siègen...</td>\n",
       "      <td>Ñii moom tamit ay takk-der nañ. Ñi ñi ngi bàyy...</td>\n",
       "      <td>Lii ab néeg la, néeg bi dañ kooale, néeg bi d...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>196</th>\n",
       "      <td>Ceci, cependant, on a l'habitude de faire les ...</td>\n",
       "      <td>Lii nag dañ ciy faral di def ndugg maanaam jig...</td>\n",
       "      <td>Waaw nataal bii de ay bunt yu dóomu-taal moo ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>197</th>\n",
       "      <td>À l'intérieur de cette photo je vois beaucoup ...</td>\n",
       "      <td>Ci biir nataal bii maa ngi ciy gis ay batã, ba...</td>\n",
       "      <td>Waaw nataal bii de ay bunt yu dóomu-taal moo ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>198 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                    original_sentences  \\\n",
       "0                                    C'est des femmes.   \n",
       "1                           Cet homme qui avait voulu.   \n",
       "2                                             Par ici?   \n",
       "3                                         Qu'il entre!   \n",
       "4                     À l'intérieur si tu ne veux pas!   \n",
       "..                                                 ...   \n",
       "193  Ceux-là, cependant, sont des cases. Celle qui ...   \n",
       "194  On voit sur la photo beaucoup de personnes sor...   \n",
       "195  Ceux-là, aussi, sont des gendarmes. Ils siègen...   \n",
       "196  Ceci, cependant, on a l'habitude de faire les ...   \n",
       "197  À l'intérieur de cette photo je vois beaucoup ...   \n",
       "\n",
       "                                          translations  \\\n",
       "0                                         Jigéen lanu.   \n",
       "1                                    Góor gii bëggóon.   \n",
       "2                                              Ci fii?   \n",
       "3                           Na dugg ci biir su bëggée!   \n",
       "4                                  Ci biir soo bëggul!   \n",
       "..                                                 ...   \n",
       "193  Waaw lii nag ay néegi ñax la, néegi ñax bi ci ...   \n",
       "194  Ñu gis ci nataal bi ay nit ñu bari ñu génn ci ...   \n",
       "195  Ñii moom tamit ay takk-der nañ. Ñi ñi ngi bàyy...   \n",
       "196  Lii nag dañ ciy faral di def ndugg maanaam jig...   \n",
       "197  Ci biir nataal bii maa ngi ciy gis ay batã, ba...   \n",
       "\n",
       "                                           predictions  \n",
       "0                                              Nit la.  \n",
       "1                                    Góor gii ŋga dem.  \n",
       "2                                              Ci wax?  \n",
       "3                                             Koo dem!  \n",
       "4                                             Ci biir!  \n",
       "..                                                 ...  \n",
       "193   Lii ab néeg la, néeg bi dañ kooale, néeg bi d...  \n",
       "194   Nataal bii de gis naa ci benn bool bu weex ak...  \n",
       "195   Lii ab néeg la, néeg bi dañ kooale, néeg bi d...  \n",
       "196   Waaw nataal bii de ay bunt yu dóomu-taal moo ...  \n",
       "197   Waaw nataal bii de ay bunt yu dóomu-taal moo ...  \n",
       "\n",
       "[198 rows x 3 columns]"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prediction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "----------------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Wolof-French v5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "➡️ Import the libraries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Global seed set to 0\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from wolof_translate import *\n",
    "\n",
    "# specify a seed for everything\n",
    "lt.seed_everything(0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "➡️ Function to recuperate datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting wolof-translate/wolof_translate/utils/recuperate_datasets.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile wolof-translate/wolof_translate/utils/recuperate_datasets.py\n",
    "from wolof_translate import *\n",
    "\n",
    "def recuperate_datasets(char_p: float, word_p: float, max_len: int, end_mark: int, tokenizer: T5TokenizerFast,\n",
    "                        corpus_1: str = 'french', corpus_2: str = 'wolof', \n",
    "                        train_file: str = 'data/extractions/new_data/train_set.csv', \n",
    "                        test_file: str = 'data/extractions/new_data/test_file.csv'):\n",
    "\n",
    "  # Let us recuperate the end_mark adding option\n",
    "  if end_mark == 1:\n",
    "    # Create augmentation to add on French sentences\n",
    "    fr_augmentation_1 = TransformerSequences(nac.KeyboardAug(aug_char_p=char_p, aug_word_p=word_p,\n",
    "                                                             aug_word_max = max_len),\n",
    "                                          remove_mark_space, delete_guillemet_space, add_mark_space)\n",
    "\n",
    "    fr_augmentation_2 = TransformerSequences(remove_mark_space, delete_guillemet_space, add_mark_space)\n",
    "    \n",
    "  else:\n",
    "    \n",
    "    if end_mark == 2:\n",
    "\n",
    "      end_mark_fn = partial(add_end_mark, end_mark_to_remove = '!', replace = True)\n",
    "    \n",
    "    elif end_mark == 3:\n",
    "\n",
    "      end_mark_fn = partial(add_end_mark)\n",
    "    \n",
    "    elif end_mark == 4:\n",
    "\n",
    "      end_mark_fn = partial(add_end_mark, end_mark_to_remove = '!')\n",
    "    \n",
    "    else:  \n",
    "        \n",
    "        raise ValueError(f'No end mark number {end_mark}')\n",
    "\n",
    "    # Create augmentation to add on French sentences\n",
    "    fr_augmentation_1 = TransformerSequences(nac.KeyboardAug(aug_char_p=char_p, aug_word_p=word_p,\n",
    "                                                             aug_word_max = max_len),\n",
    "                                          remove_mark_space, delete_guillemet_space, add_mark_space, end_mark_fn)\n",
    "    \n",
    "    fr_augmentation_2 = TransformerSequences(remove_mark_space, delete_guillemet_space, add_mark_space, end_mark_fn)\n",
    "    \n",
    "  # Recuperate the train dataset\n",
    "  train_dataset_aug = SentenceDataset(train_file,\n",
    "                                        tokenizer,\n",
    "                                        truncation = False,\n",
    "                                        cp1_transformer = fr_augmentation_1,\n",
    "                                        cp2_transformer = fr_augmentation_2,\n",
    "                                        corpus_1=corpus_1,\n",
    "                                        corpus_2=corpus_2\n",
    "                                        )\n",
    "\n",
    "  # Recuperate the valid dataset\n",
    "  valid_dataset = SentenceDataset(test_file,\n",
    "                                        tokenizer,\n",
    "                                        cp1_transformer = fr_augmentation_2,\n",
    "                                        cp2_transformer = fr_augmentation_2,\n",
    "                                        corpus_1=corpus_1,\n",
    "                                        corpus_2=corpus_2,\n",
    "                                        truncation = False)\n",
    "  \n",
    "  # Return the datasets\n",
    "  return train_dataset_aug, valid_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "%run wolof-translate/wolof_translate/utils/recuperate_datasets.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "➡️ Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# initialize the configurations\n",
    "config = {\n",
    "    'epochs': 29,\n",
    "    'max_epoch': None,\n",
    "    'log_step': 1,\n",
    "    'metric_for_best_model': 'test_loss',\n",
    "    'metric_objective': 'minimize',\n",
    "    'corpus_1': 'wolof',\n",
    "    'corpus_2': 'french',\n",
    "    'train_file': 'data/extractions/new_data/train_set.csv',\n",
    "    'test_file': 'data/extractions/new_data/valid_set.csv',\n",
    "    'drop_out_rate': 0.291121690756753,\n",
    "    'd_model': 512,\n",
    "    'n_head': 8,\n",
    "    'dim_ff': 2024,\n",
    "    'n_encoders': 6,\n",
    "    'n_decoders': 6,\n",
    "    'learning_rate': 1e-3,\n",
    "    'weight_decay': 0.0,\n",
    "    'char_p': 0.5275538662009825,\n",
    "    'word_p': 0.8981250882159111,\n",
    "    'end_mark': 3,\n",
    "    'label_smoothing': 0.1,\n",
    "    'max_len': 20,\n",
    "    'random_state': 0,\n",
    "    'boundaries': [2, 23, 43, 64, 84, 104],\n",
    "    'batch_sizes': [256, 128, 64, 32, 16, 8, 4],\n",
    "    'batch_size': None, \n",
    "    'warmup_init': False,\n",
    "    'relative_step': False,\n",
    "    'num_workers': 0,\n",
    "    'pin_memory': False,\n",
    "    # --------------------> Must be changed when continuing a training\n",
    "    'model_dir': 't5_small_v5_wf',\n",
    "    'new_model_dir': 't5_small_v5_wf',\n",
    "    'continue': False, # --------------------------> Must be changed when continuing training\n",
    "    'logging_dir': 'data/logs/t5_small_wf',\n",
    "    'save_best': True,\n",
    "    'tokenizer_path': 'wolof-translate/wolof_translate/tokenizers/t5_tokenizers/tokenizer_v4.model',\n",
    "    'data_directory': 'data/extractions/new_data/',\n",
    "    'data_file': 'ad_sentences.csv',\n",
    "    'version': 5,\n",
    "    # in the case of a distributed training\n",
    "    'backend': None,\n",
    "    'hosts': [],\n",
    "    'current_host': None,\n",
    "    'num_gpus': 5,\n",
    "    'logger': None,\n",
    "    'return_trainer': True,\n",
    "    'include_split': True,\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting wolof-translate/wolof_translate/utils/hg_training.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile wolof-translate/wolof_translate/utils/hg_training.py\n",
    "from wolof_translate import *\n",
    "import warnings\n",
    "\n",
    "def train(config: dict):\n",
    "    \n",
    "    # ---------------------------------------\n",
    "    # add distribution if necessary (https://github.com/aws/amazon-sagemaker-examples/blob/main/sagemaker-python-sdk/pytorch_mnist/mnist.py)\n",
    "    \n",
    "    logger = config['logger']\n",
    "    \n",
    "    is_distributed = len(config['hosts']) > 1 and config['backend'] is not None\n",
    "    \n",
    "    use_cuda = config['num_gpus'] > 0\n",
    "    \n",
    "    config.update({\"num_workers\": 1, \"pin_memory\": True} if use_cuda else {})\n",
    "\n",
    "    if not logger is None:\n",
    "        \n",
    "        logger.debug(\"Distributed training - {}\".format(is_distributed))\n",
    "        \n",
    "        logger.debug(\"Number of gpus available - {}\".format(config['num_gpus']))\n",
    "        \n",
    "    if is_distributed:\n",
    "        # Initialize the distributed environment.\n",
    "        world_size = len(config['hosts'])\n",
    "        \n",
    "        os.environ[\"WORLD_SIZE\"] = str(world_size)\n",
    "        \n",
    "        host_rank = config['hosts'].index(config['current_host'])\n",
    "        \n",
    "        os.environ[\"RANK\"] = str(host_rank)\n",
    "        \n",
    "        dist.init_process_group(backend=config['backend'], rank=host_rank, world_size=world_size)\n",
    "        \n",
    "        if not logger is None: logger.info(\n",
    "            \"Initialized the distributed environment: '{}' backend on {} nodes. \".format(\n",
    "                config['backend'], dist.get_world_size()\n",
    "            )\n",
    "            + \"Current host rank is {}. Number of gpus: {}\".format(dist.get_rank(), config['num_gpus'])\n",
    "        )\n",
    "    # ---------------------------------------\n",
    "    \n",
    "    # split the data\n",
    "    if config['include_split']: split_data(config['random_state'], config['data_directory'], config['data_file'])\n",
    "\n",
    "    # recuperate the tokenizer\n",
    "    tokenizer = T5TokenizerFast(config['tokenizer_path'])\n",
    "    \n",
    "    # Initialize the model name\n",
    "    model_name = 't5-small'\n",
    "\n",
    "    # import the model with its pre-trained weights\n",
    "    model = T5ForConditionalGeneration.from_pretrained(model_name)\n",
    "\n",
    "    # resize the token embeddings\n",
    "    model.resize_token_embeddings(len(tokenizer))\n",
    "    \n",
    "    # recuperate train and test set\n",
    "    train_dataset, test_dataset = recuperate_datasets(config['char_p'],\n",
    "                                                        config['word_p'], config['max_len'],\n",
    "                                                        config['end_mark'], tokenizer, config['corpus_1'],\n",
    "                                                        config['corpus_2'],\n",
    "                                                        config['train_file'], config['test_file'])\n",
    "    \n",
    "    # initialize the evaluation object\n",
    "    evaluation = TranslationEvaluation(tokenizer, train_dataset.decode)\n",
    "\n",
    "    # let us initialize the trainer\n",
    "    trainer = ModelRunner(model = model, version=config['version'], seed = 0, evaluation = evaluation, optimizer = Adafactor)\n",
    "\n",
    "    #-------------------------------------\n",
    "    # in the case when the linear learning rate scheduler with warmup is used\n",
    "    \n",
    "    # let us calculate the appropriate warmup steps (let us take a max epoch of 100)\n",
    "    # length = len(train_dataset)\n",
    "\n",
    "    # n_steps = length // config['batch_size']\n",
    "\n",
    "    # num_steps = config['max_epoch'] * n_steps\n",
    "\n",
    "    # warmup_steps = (config['max_epoch'] * n_steps) * config['warmup_ratio']\n",
    "\n",
    "    # Initialize the scheduler parameters\n",
    "    # scheduler_args = {'num_warmup_steps': warmup_steps, 'num_training_steps': num_steps}\n",
    "    #-------------------------------------\n",
    "\n",
    "    # Initialize the optimizer parameters\n",
    "    optimizer_args = {\n",
    "        'lr': config['learning_rate'],\n",
    "        'weight_decay': config['weight_decay'],\n",
    "        # 'betas': (0.9, 0.98),\n",
    "        'warmup_init': config['warmup_init'],\n",
    "        'relative_step': config['relative_step']\n",
    "    }\n",
    "\n",
    "    # ----------------------------\n",
    "    # initialize the bucket samplers for distributed environment\n",
    "    boundaries = config['boundaries']\n",
    "    batch_sizes = config['batch_sizes']\n",
    "\n",
    "    train_sampler = SequenceLengthBatchSampler(train_dataset,\n",
    "                                                boundaries = boundaries,\n",
    "                                                batch_sizes = batch_sizes)\n",
    "\n",
    "    test_sampler = SequenceLengthBatchSampler(test_dataset,\n",
    "                                                boundaries = boundaries,\n",
    "                                                batch_sizes = batch_sizes)\n",
    "\n",
    "    # ------------------------------\n",
    "    # initialize a bucket sampler with fixed batch size in the case of single machine\n",
    "    # with parallelization on multiple gpus\n",
    "    # train_sampler = BucketSampler(train_dataset, config['batch_size'])\n",
    "\n",
    "    # test_sampler = BucketSampler(test_dataset, config['batch_size'])\n",
    "    \n",
    "    # ------------------------------\n",
    "\n",
    "    # Initialize the loaders parameters\n",
    "    train_loader_args = {'batch_sampler': train_sampler, 'collate_fn': collate_fn,\n",
    "                        'num_workers': config['num_workers'], 'pin_memory': config['pin_memory']}\n",
    "\n",
    "    test_loader_args = {'batch_sampler': test_sampler, 'collate_fn': collate_fn,\n",
    "                        'num_workers': config['num_workers'], 'pin_memory': config['pin_memory']}\n",
    "\n",
    "    # Add the datasets and hyperparameters to trainer\n",
    "    trainer.compile(train_dataset, test_dataset, tokenizer, train_loader_args,\n",
    "                    test_loader_args, optimizer_kwargs = optimizer_args,\n",
    "                    # lr_scheduler=get_linear_schedule_with_warmup,\n",
    "                    # lr_scheduler_kwargs=scheduler_args,\n",
    "                    predict_with_generate = True,\n",
    "                    hugging_face = True,\n",
    "                    is_distributed=is_distributed,\n",
    "                    logging_dir=config['logging_dir'],\n",
    "                    dist=dist\n",
    "                    )\n",
    "\n",
    "    # load the model\n",
    "    trainer.load(config['model_dir'], load_best = not config['continue'])\n",
    "    \n",
    "    # Train the model\n",
    "    trainer.train(config['epochs'] - trainer.current_epoch, auto_save = True, log_step = config['log_step'], saving_directory=config['new_model_dir'], save_best = config['save_best'],\n",
    "                  metric_for_best_model = config['metric_for_best_model'], metric_objective = config['metric_objective'])\n",
    "    \n",
    "    if config['return_trainer']:\n",
    "        \n",
    "        return trainer\n",
    "    \n",
    "    return None\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Below train and save if we want."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "from wolof_translate.utils.hg_training import train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/25 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "For epoch 6: \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train batch number 2:   0%|          | 0/32 [00:00<?, ?batches/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train batch number 33: 100%|██████████| 32/32 [00:04<00:00,  6.64batches/s]\n",
      "Test batch number 2:   0%|          | 0/7 [00:00<?, ?batches/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Test batch number 8: 100%|██████████| 7/7 [00:03<00:00,  1.75batches/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Metrics: {'train_loss': 3.025106288450267, 'test_loss': 3.0578086183528708, 'bleu': 1.330838383838384, 'gen_len': 19.964669191919192}\n",
      "\n",
      "=============================\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  4%|▍         | 1/25 [00:09<03:43,  9.31s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "For epoch 7: \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train batch number 2:   0%|          | 0/32 [00:00<?, ?batches/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train batch number 33: 100%|██████████| 32/32 [00:04<00:00,  6.60batches/s]\n",
      "Test batch number 2:   0%|          | 0/7 [00:00<?, ?batches/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Test batch number 8: 100%|██████████| 7/7 [00:03<00:00,  1.96batches/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Metrics: {'train_loss': 2.888327236344436, 'test_loss': 2.932071378736785, 'bleu': 1.3087666666666669, 'gen_len': 16.136386363636365}\n",
      "\n",
      "=============================\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  8%|▊         | 2/25 [00:19<03:49,  9.96s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "For epoch 8: \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train batch number 2:   0%|          | 0/32 [00:00<?, ?batches/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train batch number 33: 100%|██████████| 32/32 [00:04<00:00,  6.56batches/s]\n",
      "Test batch number 2:   0%|          | 0/7 [00:00<?, ?batches/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Test batch number 8: 100%|██████████| 7/7 [00:03<00:00,  1.95batches/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Metrics: {'train_loss': 2.7692004121759877, 'test_loss': 2.849424434430672, 'bleu': 1.4679595959595961, 'gen_len': 16.43939494949495}\n",
      "\n",
      "=============================\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 12%|█▏        | 3/25 [00:30<03:44, 10.19s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "For epoch 9: \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train batch number 2:   0%|          | 0/32 [00:00<?, ?batches/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train batch number 33: 100%|██████████| 32/32 [00:04<00:00,  6.53batches/s]\n",
      "Test batch number 2:   0%|          | 0/7 [00:00<?, ?batches/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Test batch number 8: 100%|██████████| 7/7 [00:03<00:00,  1.77batches/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Metrics: {'train_loss': 2.663334673526241, 'test_loss': 2.788634987792584, 'bleu': 1.5415909090909092, 'gen_len': 16.055548484848487}\n",
      "\n",
      "=============================\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 16%|█▌        | 4/25 [00:41<03:39, 10.45s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "For epoch 10: \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train batch number 2:   0%|          | 0/32 [00:00<?, ?batches/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train batch number 33: 100%|██████████| 32/32 [00:04<00:00,  6.53batches/s]\n",
      "Test batch number 2:   0%|          | 0/7 [00:00<?, ?batches/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Test batch number 8: 100%|██████████| 7/7 [00:04<00:00,  1.74batches/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Metrics: {'train_loss': 2.5818608457117724, 'test_loss': 2.7095980692391444, 'bleu': 1.8785469696969699, 'gen_len': 13.808072222222222}\n",
      "\n",
      "=============================\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 20%|██        | 5/25 [00:51<03:32, 10.62s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "For epoch 11: \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train batch number 2:   0%|          | 0/32 [00:00<?, ?batches/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train batch number 33: 100%|██████████| 32/32 [00:04<00:00,  6.60batches/s]\n",
      "Test batch number 2:   0%|          | 0/7 [00:00<?, ?batches/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Test batch number 8: 100%|██████████| 7/7 [00:04<00:00,  1.73batches/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Metrics: {'train_loss': 2.4917865036259266, 'test_loss': 2.6703582159196495, 'bleu': 2.21769898989899, 'gen_len': 15.121238383838381}\n",
      "\n",
      "=============================\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 24%|██▍       | 6/25 [01:02<03:23, 10.71s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "For epoch 12: \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train batch number 2:   0%|          | 0/32 [00:00<?, ?batches/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train batch number 33: 100%|██████████| 32/32 [00:04<00:00,  6.59batches/s]\n",
      "Test batch number 2:   0%|          | 0/7 [00:00<?, ?batches/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Test batch number 8: 100%|██████████| 7/7 [00:03<00:00,  1.82batches/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Metrics: {'train_loss': 2.4179942812970907, 'test_loss': 2.6327337351712314, 'bleu': 2.2543474747474748, 'gen_len': 16.212137373737374}\n",
      "\n",
      "=============================\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 28%|██▊       | 7/25 [01:13<03:12, 10.70s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "For epoch 13: \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train batch number 2:   0%|          | 0/32 [00:00<?, ?batches/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train batch number 33: 100%|██████████| 32/32 [00:04<00:00,  6.58batches/s]\n",
      "Test batch number 2:   0%|          | 0/7 [00:00<?, ?batches/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Test batch number 8: 100%|██████████| 7/7 [00:02<00:00,  2.44batches/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Metrics: {'train_loss': 2.362779219643667, 'test_loss': 2.6032126094355728, 'bleu': 2.085411616161616, 'gen_len': 12.797957575757577}\n",
      "\n",
      "=============================\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 32%|███▏      | 8/25 [01:23<02:56, 10.40s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "For epoch 14: \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train batch number 2:   0%|          | 0/32 [00:00<?, ?batches/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train batch number 33: 100%|██████████| 32/32 [00:04<00:00,  6.43batches/s]\n",
      "Test batch number 2:   0%|          | 0/7 [00:00<?, ?batches/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Test batch number 8: 100%|██████████| 7/7 [00:03<00:00,  1.75batches/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Metrics: {'train_loss': 2.292300087677235, 'test_loss': 2.561257794649914, 'bleu': 2.3605954545454546, 'gen_len': 15.70708686868687}\n",
      "\n",
      "=============================\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 36%|███▌      | 9/25 [01:34<02:49, 10.58s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "For epoch 15: \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train batch number 2:   0%|          | 0/32 [00:00<?, ?batches/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train batch number 33: 100%|██████████| 32/32 [00:04<00:00,  6.51batches/s]\n",
      "Test batch number 2:   0%|          | 0/7 [00:00<?, ?batches/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Test batch number 8: 100%|██████████| 7/7 [00:03<00:00,  1.86batches/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Metrics: {'train_loss': 2.2410037018391695, 'test_loss': 2.5626361875823047, 'bleu': 2.6147292929292933, 'gen_len': 15.65151666666667}\n",
      "\n",
      "=============================\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 40%|████      | 10/25 [01:43<02:33, 10.23s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "For epoch 16: \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train batch number 2:   0%|          | 0/32 [00:00<?, ?batches/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train batch number 33: 100%|██████████| 32/32 [00:04<00:00,  6.47batches/s]\n",
      "Test batch number 2:   0%|          | 0/7 [00:00<?, ?batches/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Test batch number 8: 100%|██████████| 7/7 [00:03<00:00,  2.07batches/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Metrics: {'train_loss': 2.1965482313372076, 'test_loss': 2.527354256071226, 'bleu': 2.484096969696969, 'gen_len': 13.2525}\n",
      "\n",
      "=============================\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 44%|████▍     | 11/25 [01:54<02:23, 10.26s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "For epoch 17: \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train batch number 2:   0%|          | 0/32 [00:00<?, ?batches/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train batch number 33: 100%|██████████| 32/32 [00:04<00:00,  6.45batches/s]\n",
      "Test batch number 2:   0%|          | 0/7 [00:00<?, ?batches/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Test batch number 8: 100%|██████████| 7/7 [00:03<00:00,  1.93batches/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Metrics: {'train_loss': 2.1407281559226945, 'test_loss': 2.497628905556419, 'bleu': 2.9381095959595958, 'gen_len': 14.272710606060606}\n",
      "\n",
      "=============================\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 48%|████▊     | 12/25 [02:04<02:14, 10.36s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "For epoch 18: \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train batch number 2:   0%|          | 0/32 [00:00<?, ?batches/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train batch number 33: 100%|██████████| 32/32 [00:04<00:00,  6.53batches/s]\n",
      "Test batch number 2:   0%|          | 0/7 [00:00<?, ?batches/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Test batch number 8: 100%|██████████| 7/7 [00:03<00:00,  1.76batches/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Metrics: {'train_loss': 2.0784834201844107, 'test_loss': 2.497836967911384, 'bleu': 2.899290404040405, 'gen_len': 15.631311111111112}\n",
      "\n",
      "=============================\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 52%|█████▏    | 13/25 [02:14<02:01, 10.14s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "For epoch 19: \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train batch number 2:   0%|          | 0/32 [00:00<?, ?batches/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train batch number 33: 100%|██████████| 32/32 [00:04<00:00,  6.49batches/s]\n",
      "Test batch number 2:   0%|          | 0/7 [00:00<?, ?batches/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Test batch number 8: 100%|██████████| 7/7 [00:03<00:00,  1.89batches/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Metrics: {'train_loss': 2.05341728807325, 'test_loss': 2.481565406828216, 'bleu': 2.6675671717171716, 'gen_len': 16.57071616161616}\n",
      "\n",
      "=============================\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 56%|█████▌    | 14/25 [02:24<01:53, 10.28s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "For epoch 20: \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train batch number 2:   0%|          | 0/32 [00:00<?, ?batches/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train batch number 33: 100%|██████████| 32/32 [00:04<00:00,  6.41batches/s]\n",
      "Test batch number 2:   0%|          | 0/7 [00:00<?, ?batches/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Test batch number 8: 100%|██████████| 7/7 [00:04<00:00,  1.68batches/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Metrics: {'train_loss': 2.002027263300544, 'test_loss': 2.461949396615077, 'bleu': 3.311483333333334, 'gen_len': 15.63637171717172}\n",
      "\n",
      "=============================\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 60%|██████    | 15/25 [02:36<01:45, 10.55s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "For epoch 21: \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train batch number 2:   0%|          | 0/32 [00:00<?, ?batches/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train batch number 33: 100%|██████████| 32/32 [00:04<00:00,  6.41batches/s]\n",
      "Test batch number 2:   0%|          | 0/7 [00:00<?, ?batches/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Test batch number 8: 100%|██████████| 7/7 [00:03<00:00,  1.78batches/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Metrics: {'train_loss': 1.9573933808280268, 'test_loss': 2.440742301218437, 'bleu': 3.6045080808080816, 'gen_len': 16.005034343434343}\n",
      "\n",
      "=============================\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 64%|██████▍   | 16/25 [02:47<01:36, 10.74s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "For epoch 22: \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train batch number 2:   0%|          | 0/32 [00:00<?, ?batches/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train batch number 33: 100%|██████████| 32/32 [00:04<00:00,  6.43batches/s]\n",
      "Test batch number 2:   0%|          | 0/7 [00:00<?, ?batches/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Test batch number 8: 100%|██████████| 7/7 [00:03<00:00,  1.82batches/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Metrics: {'train_loss': 1.9155979083354977, 'test_loss': 2.457561742175709, 'bleu': 4.930958080808081, 'gen_len': 16.479829292929296}\n",
      "\n",
      "=============================\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 68%|██████▊   | 17/25 [02:56<01:23, 10.40s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "For epoch 23: \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train batch number 2:   0%|          | 0/32 [00:00<?, ?batches/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train batch number 33: 100%|██████████| 32/32 [00:05<00:00,  6.38batches/s]\n",
      "Test batch number 2:   0%|          | 0/7 [00:00<?, ?batches/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Test batch number 8: 100%|██████████| 7/7 [00:03<00:00,  1.87batches/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Metrics: {'train_loss': 1.8723649379952205, 'test_loss': 2.454600731531779, 'bleu': 4.282234343434344, 'gen_len': 15.005060101010102}\n",
      "\n",
      "=============================\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 72%|███████▏  | 18/25 [03:06<01:11, 10.16s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "For epoch 24: \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train batch number 2:   0%|          | 0/32 [00:00<?, ?batches/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train batch number 33: 100%|██████████| 32/32 [00:04<00:00,  6.52batches/s]\n",
      "Test batch number 2:   0%|          | 0/7 [00:00<?, ?batches/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Test batch number 8: 100%|██████████| 7/7 [00:03<00:00,  1.75batches/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Metrics: {'train_loss': 1.8316938165318433, 'test_loss': 2.439377421080464, 'bleu': 4.814183333333334, 'gen_len': 16.166674242424243}\n",
      "\n",
      "=============================\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 76%|███████▌  | 19/25 [03:17<01:02, 10.38s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "For epoch 25: \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train batch number 2:   0%|          | 0/32 [00:00<?, ?batches/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train batch number 33: 100%|██████████| 32/32 [00:04<00:00,  6.48batches/s]\n",
      "Test batch number 2:   0%|          | 0/7 [00:00<?, ?batches/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Test batch number 8: 100%|██████████| 7/7 [00:04<00:00,  1.73batches/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Metrics: {'train_loss': 1.7985615124814047, 'test_loss': 2.456443261618566, 'bleu': 4.883823232323234, 'gen_len': 16.010118686868687}\n",
      "\n",
      "=============================\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 80%|████████  | 20/25 [03:27<00:51, 10.22s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "For epoch 26: \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train batch number 2:   0%|          | 0/32 [00:00<?, ?batches/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train batch number 33: 100%|██████████| 32/32 [00:04<00:00,  6.44batches/s]\n",
      "Test batch number 2:   0%|          | 0/7 [00:00<?, ?batches/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Test batch number 8: 100%|██████████| 7/7 [00:03<00:00,  1.79batches/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Metrics: {'train_loss': 1.7628219490334776, 'test_loss': 2.4467217958334717, 'bleu': 4.541628787878789, 'gen_len': 16.626272222222223}\n",
      "\n",
      "=============================\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 84%|████████▍ | 21/25 [03:36<00:40, 10.04s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "For epoch 27: \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train batch number 2:   0%|          | 0/32 [00:00<?, ?batches/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train batch number 33: 100%|██████████| 32/32 [00:04<00:00,  6.44batches/s]\n",
      "Test batch number 2:   0%|          | 0/7 [00:00<?, ?batches/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Test batch number 8: 100%|██████████| 7/7 [00:03<00:00,  2.14batches/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Metrics: {'train_loss': 1.7351601044638563, 'test_loss': 2.4418391073592987, 'bleu': 4.809811616161616, 'gen_len': 13.74244191919192}\n",
      "\n",
      "=============================\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 88%|████████▊ | 22/25 [03:45<00:29,  9.72s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "For epoch 28: \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train batch number 2:   0%|          | 0/32 [00:00<?, ?batches/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train batch number 33: 100%|██████████| 32/32 [00:04<00:00,  6.43batches/s]\n",
      "Test batch number 2:   0%|          | 0/7 [00:00<?, ?batches/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Test batch number 8: 100%|██████████| 7/7 [00:03<00:00,  1.84batches/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Metrics: {'train_loss': 1.6943547316249008, 'test_loss': 2.4684393875526665, 'bleu': 4.866994444444445, 'gen_len': 15.510115151515153}\n",
      "\n",
      "=============================\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 92%|█████████▏| 23/25 [03:55<00:19,  9.65s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "For epoch 29: \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train batch number 2:   0%|          | 0/32 [00:00<?, ?batches/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train batch number 33: 100%|██████████| 32/32 [00:04<00:00,  6.45batches/s]\n",
      "Test batch number 2:   0%|          | 0/7 [00:00<?, ?batches/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Test batch number 8: 100%|██████████| 7/7 [00:03<00:00,  1.76batches/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Metrics: {'train_loss': 1.6575911842065094, 'test_loss': 2.433619892958439, 'bleu': 5.274884848484849, 'gen_len': 15.570673232323234}\n",
      "\n",
      "=============================\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 96%|█████████▌| 24/25 [04:06<00:10, 10.04s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "For epoch 30: \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train batch number 2:   0%|          | 0/32 [00:00<?, ?batches/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train batch number 33: 100%|██████████| 32/32 [00:04<00:00,  6.44batches/s]\n",
      "Test batch number 2:   0%|          | 0/7 [00:00<?, ?batches/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Test batch number 8: 100%|██████████| 7/7 [00:03<00:00,  1.98batches/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Metrics: {'train_loss': 1.6241250370666247, 'test_loss': 2.4374969884602713, 'bleu': 4.865085353535354, 'gen_len': 15.782837878787882}\n",
      "\n",
      "=============================\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 25/25 [04:15<00:00, 10.22s/it]\n"
     ]
    }
   ],
   "source": [
    "# with warnings.catch_warnings():\n",
    "    # warnings.simplefilter(\"ignore\")\n",
    "trainer = train(config)\n",
    "\n",
    "# save if necessary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/20 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "For epoch 31: \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train batch number 2:   0%|          | 0/33 [00:00<?, ?batches/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train batch number 34: 100%|██████████| 33/33 [00:04<00:00,  6.74batches/s]\n",
      "Test batch number 2:   0%|          | 0/7 [00:00<?, ?batches/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Test batch number 8: 100%|██████████| 7/7 [00:03<00:00,  1.99batches/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Metrics: {'train_loss': 1.6031698549495028, 'test_loss': 2.48018310286782, 'bleu': 5.082866161616162, 'gen_len': 14.929275757575759}\n",
      "\n",
      "=============================\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  5%|▌         | 1/20 [00:09<02:54,  9.20s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "For epoch 32: \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train batch number 2:   0%|          | 0/33 [00:00<?, ?batches/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train batch number 34: 100%|██████████| 33/33 [00:04<00:00,  6.66batches/s]\n",
      "Test batch number 2:   0%|          | 0/7 [00:00<?, ?batches/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Test batch number 8: 100%|██████████| 7/7 [00:03<00:00,  1.83batches/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Metrics: {'train_loss': 1.56718063592006, 'test_loss': 2.4559369737451733, 'bleu': 5.706334848484849, 'gen_len': 15.72220505050505}\n",
      "\n",
      "=============================\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 10%|█         | 2/20 [00:18<02:50,  9.47s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "For epoch 33: \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train batch number 2:   0%|          | 0/33 [00:00<?, ?batches/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train batch number 34: 100%|██████████| 33/33 [00:04<00:00,  6.62batches/s]\n",
      "Test batch number 2:   0%|          | 0/7 [00:00<?, ?batches/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Test batch number 8: 100%|██████████| 7/7 [00:03<00:00,  1.79batches/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Metrics: {'train_loss': 1.5363509753774343, 'test_loss': 2.462271097934608, 'bleu': 4.999496464646464, 'gen_len': 16.78284292929293}\n",
      "\n",
      "=============================\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 15%|█▌        | 3/20 [00:28<02:42,  9.58s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "For epoch 34: \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train batch number 2:   0%|          | 0/33 [00:00<?, ?batches/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train batch number 34: 100%|██████████| 33/33 [00:04<00:00,  6.68batches/s]\n",
      "Test batch number 2:   0%|          | 0/7 [00:00<?, ?batches/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Test batch number 8: 100%|██████████| 7/7 [00:03<00:00,  1.76batches/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Metrics: {'train_loss': 1.4993085373171824, 'test_loss': 2.4647206417237872, 'bleu': 5.2097772727272735, 'gen_len': 16.76260101010101}\n",
      "\n",
      "=============================\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 20%|██        | 4/20 [00:38<02:33,  9.62s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "For epoch 35: \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train batch number 2:   0%|          | 0/33 [00:00<?, ?batches/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train batch number 34: 100%|██████████| 33/33 [00:04<00:00,  6.65batches/s]\n",
      "Test batch number 2:   0%|          | 0/7 [00:00<?, ?batches/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Test batch number 8: 100%|██████████| 7/7 [00:03<00:00,  2.18batches/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Metrics: {'train_loss': 1.4714344369505565, 'test_loss': 2.52563930039454, 'bleu': 5.206471717171718, 'gen_len': 13.84849494949495}\n",
      "\n",
      "=============================\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 25%|██▌       | 5/20 [00:47<02:20,  9.37s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "For epoch 36: \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train batch number 2:   0%|          | 0/33 [00:00<?, ?batches/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train batch number 34: 100%|██████████| 33/33 [00:04<00:00,  6.63batches/s]\n",
      "Test batch number 2:   0%|          | 0/7 [00:00<?, ?batches/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Test batch number 8: 100%|██████████| 7/7 [00:03<00:00,  1.97batches/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Metrics: {'train_loss': 1.4317455306828286, 'test_loss': 2.4958050130593654, 'bleu': 4.841671717171717, 'gen_len': 14.994919191919193}\n",
      "\n",
      "=============================\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 30%|███       | 6/20 [00:56<02:11,  9.39s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "For epoch 37: \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train batch number 2:   0%|          | 0/33 [00:00<?, ?batches/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train batch number 34: 100%|██████████| 33/33 [00:04<00:00,  6.71batches/s]\n",
      "Test batch number 2:   0%|          | 0/7 [00:00<?, ?batches/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Test batch number 8: 100%|██████████| 7/7 [00:04<00:00,  1.71batches/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Metrics: {'train_loss': 1.4115574834544322, 'test_loss': 2.4901456832885747, 'bleu': 5.709369696969697, 'gen_len': 16.217164646464646}\n",
      "\n",
      "=============================\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 35%|███▌      | 7/20 [01:06<02:04,  9.55s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "For epoch 38: \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train batch number 2:   0%|          | 0/33 [00:00<?, ?batches/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train batch number 34: 100%|██████████| 33/33 [00:05<00:00,  6.58batches/s]\n",
      "Test batch number 2:   0%|          | 0/7 [00:00<?, ?batches/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Test batch number 8: 100%|██████████| 7/7 [00:03<00:00,  1.84batches/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Metrics: {'train_loss': 1.3803954382134265, 'test_loss': 2.5174373015008786, 'bleu': 5.082258080808081, 'gen_len': 14.899022222222225}\n",
      "\n",
      "=============================\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 40%|████      | 8/20 [01:16<01:55,  9.59s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "For epoch 39: \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train batch number 2:   0%|          | 0/33 [00:00<?, ?batches/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train batch number 34: 100%|██████████| 33/33 [00:04<00:00,  6.64batches/s]\n",
      "Test batch number 2:   0%|          | 0/7 [00:00<?, ?batches/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Test batch number 8: 100%|██████████| 7/7 [00:03<00:00,  2.33batches/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Metrics: {'train_loss': 1.3583325295603628, 'test_loss': 2.531702978442414, 'bleu': 5.318264646464646, 'gen_len': 12.757601515151515}\n",
      "\n",
      "=============================\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 45%|████▌     | 9/20 [01:24<01:42,  9.35s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "For epoch 40: \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train batch number 2:   0%|          | 0/33 [00:00<?, ?batches/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train batch number 34: 100%|██████████| 33/33 [00:05<00:00,  6.52batches/s]\n",
      "Test batch number 2:   0%|          | 0/7 [00:00<?, ?batches/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Test batch number 8: 100%|██████████| 7/7 [00:03<00:00,  1.91batches/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Metrics: {'train_loss': 1.3195869691721784, 'test_loss': 2.5422805367094097, 'bleu': 5.3666823232323235, 'gen_len': 15.166634848484849}\n",
      "\n",
      "=============================\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 50%|█████     | 10/20 [01:34<01:34,  9.43s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "For epoch 41: \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train batch number 2:   0%|          | 0/33 [00:00<?, ?batches/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train batch number 34: 100%|██████████| 33/33 [00:04<00:00,  6.67batches/s]\n",
      "Test batch number 2:   0%|          | 0/7 [00:00<?, ?batches/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Test batch number 8: 100%|██████████| 7/7 [00:03<00:00,  1.86batches/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Metrics: {'train_loss': 1.297868958158934, 'test_loss': 2.5708903038140503, 'bleu': 4.87779494949495, 'gen_len': 15.833323232323233}\n",
      "\n",
      "=============================\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 55%|█████▌    | 11/20 [01:44<01:25,  9.45s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "For epoch 42: \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train batch number 2:   0%|          | 0/33 [00:00<?, ?batches/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train batch number 34: 100%|██████████| 33/33 [00:04<00:00,  6.62batches/s]\n",
      "Test batch number 2:   0%|          | 0/7 [00:00<?, ?batches/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Test batch number 8: 100%|██████████| 7/7 [00:03<00:00,  1.88batches/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Metrics: {'train_loss': 1.2691234390449861, 'test_loss': 2.5951932126825508, 'bleu': 5.3977151515151505, 'gen_len': 15.232341414141414}\n",
      "\n",
      "=============================\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 60%|██████    | 12/20 [01:53<01:15,  9.48s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "For epoch 43: \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train batch number 2:   0%|          | 0/33 [00:00<?, ?batches/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train batch number 34: 100%|██████████| 33/33 [00:05<00:00,  6.54batches/s]\n",
      "Test batch number 2:   0%|          | 0/7 [00:00<?, ?batches/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Test batch number 8: 100%|██████████| 7/7 [00:03<00:00,  1.87batches/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Metrics: {'train_loss': 1.2421140144853033, 'test_loss': 2.590024545939282, 'bleu': 6.711281818181818, 'gen_len': 13.772703535353537}\n",
      "\n",
      "=============================\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 65%|██████▌   | 13/20 [02:03<01:06,  9.52s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "For epoch 44: \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train batch number 2:   0%|          | 0/33 [00:00<?, ?batches/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train batch number 34: 100%|██████████| 33/33 [00:04<00:00,  6.61batches/s]\n",
      "Test batch number 2:   0%|          | 0/7 [00:00<?, ?batches/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Test batch number 8: 100%|██████████| 7/7 [00:03<00:00,  1.78batches/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Metrics: {'train_loss': 1.2233287424018762, 'test_loss': 2.5700962663900975, 'bleu': 5.34880303030303, 'gen_len': 16.36870303030303}\n",
      "\n",
      "=============================\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 70%|███████   | 14/20 [02:12<00:57,  9.59s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "For epoch 45: \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train batch number 2:   0%|          | 0/33 [00:00<?, ?batches/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train batch number 34: 100%|██████████| 33/33 [00:05<00:00,  6.53batches/s]\n",
      "Test batch number 2:   0%|          | 0/7 [00:00<?, ?batches/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Test batch number 8: 100%|██████████| 7/7 [00:03<00:00,  1.81batches/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Metrics: {'train_loss': 1.2071754128061318, 'test_loss': 2.6190930038991604, 'bleu': 5.325007070707071, 'gen_len': 15.06563181818182}\n",
      "\n",
      "=============================\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 75%|███████▌  | 15/20 [02:22<00:48,  9.62s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "For epoch 46: \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train batch number 2:   0%|          | 0/33 [00:00<?, ?batches/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train batch number 34: 100%|██████████| 33/33 [00:05<00:00,  6.56batches/s]\n",
      "Test batch number 2:   0%|          | 0/7 [00:00<?, ?batches/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Test batch number 8: 100%|██████████| 7/7 [00:03<00:00,  1.79batches/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Metrics: {'train_loss': 1.167711973529613, 'test_loss': 2.5440620123737996, 'bleu': 6.071860606060607, 'gen_len': 15.292952525252526}\n",
      "\n",
      "=============================\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 80%|████████  | 16/20 [02:32<00:38,  9.66s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "For epoch 47: \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train batch number 2:   0%|          | 0/33 [00:00<?, ?batches/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train batch number 34: 100%|██████████| 33/33 [00:05<00:00,  6.55batches/s]\n",
      "Test batch number 2:   0%|          | 0/7 [00:00<?, ?batches/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Test batch number 8: 100%|██████████| 7/7 [00:03<00:00,  1.83batches/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Metrics: {'train_loss': 1.151745243541128, 'test_loss': 2.610427420548719, 'bleu': 6.87779696969697, 'gen_len': 14.500006060606061}\n",
      "\n",
      "=============================\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 85%|████████▌ | 17/20 [02:42<00:28,  9.65s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "For epoch 48: \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train batch number 2:   0%|          | 0/33 [00:00<?, ?batches/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train batch number 34: 100%|██████████| 33/33 [00:05<00:00,  6.51batches/s]\n",
      "Test batch number 2:   0%|          | 0/7 [00:00<?, ?batches/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Test batch number 8: 100%|██████████| 7/7 [00:03<00:00,  1.76batches/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Metrics: {'train_loss': 1.1192170883152472, 'test_loss': 2.5948631040977714, 'bleu': 5.628589393939393, 'gen_len': 16.36867777777778}\n",
      "\n",
      "=============================\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 90%|█████████ | 18/20 [02:51<00:19,  9.70s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "For epoch 49: \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train batch number 2:   0%|          | 0/33 [00:00<?, ?batches/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train batch number 34: 100%|██████████| 33/33 [00:05<00:00,  6.54batches/s]\n",
      "Test batch number 2:   0%|          | 0/7 [00:00<?, ?batches/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Test batch number 8: 100%|██████████| 7/7 [00:03<00:00,  2.09batches/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Metrics: {'train_loss': 1.0961309396564627, 'test_loss': 2.650956584949686, 'bleu': 7.354377777777779, 'gen_len': 14.343432323232324}\n",
      "\n",
      "=============================\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 95%|█████████▌| 19/20 [03:01<00:09,  9.55s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "For epoch 50: \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train batch number 2:   0%|          | 0/33 [00:00<?, ?batches/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train batch number 34: 100%|██████████| 33/33 [00:05<00:00,  6.53batches/s]\n",
      "Test batch number 2:   0%|          | 0/7 [00:00<?, ?batches/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Test batch number 8: 100%|██████████| 7/7 [00:03<00:00,  1.94batches/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Metrics: {'train_loss': 1.0732805997670491, 'test_loss': 2.6258159791580358, 'bleu': 6.835085353535354, 'gen_len': 14.914118686868687}\n",
      "\n",
      "=============================\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 20/20 [03:10<00:00,  9.53s/it]\n"
     ]
    }
   ],
   "source": [
    "# with warnings.catch_warnings():\n",
    "    # warnings.simplefilter(\"ignore\")\n",
    "trainer = train(config)\n",
    "\n",
    "# save if necessary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0it [00:00, ?it/s]\n"
     ]
    }
   ],
   "source": [
    "# with warnings.catch_warnings():\n",
    "    # warnings.simplefilter(\"ignore\")\n",
    "trainer = train(config)\n",
    "\n",
    "# save if necessary"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "➡️ Predictions\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluation batch number 2:   0%|          | 0/6 [00:00<?, ?batches/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluation batch number 7: 100%|██████████| 6/6 [00:03<00:00,  1.73batches/s]\n"
     ]
    }
   ],
   "source": [
    "if not trainer is None:\n",
    "    \n",
    "    # recuperate the tokenizer\n",
    "    tokenizer = T5TokenizerFast(config['tokenizer_path'])\n",
    "    \n",
    "    # recuperate the test dataset\n",
    "    # initialize the transformation sequence\n",
    "    end_mark_fn = partial(add_end_mark)\n",
    "    augmentation = TransformerSequences(remove_mark_space, delete_guillemet_space, add_mark_space, end_mark_fn)\n",
    "\n",
    "\n",
    "    # let us get the test set\n",
    "    test_dataset = SentenceDataset(f\"{config['data_directory']}test_set.csv\",\n",
    "                                            tokenizer = tokenizer,\n",
    "                                            cp1_transformer = augmentation,\n",
    "                                            cp2_transformer = augmentation,\n",
    "                                            corpus_1=config['corpus_1'],\n",
    "                                            corpus_2=config['corpus_2'],\n",
    "                                            truncation = False)\n",
    "\n",
    "    # initialize the bucket samplers for distributed environment\n",
    "    boundaries = config['boundaries']\n",
    "    batch_sizes = config['batch_sizes']\n",
    "\n",
    "    test_sampler = SequenceLengthBatchSampler(test_dataset,\n",
    "                                                boundaries = boundaries,\n",
    "                                                batch_sizes = batch_sizes)\n",
    "\n",
    "    test_loader_args = {'batch_sampler': test_sampler, 'collate_fn': collate_fn,\n",
    "                            'num_workers': config['num_workers'], 'pin_memory': config['pin_memory']}\n",
    "\n",
    "    metrics, prediction = trainer.evaluate(test_dataset, test_loader_args)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'test_loss': 2.1652301561952845,\n",
       " 'bleu': 4.696123737373738,\n",
       " 'gen_len': 10.767653535353533}"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>original_sentences</th>\n",
       "      <th>translations</th>\n",
       "      <th>predictions</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Góor gi dem na ci biir.</td>\n",
       "      <td>L'homme est allé à l'intérieur.</td>\n",
       "      <td>Il est parti.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Ana kooku.</td>\n",
       "      <td>Où est celui-là?</td>\n",
       "      <td>Où est le maître.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Gis naa xale ba.</td>\n",
       "      <td>J'ai vu l'enfant.</td>\n",
       "      <td>J'ai vu.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Deŋkël lëf li kenn ki!</td>\n",
       "      <td>Confie la chose à l'un!</td>\n",
       "      <td>Il est là-bas, là-bas,</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Sama aw xarit!</td>\n",
       "      <td>Un ami à moi!</td>\n",
       "      <td>C'est celui-là!</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>193</th>\n",
       "      <td>Waaw lii nag ay néegi ñax la, néegi ñax bi ci ...</td>\n",
       "      <td>Ceux-là, cependant, sont des cases. Celle qui ...</td>\n",
       "      <td>Je suis en gris, d'autres d'autres d'autres d...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>194</th>\n",
       "      <td>Ñu gis ci nataal bi ay nit ñu bari ñu génn ci ...</td>\n",
       "      <td>On voit sur la photo beaucoup de personnes sor...</td>\n",
       "      <td>Ceux-ci sont de vieilles pourr que tu allais ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>195</th>\n",
       "      <td>Ñii moom tamit ay takk-der nañ. Ñi ñi ngi bàyy...</td>\n",
       "      <td>Ceux-là, aussi, sont des gendarmes. Ils siègen...</td>\n",
       "      <td>Ceci est une photo sur laquelle je vois un ch...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>196</th>\n",
       "      <td>Lii nag dañ ciy faral di def ndugg maanaam jig...</td>\n",
       "      <td>Ceci, cependant, on a l'habitude de faire les ...</td>\n",
       "      <td>Ceci est une théière, similaire au poisson sé...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>197</th>\n",
       "      <td>Ci biir nataal bii maa ngi ciy gis ay batã, ba...</td>\n",
       "      <td>À l'intérieur de cette photo je vois beaucoup ...</td>\n",
       "      <td>Sur cette photo ci, j'y vois des chambres ave...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>198 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                    original_sentences  \\\n",
       "0                              Góor gi dem na ci biir.   \n",
       "1                                           Ana kooku.   \n",
       "2                                     Gis naa xale ba.   \n",
       "3                               Deŋkël lëf li kenn ki!   \n",
       "4                                       Sama aw xarit!   \n",
       "..                                                 ...   \n",
       "193  Waaw lii nag ay néegi ñax la, néegi ñax bi ci ...   \n",
       "194  Ñu gis ci nataal bi ay nit ñu bari ñu génn ci ...   \n",
       "195  Ñii moom tamit ay takk-der nañ. Ñi ñi ngi bàyy...   \n",
       "196  Lii nag dañ ciy faral di def ndugg maanaam jig...   \n",
       "197  Ci biir nataal bii maa ngi ciy gis ay batã, ba...   \n",
       "\n",
       "                                          translations  \\\n",
       "0                      L'homme est allé à l'intérieur.   \n",
       "1                                     Où est celui-là?   \n",
       "2                                    J'ai vu l'enfant.   \n",
       "3                              Confie la chose à l'un!   \n",
       "4                                        Un ami à moi!   \n",
       "..                                                 ...   \n",
       "193  Ceux-là, cependant, sont des cases. Celle qui ...   \n",
       "194  On voit sur la photo beaucoup de personnes sor...   \n",
       "195  Ceux-là, aussi, sont des gendarmes. Ils siègen...   \n",
       "196  Ceci, cependant, on a l'habitude de faire les ...   \n",
       "197  À l'intérieur de cette photo je vois beaucoup ...   \n",
       "\n",
       "                                           predictions  \n",
       "0                                        Il est parti.  \n",
       "1                                    Où est le maître.  \n",
       "2                                             J'ai vu.  \n",
       "3                               Il est là-bas, là-bas,  \n",
       "4                                      C'est celui-là!  \n",
       "..                                                 ...  \n",
       "193   Je suis en gris, d'autres d'autres d'autres d...  \n",
       "194   Ceux-ci sont de vieilles pourr que tu allais ...  \n",
       "195   Ceci est une photo sur laquelle je vois un ch...  \n",
       "196   Ceci est une théière, similaire au poisson sé...  \n",
       "197   Sur cette photo ci, j'y vois des chambres ave...  \n",
       "\n",
       "[198 rows x 3 columns]"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prediction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-------------------------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### French-Wolof v6"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "➡️ Import the libraries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Global seed set to 0\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from wolof_translate import *\n",
    "\n",
    "# specify a seed for everything\n",
    "lt.seed_everything(0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "➡️ Function to recuperate datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting wolof-translate/wolof_translate/utils/recuperate_datasets.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile wolof-translate/wolof_translate/utils/recuperate_datasets.py\n",
    "from wolof_translate import *\n",
    "\n",
    "def recuperate_datasets(char_p: float, word_p: float, max_len: int, end_mark: int, tokenizer: T5TokenizerFast,\n",
    "                        corpus_1: str = 'french', corpus_2: str = 'wolof', \n",
    "                        train_file: str = 'data/extractions/new_data/train_set.csv', \n",
    "                        test_file: str = 'data/extractions/new_data/test_file.csv'):\n",
    "\n",
    "  # Let us recuperate the end_mark adding option\n",
    "  if end_mark == 1:\n",
    "    # Create augmentation to add on French sentences\n",
    "    fr_augmentation_1 = TransformerSequences(nac.KeyboardAug(aug_char_p=char_p, aug_word_p=word_p,\n",
    "                                                             aug_word_max = max_len),\n",
    "                                          remove_mark_space, delete_guillemet_space, add_mark_space)\n",
    "\n",
    "    fr_augmentation_2 = TransformerSequences(remove_mark_space, delete_guillemet_space, add_mark_space)\n",
    "    \n",
    "  else:\n",
    "    \n",
    "    if end_mark == 2:\n",
    "\n",
    "      end_mark_fn = partial(add_end_mark, end_mark_to_remove = '!', replace = True)\n",
    "    \n",
    "    elif end_mark == 3:\n",
    "\n",
    "      end_mark_fn = partial(add_end_mark)\n",
    "    \n",
    "    elif end_mark == 4:\n",
    "\n",
    "      end_mark_fn = partial(add_end_mark, end_mark_to_remove = '!')\n",
    "    \n",
    "    else:  \n",
    "        \n",
    "        raise ValueError(f'No end mark number {end_mark}')\n",
    "\n",
    "    # Create augmentation to add on French sentences\n",
    "    fr_augmentation_1 = TransformerSequences(nac.KeyboardAug(aug_char_p=char_p, aug_word_p=word_p,\n",
    "                                                             aug_word_max = max_len),\n",
    "                                          remove_mark_space, delete_guillemet_space, add_mark_space, end_mark_fn)\n",
    "    \n",
    "    fr_augmentation_2 = TransformerSequences(remove_mark_space, delete_guillemet_space, add_mark_space, end_mark_fn)\n",
    "    \n",
    "  # Recuperate the train dataset\n",
    "  train_dataset_aug = SentenceDataset(train_file,\n",
    "                                        tokenizer,\n",
    "                                        truncation = False,\n",
    "                                        cp1_transformer = fr_augmentation_1,\n",
    "                                        cp2_transformer = fr_augmentation_2,\n",
    "                                        corpus_1=corpus_1,\n",
    "                                        corpus_2=corpus_2\n",
    "                                        )\n",
    "\n",
    "  # Recuperate the valid dataset\n",
    "  valid_dataset = SentenceDataset(test_file,\n",
    "                                        tokenizer,\n",
    "                                        cp1_transformer = fr_augmentation_2,\n",
    "                                        cp2_transformer = fr_augmentation_2,\n",
    "                                        corpus_1=corpus_1,\n",
    "                                        corpus_2=corpus_2,\n",
    "                                        truncation = False)\n",
    "  \n",
    "  # Return the datasets\n",
    "  return train_dataset_aug, valid_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "%run wolof-translate/wolof_translate/utils/recuperate_datasets.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "➡️ Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "# initialize the configurations\n",
    "config = {\n",
    "    'epochs': 15,\n",
    "    'max_epoch': None,\n",
    "    'log_step': 1,\n",
    "    'metric_for_best_model': 'test_loss',\n",
    "    'metric_objective': 'minimize',\n",
    "    'corpus_1': 'french',\n",
    "    'corpus_2': 'wolof',\n",
    "    'train_file': 'data/extractions/new_data/train_set.csv',\n",
    "    'test_file': 'data/extractions/new_data/valid_set.csv',\n",
    "    'drop_out_rate': 0.02121451891074674,\n",
    "    'd_model': 512,\n",
    "    'n_head': 8,\n",
    "    'dim_ff': 2024,\n",
    "    'n_encoders': 6,\n",
    "    'n_decoders': 6,\n",
    "    'learning_rate': 0.0012924460038848235,\n",
    "    'weight_decay': 0.02121451891074674,\n",
    "    'char_p': 0.4488567119340453,\n",
    "    'word_p': 0.8710480007380237,\n",
    "    'end_mark': 3,\n",
    "    'label_smoothing': 0.1,\n",
    "    'max_len': 20,\n",
    "    'random_state': 0,\n",
    "    'boundaries': [2, 31, 59, 87, 115, 143, 171],\n",
    "    'batch_sizes': [256, 128, 64, 32, 16, 8, 4, 2],\n",
    "    'batch_size': None, \n",
    "    'warmup_init': False,\n",
    "    'relative_step': False,\n",
    "    'num_workers': 0,\n",
    "    'pin_memory': False,\n",
    "    # --------------------> Must be changed when continuing a training\n",
    "    'model_dir': 't5_small_v6_fw',\n",
    "    'new_model_dir': 't5_small_v6_fw',\n",
    "    'continue': True, # --------------------------> Must be changed when continuing training\n",
    "    'logging_dir': 'data/logs/t5_small_fw',\n",
    "    'save_best': True,\n",
    "    'tokenizer_path': 'wolof-translate/wolof_translate/tokenizers/t5_tokenizers/tokenizer_v5.model',\n",
    "    'data_directory': 'data/extractions/new_data/',\n",
    "    'data_file': 'corpora_v6.csv',\n",
    "    'version': 6,\n",
    "    # in the case of a distributed training\n",
    "    'backend': None,\n",
    "    'hosts': [],\n",
    "    'current_host': None,\n",
    "    'num_gpus': 5,\n",
    "    'logger': None,\n",
    "    'return_trainer': True,\n",
    "    'include_split': True,\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting wolof-translate/wolof_translate/utils/hg_training.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile wolof-translate/wolof_translate/utils/hg_training.py\n",
    "from wolof_translate import *\n",
    "import warnings\n",
    "\n",
    "def train(config: dict):\n",
    "    \n",
    "    # ---------------------------------------\n",
    "    # add distribution if necessary (https://github.com/aws/amazon-sagemaker-examples/blob/main/sagemaker-python-sdk/pytorch_mnist/mnist.py)\n",
    "    \n",
    "    logger = config['logger']\n",
    "    \n",
    "    is_distributed = len(config['hosts']) > 1 and config['backend'] is not None\n",
    "    \n",
    "    use_cuda = config['num_gpus'] > 0\n",
    "    \n",
    "    config.update({\"num_workers\": 1, \"pin_memory\": True} if use_cuda else {})\n",
    "\n",
    "    if not logger is None:\n",
    "        \n",
    "        logger.debug(\"Distributed training - {}\".format(is_distributed))\n",
    "        \n",
    "        logger.debug(\"Number of gpus available - {}\".format(config['num_gpus']))\n",
    "        \n",
    "    if is_distributed:\n",
    "        # Initialize the distributed environment.\n",
    "        world_size = len(config['hosts'])\n",
    "        \n",
    "        os.environ[\"WORLD_SIZE\"] = str(world_size)\n",
    "        \n",
    "        host_rank = config['hosts'].index(config['current_host'])\n",
    "        \n",
    "        os.environ[\"RANK\"] = str(host_rank)\n",
    "        \n",
    "        dist.init_process_group(backend=config['backend'], rank=host_rank, world_size=world_size)\n",
    "        \n",
    "        if not logger is None: logger.info(\n",
    "            \"Initialized the distributed environment: '{}' backend on {} nodes. \".format(\n",
    "                config['backend'], dist.get_world_size()\n",
    "            )\n",
    "            + \"Current host rank is {}. Number of gpus: {}\".format(dist.get_rank(), config['num_gpus'])\n",
    "        )\n",
    "    # ---------------------------------------\n",
    "    \n",
    "    # split the data\n",
    "    if config['include_split']: split_data(config['random_state'], config['data_directory'], config['data_file'])\n",
    "\n",
    "    # recuperate the tokenizer\n",
    "    tokenizer = T5TokenizerFast(config['tokenizer_path'])\n",
    "    \n",
    "    # Initialize the model name\n",
    "    model_name = 't5-small'\n",
    "\n",
    "    # import the model with its pre-trained weights\n",
    "    model = T5ForConditionalGeneration.from_pretrained(model_name)\n",
    "\n",
    "    # resize the token embeddings\n",
    "    model.resize_token_embeddings(len(tokenizer))\n",
    "    \n",
    "    # recuperate train and test set\n",
    "    train_dataset, test_dataset = recuperate_datasets(config['char_p'],\n",
    "                                                        config['word_p'], config['max_len'],\n",
    "                                                        config['end_mark'], tokenizer, config['corpus_1'],\n",
    "                                                        config['corpus_2'],\n",
    "                                                        config['train_file'], config['test_file'])\n",
    "    \n",
    "    # initialize the evaluation object\n",
    "    evaluation = TranslationEvaluation(tokenizer, train_dataset.decode)\n",
    "\n",
    "    # let us initialize the trainer\n",
    "    trainer = ModelRunner(model = model, version=config['version'], seed = 0, evaluation = evaluation, optimizer = Adafactor)\n",
    "\n",
    "    #-------------------------------------\n",
    "    # in the case when the linear learning rate scheduler with warmup is used\n",
    "    \n",
    "    # let us calculate the appropriate warmup steps (let us take a max epoch of 100)\n",
    "    # length = len(train_dataset)\n",
    "\n",
    "    # n_steps = length // config['batch_size']\n",
    "\n",
    "    # num_steps = config['max_epoch'] * n_steps\n",
    "\n",
    "    # warmup_steps = (config['max_epoch'] * n_steps) * config['warmup_ratio']\n",
    "\n",
    "    # Initialize the scheduler parameters\n",
    "    # scheduler_args = {'num_warmup_steps': warmup_steps, 'num_training_steps': num_steps}\n",
    "    #-------------------------------------\n",
    "\n",
    "    # Initialize the optimizer parameters\n",
    "    optimizer_args = {\n",
    "        'lr': config['learning_rate'],\n",
    "        'weight_decay': config['weight_decay'],\n",
    "        # 'betas': (0.9, 0.98),\n",
    "        'warmup_init': config['warmup_init'],\n",
    "        'relative_step': config['relative_step']\n",
    "    }\n",
    "\n",
    "    # ----------------------------\n",
    "    # initialize the bucket samplers for distributed environment\n",
    "    boundaries = config['boundaries']\n",
    "    batch_sizes = config['batch_sizes']\n",
    "\n",
    "    train_sampler = SequenceLengthBatchSampler(train_dataset,\n",
    "                                                boundaries = boundaries,\n",
    "                                                batch_sizes = batch_sizes)\n",
    "\n",
    "    test_sampler = SequenceLengthBatchSampler(test_dataset,\n",
    "                                                boundaries = boundaries,\n",
    "                                                batch_sizes = batch_sizes)\n",
    "\n",
    "    # ------------------------------\n",
    "    # initialize a bucket sampler with fixed batch size in the case of single machine\n",
    "    # with parallelization on multiple gpus\n",
    "    # train_sampler = BucketSampler(train_dataset, config['batch_size'])\n",
    "\n",
    "    # test_sampler = BucketSampler(test_dataset, config['batch_size'])\n",
    "    \n",
    "    # ------------------------------\n",
    "\n",
    "    # Initialize the loaders parameters\n",
    "    train_loader_args = {'batch_sampler': train_sampler, 'collate_fn': collate_fn,\n",
    "                        'num_workers': config['num_workers'], 'pin_memory': config['pin_memory']}\n",
    "\n",
    "    test_loader_args = {'batch_sampler': test_sampler, 'collate_fn': collate_fn,\n",
    "                        'num_workers': config['num_workers'], 'pin_memory': config['pin_memory']}\n",
    "\n",
    "    # Add the datasets and hyperparameters to trainer\n",
    "    trainer.compile(train_dataset, test_dataset, tokenizer, train_loader_args,\n",
    "                    test_loader_args, optimizer_kwargs = optimizer_args,\n",
    "                    # lr_scheduler=get_linear_schedule_with_warmup,\n",
    "                    # lr_scheduler_kwargs=scheduler_args,\n",
    "                    predict_with_generate = True,\n",
    "                    hugging_face = True,\n",
    "                    is_distributed=is_distributed,\n",
    "                    logging_dir=config['logging_dir'],\n",
    "                    dist=dist\n",
    "                    )\n",
    "\n",
    "    # load the model\n",
    "    trainer.load(config['model_dir'], load_best = not config['continue'])\n",
    "    \n",
    "    # Train the model\n",
    "    trainer.train(config['epochs'] - trainer.current_epoch, auto_save = True, log_step = config['log_step'], saving_directory=config['new_model_dir'], save_best = config['save_best'],\n",
    "                  metric_for_best_model = config['metric_for_best_model'], metric_objective = config['metric_objective'])\n",
    "    \n",
    "    if config['return_trainer']:\n",
    "        \n",
    "        return trainer\n",
    "    \n",
    "    return None\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Below train and save if we want."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "from wolof_translate.utils.hg_training import train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/25 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "For epoch 6: \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train batch number 2:   0%|          | 0/106 [00:00<?, ?batches/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train batch number 107: 100%|██████████| 106/106 [00:16<00:00,  6.55batches/s]\n",
      "Test batch number 2:   0%|          | 0/9 [00:00<?, ?batches/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Test batch number 10: 100%|██████████| 9/9 [00:06<00:00,  1.44batches/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Metrics: {'train_loss': 2.4375888028413804, 'test_loss': 2.966145953932008, 'bleu': 2.3349269230769227, 'gen_len': 21.185333566433567}\n",
      "\n",
      "=============================\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  4%|▍         | 1/25 [00:24<09:49, 24.57s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "For epoch 7: \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train batch number 2:   0%|          | 0/106 [00:00<?, ?batches/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train batch number 107: 100%|██████████| 106/106 [00:16<00:00,  6.62batches/s]\n",
      "Test batch number 3:  11%|█         | 1/9 [00:00<00:01,  4.63batches/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Test batch number 10: 100%|██████████| 9/9 [00:06<00:00,  1.29batches/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Metrics: {'train_loss': 2.3719612082175723, 'test_loss': 2.9198804418523827, 'bleu': 2.384601748251748, 'gen_len': 21.020954545454543}\n",
      "\n",
      "=============================\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  8%|▊         | 2/25 [00:49<09:33, 24.94s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "For epoch 8: \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train batch number 2:   0%|          | 0/106 [00:00<?, ?batches/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train batch number 107: 100%|██████████| 106/106 [00:16<00:00,  6.60batches/s]\n",
      "Test batch number 2:   0%|          | 0/9 [00:00<?, ?batches/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Test batch number 10: 100%|██████████| 9/9 [00:07<00:00,  1.29batches/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Metrics: {'train_loss': 2.322190919712635, 'test_loss': 2.896589669314298, 'bleu': 2.3052779720279717, 'gen_len': 19.241266433566434}\n",
      "\n",
      "=============================\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 12%|█▏        | 3/25 [01:14<09:11, 25.07s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "For epoch 9: \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train batch number 2:   0%|          | 0/106 [00:00<?, ?batches/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train batch number 107: 100%|██████████| 106/106 [00:16<00:00,  6.59batches/s]\n",
      "Test batch number 3:  11%|█         | 1/9 [00:00<00:01,  4.77batches/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Test batch number 10: 100%|██████████| 9/9 [00:06<00:00,  1.35batches/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Metrics: {'train_loss': 2.2724601538565983, 'test_loss': 2.8756876675399035, 'bleu': 2.2909538461538457, 'gen_len': 19.94757972027972}\n",
      "\n",
      "=============================\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 16%|█▌        | 4/25 [01:39<08:45, 25.01s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "For epoch 10: \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train batch number 2:   0%|          | 0/106 [00:00<?, ?batches/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train batch number 107: 100%|██████████| 106/106 [00:16<00:00,  6.60batches/s]\n",
      "Test batch number 3:  11%|█         | 1/9 [00:00<00:01,  4.79batches/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Test batch number 10: 100%|██████████| 9/9 [00:07<00:00,  1.22batches/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Metrics: {'train_loss': 2.228499875821344, 'test_loss': 2.8574944959653843, 'bleu': 2.504084615384616, 'gen_len': 20.21676433566434}\n",
      "\n",
      "=============================\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 20%|██        | 5/25 [02:05<08:24, 25.23s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "For epoch 11: \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train batch number 2:   0%|          | 0/106 [00:00<?, ?batches/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train batch number 107: 100%|██████████| 106/106 [00:16<00:00,  6.51batches/s]\n",
      "Test batch number 2:  11%|█         | 1/9 [00:00<00:01,  4.45batches/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Test batch number 10: 100%|██████████| 9/9 [00:06<00:00,  1.40batches/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Metrics: {'train_loss': 2.1911549471224245, 'test_loss': 2.8746307279680154, 'bleu': 2.0619433566433565, 'gen_len': 16.118908391608393}\n",
      "\n",
      "=============================\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 24%|██▍       | 6/25 [02:29<07:48, 24.66s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "For epoch 12: \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train batch number 2:   0%|          | 0/106 [00:00<?, ?batches/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train batch number 107: 100%|██████████| 106/106 [00:16<00:00,  6.51batches/s]\n",
      "Test batch number 3:  11%|█         | 1/9 [00:00<00:01,  4.70batches/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Test batch number 10: 100%|██████████| 9/9 [00:06<00:00,  1.37batches/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Metrics: {'train_loss': 2.150642161083681, 'test_loss': 2.8429726854070916, 'bleu': 2.8710527972027973, 'gen_len': 17.94405454545455}\n",
      "\n",
      "=============================\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 28%|██▊       | 7/25 [02:54<07:25, 24.78s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "For epoch 13: \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train batch number 2:   0%|          | 0/106 [00:00<?, ?batches/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train batch number 107: 100%|██████████| 106/106 [00:16<00:00,  6.47batches/s]\n",
      "Test batch number 2:   0%|          | 0/9 [00:00<?, ?batches/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Test batch number 10: 100%|██████████| 9/9 [00:07<00:00,  1.23batches/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Metrics: {'train_loss': 2.1094355544618604, 'test_loss': 2.833220173428942, 'bleu': 2.8371926573426576, 'gen_len': 19.74824615384615}\n",
      "\n",
      "=============================\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 32%|███▏      | 8/25 [03:19<07:07, 25.12s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "For epoch 14: \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train batch number 2:   0%|          | 0/106 [00:00<?, ?batches/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train batch number 107: 100%|██████████| 106/106 [00:16<00:00,  6.50batches/s]\n",
      "Test batch number 2:   0%|          | 0/9 [00:00<?, ?batches/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Test batch number 10: 100%|██████████| 9/9 [00:07<00:00,  1.24batches/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Metrics: {'train_loss': 2.0748607206407157, 'test_loss': 2.8436978813651557, 'bleu': 3.220885664335665, 'gen_len': 18.332169930069927}\n",
      "\n",
      "=============================\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 36%|███▌      | 9/25 [03:44<06:38, 24.89s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "For epoch 15: \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train batch number 2:   0%|          | 0/106 [00:00<?, ?batches/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train batch number 107: 100%|██████████| 106/106 [00:16<00:00,  6.47batches/s]\n",
      "Test batch number 2:   0%|          | 0/9 [00:00<?, ?batches/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Test batch number 10: 100%|██████████| 9/9 [00:08<00:00,  1.09batches/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Metrics: {'train_loss': 2.0419498727370615, 'test_loss': 2.8237611914014478, 'bleu': 2.646240909090908, 'gen_len': 20.881122377622376}\n",
      "\n",
      "=============================\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 40%|████      | 10/25 [04:11<06:22, 25.49s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "For epoch 16: \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train batch number 2:   0%|          | 0/106 [00:00<?, ?batches/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train batch number 107: 100%|██████████| 106/106 [00:16<00:00,  6.48batches/s]\n",
      "Test batch number 3:  11%|█         | 1/9 [00:00<00:01,  4.49batches/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Test batch number 10: 100%|██████████| 9/9 [00:06<00:00,  1.34batches/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Metrics: {'train_loss': 2.008480124250478, 'test_loss': 2.8501608755205066, 'bleu': 2.9293461538461547, 'gen_len': 18.723774825174825}\n",
      "\n",
      "=============================\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 44%|████▍     | 11/25 [04:34<05:49, 24.98s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "For epoch 17: \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train batch number 2:   0%|          | 0/106 [00:00<?, ?batches/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train batch number 107: 100%|██████████| 106/106 [00:16<00:00,  6.45batches/s]\n",
      "Test batch number 2:  11%|█         | 1/9 [00:00<00:01,  4.46batches/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Test batch number 10: 100%|██████████| 9/9 [00:07<00:00,  1.25batches/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Metrics: {'train_loss': 1.9747594703896154, 'test_loss': 2.825546859861254, 'bleu': 3.1001982517482514, 'gen_len': 18.353144755244756}\n",
      "\n",
      "=============================\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 48%|████▊     | 12/25 [04:59<05:22, 24.79s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "For epoch 18: \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train batch number 2:   0%|          | 0/106 [00:00<?, ?batches/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train batch number 107: 100%|██████████| 106/106 [00:16<00:00,  6.48batches/s]\n",
      "Test batch number 3:  11%|█         | 1/9 [00:00<00:01,  4.53batches/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Test batch number 10: 100%|██████████| 9/9 [00:07<00:00,  1.17batches/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Metrics: {'train_loss': 1.9440243814415847, 'test_loss': 2.86058876731179, 'bleu': 2.7641552447552447, 'gen_len': 17.059434965034967}\n",
      "\n",
      "=============================\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 52%|█████▏    | 13/25 [05:24<04:57, 24.82s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "For epoch 19: \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train batch number 2:   0%|          | 0/106 [00:00<?, ?batches/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train batch number 107: 100%|██████████| 106/106 [00:16<00:00,  6.43batches/s]\n",
      "Test batch number 2:   0%|          | 0/9 [00:00<?, ?batches/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Test batch number 10: 100%|██████████| 9/9 [00:06<00:00,  1.38batches/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Metrics: {'train_loss': 1.9121805690082347, 'test_loss': 2.853731763946427, 'bleu': 2.945455594405594, 'gen_len': 17.19578951048951}\n",
      "\n",
      "=============================\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 56%|█████▌    | 14/25 [05:47<04:29, 24.49s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "For epoch 20: \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train batch number 2:   0%|          | 0/106 [00:00<?, ?batches/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train batch number 107: 100%|██████████| 106/106 [00:16<00:00,  6.58batches/s]\n",
      "Test batch number 2:   0%|          | 0/9 [00:00<?, ?batches/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Test batch number 10: 100%|██████████| 9/9 [00:07<00:00,  1.18batches/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Metrics: {'train_loss': 1.8839237426220727, 'test_loss': 2.82736990168378, 'bleu': 2.5408059440559434, 'gen_len': 19.545450349650356}\n",
      "\n",
      "=============================\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 60%|██████    | 15/25 [06:12<04:04, 24.49s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "For epoch 21: \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train batch number 2:   0%|          | 0/106 [00:00<?, ?batches/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train batch number 107: 100%|██████████| 106/106 [00:16<00:00,  6.60batches/s]\n",
      "Test batch number 2:   0%|          | 0/9 [00:00<?, ?batches/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Test batch number 10: 100%|██████████| 9/9 [00:07<00:00,  1.20batches/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Metrics: {'train_loss': 1.8511747155170752, 'test_loss': 2.8715496763482795, 'bleu': 2.97046993006993, 'gen_len': 18.86013286713287}\n",
      "\n",
      "=============================\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 64%|██████▍   | 16/25 [06:36<03:40, 24.45s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "For epoch 22: \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train batch number 2:   0%|          | 0/106 [00:00<?, ?batches/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train batch number 107: 100%|██████████| 106/106 [00:16<00:00,  6.62batches/s]\n",
      "Test batch number 2:   0%|          | 0/9 [00:00<?, ?batches/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Test batch number 10: 100%|██████████| 9/9 [00:06<00:00,  1.34batches/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Metrics: {'train_loss': 1.8166869640298207, 'test_loss': 2.8685822203442766, 'bleu': 3.062287062937062, 'gen_len': 17.62591118881119}\n",
      "\n",
      "=============================\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 68%|██████▊   | 17/25 [07:00<03:13, 24.15s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "For epoch 23: \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train batch number 2:   0%|          | 0/106 [00:00<?, ?batches/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train batch number 107: 100%|██████████| 106/106 [00:15<00:00,  6.63batches/s]\n",
      "Test batch number 2:   0%|          | 0/9 [00:00<?, ?batches/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Test batch number 10: 100%|██████████| 9/9 [00:06<00:00,  1.33batches/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Metrics: {'train_loss': 1.78812318959505, 'test_loss': 2.864329903275817, 'bleu': 3.1684657342657347, 'gen_len': 16.423091608391605}\n",
      "\n",
      "=============================\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 72%|███████▏  | 18/25 [07:23<02:47, 23.94s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "For epoch 24: \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train batch number 2:   0%|          | 0/106 [00:00<?, ?batches/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train batch number 107: 100%|██████████| 106/106 [00:16<00:00,  6.62batches/s]\n",
      "Test batch number 2:   0%|          | 0/9 [00:00<?, ?batches/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Test batch number 10: 100%|██████████| 9/9 [00:06<00:00,  1.35batches/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Metrics: {'train_loss': 1.7567113254286078, 'test_loss': 2.889874550012442, 'bleu': 4.056932867132867, 'gen_len': 18.95105664335664}\n",
      "\n",
      "=============================\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 76%|███████▌  | 19/25 [07:47<02:22, 23.79s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "For epoch 25: \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train batch number 2:   0%|          | 0/106 [00:00<?, ?batches/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train batch number 107: 100%|██████████| 106/106 [00:15<00:00,  6.66batches/s]\n",
      "Test batch number 3:  11%|█         | 1/9 [00:00<00:01,  4.60batches/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Test batch number 10: 100%|██████████| 9/9 [00:07<00:00,  1.19batches/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Metrics: {'train_loss': 1.7311453506396117, 'test_loss': 2.900331260441066, 'bleu': 2.816500699300699, 'gen_len': 18.590922377622377}\n",
      "\n",
      "=============================\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 80%|████████  | 20/25 [08:11<01:59, 23.90s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "For epoch 26: \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train batch number 2:   0%|          | 0/106 [00:00<?, ?batches/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train batch number 107: 100%|██████████| 106/106 [00:15<00:00,  6.70batches/s]\n",
      "Test batch number 2:   0%|          | 0/9 [00:00<?, ?batches/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Test batch number 10: 100%|██████████| 9/9 [00:07<00:00,  1.28batches/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Metrics: {'train_loss': 1.7051444899504187, 'test_loss': 2.9014232308714543, 'bleu': 3.8829940559440552, 'gen_len': 17.115365734265733}\n",
      "\n",
      "=============================\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 84%|████████▍ | 21/25 [08:34<01:35, 23.81s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "For epoch 27: \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train batch number 2:   0%|          | 0/106 [00:00<?, ?batches/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train batch number 107: 100%|██████████| 106/106 [00:15<00:00,  6.80batches/s]\n",
      "Test batch number 3:  11%|█         | 1/9 [00:00<00:01,  4.56batches/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Test batch number 10: 100%|██████████| 9/9 [00:07<00:00,  1.21batches/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Metrics: {'train_loss': 1.6735967323281744, 'test_loss': 2.9207512398699786, 'bleu': 3.4692776223776223, 'gen_len': 19.27973776223776}\n",
      "\n",
      "=============================\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 88%|████████▊ | 22/25 [08:58<01:11, 23.78s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "For epoch 28: \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train batch number 2:   0%|          | 0/106 [00:00<?, ?batches/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train batch number 107: 100%|██████████| 106/106 [00:15<00:00,  6.88batches/s]\n",
      "Test batch number 2:   0%|          | 0/9 [00:00<?, ?batches/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Test batch number 10: 100%|██████████| 9/9 [00:07<00:00,  1.16batches/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Metrics: {'train_loss': 1.651091969216501, 'test_loss': 2.894568519992428, 'bleu': 2.565493356643356, 'gen_len': 19.828672727272732}\n",
      "\n",
      "=============================\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 92%|█████████▏| 23/25 [09:22<00:47, 23.80s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "For epoch 29: \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train batch number 2:   0%|          | 0/106 [00:00<?, ?batches/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train batch number 107: 100%|██████████| 106/106 [00:15<00:00,  6.92batches/s]\n",
      "Test batch number 2:  11%|█         | 1/9 [00:00<00:01,  4.50batches/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Test batch number 10: 100%|██████████| 9/9 [00:07<00:00,  1.16batches/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Metrics: {'train_loss': 1.6179291179569009, 'test_loss': 2.9350514862087227, 'bleu': 2.698149300699301, 'gen_len': 20.583897202797203}\n",
      "\n",
      "=============================\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 96%|█████████▌| 24/25 [09:46<00:23, 23.81s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "For epoch 30: \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train batch number 2:   0%|          | 0/106 [00:00<?, ?batches/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train batch number 107: 100%|██████████| 106/106 [00:15<00:00,  6.90batches/s]\n",
      "Test batch number 2:   0%|          | 0/9 [00:00<?, ?batches/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Test batch number 10: 100%|██████████| 9/9 [00:06<00:00,  1.29batches/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Metrics: {'train_loss': 1.5896231090079034, 'test_loss': 2.94225197238522, 'bleu': 4.267848601398602, 'gen_len': 18.486016083916084}\n",
      "\n",
      "=============================\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 25/25 [10:09<00:00, 24.37s/it]\n"
     ]
    }
   ],
   "source": [
    "# with warnings.catch_warnings():\n",
    "    # warnings.simplefilter(\"ignore\")\n",
    "trainer = train(config)\n",
    "\n",
    "# save if necessary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0it [00:00, ?it/s]\n"
     ]
    }
   ],
   "source": [
    "# with warnings.catch_warnings():\n",
    "    # warnings.simplefilter(\"ignore\")\n",
    "trainer = train(config)\n",
    "\n",
    "# save if necessary"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "➡️ Predictions\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluation batch number 2:   0%|          | 0/7 [00:00<?, ?batches/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluation batch number 8: 100%|██████████| 7/7 [00:04<00:00,  1.53batches/s]\n"
     ]
    }
   ],
   "source": [
    "if not trainer is None:\n",
    "    \n",
    "    # recuperate the tokenizer\n",
    "    tokenizer = T5TokenizerFast(config['tokenizer_path'])\n",
    "    \n",
    "    # recuperate the test dataset\n",
    "    # initialize the transformation sequence\n",
    "    end_mark_fn = partial(add_end_mark)\n",
    "    augmentation = TransformerSequences(remove_mark_space, delete_guillemet_space, add_mark_space, end_mark_fn)\n",
    "\n",
    "\n",
    "    # let us get the test set\n",
    "    test_dataset = SentenceDataset(f\"{config['data_directory']}test_set.csv\",\n",
    "                                            tokenizer = tokenizer,\n",
    "                                            cp1_transformer = augmentation,\n",
    "                                            cp2_transformer = augmentation,\n",
    "                                            corpus_1=config['corpus_1'],\n",
    "                                            corpus_2=config['corpus_2'],\n",
    "                                            truncation = False)\n",
    "\n",
    "    # initialize the bucket samplers for distributed environment\n",
    "    boundaries = config['boundaries']\n",
    "    batch_sizes = config['batch_sizes']\n",
    "\n",
    "    test_sampler = SequenceLengthBatchSampler(test_dataset,\n",
    "                                                boundaries = boundaries,\n",
    "                                                batch_sizes = batch_sizes)\n",
    "\n",
    "    test_loader_args = {'batch_sampler': test_sampler, 'collate_fn': collate_fn,\n",
    "                            'num_workers': config['num_workers'], 'pin_memory': config['pin_memory']}\n",
    "\n",
    "    metrics, prediction = trainer.evaluate(test_dataset, test_loader_args)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'test_loss': 2.927400617332725,\n",
       " 'bleu': 4.409472027972028,\n",
       " 'gen_len': 14.54896958041958}"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>original_sentences</th>\n",
       "      <th>translations</th>\n",
       "      <th>predictions</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Toutes les portes étaient ouvertes.</td>\n",
       "      <td>Bunt yi yépp a tëjju woon.</td>\n",
       "      <td>Gis ŋga nit ki.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Ici alors, si tu refuses!</td>\n",
       "      <td>Cii fii kon, soo bëggul!</td>\n",
       "      <td>Kooku dem!</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Celui qui est en haut!</td>\n",
       "      <td>Kenn ki ci kaw!</td>\n",
       "      <td>Mi ŋgi fi!</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>C'est votre ami!</td>\n",
       "      <td>Seen xarit la!</td>\n",
       "      <td>Nit ku baax la!</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Devant toi.</td>\n",
       "      <td>Cha kanam.</td>\n",
       "      <td>De moomu.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>281</th>\n",
       "      <td>Ce n'est que longtemps après, quand l'égoïsme ...</td>\n",
       "      <td>Teg nañ ciy ati-at ma door a jëli ni jigéen, n...</td>\n",
       "      <td>Xare bi jeex, ñu tekk ci ñaar-fukki at, ma àn...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>282</th>\n",
       "      <td>J'ai ressenti de l'étonnement, et même de l'in...</td>\n",
       "      <td>Li wóor te wér moo di ne bi loolu lépp weesoo,...</td>\n",
       "      <td>Jigéen ñu bare, seen der jóge woon ca seen xe...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>283</th>\n",
       "      <td>À quel point les arbres aux troncs rectilignes...</td>\n",
       "      <td>Dàtti garab yaa ngi lunk, sànneeku jëm ca kow,...</td>\n",
       "      <td>Ci dénd yi muy nettaliy xol di : « Moo, góor ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>284</th>\n",
       "      <td>Je peux ressentir l'émotion qu'il éprouve à tr...</td>\n",
       "      <td>Li koy yëngal noonu, xam naa ko. Lan moo ko dà...</td>\n",
       "      <td>Li muy nettaliy toog ci kër gi, di naka noonu...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>285</th>\n",
       "      <td>J'étais vraiment sur le pont d'un bateau. Le b...</td>\n",
       "      <td>Ku ma laajoon fan laa nekk, ma ni la : « Man? ...</td>\n",
       "      <td>Jigéen ñu bare, yàgg ci sunu biir xolub jéeri...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>286 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                    original_sentences  \\\n",
       "0                  Toutes les portes étaient ouvertes.   \n",
       "1                            Ici alors, si tu refuses!   \n",
       "2                               Celui qui est en haut!   \n",
       "3                                     C'est votre ami!   \n",
       "4                                          Devant toi.   \n",
       "..                                                 ...   \n",
       "281  Ce n'est que longtemps après, quand l'égoïsme ...   \n",
       "282  J'ai ressenti de l'étonnement, et même de l'in...   \n",
       "283  À quel point les arbres aux troncs rectilignes...   \n",
       "284  Je peux ressentir l'émotion qu'il éprouve à tr...   \n",
       "285  J'étais vraiment sur le pont d'un bateau. Le b...   \n",
       "\n",
       "                                          translations  \\\n",
       "0                           Bunt yi yépp a tëjju woon.   \n",
       "1                             Cii fii kon, soo bëggul!   \n",
       "2                                      Kenn ki ci kaw!   \n",
       "3                                       Seen xarit la!   \n",
       "4                                           Cha kanam.   \n",
       "..                                                 ...   \n",
       "281  Teg nañ ciy ati-at ma door a jëli ni jigéen, n...   \n",
       "282  Li wóor te wér moo di ne bi loolu lépp weesoo,...   \n",
       "283  Dàtti garab yaa ngi lunk, sànneeku jëm ca kow,...   \n",
       "284  Li koy yëngal noonu, xam naa ko. Lan moo ko dà...   \n",
       "285  Ku ma laajoon fan laa nekk, ma ni la : « Man? ...   \n",
       "\n",
       "                                           predictions  \n",
       "0                                      Gis ŋga nit ki.  \n",
       "1                                           Kooku dem!  \n",
       "2                                           Mi ŋgi fi!  \n",
       "3                                      Nit ku baax la!  \n",
       "4                                            De moomu.  \n",
       "..                                                 ...  \n",
       "281   Xare bi jeex, ñu tekk ci ñaar-fukki at, ma àn...  \n",
       "282   Jigéen ñu bare, seen der jóge woon ca seen xe...  \n",
       "283   Ci dénd yi muy nettaliy xol di : « Moo, góor ...  \n",
       "284   Li muy nettaliy toog ci kër gi, di naka noonu...  \n",
       "285   Jigéen ñu bare, yàgg ci sunu biir xolub jéeri...  \n",
       "\n",
       "[286 rows x 3 columns]"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prediction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "----------------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Wolof-French v6"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "➡️ Import the libraries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Global seed set to 0\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from wolof_translate import *\n",
    "\n",
    "# specify a seed for everything\n",
    "lt.seed_everything(0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "➡️ Function to recuperate datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting wolof-translate/wolof_translate/utils/recuperate_datasets.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile wolof-translate/wolof_translate/utils/recuperate_datasets.py\n",
    "from wolof_translate import *\n",
    "\n",
    "def recuperate_datasets(char_p: float, word_p: float, max_len: int, end_mark: int, tokenizer: T5TokenizerFast,\n",
    "                        corpus_1: str = 'french', corpus_2: str = 'wolof', \n",
    "                        train_file: str = 'data/extractions/new_data/train_set.csv', \n",
    "                        test_file: str = 'data/extractions/new_data/test_file.csv'):\n",
    "\n",
    "  # Let us recuperate the end_mark adding option\n",
    "  if end_mark == 1:\n",
    "    # Create augmentation to add on French sentences\n",
    "    fr_augmentation_1 = TransformerSequences(nac.KeyboardAug(aug_char_p=char_p, aug_word_p=word_p,\n",
    "                                                             aug_word_max = max_len),\n",
    "                                          remove_mark_space, delete_guillemet_space, add_mark_space)\n",
    "\n",
    "    fr_augmentation_2 = TransformerSequences(remove_mark_space, delete_guillemet_space, add_mark_space)\n",
    "    \n",
    "  else:\n",
    "    \n",
    "    if end_mark == 2:\n",
    "\n",
    "      end_mark_fn = partial(add_end_mark, end_mark_to_remove = '!', replace = True)\n",
    "    \n",
    "    elif end_mark == 3:\n",
    "\n",
    "      end_mark_fn = partial(add_end_mark)\n",
    "    \n",
    "    elif end_mark == 4:\n",
    "\n",
    "      end_mark_fn = partial(add_end_mark, end_mark_to_remove = '!')\n",
    "    \n",
    "    else:  \n",
    "        \n",
    "        raise ValueError(f'No end mark number {end_mark}')\n",
    "\n",
    "    # Create augmentation to add on French sentences\n",
    "    fr_augmentation_1 = TransformerSequences(nac.KeyboardAug(aug_char_p=char_p, aug_word_p=word_p,\n",
    "                                                             aug_word_max = max_len),\n",
    "                                          remove_mark_space, delete_guillemet_space, add_mark_space, end_mark_fn)\n",
    "    \n",
    "    fr_augmentation_2 = TransformerSequences(remove_mark_space, delete_guillemet_space, add_mark_space, end_mark_fn)\n",
    "    \n",
    "  # Recuperate the train dataset\n",
    "  train_dataset_aug = SentenceDataset(train_file,\n",
    "                                        tokenizer,\n",
    "                                        truncation = False,\n",
    "                                        cp1_transformer = fr_augmentation_1,\n",
    "                                        cp2_transformer = fr_augmentation_2,\n",
    "                                        corpus_1=corpus_1,\n",
    "                                        corpus_2=corpus_2\n",
    "                                        )\n",
    "\n",
    "  # Recuperate the valid dataset\n",
    "  valid_dataset = SentenceDataset(test_file,\n",
    "                                        tokenizer,\n",
    "                                        cp1_transformer = fr_augmentation_2,\n",
    "                                        cp2_transformer = fr_augmentation_2,\n",
    "                                        corpus_1=corpus_1,\n",
    "                                        corpus_2=corpus_2,\n",
    "                                        truncation = False)\n",
    "  \n",
    "  # Return the datasets\n",
    "  return train_dataset_aug, valid_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "%run wolof-translate/wolof_translate/utils/recuperate_datasets.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "➡️ Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "# initialize the configurations\n",
    "config = {\n",
    "    'epochs': 8,\n",
    "    'max_epoch': None,\n",
    "    'log_step': 1,\n",
    "    'metric_for_best_model': 'test_loss',\n",
    "    'metric_objective': 'minimize',\n",
    "    'corpus_1': 'wolof',\n",
    "    'corpus_2': 'french',\n",
    "    'train_file': 'data/extractions/new_data/train_set.csv',\n",
    "    'test_file': 'data/extractions/new_data/valid_set.csv',\n",
    "    'drop_out_rate': 0.291121690756753,\n",
    "    'd_model': 512,\n",
    "    'n_head': 8,\n",
    "    'dim_ff': 2024,\n",
    "    'n_encoders': 6,\n",
    "    'n_decoders': 6,\n",
    "    'learning_rate': 0.004976535748221598,\n",
    "    'weight_decay': 0.01725680581796274,\n",
    "    'char_p': 0.6630185468549884,\n",
    "    'word_p': 0.819968675819829,\n",
    "    'end_mark': 3,\n",
    "    'label_smoothing': 0.1,\n",
    "    'max_len': 20,\n",
    "    'random_state': 0,\n",
    "    'boundaries': [2, 31, 59, 87, 115, 143, 171],\n",
    "    'batch_sizes': [256, 128, 64, 32, 16, 8, 4, 2],\n",
    "    'batch_size': None, \n",
    "    'warmup_init': False,\n",
    "    'relative_step': False,\n",
    "    'num_workers': 0,\n",
    "    'pin_memory': False,\n",
    "    # --------------------> Must be changed when continuing a training\n",
    "    'model_dir': 't5_small_v6_wf',\n",
    "    'new_model_dir': 't5_small_v6_wf',\n",
    "    'continue': True, # --------------------------> Must be changed when continuing training\n",
    "    'logging_dir': 'data/logs/t5_small_wf',\n",
    "    'save_best': True,\n",
    "    'tokenizer_path': 'wolof-translate/wolof_translate/tokenizers/t5_tokenizers/tokenizer_v5.model',\n",
    "    'data_directory': 'data/extractions/new_data/',\n",
    "    'data_file': 'corpora_v6.csv',\n",
    "    'version': 6,\n",
    "    # in the case of a distributed training\n",
    "    'backend': None,\n",
    "    'hosts': [],\n",
    "    'current_host': None,\n",
    "    'num_gpus': 5,\n",
    "    'logger': None,\n",
    "    'return_trainer': True,\n",
    "    'include_split': True,\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting wolof-translate/wolof_translate/utils/hg_training.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile wolof-translate/wolof_translate/utils/hg_training.py\n",
    "from wolof_translate import *\n",
    "import warnings\n",
    "\n",
    "def train(config: dict):\n",
    "    \n",
    "    # ---------------------------------------\n",
    "    # add distribution if necessary (https://github.com/aws/amazon-sagemaker-examples/blob/main/sagemaker-python-sdk/pytorch_mnist/mnist.py)\n",
    "    \n",
    "    logger = config['logger']\n",
    "    \n",
    "    is_distributed = len(config['hosts']) > 1 and config['backend'] is not None\n",
    "    \n",
    "    use_cuda = config['num_gpus'] > 0\n",
    "    \n",
    "    config.update({\"num_workers\": 1, \"pin_memory\": True} if use_cuda else {})\n",
    "\n",
    "    if not logger is None:\n",
    "        \n",
    "        logger.debug(\"Distributed training - {}\".format(is_distributed))\n",
    "        \n",
    "        logger.debug(\"Number of gpus available - {}\".format(config['num_gpus']))\n",
    "        \n",
    "    if is_distributed:\n",
    "        # Initialize the distributed environment.\n",
    "        world_size = len(config['hosts'])\n",
    "        \n",
    "        os.environ[\"WORLD_SIZE\"] = str(world_size)\n",
    "        \n",
    "        host_rank = config['hosts'].index(config['current_host'])\n",
    "        \n",
    "        os.environ[\"RANK\"] = str(host_rank)\n",
    "        \n",
    "        dist.init_process_group(backend=config['backend'], rank=host_rank, world_size=world_size)\n",
    "        \n",
    "        if not logger is None: logger.info(\n",
    "            \"Initialized the distributed environment: '{}' backend on {} nodes. \".format(\n",
    "                config['backend'], dist.get_world_size()\n",
    "            )\n",
    "            + \"Current host rank is {}. Number of gpus: {}\".format(dist.get_rank(), config['num_gpus'])\n",
    "        )\n",
    "    # ---------------------------------------\n",
    "    \n",
    "    # split the data\n",
    "    if config['include_split']: split_data(config['random_state'], config['data_directory'], config['data_file'])\n",
    "\n",
    "    # recuperate the tokenizer\n",
    "    tokenizer = T5TokenizerFast(config['tokenizer_path'])\n",
    "    \n",
    "    # Initialize the model name\n",
    "    model_name = 't5-small'\n",
    "\n",
    "    # import the model with its pre-trained weights\n",
    "    model = T5ForConditionalGeneration.from_pretrained(model_name)\n",
    "\n",
    "    # resize the token embeddings\n",
    "    model.resize_token_embeddings(len(tokenizer))\n",
    "    \n",
    "    # recuperate train and test set\n",
    "    train_dataset, test_dataset = recuperate_datasets(config['char_p'],\n",
    "                                                        config['word_p'], config['max_len'],\n",
    "                                                        config['end_mark'], tokenizer, config['corpus_1'],\n",
    "                                                        config['corpus_2'],\n",
    "                                                        config['train_file'], config['test_file'])\n",
    "    \n",
    "    # initialize the evaluation object\n",
    "    evaluation = TranslationEvaluation(tokenizer, train_dataset.decode)\n",
    "\n",
    "    # let us initialize the trainer\n",
    "    trainer = ModelRunner(model = model, version=config['version'], seed = 0, evaluation = evaluation, optimizer = Adafactor)\n",
    "\n",
    "    #-------------------------------------\n",
    "    # in the case when the linear learning rate scheduler with warmup is used\n",
    "    \n",
    "    # let us calculate the appropriate warmup steps (let us take a max epoch of 100)\n",
    "    # length = len(train_dataset)\n",
    "\n",
    "    # n_steps = length // config['batch_size']\n",
    "\n",
    "    # num_steps = config['max_epoch'] * n_steps\n",
    "\n",
    "    # warmup_steps = (config['max_epoch'] * n_steps) * config['warmup_ratio']\n",
    "\n",
    "    # Initialize the scheduler parameters\n",
    "    # scheduler_args = {'num_warmup_steps': warmup_steps, 'num_training_steps': num_steps}\n",
    "    #-------------------------------------\n",
    "\n",
    "    # Initialize the optimizer parameters\n",
    "    optimizer_args = {\n",
    "        'lr': config['learning_rate'],\n",
    "        'weight_decay': config['weight_decay'],\n",
    "        # 'betas': (0.9, 0.98),\n",
    "        'warmup_init': config['warmup_init'],\n",
    "        'relative_step': config['relative_step']\n",
    "    }\n",
    "\n",
    "    # ----------------------------\n",
    "    # initialize the bucket samplers for distributed environment\n",
    "    boundaries = config['boundaries']\n",
    "    batch_sizes = config['batch_sizes']\n",
    "\n",
    "    train_sampler = SequenceLengthBatchSampler(train_dataset,\n",
    "                                                boundaries = boundaries,\n",
    "                                                batch_sizes = batch_sizes)\n",
    "\n",
    "    test_sampler = SequenceLengthBatchSampler(test_dataset,\n",
    "                                                boundaries = boundaries,\n",
    "                                                batch_sizes = batch_sizes)\n",
    "\n",
    "    # ------------------------------\n",
    "    # initialize a bucket sampler with fixed batch size in the case of single machine\n",
    "    # with parallelization on multiple gpus\n",
    "    # train_sampler = BucketSampler(train_dataset, config['batch_size'])\n",
    "\n",
    "    # test_sampler = BucketSampler(test_dataset, config['batch_size'])\n",
    "    \n",
    "    # ------------------------------\n",
    "\n",
    "    # Initialize the loaders parameters\n",
    "    train_loader_args = {'batch_sampler': train_sampler, 'collate_fn': collate_fn,\n",
    "                        'num_workers': config['num_workers'], 'pin_memory': config['pin_memory']}\n",
    "\n",
    "    test_loader_args = {'batch_sampler': test_sampler, 'collate_fn': collate_fn,\n",
    "                        'num_workers': config['num_workers'], 'pin_memory': config['pin_memory']}\n",
    "\n",
    "    # Add the datasets and hyperparameters to trainer\n",
    "    trainer.compile(train_dataset, test_dataset, tokenizer, train_loader_args,\n",
    "                    test_loader_args, optimizer_kwargs = optimizer_args,\n",
    "                    # lr_scheduler=get_linear_schedule_with_warmup,\n",
    "                    # lr_scheduler_kwargs=scheduler_args,\n",
    "                    predict_with_generate = True,\n",
    "                    hugging_face = True,\n",
    "                    is_distributed=is_distributed,\n",
    "                    logging_dir=config['logging_dir'],\n",
    "                    dist=dist\n",
    "                    )\n",
    "\n",
    "    # load the model\n",
    "    trainer.load(config['model_dir'], load_best = not config['continue'])\n",
    "    \n",
    "    # Train the model\n",
    "    trainer.train(config['epochs'] - trainer.current_epoch, auto_save = True, log_step = config['log_step'], saving_directory=config['new_model_dir'], save_best = config['save_best'],\n",
    "                  metric_for_best_model = config['metric_for_best_model'], metric_objective = config['metric_objective'])\n",
    "    \n",
    "    if config['return_trainer']:\n",
    "        \n",
    "        return trainer\n",
    "    \n",
    "    return None\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Below train and save if we want."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "from wolof_translate.utils.hg_training import train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/25 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "For epoch 6: \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train batch number 2:   0%|          | 0/86 [00:00<?, ?batches/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train batch number 87: 100%|██████████| 86/86 [00:13<00:00,  6.44batches/s]\n",
      "Test batch number 2:   0%|          | 0/9 [00:00<?, ?batches/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Test batch number 10: 100%|██████████| 9/9 [00:06<00:00,  1.33batches/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Metrics: {'train_loss': 2.2012223856491864, 'test_loss': 2.97088413972121, 'bleu': 1.3288132867132867, 'gen_len': 32.2832027972028}\n",
      "\n",
      "=============================\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  4%|▍         | 1/25 [00:20<08:14, 20.62s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "For epoch 7: \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train batch number 2:   0%|          | 0/86 [00:00<?, ?batches/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train batch number 87: 100%|██████████| 86/86 [00:13<00:00,  6.44batches/s]\n",
      "Test batch number 2:   0%|          | 0/9 [00:00<?, ?batches/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Test batch number 10: 100%|██████████| 9/9 [00:06<00:00,  1.36batches/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Metrics: {'train_loss': 2.0587681743011346, 'test_loss': 2.920340236250337, 'bleu': 1.5949433566433566, 'gen_len': 31.53494195804196}\n",
      "\n",
      "=============================\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  8%|▊         | 2/25 [00:42<08:17, 21.65s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "For epoch 8: \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train batch number 2:   0%|          | 0/86 [00:00<?, ?batches/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train batch number 87: 100%|██████████| 86/86 [00:13<00:00,  6.48batches/s]\n",
      "Test batch number 2:   0%|          | 0/9 [00:00<?, ?batches/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Test batch number 10: 100%|██████████| 9/9 [00:06<00:00,  1.41batches/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Metrics: {'train_loss': 1.9235682864349695, 'test_loss': 2.907906102133798, 'bleu': 1.9670594405594404, 'gen_len': 30.622374825174827}\n",
      "\n",
      "=============================\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 12%|█▏        | 3/25 [01:04<07:57, 21.72s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "For epoch 9: \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train batch number 2:   0%|          | 0/86 [00:00<?, ?batches/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train batch number 87: 100%|██████████| 86/86 [00:13<00:00,  6.40batches/s]\n",
      "Test batch number 2:   0%|          | 0/9 [00:00<?, ?batches/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Test batch number 10: 100%|██████████| 9/9 [00:06<00:00,  1.32batches/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Metrics: {'train_loss': 1.8176907369232496, 'test_loss': 2.957092210129424, 'bleu': 1.7479314685314682, 'gen_len': 31.54194195804196}\n",
      "\n",
      "=============================\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 16%|█▌        | 4/25 [01:25<07:30, 21.44s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "For epoch 10: \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train batch number 2:   0%|          | 0/86 [00:00<?, ?batches/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train batch number 87: 100%|██████████| 86/86 [00:13<00:00,  6.43batches/s]\n",
      "Test batch number 2:   0%|          | 0/9 [00:00<?, ?batches/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Test batch number 10: 100%|██████████| 9/9 [00:07<00:00,  1.27batches/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Metrics: {'train_loss': 1.7026850958812894, 'test_loss': 3.0525858285543808, 'bleu': 1.4636772727272727, 'gen_len': 30.437095804195803}\n",
      "\n",
      "=============================\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 20%|██        | 5/25 [01:46<07:06, 21.34s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "For epoch 11: \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train batch number 2:   0%|          | 0/86 [00:00<?, ?batches/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train batch number 87: 100%|██████████| 86/86 [00:13<00:00,  6.42batches/s]\n",
      "Test batch number 2:   0%|          | 0/9 [00:00<?, ?batches/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Test batch number 10: 100%|██████████| 9/9 [00:06<00:00,  1.35batches/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Metrics: {'train_loss': 1.6265092072499394, 'test_loss': 3.011158271269365, 'bleu': 2.3412748251748248, 'gen_len': 30.237743356643353}\n",
      "\n",
      "=============================\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 24%|██▍       | 6/25 [02:07<06:42, 21.17s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "For epoch 12: \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train batch number 2:   0%|          | 0/86 [00:00<?, ?batches/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train batch number 87: 100%|██████████| 86/86 [00:13<00:00,  6.41batches/s]\n",
      "Test batch number 2:   0%|          | 0/9 [00:00<?, ?batches/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Test batch number 10: 100%|██████████| 9/9 [00:07<00:00,  1.24batches/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Metrics: {'train_loss': 1.5177782061035368, 'test_loss': 3.071008787288532, 'bleu': 3.338223426573427, 'gen_len': 30.769244055944053}\n",
      "\n",
      "=============================\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 28%|██▊       | 7/25 [02:29<06:22, 21.25s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "For epoch 13: \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train batch number 2:   0%|          | 0/86 [00:00<?, ?batches/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train batch number 87: 100%|██████████| 86/86 [00:13<00:00,  6.37batches/s]\n",
      "Test batch number 2:   0%|          | 0/9 [00:00<?, ?batches/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Test batch number 10: 100%|██████████| 9/9 [00:06<00:00,  1.29batches/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Metrics: {'train_loss': 1.4270186270617693, 'test_loss': 3.084889536970979, 'bleu': 2.645886013986014, 'gen_len': 29.01398391608392}\n",
      "\n",
      "=============================\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 32%|███▏      | 8/25 [02:50<06:00, 21.22s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "For epoch 14: \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train batch number 2:   0%|          | 0/86 [00:00<?, ?batches/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train batch number 87: 100%|██████████| 86/86 [00:13<00:00,  6.41batches/s]\n",
      "Test batch number 2:   0%|          | 0/9 [00:00<?, ?batches/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Test batch number 10: 100%|██████████| 9/9 [00:06<00:00,  1.33batches/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Metrics: {'train_loss': 1.3568135406520627, 'test_loss': 3.039921688866782, 'bleu': 2.096099300699301, 'gen_len': 27.216797902097902}\n",
      "\n",
      "=============================\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 36%|███▌      | 9/25 [03:11<05:37, 21.12s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "For epoch 15: \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train batch number 2:   0%|          | 0/86 [00:00<?, ?batches/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train batch number 87: 100%|██████████| 86/86 [00:13<00:00,  6.50batches/s]\n",
      "Test batch number 2:   0%|          | 0/9 [00:00<?, ?batches/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Test batch number 10: 100%|██████████| 9/9 [00:07<00:00,  1.28batches/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Metrics: {'train_loss': 1.2849820408372312, 'test_loss': 3.1051486388786693, 'bleu': 3.5916318181818183, 'gen_len': 29.580393006993006}\n",
      "\n",
      "=============================\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 40%|████      | 10/25 [03:32<05:16, 21.07s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "For epoch 16: \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train batch number 2:   0%|          | 0/86 [00:00<?, ?batches/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train batch number 87: 100%|██████████| 86/86 [00:13<00:00,  6.45batches/s]\n",
      "Test batch number 2:   0%|          | 0/9 [00:00<?, ?batches/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Test batch number 10: 100%|██████████| 9/9 [00:06<00:00,  1.36batches/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Metrics: {'train_loss': 1.2035034914815803, 'test_loss': 3.0593154080264218, 'bleu': 3.5271048951048947, 'gen_len': 27.00347762237762}\n",
      "\n",
      "=============================\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 44%|████▍     | 11/25 [03:52<04:53, 20.95s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "For epoch 17: \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train batch number 2:   0%|          | 0/86 [00:00<?, ?batches/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train batch number 87: 100%|██████████| 86/86 [00:13<00:00,  6.45batches/s]\n",
      "Test batch number 2:   0%|          | 0/9 [00:00<?, ?batches/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Test batch number 10: 100%|██████████| 9/9 [00:06<00:00,  1.32batches/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Metrics: {'train_loss': 1.1423003066430517, 'test_loss': 3.2354436237495254, 'bleu': 3.003247552447552, 'gen_len': 29.86014405594406}\n",
      "\n",
      "=============================\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 48%|████▊     | 12/25 [04:13<04:32, 20.94s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "For epoch 18: \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train batch number 2:   0%|          | 0/86 [00:00<?, ?batches/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train batch number 87: 100%|██████████| 86/86 [00:13<00:00,  6.46batches/s]\n",
      "Test batch number 2:   0%|          | 0/9 [00:00<?, ?batches/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Test batch number 10: 100%|██████████| 9/9 [00:07<00:00,  1.26batches/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Metrics: {'train_loss': 1.0782466187525848, 'test_loss': 3.068224074957254, 'bleu': 3.4960227272727273, 'gen_len': 30.300693706293707}\n",
      "\n",
      "=============================\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 52%|█████▏    | 13/25 [04:35<04:12, 21.03s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "For epoch 19: \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train batch number 2:   0%|          | 0/86 [00:00<?, ?batches/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train batch number 87: 100%|██████████| 86/86 [00:13<00:00,  6.51batches/s]\n",
      "Test batch number 2:   0%|          | 0/9 [00:00<?, ?batches/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Test batch number 10: 100%|██████████| 9/9 [00:06<00:00,  1.34batches/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Metrics: {'train_loss': 1.0126017112078027, 'test_loss': 3.157427514349664, 'bleu': 3.7345384615384614, 'gen_len': 30.650358041958047}\n",
      "\n",
      "=============================\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 56%|█████▌    | 14/25 [04:55<03:50, 20.94s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "For epoch 20: \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train batch number 2:   0%|          | 0/86 [00:00<?, ?batches/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train batch number 87: 100%|██████████| 86/86 [00:13<00:00,  6.49batches/s]\n",
      "Test batch number 2:   0%|          | 0/9 [00:00<?, ?batches/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Test batch number 10: 100%|██████████| 9/9 [00:07<00:00,  1.28batches/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Metrics: {'train_loss': 0.9505923225250786, 'test_loss': 3.2311591268419386, 'bleu': 4.545248951048951, 'gen_len': 29.867105594405597}\n",
      "\n",
      "=============================\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 60%|██████    | 15/25 [05:16<03:29, 20.99s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "For epoch 21: \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train batch number 2:   0%|          | 0/86 [00:00<?, ?batches/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train batch number 87: 100%|██████████| 86/86 [00:13<00:00,  6.44batches/s]\n",
      "Test batch number 2:   0%|          | 0/9 [00:00<?, ?batches/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Test batch number 10: 100%|██████████| 9/9 [00:07<00:00,  1.28batches/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Metrics: {'train_loss': 0.9221569982401709, 'test_loss': 3.342424379362093, 'bleu': 4.577914685314686, 'gen_len': 29.97205734265734}\n",
      "\n",
      "=============================\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 64%|██████▍   | 16/25 [05:38<03:09, 21.03s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "For epoch 22: \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train batch number 2:   0%|          | 0/86 [00:00<?, ?batches/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train batch number 87: 100%|██████████| 86/86 [00:13<00:00,  6.49batches/s]\n",
      "Test batch number 2:   0%|          | 0/9 [00:00<?, ?batches/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Test batch number 10: 100%|██████████| 9/9 [00:07<00:00,  1.24batches/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Metrics: {'train_loss': 0.8686813425580567, 'test_loss': 3.3138423116057067, 'bleu': 4.567988811188811, 'gen_len': 29.0175013986014}\n",
      "\n",
      "=============================\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 68%|██████▊   | 17/25 [05:59<02:48, 21.08s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "For epoch 23: \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train batch number 2:   0%|          | 0/86 [00:00<?, ?batches/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train batch number 87: 100%|██████████| 86/86 [00:13<00:00,  6.44batches/s]\n",
      "Test batch number 2:   0%|          | 0/9 [00:00<?, ?batches/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Test batch number 10: 100%|██████████| 9/9 [00:07<00:00,  1.25batches/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Metrics: {'train_loss': 0.805647796151038, 'test_loss': 3.304755932801253, 'bleu': 5.165895104895106, 'gen_len': 30.206317482517484}\n",
      "\n",
      "=============================\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 72%|███████▏  | 18/25 [06:20<02:28, 21.14s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "For epoch 24: \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train batch number 2:   0%|          | 0/86 [00:00<?, ?batches/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train batch number 87: 100%|██████████| 86/86 [00:13<00:00,  6.44batches/s]\n",
      "Test batch number 2:   0%|          | 0/9 [00:00<?, ?batches/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Test batch number 10: 100%|██████████| 9/9 [00:06<00:00,  1.30batches/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Metrics: {'train_loss': 0.7463729696834536, 'test_loss': 3.153714696844141, 'bleu': 5.976082167832168, 'gen_len': 25.657329370629373}\n",
      "\n",
      "=============================\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 76%|███████▌  | 19/25 [06:41<02:06, 21.10s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "For epoch 25: \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train batch number 2:   0%|          | 0/86 [00:00<?, ?batches/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train batch number 87: 100%|██████████| 86/86 [00:13<00:00,  6.43batches/s]\n",
      "Test batch number 2:   0%|          | 0/9 [00:00<?, ?batches/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Test batch number 10: 100%|██████████| 9/9 [00:06<00:00,  1.37batches/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Metrics: {'train_loss': 0.7106834085496115, 'test_loss': 3.3087262707156735, 'bleu': 4.279994055944056, 'gen_len': 28.41257832167832}\n",
      "\n",
      "=============================\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 80%|████████  | 20/25 [07:02<01:44, 20.96s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "For epoch 26: \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train batch number 2:   0%|          | 0/86 [00:00<?, ?batches/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train batch number 87: 100%|██████████| 86/86 [00:13<00:00,  6.21batches/s]\n",
      "Test batch number 2:   0%|          | 0/9 [00:00<?, ?batches/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Test batch number 10: 100%|██████████| 9/9 [00:07<00:00,  1.26batches/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Metrics: {'train_loss': 0.6612085418610337, 'test_loss': 3.3331449115192973, 'bleu': 4.954048951048952, 'gen_len': 28.080423776223782}\n",
      "\n",
      "=============================\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 84%|████████▍ | 21/25 [07:23<01:24, 21.18s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "For epoch 27: \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train batch number 2:   0%|          | 0/86 [00:00<?, ?batches/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train batch number 87: 100%|██████████| 86/86 [00:13<00:00,  6.35batches/s]\n",
      "Test batch number 2:   0%|          | 0/9 [00:00<?, ?batches/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Test batch number 10: 100%|██████████| 9/9 [00:07<00:00,  1.25batches/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Metrics: {'train_loss': 0.626731078167722, 'test_loss': 3.367448740072183, 'bleu': 6.475375174825176, 'gen_len': 28.678298601398602}\n",
      "\n",
      "=============================\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 88%|████████▊ | 22/25 [07:45<01:03, 21.27s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "For epoch 28: \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train batch number 2:   0%|          | 0/86 [00:00<?, ?batches/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train batch number 87: 100%|██████████| 86/86 [00:13<00:00,  6.36batches/s]\n",
      "Test batch number 2:   0%|          | 0/9 [00:00<?, ?batches/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Test batch number 10: 100%|██████████| 9/9 [00:07<00:00,  1.25batches/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Metrics: {'train_loss': 0.589140331246786, 'test_loss': 3.37711390581998, 'bleu': 6.344759090909091, 'gen_len': 30.244778321678325}\n",
      "\n",
      "=============================\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 92%|█████████▏| 23/25 [08:06<00:42, 21.32s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "For epoch 29: \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train batch number 2:   0%|          | 0/86 [00:00<?, ?batches/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train batch number 87: 100%|██████████| 86/86 [00:13<00:00,  6.34batches/s]\n",
      "Test batch number 2:   0%|          | 0/9 [00:00<?, ?batches/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Test batch number 10: 100%|██████████| 9/9 [00:06<00:00,  1.30batches/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Metrics: {'train_loss': 0.5504833005073876, 'test_loss': 3.431763330539623, 'bleu': 7.882058041958043, 'gen_len': 27.56643636363637}\n",
      "\n",
      "=============================\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 96%|█████████▌| 24/25 [08:27<00:21, 21.29s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "For epoch 30: \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train batch number 2:   0%|          | 0/86 [00:00<?, ?batches/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train batch number 87: 100%|██████████| 86/86 [00:13<00:00,  6.29batches/s]\n",
      "Test batch number 2:   0%|          | 0/9 [00:00<?, ?batches/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Test batch number 10: 100%|██████████| 9/9 [00:07<00:00,  1.15batches/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Metrics: {'train_loss': 0.5199076705752737, 'test_loss': 3.3950424027609665, 'bleu': 6.214842657342657, 'gen_len': 27.10490769230769}\n",
      "\n",
      "=============================\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 25/25 [08:50<00:00, 21.21s/it]\n"
     ]
    }
   ],
   "source": [
    "# with warnings.catch_warnings():\n",
    "    # warnings.simplefilter(\"ignore\")\n",
    "trainer = train(config)\n",
    "\n",
    "# save if necessary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0it [00:00, ?it/s]\n"
     ]
    }
   ],
   "source": [
    "# with warnings.catch_warnings():\n",
    "    # warnings.simplefilter(\"ignore\")\n",
    "trainer = train(config)\n",
    "\n",
    "# save if necessary"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "➡️ Predictions\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluation batch number 2:   0%|          | 0/7 [00:00<?, ?batches/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluation batch number 8: 100%|██████████| 7/7 [00:04<00:00,  1.46batches/s]\n"
     ]
    }
   ],
   "source": [
    "if not trainer is None:\n",
    "    \n",
    "    # recuperate the tokenizer\n",
    "    tokenizer = T5TokenizerFast(config['tokenizer_path'])\n",
    "    \n",
    "    # recuperate the test dataset\n",
    "    # initialize the transformation sequence\n",
    "    end_mark_fn = partial(add_end_mark)\n",
    "    augmentation = TransformerSequences(remove_mark_space, delete_guillemet_space, add_mark_space, end_mark_fn)\n",
    "\n",
    "\n",
    "    # let us get the test set\n",
    "    test_dataset = SentenceDataset(f\"{config['data_directory']}test_set.csv\",\n",
    "                                            tokenizer = tokenizer,\n",
    "                                            cp1_transformer = augmentation,\n",
    "                                            cp2_transformer = augmentation,\n",
    "                                            corpus_1=config['corpus_1'],\n",
    "                                            corpus_2=config['corpus_2'],\n",
    "                                            truncation = False)\n",
    "\n",
    "    # initialize the bucket samplers for distributed environment\n",
    "    boundaries = config['boundaries']\n",
    "    batch_sizes = config['batch_sizes']\n",
    "\n",
    "    test_sampler = SequenceLengthBatchSampler(test_dataset,\n",
    "                                                boundaries = boundaries,\n",
    "                                                batch_sizes = batch_sizes)\n",
    "\n",
    "    test_loader_args = {'batch_sampler': test_sampler, 'collate_fn': collate_fn,\n",
    "                            'num_workers': config['num_workers'], 'pin_memory': config['pin_memory']}\n",
    "\n",
    "    metrics, prediction = trainer.evaluate(test_dataset, test_loader_args)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'test_loss': 3.406999619690688,\n",
       " 'bleu': 5.349946153846154,\n",
       " 'gen_len': 24.363641608391607}"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>original_sentences</th>\n",
       "      <th>translations</th>\n",
       "      <th>predictions</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Nit la ku baax.</td>\n",
       "      <td>C'est un homme gentil.</td>\n",
       "      <td>C'est un homme de un homme gentil.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Góor gi dana dem.</td>\n",
       "      <td>L'homme ira.</td>\n",
       "      <td>L'homme qui allait partir.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Looy ñaan?</td>\n",
       "      <td>Tu pries pour quoi?</td>\n",
       "      <td>Qu'as-tu acheté?</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Maa di dem.</td>\n",
       "      <td>C'est moi qui pars.</td>\n",
       "      <td>C'est moi qui vais partir de partir.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Ci biir.</td>\n",
       "      <td>À l'intérieur.</td>\n",
       "      <td>Chez celui qu'il a indiqué.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>281</th>\n",
       "      <td>Teg nañ ciy ati-at ma door a jëli ni jigéen, n...</td>\n",
       "      <td>Ce n'est que longtemps après, quand l'égoïsme ...</td>\n",
       "      <td>Je me souviens de la violence en Afrique. Non...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>282</th>\n",
       "      <td>Li wóor te wér moo di ne bi loolu lépp weesoo,...</td>\n",
       "      <td>J'ai ressenti de l'étonnement, et même de l'in...</td>\n",
       "      <td>Je me souviens de la violence en Afrique. Non...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>283</th>\n",
       "      <td>Dàtti garab yaa ngi lunk, sànneeku jëm ca kow,...</td>\n",
       "      <td>À quel point les arbres aux troncs rectilignes...</td>\n",
       "      <td>Je me souviens de la violence en Afrique. Non...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>284</th>\n",
       "      <td>Li koy yëngal noonu, xam naa ko. Lan moo ko dà...</td>\n",
       "      <td>Je peux ressentir l'émotion qu'il éprouve à tr...</td>\n",
       "      <td>Jusqu'à présent, on se moquait des planteurs ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>285</th>\n",
       "      <td>Ku ma laajoon fan laa nekk, ma ni la : « Man? ...</td>\n",
       "      <td>J'étais vraiment sur le pont d'un bateau. Le b...</td>\n",
       "      <td>Une force électrique qu'il fondait cette just...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>286 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                    original_sentences  \\\n",
       "0                                      Nit la ku baax.   \n",
       "1                                    Góor gi dana dem.   \n",
       "2                                           Looy ñaan?   \n",
       "3                                          Maa di dem.   \n",
       "4                                             Ci biir.   \n",
       "..                                                 ...   \n",
       "281  Teg nañ ciy ati-at ma door a jëli ni jigéen, n...   \n",
       "282  Li wóor te wér moo di ne bi loolu lépp weesoo,...   \n",
       "283  Dàtti garab yaa ngi lunk, sànneeku jëm ca kow,...   \n",
       "284  Li koy yëngal noonu, xam naa ko. Lan moo ko dà...   \n",
       "285  Ku ma laajoon fan laa nekk, ma ni la : « Man? ...   \n",
       "\n",
       "                                          translations  \\\n",
       "0                               C'est un homme gentil.   \n",
       "1                                         L'homme ira.   \n",
       "2                                  Tu pries pour quoi?   \n",
       "3                                  C'est moi qui pars.   \n",
       "4                                       À l'intérieur.   \n",
       "..                                                 ...   \n",
       "281  Ce n'est que longtemps après, quand l'égoïsme ...   \n",
       "282  J'ai ressenti de l'étonnement, et même de l'in...   \n",
       "283  À quel point les arbres aux troncs rectilignes...   \n",
       "284  Je peux ressentir l'émotion qu'il éprouve à tr...   \n",
       "285  J'étais vraiment sur le pont d'un bateau. Le b...   \n",
       "\n",
       "                                           predictions  \n",
       "0                   C'est un homme de un homme gentil.  \n",
       "1                           L'homme qui allait partir.  \n",
       "2                                     Qu'as-tu acheté?  \n",
       "3                 C'est moi qui vais partir de partir.  \n",
       "4                          Chez celui qu'il a indiqué.  \n",
       "..                                                 ...  \n",
       "281   Je me souviens de la violence en Afrique. Non...  \n",
       "282   Je me souviens de la violence en Afrique. Non...  \n",
       "283   Je me souviens de la violence en Afrique. Non...  \n",
       "284   Jusqu'à présent, on se moquait des planteurs ...  \n",
       "285   Une force électrique qu'il fondait cette just...  \n",
       "\n",
       "[286 rows x 3 columns]"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prediction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "--------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### French-Wolof v7"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "➡️ Import the libraries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Global seed set to 0\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from wolof_translate import *\n",
    "\n",
    "# specify a seed for everything\n",
    "lt.seed_everything(0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "➡️ Function to recuperate datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting wolof-translate/wolof_translate/utils/recuperate_datasets.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile wolof-translate/wolof_translate/utils/recuperate_datasets.py\n",
    "from wolof_translate import *\n",
    "\n",
    "def recuperate_datasets(char_p: float, word_p: float, max_len: int, end_mark: int, tokenizer: T5TokenizerFast,\n",
    "                        corpus_1: str = 'french', corpus_2: str = 'wolof', \n",
    "                        train_file: str = 'data/extractions/new_data/train_set.csv', \n",
    "                        test_file: str = 'data/extractions/new_data/test_file.csv'):\n",
    "\n",
    "  # Let us recuperate the end_mark adding option\n",
    "  if end_mark == 1:\n",
    "    # Create augmentation to add on French sentences\n",
    "    fr_augmentation_1 = TransformerSequences(nac.KeyboardAug(aug_char_p=char_p, aug_word_p=word_p,\n",
    "                                                             aug_word_max = max_len),\n",
    "                                          remove_mark_space, delete_guillemet_space, add_mark_space)\n",
    "\n",
    "    fr_augmentation_2 = TransformerSequences(remove_mark_space, delete_guillemet_space, add_mark_space)\n",
    "    \n",
    "  else:\n",
    "    \n",
    "    if end_mark == 2:\n",
    "\n",
    "      end_mark_fn = partial(add_end_mark, end_mark_to_remove = '!', replace = True)\n",
    "    \n",
    "    elif end_mark == 3:\n",
    "\n",
    "      end_mark_fn = partial(add_end_mark)\n",
    "    \n",
    "    elif end_mark == 4:\n",
    "\n",
    "      end_mark_fn = partial(add_end_mark, end_mark_to_remove = '!')\n",
    "    \n",
    "    else:  \n",
    "        \n",
    "        raise ValueError(f'No end mark number {end_mark}')\n",
    "\n",
    "    # Create augmentation to add on French sentences\n",
    "    fr_augmentation_1 = TransformerSequences(nac.KeyboardAug(aug_char_p=char_p, aug_word_p=word_p,\n",
    "                                                             aug_word_max = max_len),\n",
    "                                          remove_mark_space, delete_guillemet_space, add_mark_space, end_mark_fn)\n",
    "    \n",
    "    fr_augmentation_2 = TransformerSequences(remove_mark_space, delete_guillemet_space, add_mark_space, end_mark_fn)\n",
    "    \n",
    "  # Recuperate the train dataset\n",
    "  train_dataset_aug = SentenceDataset(train_file,\n",
    "                                        tokenizer,\n",
    "                                        truncation = False,\n",
    "                                        cp1_transformer = fr_augmentation_1,\n",
    "                                        cp2_transformer = fr_augmentation_2,\n",
    "                                        corpus_1=corpus_1,\n",
    "                                        corpus_2=corpus_2\n",
    "                                        )\n",
    "\n",
    "  # Recuperate the valid dataset\n",
    "  valid_dataset = SentenceDataset(test_file,\n",
    "                                        tokenizer,\n",
    "                                        cp1_transformer = fr_augmentation_2,\n",
    "                                        cp2_transformer = fr_augmentation_2,\n",
    "                                        corpus_1=corpus_1,\n",
    "                                        corpus_2=corpus_2,\n",
    "                                        truncation = False)\n",
    "  \n",
    "  # Return the datasets\n",
    "  return train_dataset_aug, valid_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "%run wolof-translate/wolof_translate/utils/recuperate_datasets.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "➡️ Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "# initialize the configurations\n",
    "config = {\n",
    "    'epochs': 7,\n",
    "    'max_epoch': None,\n",
    "    'log_step': 1,\n",
    "    'metric_for_best_model': 'test_loss',\n",
    "    'metric_objective': 'minimize',\n",
    "    'corpus_1': 'french',\n",
    "    'corpus_2': 'wolof',\n",
    "    'train_file': 'data/extractions/new_data/train_set.csv',\n",
    "    'test_file': 'data/extractions/new_data/valid_set.csv',\n",
    "    'drop_out_rate': 0.02121451891074674,\n",
    "    'd_model': 512,\n",
    "    'n_head': 8,\n",
    "    'dim_ff': 2024,\n",
    "    'n_encoders': 6,\n",
    "    'n_decoders': 6,\n",
    "    'learning_rate': 0.0061194459986928336,\n",
    "    'weight_decay': 0.004113564518012536,\n",
    "    'char_p': 0.14939770357468674,\n",
    "    'word_p': 0.05670290376911641,\n",
    "    'end_mark': 3,\n",
    "    'label_smoothing': 0.1,\n",
    "    'max_len': 20,\n",
    "    'random_state': 0,\n",
    "    'boundaries': [2, 30, 57, 84, 112, 139, 166],\n",
    "    'batch_sizes': [256, 128, 64, 32, 16, 8, 4, 2],\n",
    "    'batch_size': None, \n",
    "    'warmup_init': False,\n",
    "    'relative_step': False,\n",
    "    'num_workers': 0,\n",
    "    'pin_memory': False,\n",
    "    # --------------------> Must be changed when continuing a training\n",
    "    'model_dir': 't5_small_v7_fw',\n",
    "    'new_model_dir': 't5_small_v7_fw',\n",
    "    'continue': True, # --------------------------> Must be changed when continuing training\n",
    "    'logging_dir': 'data/logs/t5_small_fw',\n",
    "    'save_best': True,\n",
    "    'tokenizer_path': 'wolof-translate/wolof_translate/tokenizers/t5_tokenizers/tokenizer_v6.model',\n",
    "    'data_directory': 'data/extractions/new_data/',\n",
    "    'data_file': 'corpora_v7.csv',\n",
    "    'version': 7,\n",
    "    # in the case of a distributed training\n",
    "    'backend': None,\n",
    "    'hosts': [],\n",
    "    'current_host': None,\n",
    "    'num_gpus': 5,\n",
    "    'logger': None,\n",
    "    'return_trainer': True,\n",
    "    'include_split': True,\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting wolof-translate/wolof_translate/utils/hg_training.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile wolof-translate/wolof_translate/utils/hg_training.py\n",
    "from wolof_translate import *\n",
    "import warnings\n",
    "\n",
    "def train(config: dict):\n",
    "    \n",
    "    # ---------------------------------------\n",
    "    # add distribution if necessary (https://github.com/aws/amazon-sagemaker-examples/blob/main/sagemaker-python-sdk/pytorch_mnist/mnist.py)\n",
    "    \n",
    "    logger = config['logger']\n",
    "    \n",
    "    is_distributed = len(config['hosts']) > 1 and config['backend'] is not None\n",
    "    \n",
    "    use_cuda = config['num_gpus'] > 0\n",
    "    \n",
    "    config.update({\"num_workers\": 1, \"pin_memory\": True} if use_cuda else {})\n",
    "\n",
    "    if not logger is None:\n",
    "        \n",
    "        logger.debug(\"Distributed training - {}\".format(is_distributed))\n",
    "        \n",
    "        logger.debug(\"Number of gpus available - {}\".format(config['num_gpus']))\n",
    "        \n",
    "    if is_distributed:\n",
    "        # Initialize the distributed environment.\n",
    "        world_size = len(config['hosts'])\n",
    "        \n",
    "        os.environ[\"WORLD_SIZE\"] = str(world_size)\n",
    "        \n",
    "        host_rank = config['hosts'].index(config['current_host'])\n",
    "        \n",
    "        os.environ[\"RANK\"] = str(host_rank)\n",
    "        \n",
    "        dist.init_process_group(backend=config['backend'], rank=host_rank, world_size=world_size)\n",
    "        \n",
    "        if not logger is None: logger.info(\n",
    "            \"Initialized the distributed environment: '{}' backend on {} nodes. \".format(\n",
    "                config['backend'], dist.get_world_size()\n",
    "            )\n",
    "            + \"Current host rank is {}. Number of gpus: {}\".format(dist.get_rank(), config['num_gpus'])\n",
    "        )\n",
    "    # ---------------------------------------\n",
    "    \n",
    "    # split the data\n",
    "    if config['include_split']: split_data(config['random_state'], config['data_directory'], config['data_file'])\n",
    "\n",
    "    # recuperate the tokenizer\n",
    "    tokenizer = T5TokenizerFast(config['tokenizer_path'])\n",
    "    \n",
    "    # Initialize the model name\n",
    "    model_name = 't5-small'\n",
    "\n",
    "    # import the model with its pre-trained weights\n",
    "    model = T5ForConditionalGeneration.from_pretrained(model_name)\n",
    "\n",
    "    # resize the token embeddings\n",
    "    model.resize_token_embeddings(len(tokenizer))\n",
    "    \n",
    "    # recuperate train and test set\n",
    "    train_dataset, test_dataset = recuperate_datasets(config['char_p'],\n",
    "                                                        config['word_p'], config['max_len'],\n",
    "                                                        config['end_mark'], tokenizer, config['corpus_1'],\n",
    "                                                        config['corpus_2'],\n",
    "                                                        config['train_file'], config['test_file'])\n",
    "    \n",
    "    # initialize the evaluation object\n",
    "    evaluation = TranslationEvaluation(tokenizer, train_dataset.decode)\n",
    "\n",
    "    # let us initialize the trainer\n",
    "    trainer = ModelRunner(model = model, version=config['version'], seed = 0, evaluation = evaluation, optimizer = Adafactor)\n",
    "\n",
    "    #-------------------------------------\n",
    "    # in the case when the linear learning rate scheduler with warmup is used\n",
    "    \n",
    "    # let us calculate the appropriate warmup steps (let us take a max epoch of 100)\n",
    "    # length = len(train_dataset)\n",
    "\n",
    "    # n_steps = length // config['batch_size']\n",
    "\n",
    "    # num_steps = config['max_epoch'] * n_steps\n",
    "\n",
    "    # warmup_steps = (config['max_epoch'] * n_steps) * config['warmup_ratio']\n",
    "\n",
    "    # Initialize the scheduler parameters\n",
    "    # scheduler_args = {'num_warmup_steps': warmup_steps, 'num_training_steps': num_steps}\n",
    "    #-------------------------------------\n",
    "\n",
    "    # Initialize the optimizer parameters\n",
    "    optimizer_args = {\n",
    "        'lr': config['learning_rate'],\n",
    "        'weight_decay': config['weight_decay'],\n",
    "        # 'betas': (0.9, 0.98),\n",
    "        'warmup_init': config['warmup_init'],\n",
    "        'relative_step': config['relative_step']\n",
    "    }\n",
    "\n",
    "    # ----------------------------\n",
    "    # initialize the bucket samplers for distributed environment\n",
    "    boundaries = config['boundaries']\n",
    "    batch_sizes = config['batch_sizes']\n",
    "\n",
    "    train_sampler = SequenceLengthBatchSampler(train_dataset,\n",
    "                                                boundaries = boundaries,\n",
    "                                                batch_sizes = batch_sizes)\n",
    "\n",
    "    test_sampler = SequenceLengthBatchSampler(test_dataset,\n",
    "                                                boundaries = boundaries,\n",
    "                                                batch_sizes = batch_sizes)\n",
    "\n",
    "    # ------------------------------\n",
    "    # initialize a bucket sampler with fixed batch size in the case of single machine\n",
    "    # with parallelization on multiple gpus\n",
    "    # train_sampler = BucketSampler(train_dataset, config['batch_size'])\n",
    "\n",
    "    # test_sampler = BucketSampler(test_dataset, config['batch_size'])\n",
    "    \n",
    "    # ------------------------------\n",
    "\n",
    "    # Initialize the loaders parameters\n",
    "    train_loader_args = {'batch_sampler': train_sampler, 'collate_fn': collate_fn,\n",
    "                        'num_workers': config['num_workers'], 'pin_memory': config['pin_memory']}\n",
    "\n",
    "    test_loader_args = {'batch_sampler': test_sampler, 'collate_fn': collate_fn,\n",
    "                        'num_workers': config['num_workers'], 'pin_memory': config['pin_memory']}\n",
    "\n",
    "    # Add the datasets and hyperparameters to trainer\n",
    "    trainer.compile(train_dataset, test_dataset, tokenizer, train_loader_args,\n",
    "                    test_loader_args, optimizer_kwargs = optimizer_args,\n",
    "                    # lr_scheduler=get_linear_schedule_with_warmup,\n",
    "                    # lr_scheduler_kwargs=scheduler_args,\n",
    "                    predict_with_generate = True,\n",
    "                    hugging_face = True,\n",
    "                    is_distributed=is_distributed,\n",
    "                    logging_dir=config['logging_dir'],\n",
    "                    dist=dist\n",
    "                    )\n",
    "\n",
    "    # load the model\n",
    "    trainer.load(config['model_dir'], load_best = not config['continue'])\n",
    "    \n",
    "    # Train the model\n",
    "    trainer.train(config['epochs'] - trainer.current_epoch, auto_save = True, log_step = config['log_step'], saving_directory=config['new_model_dir'], save_best = config['save_best'],\n",
    "                  metric_for_best_model = config['metric_for_best_model'], metric_objective = config['metric_objective'])\n",
    "    \n",
    "    if config['return_trainer']:\n",
    "        \n",
    "        return trainer\n",
    "    \n",
    "    return None\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Below train and save if we want."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "from wolof_translate.utils.hg_training import train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/25 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "For epoch 6: \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train batch number 2:   0%|          | 0/66 [00:00<?, ?batches/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train batch number 67: 100%|██████████| 66/66 [00:13<00:00,  4.80batches/s]\n",
      "Test batch number 2:   0%|          | 0/11 [00:00<?, ?batches/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Test batch number 12: 100%|██████████| 11/11 [00:09<00:00,  1.21batches/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Metrics: {'train_loss': 2.6273809150817358, 'test_loss': 3.429296156483838, 'bleu': 1.5858124786324788, 'gen_len': 19.186319658119654}\n",
      "\n",
      "=============================\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  4%|▍         | 1/25 [00:23<09:20, 23.35s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "For epoch 7: \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train batch number 2:   0%|          | 0/66 [00:00<?, ?batches/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train batch number 67: 100%|██████████| 66/66 [00:13<00:00,  4.76batches/s]\n",
      "Test batch number 2:   0%|          | 0/11 [00:00<?, ?batches/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Test batch number 12: 100%|██████████| 11/11 [00:05<00:00,  1.86batches/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Metrics: {'train_loss': 2.4541173095474713, 'test_loss': 3.404978168520153, 'bleu': 1.8367466666666665, 'gen_len': 18.454717606837605}\n",
      "\n",
      "=============================\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  8%|▊         | 2/25 [00:51<09:59, 26.06s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "For epoch 8: \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train batch number 2:   0%|          | 0/66 [00:00<?, ?batches/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train batch number 67: 100%|██████████| 66/66 [00:13<00:00,  4.77batches/s]\n",
      "Test batch number 3:   9%|▉         | 1/11 [00:00<00:02,  4.63batches/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Test batch number 12: 100%|██████████| 11/11 [00:06<00:00,  1.63batches/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Metrics: {'train_loss': 2.2870384227831813, 'test_loss': 3.443212355915298, 'bleu': 1.8307552136752137, 'gen_len': 19.482052136752134}\n",
      "\n",
      "=============================\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 12%|█▏        | 3/25 [01:12<08:47, 23.97s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "For epoch 9: \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train batch number 2:   0%|          | 0/66 [00:00<?, ?batches/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train batch number 67: 100%|██████████| 66/66 [00:13<00:00,  4.75batches/s]\n",
      "Test batch number 3:   9%|▉         | 1/11 [00:00<00:02,  4.99batches/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Test batch number 12: 100%|██████████| 11/11 [00:06<00:00,  1.66batches/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Metrics: {'train_loss': 2.1352149050545752, 'test_loss': 3.438325615418263, 'bleu': 2.647915384615385, 'gen_len': 18.136759145299145}\n",
      "\n",
      "=============================\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 16%|█▌        | 4/25 [01:35<08:09, 23.33s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "For epoch 10: \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train batch number 2:   0%|          | 0/66 [00:00<?, ?batches/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train batch number 67: 100%|██████████| 66/66 [00:13<00:00,  4.79batches/s]\n",
      "Test batch number 3:   9%|▉         | 1/11 [00:00<00:02,  4.87batches/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Test batch number 12: 100%|██████████| 11/11 [00:06<00:00,  1.63batches/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Metrics: {'train_loss': 1.986386069497356, 'test_loss': 3.5020873004554685, 'bleu': 2.401228205128205, 'gen_len': 20.008519145299143}\n",
      "\n",
      "=============================\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 20%|██        | 5/25 [01:58<07:49, 23.50s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "For epoch 11: \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train batch number 2:   0%|          | 0/66 [00:00<?, ?batches/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train batch number 67: 100%|██████████| 66/66 [00:13<00:00,  4.76batches/s]\n",
      "Test batch number 3:   9%|▉         | 1/11 [00:00<00:02,  4.78batches/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Test batch number 12: 100%|██████████| 11/11 [00:06<00:00,  1.62batches/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Metrics: {'train_loss': 1.8512997479776316, 'test_loss': 3.569902686583691, 'bleu': 2.6901911111111105, 'gen_len': 19.938464273504273}\n",
      "\n",
      "=============================\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 24%|██▍       | 6/25 [02:20<07:14, 22.89s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "For epoch 12: \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train batch number 2:   0%|          | 0/66 [00:00<?, ?batches/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train batch number 67: 100%|██████████| 66/66 [00:13<00:00,  4.79batches/s]\n",
      "Test batch number 3:   9%|▉         | 1/11 [00:00<00:02,  4.87batches/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Test batch number 12: 100%|██████████| 11/11 [00:06<00:00,  1.77batches/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Metrics: {'train_loss': 1.6897891996582928, 'test_loss': 3.564492826380281, 'bleu': 3.600856581196582, 'gen_len': 19.615387350427348}\n",
      "\n",
      "=============================\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 28%|██▊       | 7/25 [02:41<06:40, 22.26s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "For epoch 13: \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train batch number 2:   0%|          | 0/66 [00:00<?, ?batches/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train batch number 67: 100%|██████████| 66/66 [00:13<00:00,  4.77batches/s]\n",
      "Test batch number 3:   9%|▉         | 1/11 [00:00<00:02,  4.93batches/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Test batch number 12: 100%|██████████| 11/11 [00:06<00:00,  1.72batches/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Metrics: {'train_loss': 1.5803535098291233, 'test_loss': 3.5194729707179926, 'bleu': 3.8159319658119655, 'gen_len': 18.323057435897432}\n",
      "\n",
      "=============================\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 32%|███▏      | 8/25 [03:02<06:12, 21.91s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "For epoch 14: \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train batch number 2:   0%|          | 0/66 [00:00<?, ?batches/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train batch number 67: 100%|██████████| 66/66 [00:13<00:00,  4.78batches/s]\n",
      "Test batch number 3:   9%|▉         | 1/11 [00:00<00:02,  4.77batches/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Test batch number 12: 100%|██████████| 11/11 [00:06<00:00,  1.65batches/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Metrics: {'train_loss': 1.4440055558724063, 'test_loss': 3.507848788530399, 'bleu': 4.121050256410257, 'gen_len': 18.683748888888886}\n",
      "\n",
      "=============================\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 36%|███▌      | 9/25 [03:24<05:49, 21.85s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "For epoch 15: \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train batch number 2:   0%|          | 0/66 [00:00<?, ?batches/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train batch number 67: 100%|██████████| 66/66 [00:14<00:00,  4.65batches/s]\n",
      "Test batch number 2:   9%|▉         | 1/11 [00:00<00:02,  4.58batches/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Test batch number 12: 100%|██████████| 11/11 [00:06<00:00,  1.81batches/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Metrics: {'train_loss': 1.3341362899260145, 'test_loss': 3.62996331891443, 'bleu': 4.754367692307693, 'gen_len': 18.868364786324786}\n",
      "\n",
      "=============================\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 40%|████      | 10/25 [03:45<05:25, 21.71s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "For epoch 16: \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train batch number 2:   0%|          | 0/66 [00:00<?, ?batches/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train batch number 67: 100%|██████████| 66/66 [00:13<00:00,  4.73batches/s]\n",
      "Test batch number 3:   9%|▉         | 1/11 [00:00<00:02,  4.71batches/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Test batch number 12: 100%|██████████| 11/11 [00:06<00:00,  1.64batches/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Metrics: {'train_loss': 1.217524775293543, 'test_loss': 3.761451134314904, 'bleu': 4.313910769230768, 'gen_len': 19.651287521367518}\n",
      "\n",
      "=============================\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 44%|████▍     | 11/25 [04:07<05:03, 21.65s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "For epoch 17: \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train batch number 2:   0%|          | 0/66 [00:00<?, ?batches/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train batch number 67: 100%|██████████| 66/66 [00:13<00:00,  4.75batches/s]\n",
      "Test batch number 3:   9%|▉         | 1/11 [00:00<00:02,  4.69batches/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Test batch number 12: 100%|██████████| 11/11 [00:06<00:00,  1.74batches/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Metrics: {'train_loss': 1.116343072042749, 'test_loss': 3.5930790754464947, 'bleu': 5.23972888888889, 'gen_len': 18.610275897435898}\n",
      "\n",
      "=============================\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 48%|████▊     | 12/25 [04:31<04:50, 22.32s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "For epoch 18: \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train batch number 2:   0%|          | 0/66 [00:00<?, ?batches/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train batch number 67: 100%|██████████| 66/66 [00:13<00:00,  4.73batches/s]\n",
      "Test batch number 3:   9%|▉         | 1/11 [00:00<00:02,  4.71batches/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Test batch number 12: 100%|██████████| 11/11 [00:06<00:00,  1.79batches/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Metrics: {'train_loss': 1.0311605998779383, 'test_loss': 3.714331290253207, 'bleu': 4.78909076923077, 'gen_len': 19.056409743589743}\n",
      "\n",
      "=============================\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 52%|█████▏    | 13/25 [04:52<04:23, 21.93s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "For epoch 19: \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train batch number 2:   0%|          | 0/66 [00:00<?, ?batches/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train batch number 67: 100%|██████████| 66/66 [00:13<00:00,  4.73batches/s]\n",
      "Test batch number 3:   9%|▉         | 1/11 [00:00<00:02,  4.72batches/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Test batch number 12: 100%|██████████| 11/11 [00:05<00:00,  2.13batches/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Metrics: {'train_loss': 0.9403659137550634, 'test_loss': 3.700549722329164, 'bleu': 6.10097641025641, 'gen_len': 17.04446376068376}\n",
      "\n",
      "=============================\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 56%|█████▌    | 14/25 [05:12<03:55, 21.37s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "For epoch 20: \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train batch number 2:   0%|          | 0/66 [00:00<?, ?batches/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train batch number 67: 100%|██████████| 66/66 [00:14<00:00,  4.70batches/s]\n",
      "Test batch number 3:   9%|▉         | 1/11 [00:00<00:02,  4.88batches/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Test batch number 12: 100%|██████████| 11/11 [00:06<00:00,  1.78batches/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Metrics: {'train_loss': 0.8617357562644241, 'test_loss': 3.7089439701830225, 'bleu': 6.111125982905984, 'gen_len': 18.157270769230767}\n",
      "\n",
      "=============================\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 60%|██████    | 15/25 [05:33<03:32, 21.30s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "For epoch 21: \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train batch number 2:   0%|          | 0/66 [00:00<?, ?batches/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train batch number 67: 100%|██████████| 66/66 [00:14<00:00,  4.67batches/s]\n",
      "Test batch number 2:   0%|          | 0/11 [00:00<?, ?batches/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Test batch number 12: 100%|██████████| 11/11 [00:06<00:00,  1.65batches/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Metrics: {'train_loss': 0.7813965496348204, 'test_loss': 3.7829569767682982, 'bleu': 5.685519829059829, 'gen_len': 18.36067333333333}\n",
      "\n",
      "=============================\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 64%|██████▍   | 16/25 [05:55<03:12, 21.43s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "For epoch 22: \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train batch number 2:   0%|          | 0/66 [00:00<?, ?batches/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train batch number 67: 100%|██████████| 66/66 [00:14<00:00,  4.63batches/s]\n",
      "Test batch number 3:   9%|▉         | 1/11 [00:00<00:02,  4.81batches/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Test batch number 12: 100%|██████████| 11/11 [00:06<00:00,  1.80batches/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Metrics: {'train_loss': 0.7253713563683709, 'test_loss': 3.7712708505809815, 'bleu': 6.514730427350428, 'gen_len': 17.788026495726495}\n",
      "\n",
      "=============================\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 68%|██████▊   | 17/25 [06:17<02:53, 21.63s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "For epoch 23: \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train batch number 2:   0%|          | 0/66 [00:00<?, ?batches/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train batch number 67: 100%|██████████| 66/66 [00:14<00:00,  4.59batches/s]\n",
      "Test batch number 3:   9%|▉         | 1/11 [00:00<00:02,  4.75batches/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Test batch number 12: 100%|██████████| 11/11 [00:06<00:00,  1.70batches/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Metrics: {'train_loss': 0.6614922964706429, 'test_loss': 3.760771334884514, 'bleu': 7.574549059829059, 'gen_len': 17.596589059829057}\n",
      "\n",
      "=============================\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 72%|███████▏  | 18/25 [06:39<02:31, 21.66s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "For epoch 24: \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train batch number 2:   0%|          | 0/66 [00:00<?, ?batches/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train batch number 67: 100%|██████████| 66/66 [00:14<00:00,  4.58batches/s]\n",
      "Test batch number 3:   9%|▉         | 1/11 [00:00<00:02,  4.75batches/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Test batch number 12: 100%|██████████| 11/11 [00:06<00:00,  1.67batches/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Metrics: {'train_loss': 0.6032347261150415, 'test_loss': 3.9356272681146605, 'bleu': 7.386919999999999, 'gen_len': 18.449588547008545}\n",
      "\n",
      "=============================\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 76%|███████▌  | 19/25 [07:01<02:10, 21.78s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "For epoch 25: \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train batch number 2:   0%|          | 0/66 [00:00<?, ?batches/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train batch number 67: 100%|██████████| 66/66 [00:14<00:00,  4.56batches/s]\n",
      "Test batch number 2:   0%|          | 0/11 [00:00<?, ?batches/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Test batch number 12: 100%|██████████| 11/11 [00:05<00:00,  2.06batches/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Metrics: {'train_loss': 0.5677011773528785, 'test_loss': 4.0033436685545825, 'bleu': 7.683991111111111, 'gen_len': 17.141903247863244}\n",
      "\n",
      "=============================\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 80%|████████  | 20/25 [07:21<01:47, 21.47s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "For epoch 26: \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train batch number 2:   0%|          | 0/66 [00:00<?, ?batches/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train batch number 67: 100%|██████████| 66/66 [00:14<00:00,  4.54batches/s]\n",
      "Test batch number 3:   9%|▉         | 1/11 [00:00<00:02,  4.69batches/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Test batch number 12: 100%|██████████| 11/11 [00:06<00:00,  1.82batches/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Metrics: {'train_loss': 0.5069852984574893, 'test_loss': 3.9822296272995126, 'bleu': 7.061915042735043, 'gen_len': 17.222232136752133}\n",
      "\n",
      "=============================\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 84%|████████▍ | 21/25 [07:43<01:25, 21.45s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "For epoch 27: \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train batch number 2:   0%|          | 0/66 [00:00<?, ?batches/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train batch number 67: 100%|██████████| 66/66 [00:14<00:00,  4.53batches/s]\n",
      "Test batch number 3:   9%|▉         | 1/11 [00:00<00:02,  4.66batches/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Test batch number 12: 100%|██████████| 11/11 [00:06<00:00,  1.65batches/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Metrics: {'train_loss': 0.4642480768636187, 'test_loss': 4.044623067643908, 'bleu': 8.169699316239315, 'gen_len': 18.22220632478632}\n",
      "\n",
      "=============================\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 88%|████████▊ | 22/25 [08:05<01:05, 21.67s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "For epoch 28: \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train batch number 2:   0%|          | 0/66 [00:00<?, ?batches/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train batch number 67: 100%|██████████| 66/66 [00:14<00:00,  4.52batches/s]\n",
      "Test batch number 3:   9%|▉         | 1/11 [00:00<00:02,  4.67batches/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Test batch number 12: 100%|██████████| 11/11 [00:06<00:00,  1.69batches/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Metrics: {'train_loss': 0.44251986547597116, 'test_loss': 4.129232294946655, 'bleu': 7.961665470085469, 'gen_len': 17.35555213675213}\n",
      "\n",
      "=============================\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 92%|█████████▏| 23/25 [08:27<00:43, 21.79s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "For epoch 29: \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train batch number 2:   0%|          | 0/66 [00:00<?, ?batches/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train batch number 67: 100%|██████████| 66/66 [00:14<00:00,  4.49batches/s]\n",
      "Test batch number 3:   9%|▉         | 1/11 [00:00<00:02,  4.66batches/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Test batch number 12: 100%|██████████| 11/11 [00:06<00:00,  1.79batches/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Metrics: {'train_loss': 0.41743088178926147, 'test_loss': 4.249863563439785, 'bleu': 7.452027692307692, 'gen_len': 17.839301025641024}\n",
      "\n",
      "=============================\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 96%|█████████▌| 24/25 [08:49<00:21, 21.77s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "For epoch 30: \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train batch number 2:   0%|          | 0/66 [00:00<?, ?batches/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train batch number 67: 100%|██████████| 66/66 [00:14<00:00,  4.45batches/s]\n",
      "Test batch number 3:   9%|▉         | 1/11 [00:00<00:02,  4.66batches/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Test batch number 12: 100%|██████████| 11/11 [00:05<00:00,  1.87batches/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Metrics: {'train_loss': 0.3819201127741519, 'test_loss': 4.304435429206262, 'bleu': 7.999988205128205, 'gen_len': 17.58118068376068}\n",
      "\n",
      "=============================\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 25/25 [09:10<00:00, 22.04s/it]\n"
     ]
    }
   ],
   "source": [
    "# with warnings.catch_warnings():\n",
    "    # warnings.simplefilter(\"ignore\")\n",
    "trainer = train(config)\n",
    "\n",
    "# save if necessary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0it [00:00, ?it/s]\n"
     ]
    }
   ],
   "source": [
    "# with warnings.catch_warnings():\n",
    "    # warnings.simplefilter(\"ignore\")\n",
    "trainer = train(config)\n",
    "\n",
    "# save if necessary"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "➡️ Predictions\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluation batch number 2:   0%|          | 0/11 [00:00<?, ?batches/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluation batch number 12: 100%|██████████| 11/11 [00:09<00:00,  1.18batches/s]\n"
     ]
    }
   ],
   "source": [
    "if not trainer is None:\n",
    "    \n",
    "    # recuperate the tokenizer\n",
    "    tokenizer = T5TokenizerFast(config['tokenizer_path'])\n",
    "    \n",
    "    # recuperate the test dataset\n",
    "    # initialize the transformation sequence\n",
    "    end_mark_fn = partial(add_end_mark)\n",
    "    augmentation = TransformerSequences(remove_mark_space, delete_guillemet_space, add_mark_space, end_mark_fn)\n",
    "\n",
    "\n",
    "    # let us get the test set\n",
    "    test_dataset = SentenceDataset(f\"{config['data_directory']}test_set.csv\",\n",
    "                                            tokenizer = tokenizer,\n",
    "                                            cp1_transformer = augmentation,\n",
    "                                            cp2_transformer = augmentation,\n",
    "                                            corpus_1=config['corpus_1'],\n",
    "                                            corpus_2=config['corpus_2'],\n",
    "                                            truncation = False)\n",
    "\n",
    "    # initialize the bucket samplers for distributed environment\n",
    "    boundaries = config['boundaries']\n",
    "    batch_sizes = config['batch_sizes']\n",
    "\n",
    "    test_sampler = SequenceLengthBatchSampler(test_dataset,\n",
    "                                                boundaries = boundaries,\n",
    "                                                batch_sizes = batch_sizes)\n",
    "\n",
    "    test_loader_args = {'batch_sampler': test_sampler, 'collate_fn': collate_fn,\n",
    "                            'num_workers': config['num_workers'], 'pin_memory': config['pin_memory']}\n",
    "\n",
    "    metrics, prediction = trainer.evaluate(test_dataset, test_loader_args)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'test_loss': 4.216073186988504,\n",
       " 'bleu': 6.631126153846155,\n",
       " 'gen_len': 18.235899999999994}"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>original_sentences</th>\n",
       "      <th>translations</th>\n",
       "      <th>predictions</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Toi tu connais personne.</td>\n",
       "      <td>Yaw xamuloo kenn.</td>\n",
       "      <td>Yaw doonkoon ŋga suñu njiit.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Donne un à chacun!</td>\n",
       "      <td>Mayal benn ñépp!</td>\n",
       "      <td>Mayal ñépp benn!</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Je parle de ce petit.</td>\n",
       "      <td>Xale buu laa wax.</td>\n",
       "      <td>Maa ngiy féexlu ci suba.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Viens m'accompagner au marché.</td>\n",
       "      <td>Kaay gunge ma marse.</td>\n",
       "      <td>Kaay tànnal ma marse ba.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Où est la récompense?</td>\n",
       "      <td>Ana njëgu guró gi?</td>\n",
       "      <td>Ana boroom kër gi?</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>580</th>\n",
       "      <td>Je peux ressentir l'émotion qu'il éprouve à tr...</td>\n",
       "      <td>Li koy yëngal noonu, xam naa ko. Lan moo ko dà...</td>\n",
       "      <td>Coppite gi may njort doon na àgg ba ci waxina...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>581</th>\n",
       "      <td>C'était avant la guerre, avant la solitude et ...</td>\n",
       "      <td>Walla ma ni boog loolu la doon gént, laata xar...</td>\n",
       "      <td>Su dee, su ma juumulee, baayam « District Off...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>582</th>\n",
       "      <td>Je ne m'en souviens pas, mais j'ai dû crier, h...</td>\n",
       "      <td>Xam naa damaa waroon a yuuxu keroog ndax sama ...</td>\n",
       "      <td>Sama xel dellu ci Afrig, am kàddu gu riir ci ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>583</th>\n",
       "      <td>Ou bien les récits de grands Blancs qui voyage...</td>\n",
       "      <td>Walla boog mu mas maa may ci jaloorey ponkali ...</td>\n",
       "      <td>Du lu war a jaaxal kenn. Su dee loolu la, maa...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>584</th>\n",
       "      <td>Dans la case que nous habitions (le mot case a...</td>\n",
       "      <td>Ñu ngi dëkkoon ci benn néegu-ňax. Baatu “néegu...</td>\n",
       "      <td>Waaw, Ibadaŋ ak Kótónu, naka noonu. Mbirum Bo...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>585 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                    original_sentences  \\\n",
       "0                             Toi tu connais personne.   \n",
       "1                                   Donne un à chacun!   \n",
       "2                                Je parle de ce petit.   \n",
       "3                       Viens m'accompagner au marché.   \n",
       "4                                Où est la récompense?   \n",
       "..                                                 ...   \n",
       "580  Je peux ressentir l'émotion qu'il éprouve à tr...   \n",
       "581  C'était avant la guerre, avant la solitude et ...   \n",
       "582  Je ne m'en souviens pas, mais j'ai dû crier, h...   \n",
       "583  Ou bien les récits de grands Blancs qui voyage...   \n",
       "584  Dans la case que nous habitions (le mot case a...   \n",
       "\n",
       "                                          translations  \\\n",
       "0                                    Yaw xamuloo kenn.   \n",
       "1                                     Mayal benn ñépp!   \n",
       "2                                    Xale buu laa wax.   \n",
       "3                                 Kaay gunge ma marse.   \n",
       "4                                   Ana njëgu guró gi?   \n",
       "..                                                 ...   \n",
       "580  Li koy yëngal noonu, xam naa ko. Lan moo ko dà...   \n",
       "581  Walla ma ni boog loolu la doon gént, laata xar...   \n",
       "582  Xam naa damaa waroon a yuuxu keroog ndax sama ...   \n",
       "583  Walla boog mu mas maa may ci jaloorey ponkali ...   \n",
       "584  Ñu ngi dëkkoon ci benn néegu-ňax. Baatu “néegu...   \n",
       "\n",
       "                                           predictions  \n",
       "0                         Yaw doonkoon ŋga suñu njiit.  \n",
       "1                                     Mayal ñépp benn!  \n",
       "2                             Maa ngiy féexlu ci suba.  \n",
       "3                             Kaay tànnal ma marse ba.  \n",
       "4                                   Ana boroom kër gi?  \n",
       "..                                                 ...  \n",
       "580   Coppite gi may njort doon na àgg ba ci waxina...  \n",
       "581   Su dee, su ma juumulee, baayam « District Off...  \n",
       "582   Sama xel dellu ci Afrig, am kàddu gu riir ci ...  \n",
       "583   Du lu war a jaaxal kenn. Su dee loolu la, maa...  \n",
       "584   Waaw, Ibadaŋ ak Kótónu, naka noonu. Mbirum Bo...  \n",
       "\n",
       "[585 rows x 3 columns]"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prediction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "----------------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Wolof-French v7"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "➡️ Import the libraries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Global seed set to 0\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from wolof_translate import *\n",
    "\n",
    "# specify a seed for everything\n",
    "lt.seed_everything(0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "➡️ Function to recuperate datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting wolof-translate/wolof_translate/utils/recuperate_datasets.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile wolof-translate/wolof_translate/utils/recuperate_datasets.py\n",
    "from wolof_translate import *\n",
    "\n",
    "def recuperate_datasets(char_p: float, word_p: float, max_len: int, end_mark: int, tokenizer: T5TokenizerFast,\n",
    "                        corpus_1: str = 'french', corpus_2: str = 'wolof', \n",
    "                        train_file: str = 'data/extractions/new_data/train_set.csv', \n",
    "                        test_file: str = 'data/extractions/new_data/test_file.csv'):\n",
    "\n",
    "  # Let us recuperate the end_mark adding option\n",
    "  if end_mark == 1:\n",
    "    # Create augmentation to add on French sentences\n",
    "    fr_augmentation_1 = TransformerSequences(nac.KeyboardAug(aug_char_p=char_p, aug_word_p=word_p,\n",
    "                                                             aug_word_max = max_len),\n",
    "                                          remove_mark_space, delete_guillemet_space, add_mark_space)\n",
    "\n",
    "    fr_augmentation_2 = TransformerSequences(remove_mark_space, delete_guillemet_space, add_mark_space)\n",
    "    \n",
    "  else:\n",
    "    \n",
    "    if end_mark == 2:\n",
    "\n",
    "      end_mark_fn = partial(add_end_mark, end_mark_to_remove = '!', replace = True)\n",
    "    \n",
    "    elif end_mark == 3:\n",
    "\n",
    "      end_mark_fn = partial(add_end_mark)\n",
    "    \n",
    "    elif end_mark == 4:\n",
    "\n",
    "      end_mark_fn = partial(add_end_mark, end_mark_to_remove = '!')\n",
    "    \n",
    "    else:  \n",
    "        \n",
    "        raise ValueError(f'No end mark number {end_mark}')\n",
    "\n",
    "    # Create augmentation to add on French sentences\n",
    "    fr_augmentation_1 = TransformerSequences(nac.KeyboardAug(aug_char_p=char_p, aug_word_p=word_p,\n",
    "                                                             aug_word_max = max_len),\n",
    "                                          remove_mark_space, delete_guillemet_space, add_mark_space, end_mark_fn)\n",
    "    \n",
    "    fr_augmentation_2 = TransformerSequences(remove_mark_space, delete_guillemet_space, add_mark_space, end_mark_fn)\n",
    "    \n",
    "  # Recuperate the train dataset\n",
    "  train_dataset_aug = SentenceDataset(train_file,\n",
    "                                        tokenizer,\n",
    "                                        truncation = False,\n",
    "                                        cp1_transformer = fr_augmentation_1,\n",
    "                                        cp2_transformer = fr_augmentation_2,\n",
    "                                        corpus_1=corpus_1,\n",
    "                                        corpus_2=corpus_2\n",
    "                                        )\n",
    "\n",
    "  # Recuperate the valid dataset\n",
    "  valid_dataset = SentenceDataset(test_file,\n",
    "                                        tokenizer,\n",
    "                                        cp1_transformer = fr_augmentation_2,\n",
    "                                        cp2_transformer = fr_augmentation_2,\n",
    "                                        corpus_1=corpus_1,\n",
    "                                        corpus_2=corpus_2,\n",
    "                                        truncation = False)\n",
    "  \n",
    "  # Return the datasets\n",
    "  return train_dataset_aug, valid_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "%run wolof-translate/wolof_translate/utils/recuperate_datasets.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "➡️ Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "# initialize the configurations\n",
    "config = {\n",
    "    'epochs': 20,\n",
    "    'max_epoch': None,\n",
    "    'log_step': 1,\n",
    "    'metric_for_best_model': 'test_loss',\n",
    "    'metric_objective': 'minimize',\n",
    "    'corpus_1': 'wolof',\n",
    "    'corpus_2': 'french',\n",
    "    'train_file': 'data/extractions/new_data/train_set.csv',\n",
    "    'test_file': 'data/extractions/new_data/valid_set.csv',\n",
    "    'drop_out_rate': 0.291121690756753,\n",
    "    'd_model': 512,\n",
    "    'n_head': 8,\n",
    "    'dim_ff': 2024,\n",
    "    'n_encoders': 6,\n",
    "    'n_decoders': 6,\n",
    "    'learning_rate': 0.002880919287770637,\n",
    "    'weight_decay': 0.0003390262718277915,\n",
    "    'char_p': 0.018203086625721746,\n",
    "    'word_p': 0.02404347487358639,\n",
    "    'end_mark': 3,\n",
    "    'label_smoothing': 0.1,\n",
    "    'max_len': 20,\n",
    "    'random_state': 0,\n",
    "    'boundaries': [2, 30, 57, 84, 112, 139, 166],\n",
    "    'batch_sizes': [256, 128, 64, 32, 16, 8, 4, 2],\n",
    "    'batch_size': None, \n",
    "    'warmup_init': False,\n",
    "    'relative_step': False,\n",
    "    'num_workers': 0,\n",
    "    'pin_memory': False,\n",
    "    # --------------------> Must be changed when continuing a training\n",
    "    'model_dir': 't5_small_v7_wf',\n",
    "    'new_model_dir': 't5_small_v7_wf',\n",
    "    'continue': True, # --------------------------> Must be changed when continuing training\n",
    "    'logging_dir': 'data/logs/t5_small_wf',\n",
    "    'save_best': True,\n",
    "    'tokenizer_path': 'wolof-translate/wolof_translate/tokenizers/t5_tokenizers/tokenizer_v6.model',\n",
    "    'data_directory': 'data/extractions/new_data/',\n",
    "    'data_file': 'corpora_v7.csv',\n",
    "    'version': 7,\n",
    "    # in the case of a distributed training\n",
    "    'backend': None,\n",
    "    'hosts': [],\n",
    "    'current_host': None,\n",
    "    'num_gpus': 5,\n",
    "    'logger': None,\n",
    "    'return_trainer': True,\n",
    "    'include_split': True,\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting wolof-translate/wolof_translate/utils/hg_training.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile wolof-translate/wolof_translate/utils/hg_training.py\n",
    "from wolof_translate import *\n",
    "import warnings\n",
    "\n",
    "def train(config: dict):\n",
    "    \n",
    "    # ---------------------------------------\n",
    "    # add distribution if necessary (https://github.com/aws/amazon-sagemaker-examples/blob/main/sagemaker-python-sdk/pytorch_mnist/mnist.py)\n",
    "    \n",
    "    logger = config['logger']\n",
    "    \n",
    "    is_distributed = len(config['hosts']) > 1 and config['backend'] is not None\n",
    "    \n",
    "    use_cuda = config['num_gpus'] > 0\n",
    "    \n",
    "    config.update({\"num_workers\": 1, \"pin_memory\": True} if use_cuda else {})\n",
    "\n",
    "    if not logger is None:\n",
    "        \n",
    "        logger.debug(\"Distributed training - {}\".format(is_distributed))\n",
    "        \n",
    "        logger.debug(\"Number of gpus available - {}\".format(config['num_gpus']))\n",
    "        \n",
    "    if is_distributed:\n",
    "        # Initialize the distributed environment.\n",
    "        world_size = len(config['hosts'])\n",
    "        \n",
    "        os.environ[\"WORLD_SIZE\"] = str(world_size)\n",
    "        \n",
    "        host_rank = config['hosts'].index(config['current_host'])\n",
    "        \n",
    "        os.environ[\"RANK\"] = str(host_rank)\n",
    "        \n",
    "        dist.init_process_group(backend=config['backend'], rank=host_rank, world_size=world_size)\n",
    "        \n",
    "        if not logger is None: logger.info(\n",
    "            \"Initialized the distributed environment: '{}' backend on {} nodes. \".format(\n",
    "                config['backend'], dist.get_world_size()\n",
    "            )\n",
    "            + \"Current host rank is {}. Number of gpus: {}\".format(dist.get_rank(), config['num_gpus'])\n",
    "        )\n",
    "    # ---------------------------------------\n",
    "    \n",
    "    # split the data\n",
    "    if config['include_split']: split_data(config['random_state'], config['data_directory'], config['data_file'])\n",
    "\n",
    "    # recuperate the tokenizer\n",
    "    tokenizer = T5TokenizerFast(config['tokenizer_path'])\n",
    "    \n",
    "    # Initialize the model name\n",
    "    model_name = 't5-small'\n",
    "\n",
    "    # import the model with its pre-trained weights\n",
    "    model = T5ForConditionalGeneration.from_pretrained(model_name)\n",
    "\n",
    "    # resize the token embeddings\n",
    "    model.resize_token_embeddings(len(tokenizer))\n",
    "    \n",
    "    # recuperate train and test set\n",
    "    train_dataset, test_dataset = recuperate_datasets(config['char_p'],\n",
    "                                                        config['word_p'], config['max_len'],\n",
    "                                                        config['end_mark'], tokenizer, config['corpus_1'],\n",
    "                                                        config['corpus_2'],\n",
    "                                                        config['train_file'], config['test_file'])\n",
    "    \n",
    "    # initialize the evaluation object\n",
    "    evaluation = TranslationEvaluation(tokenizer, train_dataset.decode)\n",
    "\n",
    "    # let us initialize the trainer\n",
    "    trainer = ModelRunner(model = model, version=config['version'], seed = 0, evaluation = evaluation, optimizer = Adafactor)\n",
    "\n",
    "    #-------------------------------------\n",
    "    # in the case when the linear learning rate scheduler with warmup is used\n",
    "    \n",
    "    # let us calculate the appropriate warmup steps (let us take a max epoch of 100)\n",
    "    # length = len(train_dataset)\n",
    "\n",
    "    # n_steps = length // config['batch_size']\n",
    "\n",
    "    # num_steps = config['max_epoch'] * n_steps\n",
    "\n",
    "    # warmup_steps = (config['max_epoch'] * n_steps) * config['warmup_ratio']\n",
    "\n",
    "    # Initialize the scheduler parameters\n",
    "    # scheduler_args = {'num_warmup_steps': warmup_steps, 'num_training_steps': num_steps}\n",
    "    #-------------------------------------\n",
    "\n",
    "    # Initialize the optimizer parameters\n",
    "    optimizer_args = {\n",
    "        'lr': config['learning_rate'],\n",
    "        'weight_decay': config['weight_decay'],\n",
    "        # 'betas': (0.9, 0.98),\n",
    "        'warmup_init': config['warmup_init'],\n",
    "        'relative_step': config['relative_step']\n",
    "    }\n",
    "\n",
    "    # ----------------------------\n",
    "    # initialize the bucket samplers for distributed environment\n",
    "    boundaries = config['boundaries']\n",
    "    batch_sizes = config['batch_sizes']\n",
    "\n",
    "    train_sampler = SequenceLengthBatchSampler(train_dataset,\n",
    "                                                boundaries = boundaries,\n",
    "                                                batch_sizes = batch_sizes)\n",
    "\n",
    "    test_sampler = SequenceLengthBatchSampler(test_dataset,\n",
    "                                                boundaries = boundaries,\n",
    "                                                batch_sizes = batch_sizes)\n",
    "\n",
    "    # ------------------------------\n",
    "    # initialize a bucket sampler with fixed batch size in the case of single machine\n",
    "    # with parallelization on multiple gpus\n",
    "    # train_sampler = BucketSampler(train_dataset, config['batch_size'])\n",
    "\n",
    "    # test_sampler = BucketSampler(test_dataset, config['batch_size'])\n",
    "    \n",
    "    # ------------------------------\n",
    "\n",
    "    # Initialize the loaders parameters\n",
    "    train_loader_args = {'batch_sampler': train_sampler, 'collate_fn': collate_fn,\n",
    "                        'num_workers': config['num_workers'], 'pin_memory': config['pin_memory']}\n",
    "\n",
    "    test_loader_args = {'batch_sampler': test_sampler, 'collate_fn': collate_fn,\n",
    "                        'num_workers': config['num_workers'], 'pin_memory': config['pin_memory']}\n",
    "\n",
    "    # Add the datasets and hyperparameters to trainer\n",
    "    trainer.compile(train_dataset, test_dataset, tokenizer, train_loader_args,\n",
    "                    test_loader_args, optimizer_kwargs = optimizer_args,\n",
    "                    # lr_scheduler=get_linear_schedule_with_warmup,\n",
    "                    # lr_scheduler_kwargs=scheduler_args,\n",
    "                    predict_with_generate = True,\n",
    "                    hugging_face = True,\n",
    "                    is_distributed=is_distributed,\n",
    "                    logging_dir=config['logging_dir'],\n",
    "                    dist=dist\n",
    "                    )\n",
    "\n",
    "    # load the model\n",
    "    trainer.load(config['model_dir'], load_best = not config['continue'])\n",
    "    \n",
    "    # Train the model\n",
    "    trainer.train(config['epochs'] - trainer.current_epoch, auto_save = True, log_step = config['log_step'], saving_directory=config['new_model_dir'], save_best = config['save_best'],\n",
    "                  metric_for_best_model = config['metric_for_best_model'], metric_objective = config['metric_objective'])\n",
    "    \n",
    "    if config['return_trainer']:\n",
    "        \n",
    "        return trainer\n",
    "    \n",
    "    return None\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Below train and save if we want."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "from wolof_translate.utils.hg_training import train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/25 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "For epoch 6: \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train batch number 2:   0%|          | 0/60 [00:00<?, ?batches/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train batch number 61: 100%|██████████| 60/60 [00:23<00:00,  2.52batches/s]\n",
      "Test batch number 2:   0%|          | 0/11 [00:00<?, ?batches/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Test batch number 12: 100%|██████████| 11/11 [00:08<00:00,  1.26batches/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Metrics: {'train_loss': 3.5544145637541567, 'test_loss': 3.557906624802157, 'bleu': 0.9559466666666665, 'gen_len': 20.58971863247863}\n",
      "\n",
      "=============================\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  4%|▍         | 1/25 [00:33<13:13, 33.08s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "For epoch 7: \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train batch number 2:   0%|          | 0/60 [00:00<?, ?batches/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train batch number 61: 100%|██████████| 60/60 [00:24<00:00,  2.46batches/s]\n",
      "Test batch number 3:   9%|▉         | 1/11 [00:00<00:02,  4.59batches/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Test batch number 12: 100%|██████████| 11/11 [00:08<00:00,  1.36batches/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Metrics: {'train_loss': 3.3875999480766357, 'test_loss': 3.498508999897883, 'bleu': 1.3941232478632481, 'gen_len': 20.854708034188032}\n",
      "\n",
      "=============================\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  8%|▊         | 2/25 [01:07<13:05, 34.15s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "For epoch 8: \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train batch number 2:   0%|          | 0/60 [00:00<?, ?batches/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train batch number 61: 100%|██████████| 60/60 [00:23<00:00,  2.60batches/s]\n",
      "Test batch number 2:   0%|          | 0/11 [00:00<?, ?batches/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Test batch number 12: 100%|██████████| 11/11 [00:08<00:00,  1.23batches/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Metrics: {'train_loss': 3.2478912552775627, 'test_loss': 3.4401248059721072, 'bleu': 1.641368205128205, 'gen_len': 21.083752478632476}\n",
      "\n",
      "=============================\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 12%|█▏        | 3/25 [01:42<12:32, 34.22s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "For epoch 9: \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train batch number 2:   0%|          | 0/60 [00:00<?, ?batches/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train batch number 61: 100%|██████████| 60/60 [00:25<00:00,  2.38batches/s]\n",
      "Test batch number 2:   0%|          | 0/11 [00:00<?, ?batches/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Test batch number 12: 100%|██████████| 11/11 [00:08<00:00,  1.29batches/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Metrics: {'train_loss': 3.1177383677076347, 'test_loss': 3.4354852224007635, 'bleu': 1.7895782905982909, 'gen_len': 20.536745128205126}\n",
      "\n",
      "=============================\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 16%|█▌        | 4/25 [02:18<12:13, 34.94s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "For epoch 10: \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train batch number 2:   0%|          | 0/60 [00:00<?, ?batches/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train batch number 61: 100%|██████████| 60/60 [00:20<00:00,  2.92batches/s]\n",
      "Test batch number 2:   0%|          | 0/11 [00:00<?, ?batches/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Test batch number 12: 100%|██████████| 11/11 [00:08<00:00,  1.31batches/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Metrics: {'train_loss': 2.991359824090049, 'test_loss': 3.4468863858117, 'bleu': 2.036101538461539, 'gen_len': 21.02394803418803}\n",
      "\n",
      "=============================\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 20%|██        | 5/25 [02:48<11:01, 33.09s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "For epoch 11: \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train batch number 2:   0%|          | 0/60 [00:00<?, ?batches/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train batch number 61: 100%|██████████| 60/60 [00:24<00:00,  2.46batches/s]\n",
      "Test batch number 2:   0%|          | 0/11 [00:00<?, ?batches/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Test batch number 12: 100%|██████████| 11/11 [00:09<00:00,  1.18batches/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Metrics: {'train_loss': 2.8759532886452917, 'test_loss': 3.37109613663111, 'bleu': 2.1354916239316233, 'gen_len': 21.10767846153846}\n",
      "\n",
      "=============================\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 24%|██▍       | 6/25 [03:24<10:48, 34.12s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "For epoch 12: \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train batch number 2:   0%|          | 0/60 [00:00<?, ?batches/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train batch number 61: 100%|██████████| 60/60 [00:21<00:00,  2.75batches/s]\n",
      "Test batch number 2:   0%|          | 0/11 [00:00<?, ?batches/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Test batch number 12: 100%|██████████| 11/11 [00:08<00:00,  1.33batches/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Metrics: {'train_loss': 2.7573855085899135, 'test_loss': 3.365126796054026, 'bleu': 2.4606398290598297, 'gen_len': 20.71794444444444}\n",
      "\n",
      "=============================\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 28%|██▊       | 7/25 [03:56<10:03, 33.53s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "For epoch 13: \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train batch number 2:   0%|          | 0/60 [00:00<?, ?batches/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train batch number 61: 100%|██████████| 60/60 [00:22<00:00,  2.65batches/s]\n",
      "Test batch number 2:   0%|          | 0/11 [00:00<?, ?batches/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Test batch number 12: 100%|██████████| 11/11 [00:07<00:00,  1.42batches/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Metrics: {'train_loss': 2.6500792455428486, 'test_loss': 3.4173506288446927, 'bleu': 2.6750264957264958, 'gen_len': 20.74701675213675}\n",
      "\n",
      "=============================\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 32%|███▏      | 8/25 [04:27<09:18, 32.84s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "For epoch 14: \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train batch number 2:   0%|          | 0/60 [00:00<?, ?batches/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train batch number 61: 100%|██████████| 60/60 [00:21<00:00,  2.75batches/s]\n",
      "Test batch number 2:   0%|          | 0/11 [00:00<?, ?batches/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Test batch number 12: 100%|██████████| 11/11 [00:08<00:00,  1.36batches/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Metrics: {'train_loss': 2.5380443433368174, 'test_loss': 3.3410615073310006, 'bleu': 2.7775425641025646, 'gen_len': 20.747032307692308}\n",
      "\n",
      "=============================\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 36%|███▌      | 9/25 [05:00<08:41, 32.61s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "For epoch 15: \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train batch number 2:   0%|          | 0/60 [00:00<?, ?batches/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train batch number 61: 100%|██████████| 60/60 [00:24<00:00,  2.44batches/s]\n",
      "Test batch number 2:   0%|          | 0/11 [00:00<?, ?batches/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Test batch number 12: 100%|██████████| 11/11 [00:08<00:00,  1.27batches/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Metrics: {'train_loss': 2.4323231978254314, 'test_loss': 3.363137841836, 'bleu': 3.428517948717949, 'gen_len': 20.83416239316239}\n",
      "\n",
      "=============================\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 40%|████      | 10/25 [05:34<08:16, 33.08s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "For epoch 16: \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train batch number 2:   0%|          | 0/60 [00:00<?, ?batches/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train batch number 61: 100%|██████████| 60/60 [00:22<00:00,  2.67batches/s]\n",
      "Test batch number 2:   0%|          | 0/11 [00:00<?, ?batches/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Test batch number 12: 100%|██████████| 11/11 [00:09<00:00,  1.21batches/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Metrics: {'train_loss': 2.333548980788149, 'test_loss': 3.3440274169302397, 'bleu': 3.2440717948717945, 'gen_len': 21.005132820512816}\n",
      "\n",
      "=============================\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 44%|████▍     | 11/25 [06:06<07:40, 32.91s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "For epoch 17: \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train batch number 2:   0%|          | 0/60 [00:00<?, ?batches/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train batch number 61: 100%|██████████| 60/60 [00:24<00:00,  2.43batches/s]\n",
      "Test batch number 2:   0%|          | 0/11 [00:00<?, ?batches/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Test batch number 12: 100%|██████████| 11/11 [00:08<00:00,  1.23batches/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Metrics: {'train_loss': 2.2358776534388882, 'test_loss': 3.3254459833487484, 'bleu': 4.005057264957265, 'gen_len': 20.62908102564102}\n",
      "\n",
      "=============================\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 48%|████▊     | 12/25 [06:42<07:19, 33.81s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "For epoch 18: \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train batch number 2:   0%|          | 0/60 [00:00<?, ?batches/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train batch number 61: 100%|██████████| 60/60 [00:23<00:00,  2.58batches/s]\n",
      "Test batch number 2:   0%|          | 0/11 [00:00<?, ?batches/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Test batch number 12: 100%|██████████| 11/11 [00:09<00:00,  1.18batches/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Metrics: {'train_loss': 2.122140735738343, 'test_loss': 3.324604402444301, 'bleu': 4.092709230769231, 'gen_len': 20.7367347008547}\n",
      "\n",
      "=============================\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 52%|█████▏    | 13/25 [07:17<06:49, 34.13s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "For epoch 19: \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train batch number 2:   0%|          | 0/60 [00:00<?, ?batches/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train batch number 61: 100%|██████████| 60/60 [00:22<00:00,  2.69batches/s]\n",
      "Test batch number 2:   0%|          | 0/11 [00:00<?, ?batches/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Test batch number 12: 100%|██████████| 11/11 [00:09<00:00,  1.18batches/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Metrics: {'train_loss': 2.0318410820475084, 'test_loss': 3.3232013095138417, 'bleu': 4.237663247863248, 'gen_len': 20.85469521367521}\n",
      "\n",
      "=============================\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 56%|█████▌    | 14/25 [07:51<06:14, 34.05s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "For epoch 20: \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train batch number 2:   0%|          | 0/60 [00:00<?, ?batches/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train batch number 61: 100%|██████████| 60/60 [00:26<00:00,  2.26batches/s]\n",
      "Test batch number 2:   0%|          | 0/11 [00:00<?, ?batches/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Test batch number 12: 100%|██████████| 11/11 [00:08<00:00,  1.37batches/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Metrics: {'train_loss': 1.9317204730443407, 'test_loss': 3.3213615441933655, 'bleu': 4.656213846153847, 'gen_len': 20.470100854700853}\n",
      "\n",
      "=============================\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 60%|██████    | 15/25 [08:28<05:48, 34.87s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "For epoch 21: \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train batch number 2:   0%|          | 0/60 [00:00<?, ?batches/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train batch number 61: 100%|██████████| 60/60 [00:22<00:00,  2.64batches/s]\n",
      "Test batch number 2:   0%|          | 0/11 [00:00<?, ?batches/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Test batch number 12: 100%|██████████| 11/11 [00:08<00:00,  1.33batches/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Metrics: {'train_loss': 1.8333162675755874, 'test_loss': 3.3651895767603173, 'bleu': 4.917832820512821, 'gen_len': 20.454705299145296}\n",
      "\n",
      "=============================\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 64%|██████▍   | 16/25 [09:00<05:06, 34.06s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "For epoch 22: \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train batch number 2:   0%|          | 0/60 [00:00<?, ?batches/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train batch number 61: 100%|██████████| 60/60 [00:27<00:00,  2.22batches/s]\n",
      "Test batch number 2:   0%|          | 0/11 [00:00<?, ?batches/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Test batch number 12: 100%|██████████| 11/11 [00:08<00:00,  1.31batches/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Metrics: {'train_loss': 1.7523903528386144, 'test_loss': 3.3957172605726456, 'bleu': 5.2231699145299135, 'gen_len': 20.16238649572649}\n",
      "\n",
      "=============================\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 68%|██████▊   | 17/25 [09:36<04:38, 34.76s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "For epoch 23: \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train batch number 2:   0%|          | 0/60 [00:00<?, ?batches/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train batch number 30:  47%|████▋     | 28/60 [00:11<00:12,  2.54batches/s]\n",
      " 68%|██████▊   | 17/25 [09:47<04:36, 34.57s/it]\n",
      "\n",
      "KeyboardInterrupt\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# with warnings.catch_warnings():\n",
    "    # warnings.simplefilter(\"ignore\")\n",
    "trainer = train(config)\n",
    "\n",
    "# save if necessary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0it [00:00, ?it/s]\n"
     ]
    }
   ],
   "source": [
    "# with warnings.catch_warnings():\n",
    "    # warnings.simplefilter(\"ignore\")\n",
    "trainer = train(config)\n",
    "\n",
    "# save if necessary"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "➡️ Predictions\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluation batch number 2:   0%|          | 0/11 [00:00<?, ?batches/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluation batch number 12: 100%|██████████| 11/11 [00:10<00:00,  1.07batches/s]\n"
     ]
    }
   ],
   "source": [
    "if not trainer is None:\n",
    "    \n",
    "    # recuperate the tokenizer\n",
    "    tokenizer = T5TokenizerFast(config['tokenizer_path'])\n",
    "    \n",
    "    # recuperate the test dataset\n",
    "    # initialize the transformation sequence\n",
    "    end_mark_fn = partial(add_end_mark)\n",
    "    augmentation = TransformerSequences(remove_mark_space, delete_guillemet_space, add_mark_space, end_mark_fn)\n",
    "\n",
    "\n",
    "    # let us get the test set\n",
    "    test_dataset = SentenceDataset(f\"{config['data_directory']}test_set.csv\",\n",
    "                                            tokenizer = tokenizer,\n",
    "                                            cp1_transformer = augmentation,\n",
    "                                            cp2_transformer = augmentation,\n",
    "                                            corpus_1=config['corpus_1'],\n",
    "                                            corpus_2=config['corpus_2'],\n",
    "                                            truncation = False)\n",
    "\n",
    "    # initialize the bucket samplers for distributed environment\n",
    "    boundaries = config['boundaries']\n",
    "    batch_sizes = config['batch_sizes']\n",
    "\n",
    "    test_sampler = SequenceLengthBatchSampler(test_dataset,\n",
    "                                                boundaries = boundaries,\n",
    "                                                batch_sizes = batch_sizes)\n",
    "\n",
    "    test_loader_args = {'batch_sampler': test_sampler, 'collate_fn': collate_fn,\n",
    "                            'num_workers': config['num_workers'], 'pin_memory': config['pin_memory']}\n",
    "\n",
    "    metrics, prediction = trainer.evaluate(test_dataset, test_loader_args)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'test_loss': 3.3293098201099625,\n",
       " 'bleu': 4.558357435897436,\n",
       " 'gen_len': 21.695728376068374}"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>original_sentences</th>\n",
       "      <th>translations</th>\n",
       "      <th>predictions</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Jéndal gejju Kaasamaas.</td>\n",
       "      <td>Achète du poisson séché de Casamance.</td>\n",
       "      <td>Achète une autre chose de l</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Bët yu réy la am.</td>\n",
       "      <td>Il a de gros yeux.</td>\n",
       "      <td>C'est cette femme qu</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Asamaan saa ngiy dënnu.</td>\n",
       "      <td>Il tonne.</td>\n",
       "      <td>On y a des choses qui</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Mi xar mépp.</td>\n",
       "      <td>Ce mouton en entier.</td>\n",
       "      <td>Le mouton n'est pas celui</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Kër gu jëkk gi la.</td>\n",
       "      <td>C'est la première maison.</td>\n",
       "      <td>C'est la maison de la</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Ndax kan dem?</td>\n",
       "      <td>Afin que parte?</td>\n",
       "      <td>Afin que qui parte?</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>Jamma rek.</td>\n",
       "      <td>Paix seulement.</td>\n",
       "      <td>Je n'ai pas été à</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>Ana ŋga?</td>\n",
       "      <td>Où es-tu?</td>\n",
       "      <td>Où est l'homme?</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>Naan?</td>\n",
       "      <td>De quelle manière?</td>\n",
       "      <td>Pai?</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>Yaw xamuloo kenn.</td>\n",
       "      <td>Toi tu connais personne.</td>\n",
       "      <td>Tois ne sont pas encore arrivée</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>Ba tey maa ngi liggéey.</td>\n",
       "      <td>Je travaille encore à ce jour.</td>\n",
       "      <td>Jusqu'à présent, il</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>Ci seen baax la bokk.</td>\n",
       "      <td>Cela fait partie de leur tradition.</td>\n",
       "      <td>C'est à l'intérieur</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>Fanweer yi ñaata?</td>\n",
       "      <td>Combien les trente?</td>\n",
       "      <td>Les enfants sont des enfants qui sont</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>Ku dem?</td>\n",
       "      <td>Qui a été?</td>\n",
       "      <td>Qui est parti?</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>Ñaari xew yi dañu daje.</td>\n",
       "      <td>Les deux fêtes coïncident.</td>\n",
       "      <td>Les femmes ont les deux mainss</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>Yàlla, bàjjo bi.</td>\n",
       "      <td>Dieu, l'unique.</td>\n",
       "      <td>Dieu, nous avons fait le</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>Ana njëgu guró gi?</td>\n",
       "      <td>Où est la récompense?</td>\n",
       "      <td>Où est le Monsieur de la</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>Ban nga wax?</td>\n",
       "      <td>Lequel dis-tu?</td>\n",
       "      <td>Tu as dit cela qui tellement</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>Ba nga xam ne.</td>\n",
       "      <td>Tel point que.</td>\n",
       "      <td>Depuis que tu ne peux t</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>Keneen ŋga.</td>\n",
       "      <td>Un autre, tu es.</td>\n",
       "      <td>Tu as discuté avec qui n</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>Dafay fo.</td>\n",
       "      <td>Il joue.</td>\n",
       "      <td>Il a fait une cravate de</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>Séen naa am xar.</td>\n",
       "      <td>J'ai aperçu un mouton.</td>\n",
       "      <td>J'ai aperçu un mouton de</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>Ana waa kër gi?</td>\n",
       "      <td>Comment va ta famille?</td>\n",
       "      <td>Où sont les membres de</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>Du kenn.</td>\n",
       "      <td>Ce n'est personne.</td>\n",
       "      <td>Ce n'est pas le nuque</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>Gis ŋga kooku?</td>\n",
       "      <td>Tu as vu celui-ci?</td>\n",
       "      <td>Tu as vu celui-là que</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>Deemal waay!</td>\n",
       "      <td>Vas t'en!</td>\n",
       "      <td>Va voir le travail!</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>Jaam nga am?</td>\n",
       "      <td>Es-tu en paix?</td>\n",
       "      <td>Es-tu en paix?</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>Man xar moo réer?</td>\n",
       "      <td>Quel moutons est égaré?</td>\n",
       "      <td>Quel mouton en entier?</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>Bi ŋga dee dem.</td>\n",
       "      <td>Du moment que tu pars.</td>\n",
       "      <td>Alors que tu partais pour l</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>Bi ŋga dee dem.</td>\n",
       "      <td>Au moment où tu partais.</td>\n",
       "      <td>Alors que tu partais pour l</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30</th>\n",
       "      <td>Lu am, Yàllaa!</td>\n",
       "      <td>Advienne que pourra!</td>\n",
       "      <td>C'est à l'intérieur</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31</th>\n",
       "      <td>Léegi dafa bejjaaw.</td>\n",
       "      <td>Maintenant il a des cheveux blancs.</td>\n",
       "      <td>Maintenant l'air de la mer</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32</th>\n",
       "      <td>Dama càqaar.</td>\n",
       "      <td>J'ai des ganglions.</td>\n",
       "      <td>Je veux que tu ne peux t</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33</th>\n",
       "      <td>Danu sonn.</td>\n",
       "      <td>Nous sommes fatigués.</td>\n",
       "      <td>Nous avons faiton n'</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34</th>\n",
       "      <td>Jaam rek.</td>\n",
       "      <td>Je suis en paix.</td>\n",
       "      <td>C'est à l'intérieur</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35</th>\n",
       "      <td>Ab juló dàkkat.</td>\n",
       "      <td>Un commerçant grossiste.</td>\n",
       "      <td>Un commerçant détaillant du</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>36</th>\n",
       "      <td>La ŋga wax la.</td>\n",
       "      <td>Ce que tu as dit.</td>\n",
       "      <td>Tu as dit cela.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>37</th>\n",
       "      <td>Xolal fépp!</td>\n",
       "      <td>Fouille toute la place!</td>\n",
       "      <td>Prends l'un!</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>38</th>\n",
       "      <td>Moo doon wax.</td>\n",
       "      <td>C'est lui qui parlait.</td>\n",
       "      <td>C'est lui qui a dit</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39</th>\n",
       "      <td>Benn lañu.</td>\n",
       "      <td>Ils sont semblables.</td>\n",
       "      <td>C'est à l'intérieur</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>40</th>\n",
       "      <td>Naka-nga sant?</td>\n",
       "      <td>Comment t'appelles-tu?</td>\n",
       "      <td>Comment s'est-ce que</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>41</th>\n",
       "      <td>Ñaata lay jar?</td>\n",
       "      <td>Combien ça coute?</td>\n",
       "      <td>Combien pour te faire?</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>42</th>\n",
       "      <td>Dëddu na àdduna.</td>\n",
       "      <td>Il a quitté le monde.</td>\n",
       "      <td>Il a tué une placarde</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>43</th>\n",
       "      <td>Nanga def?</td>\n",
       "      <td>Comment vas-tu?</td>\n",
       "      <td>Tu n'es pas de l</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>44</th>\n",
       "      <td>Giskóonuma leen.</td>\n",
       "      <td>Je ne vous eusse pas vus.</td>\n",
       "      <td>Je n'ai pas de bagages</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>45</th>\n",
       "      <td>Waxtaan ak kenn kan?</td>\n",
       "      <td>Converser avec lequel?</td>\n",
       "      <td>C'est à l'intérieur</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>46</th>\n",
       "      <td>Kile la.</td>\n",
       "      <td>C'est celui-là.</td>\n",
       "      <td>C'est celui-là qui</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>47</th>\n",
       "      <td>Dangeen di waxtaan?</td>\n",
       "      <td>Vous causez?</td>\n",
       "      <td>Vous ne t'a pas de</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>48</th>\n",
       "      <td>Jigéen ñi danañu naŋgu.</td>\n",
       "      <td>Les femmes accepteront.</td>\n",
       "      <td>C'est ce que j'</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49</th>\n",
       "      <td>Naka la bindoo?</td>\n",
       "      <td>Quelle en est la forme?</td>\n",
       "      <td>Comment s'est-ce que</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50</th>\n",
       "      <td>Booy, kaay!</td>\n",
       "      <td>Jeune homme, viens!</td>\n",
       "      <td>Quand tu vas, tu ne</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>51</th>\n",
       "      <td>Mayal benn ñépp!</td>\n",
       "      <td>Donne un à chacun!</td>\n",
       "      <td>Donne-lui une vache de</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>52</th>\n",
       "      <td>Samay xarit!</td>\n",
       "      <td>Mes amis!</td>\n",
       "      <td>Un ami à moi!</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>53</th>\n",
       "      <td>Su dee dem.</td>\n",
       "      <td>S'il part.</td>\n",
       "      <td>Une fois en chemin est parti</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>54</th>\n",
       "      <td>Nit ag gaynde duñu dëkkóo.</td>\n",
       "      <td>Homme et lion ne cohabitent.</td>\n",
       "      <td>Homme et lion ne sont pas encore</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>55</th>\n",
       "      <td>Xar man moo réer?</td>\n",
       "      <td>Quel moutons est égaré?</td>\n",
       "      <td>C'est cette petite mouton</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>56</th>\n",
       "      <td>Xaj bi dafa gaañu.</td>\n",
       "      <td>Le chien est blessé.</td>\n",
       "      <td>Les chiens le pri de l</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>57</th>\n",
       "      <td>Ñaata ŋgeen?</td>\n",
       "      <td>Combien êtes-vous?</td>\n",
       "      <td>Combien pour quoi?</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>58</th>\n",
       "      <td>Góor gi di dem.</td>\n",
       "      <td>Góor giy dem.</td>\n",
       "      <td>L'homme qui part.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>59</th>\n",
       "      <td>Seetal néeg beneen!</td>\n",
       "      <td>Cherche une autre maison!</td>\n",
       "      <td>Cherche une autre chose!</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>60</th>\n",
       "      <td>Aw lawbe la.</td>\n",
       "      <td>C'est un bûcheron.</td>\n",
       "      <td>C'est une mauvaise</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>61</th>\n",
       "      <td>Poos bi dafa bënn.</td>\n",
       "      <td>La poche est trouée.</td>\n",
       "      <td>Le Est-ce que le</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>62</th>\n",
       "      <td>Cig gàttal.</td>\n",
       "      <td>En bref.</td>\n",
       "      <td>C'est à l'intérieur</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>63</th>\n",
       "      <td>Xale buu laa wax.</td>\n",
       "      <td>Je parle de ce petit.</td>\n",
       "      <td>Je parle de cet enfant là-</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>64</th>\n",
       "      <td>Deg nga Wolof?</td>\n",
       "      <td>Parles-tu Wolof?</td>\n",
       "      <td>Parles-tu Français?</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>65</th>\n",
       "      <td>Fàttali ma bés bi!</td>\n",
       "      <td>Rappelle-moi le jour!</td>\n",
       "      <td>Mon Dieu-moi le lait de</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>66</th>\n",
       "      <td>Sama bos.</td>\n",
       "      <td>Le mien.</td>\n",
       "      <td>C'est à moi qu'</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>67</th>\n",
       "      <td>Xar man a réer?</td>\n",
       "      <td>Quel mouton s'est égaré?</td>\n",
       "      <td>Quel mouton?</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>68</th>\n",
       "      <td>Ana kooku.</td>\n",
       "      <td>Où est celui-là?</td>\n",
       "      <td>Où est le tambour de la</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>69</th>\n",
       "      <td>Ak nu mu mëna deme.</td>\n",
       "      <td>De toute façon.</td>\n",
       "      <td>Avec ce qu'il a</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>70</th>\n",
       "      <td>Négël xale bu rafet bi.</td>\n",
       "      <td>Attends la belle jeune fille.</td>\n",
       "      <td>C'est à moi que j</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>71</th>\n",
       "      <td>Dafa dàŋŋaaral.</td>\n",
       "      <td>Il est hautain.</td>\n",
       "      <td>Il a une bonne chose de</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>72</th>\n",
       "      <td>Kees bi dafa dëppu.</td>\n",
       "      <td>La caisse est renversée.</td>\n",
       "      <td>Le ége de l'</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>73</th>\n",
       "      <td>Omar a ngii.</td>\n",
       "      <td>Voici Omar.</td>\n",
       "      <td>Voilà Omar aujourd'</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>74</th>\n",
       "      <td>Dafa doon liggéey.</td>\n",
       "      <td>Il travaillait.</td>\n",
       "      <td>Il a une graisse de graisse</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75</th>\n",
       "      <td>Xolal fépp!</td>\n",
       "      <td>Fouille tout endroit!</td>\n",
       "      <td>Prends l'un!</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>76</th>\n",
       "      <td>Fu mu?</td>\n",
       "      <td>Où est-ce?</td>\n",
       "      <td>Où va-t-il</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>77</th>\n",
       "      <td>Degg naa tuuti Faranse.</td>\n",
       "      <td>Je parle un peu anglais.</td>\n",
       "      <td>J'ai aperçu une jeune</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>78</th>\n",
       "      <td>Dem nañu.</td>\n",
       "      <td>Nous partons.</td>\n",
       "      <td>Ils sont partis.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>79</th>\n",
       "      <td>Dem fa suba.</td>\n",
       "      <td>Vas-y demain.</td>\n",
       "      <td>Ils sont partiss de l</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>80</th>\n",
       "      <td>Séen naa aw fas.</td>\n",
       "      <td>J'ai aperçu un cheval.</td>\n",
       "      <td>J'ai aperçu une jeune</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>81</th>\n",
       "      <td>Fee la.</td>\n",
       "      <td>C'est là-bas.</td>\n",
       "      <td>C'est une ceinture de</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>82</th>\n",
       "      <td>Góor gi deekoon na.</td>\n",
       "      <td>L'homme serait mort.</td>\n",
       "      <td>L'homme n'a pas</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>83</th>\n",
       "      <td>Sama xarit la!</td>\n",
       "      <td>C'est mon ami!</td>\n",
       "      <td>C'est une ami à</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>84</th>\n",
       "      <td>Su góor gi dee ñëw.</td>\n",
       "      <td>Quand l'homme viendra.</td>\n",
       "      <td>Si l'homme vient, l</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>85</th>\n",
       "      <td>Ñunga fa.</td>\n",
       "      <td>Ils sont là-bas.</td>\n",
       "      <td>Ils se sont rassemblés dans</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>86</th>\n",
       "      <td>Ku ñëw?</td>\n",
       "      <td>Qui est venu?</td>\n",
       "      <td>Qui a parlé?</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>87</th>\n",
       "      <td>Góor gi du bày.</td>\n",
       "      <td>L'homme ne cultivera pas.</td>\n",
       "      <td>L'homme n'a pas</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>88</th>\n",
       "      <td>Moom daal!</td>\n",
       "      <td>Lui, aussi!</td>\n",
       "      <td>C'est à l'intérieur</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>89</th>\n",
       "      <td>Sax yee koy mujje.</td>\n",
       "      <td>Il finira par les vers.</td>\n",
       "      <td>C'est à moi qu'</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>90</th>\n",
       "      <td>Mu ngi toog ne cell.</td>\n",
       "      <td>Il est assis et reste calme.</td>\n",
       "      <td>Il porte son enfant qui a dit</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>91</th>\n",
       "      <td>Ngoon saangi fi rek.</td>\n",
       "      <td>La soirée se passe bien.</td>\n",
       "      <td>C'est à l'intérieur</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>92</th>\n",
       "      <td>Ma ŋgoogule foofu.</td>\n",
       "      <td>Il est là.</td>\n",
       "      <td>Le voilà, là-bas que</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>93</th>\n",
       "      <td>Ya ŋgi te mi ŋgi!</td>\n",
       "      <td>Le voilà et le voilà!</td>\n",
       "      <td>Tu aurais été le voilà!</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>94</th>\n",
       "      <td>Danaa dem.</td>\n",
       "      <td>J'irai.</td>\n",
       "      <td>C'est lui qui a été</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>95</th>\n",
       "      <td>Benn la.</td>\n",
       "      <td>C'est unique.</td>\n",
       "      <td>C'est une personne malhonnête</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>96</th>\n",
       "      <td>Baal naa la sama wàll.</td>\n",
       "      <td>Je te cède ma part.</td>\n",
       "      <td>Je t'aimes de mon père</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>97</th>\n",
       "      <td>Bul fëggës basaŋ gi fii.</td>\n",
       "      <td>Ne secoue pas la natte ici.</td>\n",
       "      <td>Ne me tourmente pas l'homme</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>98</th>\n",
       "      <td>Yaw doŋŋ laa wax.</td>\n",
       "      <td>Je ne parle que de toi.</td>\n",
       "      <td>Je parle de toi-même que</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>99</th>\n",
       "      <td>Nekku fi.</td>\n",
       "      <td>Il n'est pas ici.</td>\n",
       "      <td>Il n'a pas de l</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>100</th>\n",
       "      <td>Mu ngi ci sa càmmoñ.</td>\n",
       "      <td>C'est à ta gauche.</td>\n",
       "      <td>Il est gonflé de ton ami</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>101</th>\n",
       "      <td>Xale yi agsi nañu.</td>\n",
       "      <td>Les enfants sont arrivés.</td>\n",
       "      <td>Les enfants sont des enfants qui sont</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>102</th>\n",
       "      <td>Am lii!</td>\n",
       "      <td>Tiens ça!</td>\n",
       "      <td>Dans ce n'est pas un</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>103</th>\n",
       "      <td>Duggal mba nga génn!</td>\n",
       "      <td>entre ou sors!</td>\n",
       "      <td>Tu as péché le riz au-</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>104</th>\n",
       "      <td>Wekkil gëñ yii!</td>\n",
       "      <td>Arrache ces dents-là!</td>\n",
       "      <td>Il a mis à l'intérieur</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>105</th>\n",
       "      <td>Dem naa.</td>\n",
       "      <td>Je suis parti.</td>\n",
       "      <td>J'ai été.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>106</th>\n",
       "      <td>Naka ngoon si?</td>\n",
       "      <td>Comment se passes la soirée?</td>\n",
       "      <td>Comment va-ils?</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>107</th>\n",
       "      <td>Dawal ma dara!</td>\n",
       "      <td>Fais-moi un acompte!</td>\n",
       "      <td>Con-moi la paix!</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>108</th>\n",
       "      <td>Keneen la?</td>\n",
       "      <td>C'est un autre?</td>\n",
       "      <td>C'est un autre qui est</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>109</th>\n",
       "      <td>Li ŋga wax loolu.</td>\n",
       "      <td>Ce que tu as dit.</td>\n",
       "      <td>Ce que tu as dit que tu</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>110</th>\n",
       "      <td>Fu mu bëgg.</td>\n",
       "      <td>Où veut-il.</td>\n",
       "      <td>Où avait-il que tu</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>111</th>\n",
       "      <td>Aw gaynde ci biir néeg bi!</td>\n",
       "      <td>Un lion dans la chambre!</td>\n",
       "      <td>Adresse-le dans la chambre</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>112</th>\n",
       "      <td>Mi ŋgi foofule!</td>\n",
       "      <td>Il est par-là!</td>\n",
       "      <td>Le voilà, là, là,</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>113</th>\n",
       "      <td>Xarit yan ño ñëw?</td>\n",
       "      <td>Quels amis sont arrivés?</td>\n",
       "      <td>C'est à l'intérieur</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>114</th>\n",
       "      <td>Naka-nga sant?</td>\n",
       "      <td>C'est comment ton nom?</td>\n",
       "      <td>Comment s'est-ce que</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>115</th>\n",
       "      <td>Jiit dafay fette.</td>\n",
       "      <td>Le scorpion pique.</td>\n",
       "      <td>C'est à cause de l</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>116</th>\n",
       "      <td>Kaay gunge ma marse.</td>\n",
       "      <td>Viens m'accompagner au marché.</td>\n",
       "      <td>Il n'a pas de l</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>117</th>\n",
       "      <td>Dinaa leen dab.</td>\n",
       "      <td>Je les rejoindrai.</td>\n",
       "      <td>J'ai le singe de l</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>118</th>\n",
       "      <td>Soo dee góor.</td>\n",
       "      <td>Si tu es un homme.</td>\n",
       "      <td>Si tu ne veux pas, tu</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>119</th>\n",
       "      <td>Góor gi demoon na.</td>\n",
       "      <td>L'homme avait été.</td>\n",
       "      <td>L'homme a donné quelque chose</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>120</th>\n",
       "      <td>Dem na.</td>\n",
       "      <td>Il est parti.</td>\n",
       "      <td>Il est parti il y a un</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>121</th>\n",
       "      <td>Duŋgeen woon ñëw.</td>\n",
       "      <td>Vous ne seriez pas venus.</td>\n",
       "      <td>Vous ne mangez pas, ne</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>122</th>\n",
       "      <td>Noonee nan?</td>\n",
       "      <td>Comment?</td>\n",
       "      <td>C'est à partir-tu</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>123</th>\n",
       "      <td>Guy la.</td>\n",
       "      <td>C'est un boabab.</td>\n",
       "      <td>C'est une personne n</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>124</th>\n",
       "      <td>Dafa tóol.</td>\n",
       "      <td>Il en manque.</td>\n",
       "      <td>Il a fait une bonne chose</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>125</th>\n",
       "      <td>Wax ma say bëgg-bëgg!</td>\n",
       "      <td>Dis-moi tes désirs!</td>\n",
       "      <td>Dis-moi de l'eau</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>126</th>\n",
       "      <td>Buuse la woon.</td>\n",
       "      <td>Il était boucher.</td>\n",
       "      <td>C'est une personne malhonnête</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>127</th>\n",
       "      <td>Fi ŋga dem foofile.</td>\n",
       "      <td>Là où tu as été.</td>\n",
       "      <td>Tu ne vas nulle que tu</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>128</th>\n",
       "      <td>Gannax yee këpp gaal gi.</td>\n",
       "      <td>Ce sont les vagues qui ont renversé la pirogue.</td>\n",
       "      <td>C'est la voiture qui avaient appris à l</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>129</th>\n",
       "      <td>Nanga dàkkaande sama mbubb mi.</td>\n",
       "      <td>Tu empèseras mon boubou.</td>\n",
       "      <td>Tu n'es ni roi ni en brousse pour que</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>130</th>\n",
       "      <td>Fépp foo dem, mu nga fa.</td>\n",
       "      <td>Partout où tu vas, il y est.</td>\n",
       "      <td>Alors, tu partes, tu ne dois rien, tu</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>131</th>\n",
       "      <td>Lu la dal?</td>\n",
       "      <td>Qu'est-ce qui t'arrive?</td>\n",
       "      <td>Qu'est-ce qui a dit qu'il</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>132</th>\n",
       "      <td>Jënd ñaa menn xar mi.</td>\n",
       "      <td>J'ai acheté le mouton.</td>\n",
       "      <td>J'ai acheté ce que tu ne te pas le</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>133</th>\n",
       "      <td>Waaye man sax dama dul ñëw!</td>\n",
       "      <td>Mais, même moi, je ne viendrai pas!</td>\n",
       "      <td>Mais, moi, moi, je n'ai pas</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>134</th>\n",
       "      <td>Suba lañuy feyyu.</td>\n",
       "      <td>On perçoit les salaires demain.</td>\n",
       "      <td>S'il n'était pas l'habitude de</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>135</th>\n",
       "      <td>Buleen aw ci yoon wii!</td>\n",
       "      <td>Ne passez pas par ce chemin!</td>\n",
       "      <td>Quand il ne mange pas à l'intérieur,</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>136</th>\n",
       "      <td>Cangat li laay defsi.</td>\n",
       "      <td>Je viens faire le bain lustral.</td>\n",
       "      <td>C'est à moi que je veux ce qui a</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>137</th>\n",
       "      <td>Jox na nit ki dara.</td>\n",
       "      <td>Il a donné à la personne quelque chose.</td>\n",
       "      <td>Il a donné à l'homme qui a fait quelque</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>138</th>\n",
       "      <td>Bar ca bar ca ; bar ca ñaar ca.</td>\n",
       "      <td>Une mesure.</td>\n",
       "      <td>Ils ont deux enfants qui s'est en proche</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>139</th>\n",
       "      <td>Sama caq dafa dagg.</td>\n",
       "      <td>Mon collier de perles est cassé.</td>\n",
       "      <td>Mon épouse est en train de la ceinture de la ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>140</th>\n",
       "      <td>Borom dëkk bi dëddu na.</td>\n",
       "      <td>Le chef du village s'est éteint.</td>\n",
       "      <td>Le veau a fait une ville de l'argent</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>141</th>\n",
       "      <td>Conkom du màndi.</td>\n",
       "      <td>Le vin de palme frais n'enivre pas.</td>\n",
       "      <td>Je ne sais pas de ce qu'il ne pas</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>142</th>\n",
       "      <td>Jaaykat bu am-bayre la.</td>\n",
       "      <td>C'est un commerçant en vue.</td>\n",
       "      <td>C'est une ceinture de ce qu'il</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>143</th>\n",
       "      <td>Coow laa ngiy awu ci sama kaaŋ.</td>\n",
       "      <td>Le bruit retentit dans mon crâne.</td>\n",
       "      <td>Je Coows dans les encenss de la awu</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>144</th>\n",
       "      <td>Damay dooleel li nga wax.</td>\n",
       "      <td>Je renforce ce que tu as dit.</td>\n",
       "      <td>J'ai dit de ce qu'on parle de</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>145</th>\n",
       "      <td>Meew mi dafa rax.</td>\n",
       "      <td>Le lait n'est pas pur.</td>\n",
       "      <td>C'est à l'intérieur que le feu qui</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>146</th>\n",
       "      <td>Yobul na ka ŋgooñ tool ya.</td>\n",
       "      <td>Il lui a amené du fourrage au champ.</td>\n",
       "      <td>Il s'est arrêtée dans une chose à</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>147</th>\n",
       "      <td>Doo dem, xanaa?</td>\n",
       "      <td>Donc, tu ne partiras pas?</td>\n",
       "      <td>Tu ne partiras donc pas, je n'ai</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>148</th>\n",
       "      <td>Safara sa gandeer na ko.</td>\n",
       "      <td>La chaleur du feu l'a envahi.</td>\n",
       "      <td>C'est ton ami qu'il a héberg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>149</th>\n",
       "      <td>Gor gii di Lawbe Ndar.</td>\n",
       "      <td>Cet homme est Laobe de Saint-Louis.</td>\n",
       "      <td>Cet homme qui a dit que c'est un Laobe</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>150</th>\n",
       "      <td>Bindal, ndax nga baña fàtte dara.</td>\n",
       "      <td>Écris, de peur de ne rien oublier.</td>\n",
       "      <td>C'est à cause de l'eau qui a</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>151</th>\n",
       "      <td>Sama gannaaw gi yépp ay metti.</td>\n",
       "      <td>J'ai mal dans tout le dos.</td>\n",
       "      <td>Mon père a dit que l'homme n'est</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>152</th>\n",
       "      <td>Ma ngii dem.</td>\n",
       "      <td>Voilà qu'il est parti.</td>\n",
       "      <td>Le voilà qui s'en va.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>153</th>\n",
       "      <td>Nit keneen kaa laa wax!</td>\n",
       "      <td>Je parle de cet autre homme que voilà!</td>\n",
       "      <td>Je parle de l'autre homme de cette dame</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>154</th>\n",
       "      <td>Jile jigéen jan ŋgeen wax?</td>\n",
       "      <td>Vous parlez de quelle dame?</td>\n",
       "      <td>C'est à l'intérieur que vous ne parle</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>155</th>\n",
       "      <td>Bul guddal wax ji!</td>\n",
       "      <td>Ne fais pas durer la discussion!</td>\n",
       "      <td>Ne peux-tu pas à l'intérieur que le</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>156</th>\n",
       "      <td>Fenn laa dul dem.</td>\n",
       "      <td>Je n'irai nulle part.</td>\n",
       "      <td>C'est l'habitude de partir qui n'</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>157</th>\n",
       "      <td>Gis naa la yaw ak moom!</td>\n",
       "      <td>Je t'ai vu avec lui!</td>\n",
       "      <td>J'ai vu l'homme que toi avec lui</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>158</th>\n",
       "      <td>Sàngara si dafa leen daan fital.</td>\n",
       "      <td>L'alcool les rendait braves.</td>\n",
       "      <td>C'est la nuit qui avaient appris qu'</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>159</th>\n",
       "      <td>Mbaa kenn génnul?</td>\n",
       "      <td>J'espère que nul n'est sorti?</td>\n",
       "      <td>Que personne n'est pas bon-il?</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>160</th>\n",
       "      <td>Mu nga ne faax ca saal ba.</td>\n",
       "      <td>Il est assis tout à son aise au salon.</td>\n",
       "      <td>Il dit que tu ne peux t'en a dit</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>161</th>\n",
       "      <td>Waxul cu li dara.</td>\n",
       "      <td>Il ne dit rien de cela.</td>\n",
       "      <td>Tu n'avais rien dit de l'autre côté</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>162</th>\n",
       "      <td>Góor gi ak xale bi génn nañu.</td>\n",
       "      <td>L'homme et l'enfant sont partis.</td>\n",
       "      <td>L'homme que l'homme que le faire de</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>163</th>\n",
       "      <td>Yaa ŋgi demuloo.</td>\n",
       "      <td>Toi que voilà, tu n'as pas été.</td>\n",
       "      <td>Te voilà, là-bas, là, là,</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>164</th>\n",
       "      <td>jëme ko ci dëkki Afrig yu ŋiis yi.</td>\n",
       "      <td>ces villages d'Afrique anémiés.</td>\n",
       "      <td>Tu le souvenir de l'Afrique qui l'a</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>165</th>\n",
       "      <td>Gaw na ay ndomboom.</td>\n",
       "      <td>Il a attaché ses ceintures talismans.</td>\n",
       "      <td>Il a des Gaws qu'il a des Gaw</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>166</th>\n",
       "      <td>Day na ni Omar léegi.</td>\n",
       "      <td>Il est aussi grand qu'Omar maintenant.</td>\n",
       "      <td>Il a tué une aujourd'hui de</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>167</th>\n",
       "      <td>Loolu jar naa deñi-kumpa.</td>\n",
       "      <td>Ça vaut la peine d'aller voir ça.</td>\n",
       "      <td>C'est là qu'il n'a pas</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>168</th>\n",
       "      <td>Kañ ŋga ka gis?</td>\n",
       "      <td>Quand l'as-tu vu?</td>\n",
       "      <td>Quand tu as vu le as-tu vu?</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>169</th>\n",
       "      <td>Ca gannaaw-ëllëg sa la dem.</td>\n",
       "      <td>Il est parti le lendemain.</td>\n",
       "      <td>C'est l'autre endroit qui a été médecin</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>170</th>\n",
       "      <td>Abdoo nu doy.</td>\n",
       "      <td>C'est Abdou qui nous satisfait.</td>\n",
       "      <td>Une deuxième, on a une bonne</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>171</th>\n",
       "      <td>Dégg ngeen jóolóoli bi?</td>\n",
       "      <td>Avez-vous entendu la cloche?</td>\n",
       "      <td>C'est vous-ce que vous vous réconciliiez</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>172</th>\n",
       "      <td>Amul bàmmeelu biir.</td>\n",
       "      <td>Il ne sait pas garder un secret.</td>\n",
       "      <td>Il n'a pas de l'enfant qui a</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>173</th>\n",
       "      <td>Tàggool ba-noppi.</td>\n",
       "      <td>Demande d'abord l'autorisation.</td>\n",
       "      <td>Ils ont été tellement de la paix de la</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>174</th>\n",
       "      <td>Am nàa mbekte ci guiss la.</td>\n",
       "      <td>Content de te rencontrer.</td>\n",
       "      <td>Je n'ai pas de l'autre bout de</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>175</th>\n",
       "      <td>Ñoo leen daan yenul seeni gaal.</td>\n",
       "      <td>Ce sont eux qui leur portaient leurs malles.</td>\n",
       "      <td>Tu ne peux pas pour que les défaire de leur</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>176</th>\n",
       "      <td>Naka waa kër ga?</td>\n",
       "      <td>Comment vont les gens de la maison?</td>\n",
       "      <td>Comment se passe à la maison?</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>177</th>\n",
       "      <td>Gor gii di Lawbe Ndar.</td>\n",
       "      <td>Cet homme qui est Laobe de Saint-Louis.</td>\n",
       "      <td>Cet homme qui a dit que c'est un Laobe</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>178</th>\n",
       "      <td>Dinga xam ne man nga digaaleel.</td>\n",
       "      <td>Tu sauras que tu as affaire à moi.</td>\n",
       "      <td>Tu as dit cette année de noir que tu ne</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>179</th>\n",
       "      <td>Demleen futbal ca mbedd ma!</td>\n",
       "      <td>Allez jouer au football dans la rue!</td>\n",
       "      <td>Allez-moi la paix de la rue qu'il</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>180</th>\n",
       "      <td>Ana doomu néeg bi?</td>\n",
       "      <td>Où est la clé de la chambre?</td>\n",
       "      <td>Où est le Monsieur à la chose à l'</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>181</th>\n",
       "      <td>Awma dencukaay bu mënti nekk.</td>\n",
       "      <td>Je n'ai pas de magasin en particulier.</td>\n",
       "      <td>C'est à une ceinture de l'eau</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>182</th>\n",
       "      <td>Xale bu baax la.</td>\n",
       "      <td>C'est un enfant qui est bien.</td>\n",
       "      <td>C'est un enfant qu'il est bon!</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>183</th>\n",
       "      <td>Jaama nga fanaan?</td>\n",
       "      <td>As-tu passé la nuit en paix?</td>\n",
       "      <td>Es-tu passé la nuit fois que tu ne peux</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>184</th>\n",
       "      <td>Mu ngiy bëgga ëpp loxo.</td>\n",
       "      <td>Ça commence à être hors de contrôle.</td>\n",
       "      <td>Il est « « « «</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>185</th>\n",
       "      <td>Yéesu dundalaat na ku dee woon.</td>\n",
       "      <td>Jésus a ressuscité quelqu'un qui était mort.</td>\n",
       "      <td>Jésus a ressuscité un mort qu'il a cousu</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>186</th>\n",
       "      <td>Góor gi doon dem.</td>\n",
       "      <td>L'homme qui devait partir.</td>\n",
       "      <td>L'homme qui a été.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>187</th>\n",
       "      <td>Loolu génn na ci sama xel.</td>\n",
       "      <td>Je n'en ai aucun souvenir.</td>\n",
       "      <td>Cela a dit que ce n'est pas encore mieux</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>188</th>\n",
       "      <td>Wooyil Musaa moom mi di dem.</td>\n",
       "      <td>Rappelle Moussa qui s'en va.</td>\n",
       "      <td>C'est la vieille de l'eau qui a</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>189</th>\n",
       "      <td>Yég na ko ba bëgga dee.</td>\n",
       "      <td>Il est bel et bien au courant.</td>\n",
       "      <td>Il l'a fait une chose à l'</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>190</th>\n",
       "      <td>Yég na ko ba bëgga dee.</td>\n",
       "      <td>Il l'a bel et bien senti.</td>\n",
       "      <td>Il l'a fait une chose à l'</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>191</th>\n",
       "      <td>Mu nga ne déjj di la xaar.</td>\n",
       "      <td>Elle est assise fermement et t'attend.</td>\n",
       "      <td>Il est debout sur le point de l'herbe qui</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>192</th>\n",
       "      <td>Basaŋ gu ñu ràbbe barax.</td>\n",
       "      <td>Une natte tissée avec du roseau.</td>\n",
       "      <td>C'est à l'extérieur qu'on appelle</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>193</th>\n",
       "      <td>Mën na def ñaari liitar.</td>\n",
       "      <td>Ça peut contenir deux litres.</td>\n",
       "      <td>Il peut le faire aussi deux deux fois en fair...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>194</th>\n",
       "      <td>Sa bopp baa ngi fees ak dëññ.</td>\n",
       "      <td>Ta tête est pleine de lentes.</td>\n",
       "      <td>Ton père a dit de Bansos de Bansos de</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>195</th>\n",
       "      <td>Fase na ko.</td>\n",
       "      <td>Il l'a répudiée.</td>\n",
       "      <td>Il l'a terrasséé à l'intérieur de</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>196</th>\n",
       "      <td>Dafa foye palaasam bi.</td>\n",
       "      <td>Il a pris son emploi à la légère.</td>\n",
       "      <td>Il a fait une bonne chose de l'argent</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>197</th>\n",
       "      <td>Weneen fas wan ŋga gis?</td>\n",
       "      <td>Quel autre cheval as-tu vu?</td>\n",
       "      <td>Quel cheval veux-tu vu?</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>198</th>\n",
       "      <td>Takkal jaaro bi ci baaraamu-digg bi!</td>\n",
       "      <td>Mets la bague au majeur!</td>\n",
       "      <td>Il n'a pas de la foudre du Baol de</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>199</th>\n",
       "      <td>Demal góor gi gis la!</td>\n",
       "      <td>Va que l'homme te voie!</td>\n",
       "      <td>Va te l'homme qui te c'est l</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>200</th>\n",
       "      <td>Su ma la jàppee, aaxajala!</td>\n",
       "      <td>Si je te prends, gare!</td>\n",
       "      <td>Si je me souviens de l'argent, je n</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>201</th>\n",
       "      <td>Doxatu gëléem, kow la jëm.</td>\n",
       "      <td>Le pet du dromadaire va en montant.</td>\n",
       "      <td>C'est l'air qui n'a pas</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>202</th>\n",
       "      <td>Nit ku baaxoon la!</td>\n",
       "      <td>C'est un homme qui fut bon!</td>\n",
       "      <td>C'est un homme gentil qui n'a pas</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>203</th>\n",
       "      <td>Benn ban ŋga gisul?</td>\n",
       "      <td>Lequel n'as-tu pas vu?</td>\n",
       "      <td>Tu as dit ces femmes-tu achetés?</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>204</th>\n",
       "      <td>Jambaar du bare wax.</td>\n",
       "      <td>Homme de courage n'abonde pas paroles.</td>\n",
       "      <td>C'est le nuque qui a dit qu'il</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>205</th>\n",
       "      <td>Nit ñooñii génn ay mbër lanu.</td>\n",
       "      <td>Ces gens qui sont sortis sont des lutteurs.</td>\n",
       "      <td>Les gens n'a rien dit de ces femmes ont</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>206</th>\n",
       "      <td>Sa yay nee na ci ŋgoon dana ñëw.</td>\n",
       "      <td>Ta mère dit qu'elle viendra ce soir.</td>\n",
       "      <td>Ta mère dit qu'il y avait une bonne</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>207</th>\n",
       "      <td>Dem ŋga te ñëw na.</td>\n",
       "      <td>Tu as été et il est venu.</td>\n",
       "      <td>Tu as été et tu as été et qu'il</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>208</th>\n",
       "      <td>Yaa doonkoon falu.</td>\n",
       "      <td>C'est toi qui eusses été élu.</td>\n",
       "      <td>C'est toi qui n'a pas été à</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>209</th>\n",
       "      <td>Bumi bi du fi buur.</td>\n",
       "      <td>Le prétendant n'y est pas roi.</td>\n",
       "      <td>Les Bum de l'argent du fi où il ne</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>210</th>\n",
       "      <td>Góor gi waxtaan na ag yaw.</td>\n",
       "      <td>L'homme a parlé avec toi.</td>\n",
       "      <td>L'homme a donné à cause de toi et moi</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>211</th>\n",
       "      <td>Kooku rekk ñëwul.</td>\n",
       "      <td>Celui-là n'est pas venu seul.</td>\n",
       "      <td>Celui-là ne peut pas aujourd'hui</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>212</th>\n",
       "      <td>Ni ka bu mu ñëw.</td>\n",
       "      <td>Dis-lui qu'il ne vienne pas.</td>\n",
       "      <td>Un enfant n'est pas une bonne chose qu</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>213</th>\n",
       "      <td>Am na ab banaana ci pañe bi.</td>\n",
       "      <td>Il y a une banane dans le panier.</td>\n",
       "      <td>Il y a une chose à l'intérieur de</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>214</th>\n",
       "      <td>Keneen, ki ñëw!</td>\n",
       "      <td>L'autre, celui qui est arrivé!</td>\n",
       "      <td>Cet autre qui est arrivé, qui est arrivé!</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>215</th>\n",
       "      <td>Bul diri xale bi ci suuf si!</td>\n",
       "      <td>Ne traîne pas l'enfant par terre!</td>\n",
       "      <td>Ne fais pas le couscous le couscous dans la m...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>216</th>\n",
       "      <td>Sàmmoon neñu ci kaaraange bëy yi.</td>\n",
       "      <td>Les chèvres ont été gardées en sécurité.</td>\n",
       "      <td>Les hommes sont des choses qui sont en colère...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>217</th>\n",
       "      <td>Tey, danga am-wërsëg.</td>\n",
       "      <td>Aujourd'hui, tu es chanceux.</td>\n",
       "      <td>Aujourd'hui, ils ne sont pas</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>218</th>\n",
       "      <td>Xalamkat bu aaytal la.</td>\n",
       "      <td>C'est un guitariste talentueux.</td>\n",
       "      <td>C'est une mauvaise mauvaise fois qui</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>219</th>\n",
       "      <td>Wekkiwuloo, dangay daaj.</td>\n",
       "      <td>Tu aggraves la situation.</td>\n",
       "      <td>Ils ont fait une cravate, tu ne pourrai</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>220</th>\n",
       "      <td>Bi mu dee dem.</td>\n",
       "      <td>Du fait qu'il doit partir.</td>\n",
       "      <td>Du moment qu'il part pour qu'il ne</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>221</th>\n",
       "      <td>Dëkk bi am na ag coppite.</td>\n",
       "      <td>Il y a un changement dans la ville.</td>\n",
       "      <td>Il a fait une affaire de l'habitude de</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>222</th>\n",
       "      <td>Suuf si nu meňň ëmb wurus.</td>\n",
       "      <td>Terre-mère au ventre gorgé d'or.</td>\n",
       "      <td>Quand tu as été si nous allions le fatigue</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>223</th>\n",
       "      <td>Joxu la woon juuti bi.</td>\n",
       "      <td>Il ne t'avait pas remis la taxe.</td>\n",
       "      <td>Il t'a donné le morceau de la suite de</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>224</th>\n",
       "      <td>Màgget ñu bare dañu dëpp.</td>\n",
       "      <td>Beaucoup de vieux sont voûtés.</td>\n",
       "      <td>Les enfants sont assiss les enfants sont assi...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>225</th>\n",
       "      <td>Demagul.</td>\n",
       "      <td>Il n'a pas encore été.</td>\n",
       "      <td>Il n'a pas encore vraiment à l'intérieur</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>226</th>\n",
       "      <td>Mu ngay ciñaag ca ëtt ba.</td>\n",
       "      <td>Il se réchauffe au soleil dans la cour.</td>\n",
       "      <td>Il est debout sur le point de la cour de l</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>227</th>\n",
       "      <td>Wax-dëgg jaambaar la.</td>\n",
       "      <td>Franchement c'est un brave.</td>\n",
       "      <td>C'est à l'intérieur que c'est</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>228</th>\n",
       "      <td>Yoon daan na ko.</td>\n",
       "      <td>La loi l'a reconnu coupable.</td>\n",
       "      <td>Il l'a fait une belle parole de l</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>229</th>\n",
       "      <td>Dajaleel call yi.</td>\n",
       "      <td>Rassemble les petits tas d'arachides.</td>\n",
       "      <td>Les enfants sont arrivéss de l'eau les enfants</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>230</th>\n",
       "      <td>Loolu la.</td>\n",
       "      <td>Voilà ce qu'il est.</td>\n",
       "      <td>C'est une personne n'est pas un</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>231</th>\n",
       "      <td>Góor gi dem.</td>\n",
       "      <td>L'homme qui s'en alla.</td>\n",
       "      <td>L'homme qui n'a pas été.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>232</th>\n",
       "      <td>Man demuma.</td>\n",
       "      <td>Moi je n'ai pas été.</td>\n",
       "      <td>Moi je n'ai pas été.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>233</th>\n",
       "      <td>Joxal téere bi doomu nit ku yaru kooku!</td>\n",
       "      <td>Donnes le livre à ce fils bien élevé!</td>\n",
       "      <td>Donne le travail à un autre qui n'as pas</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>234</th>\n",
       "      <td>Wax ko ni mu deme.</td>\n",
       "      <td>Dis-lui comment ça s'est passé.</td>\n",
       "      <td>C'est une chose de ce qu'il</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>235</th>\n",
       "      <td>Jëkkëru yoonu Yàlla.</td>\n",
       "      <td>Fils de la tante paternelle.</td>\n",
       "      <td>C'est à l'intérieur qui a fait un</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>236</th>\n",
       "      <td>Ñii dañu demul xanaa!</td>\n",
       "      <td>Ceux-ci ne partent peut-être pas!</td>\n",
       "      <td>Ceux-ci ne sont pas partis!</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>237</th>\n",
       "      <td>Gisu la woon.</td>\n",
       "      <td>Il ne t'avait pas vu.</td>\n",
       "      <td>Il n'a pas une belle belleé de</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>238</th>\n",
       "      <td>Ŋga dem ŋga bañ dem dana ñëw!</td>\n",
       "      <td>Il viendra que tu veuilles ou non!</td>\n",
       "      <td>Tu ne vas pas que tu ne viennes pas que je</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>239</th>\n",
       "      <td>Dafa ko doon dóor suba ak ngoon.</td>\n",
       "      <td>Il le battait matin et soir.</td>\n",
       "      <td>Il l'a fait tomber sur le sol et qu</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>240</th>\n",
       "      <td>Maay dem.</td>\n",
       "      <td>C'est moi qui pars.</td>\n",
       "      <td>C'est moi qui vais partir.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>241</th>\n",
       "      <td>Dund gi metti na foofa.</td>\n",
       "      <td>La vie est dure là-bas.</td>\n",
       "      <td>L'homme a donné quelque chose à l'homme</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>242</th>\n",
       "      <td>Omar dinay bàcc.</td>\n",
       "      <td>Omar élimine les parasites des yeux.</td>\n",
       "      <td>C'est Omar qui a fait une qui ne</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>243</th>\n",
       "      <td>Mësumaa dem foofa.</td>\n",
       "      <td>Je n'ai jamais été là-bas.</td>\n",
       "      <td>Je n'ai pas été à une autre jeune</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>244</th>\n",
       "      <td>Demal ba mu dellusi.</td>\n",
       "      <td>Va jusqu'à ce qu'il revienne.</td>\n",
       "      <td>Va jusqu'à ce qu'il soit fait de</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>245</th>\n",
       "      <td>Nit laa!</td>\n",
       "      <td>J'ai été quelqu'un!</td>\n",
       "      <td>C'est un homme pour l'homme qui a</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>246</th>\n",
       "      <td>Kooku ci biir.</td>\n",
       "      <td>Celui-là à l'intérieur.</td>\n",
       "      <td>Celui-là, il n'y a pas à</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>247</th>\n",
       "      <td>Dafa ko bojj paaka ci biir.</td>\n",
       "      <td>Il lui a enfoncé un couteau dans le ventre.</td>\n",
       "      <td>Il l'a l'a grifféé à l</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>248</th>\n",
       "      <td>Làq ci seen biir xal yu yànj.</td>\n",
       "      <td>Couvant des braises ardentes.</td>\n",
       "      <td>C'est à l'intérieur de l'eau</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>249</th>\n",
       "      <td>Jambaar du bare wax.</td>\n",
       "      <td>Homme de courage n'abonde pas en paroles.</td>\n",
       "      <td>C'est le nuque qui a dit qu'il</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>250</th>\n",
       "      <td>Lem ko ndax mu dëll.</td>\n",
       "      <td>Plie-le pour que cela devienne épais.</td>\n",
       "      <td>C'est le champ qu'il l'a</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>251</th>\n",
       "      <td>Nit ŋgeen!</td>\n",
       "      <td>Vous avez été quelqu'un!</td>\n",
       "      <td>L'homme n'est pas un homme de paix</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>252</th>\n",
       "      <td>Nag woowa wépp.</td>\n",
       "      <td>Ce boeuf-là, en totalité.</td>\n",
       "      <td>Il n'a pas de l'air de la</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>253</th>\n",
       "      <td>Mbaa kenn demul.</td>\n",
       "      <td>J'espère que personne n'est parti.</td>\n",
       "      <td>Que personne n'est parti.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>254</th>\n",
       "      <td>Man la góor gi woo?</td>\n",
       "      <td>C'est moi que l'homme appelle?</td>\n",
       "      <td>C'est à l'homme que c'est</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>255</th>\n",
       "      <td>Birig yi doyuñu.</td>\n",
       "      <td>Les briques ne suffisent pas.</td>\n",
       "      <td>Les enfants ses enfants sont assiss les arach...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>256</th>\n",
       "      <td>Fas yéene naa dem Kawlax suba.</td>\n",
       "      <td>J'ai pris la résolution d'aller à Kaolack demain.</td>\n",
       "      <td>J'ai été chez un autre qui a mangé le riz.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>257</th>\n",
       "      <td>Sa taal bi bërëx na ; kaay tàmbali.</td>\n",
       "      <td>Ton feu est bien pris ; viens commencer.</td>\n",
       "      <td>Ta de la ville ; il a fait une simple de l'</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>258</th>\n",
       "      <td>Jeneen jabar laa bëgg!</td>\n",
       "      <td>C'est une autre épouse que je veux!</td>\n",
       "      <td>C'est à l'intérieur que je veux!</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>259</th>\n",
       "      <td>Boo wàcce yoonu Bamako jëm Arundu.</td>\n",
       "      <td>En contrebas du chemin de Bamako à Aroundou.</td>\n",
       "      <td>Quand tu ne peux pas de Bamakor leundu de la ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>260</th>\n",
       "      <td>Ogosaa, xaritoo woon nan faak yeneen gunóor : ...</td>\n",
       "      <td>Nous avions découvert d'autres compagnons de j...</td>\n",
       "      <td>Ogoja, les insectess xaritoos xaritoos dans l...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>261</th>\n",
       "      <td>Fas yéenen la bëgg!</td>\n",
       "      <td>C'est d'autres chevaux que je veux!</td>\n",
       "      <td>C'est à l'autre que c'est à l'intérieur</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>262</th>\n",
       "      <td>Toogal ci fépp, fu leer!</td>\n",
       "      <td>Mets-toi n'importe où, où il fait clair!</td>\n",
       "      <td>Mets-toi en garçon, il y a une bonne chose!</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>263</th>\n",
       "      <td>Nit ñeñeen la gis!</td>\n",
       "      <td>C'est d'autres gens que j'ai vus!</td>\n",
       "      <td>C'est un homme pour que je te dit que je te dit!</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>264</th>\n",
       "      <td>Yaw la ndaw si sopp.</td>\n",
       "      <td>C'est toi qui aimes la jeune femme.</td>\n",
       "      <td>C'est toi qu'il est en l'eau qui a été</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>265</th>\n",
       "      <td>Gàddaay ba mana daw kàdd gi.</td>\n",
       "      <td>S'exiler pour s'éloigner de l'acacia.</td>\n",
       "      <td>S'exiler pour être loin l'enfant que l'enfant...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>266</th>\n",
       "      <td>Omar moo donn kër gi.</td>\n",
       "      <td>C'est Omar qui a hérité de la maison.</td>\n",
       "      <td>C'est Omar qui a dit que l'homme qui a dit qu</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>267</th>\n",
       "      <td>Daŋga tayal naka keneen kee.</td>\n",
       "      <td>Tu es paresseuse comme cet autre-là.</td>\n",
       "      <td>Il n'a pas de la suite de la voiture de la vo...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>268</th>\n",
       "      <td>Gisuma ko, dama gëmmoon.</td>\n",
       "      <td>Je ne l'ai pas vu, j'avais fermé les yeux.</td>\n",
       "      <td>Je n'ai pas de l'ai vu, je n'ai</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>269</th>\n",
       "      <td>Soo yaree sa doom, du la teg gàcce.</td>\n",
       "      <td>Si tu éduques ton enfant, il ne te fera pas ho...</td>\n",
       "      <td>Si tu ne veux pas, il te fait sous le répara.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>270</th>\n",
       "      <td>Dana tukki tay mbaa ëlëk.</td>\n",
       "      <td>Il voyagera aujourd'hui ou demain.</td>\n",
       "      <td>C'est à l'inste de l'eau qui a été</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>271</th>\n",
       "      <td>Demkoon doon na ma fi war.</td>\n",
       "      <td>J'ai failli être dans l'obligation d'y aller.</td>\n",
       "      <td>Il m'a dit qu'il m'a dit qu'il</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>272</th>\n",
       "      <td>Du bés bu nekk baay rey gaynde.</td>\n",
       "      <td>Ce n'est pas chaque jour que père tue un lion.</td>\n",
       "      <td>C'était une chose qui n'a pas été de la mer</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>273</th>\n",
       "      <td>Nit ki dóor na nag wi ak bant.</td>\n",
       "      <td>L'homme a frappé la vache avec un bâton.</td>\n",
       "      <td>L'homme a donné quelque chose à l'intérieur d...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>274</th>\n",
       "      <td>Fètt bi amul woon benn wersseku fàcc.</td>\n",
       "      <td>La bombe n'avait aucune chance d'exploser.</td>\n",
       "      <td>Le lest est en train de l'eau de l'eau du</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>275</th>\n",
       "      <td>Dafa doon nëbb biiram bi wànte léegi dey fés na.</td>\n",
       "      <td>Elle cachait sa grossesse, mais maintenant c'e...</td>\n",
       "      <td>Il a l'a d'un coup de l'argent à l</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>276</th>\n",
       "      <td>Faraas dóor na Berezil ñett ci dara.</td>\n",
       "      <td>La France a battu le Brésil trois à zéro.</td>\n",
       "      <td>Il a dit qu'il y a quelque chose de l'argent de</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>277</th>\n",
       "      <td>Waxtu wi dafa jot rekk, mu daldi gaawa dem.</td>\n",
       "      <td>Le moment venu, rapidement, il s'en alla.</td>\n",
       "      <td>Une fois en chemin, il ne peut pas s'en aller à</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>278</th>\n",
       "      <td>Demoon ba Ndar.</td>\n",
       "      <td>Avoir pu aller jusqu'à Saint-Louis.</td>\n",
       "      <td>Ils ont été tellement toute cette Saint-Louis.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>279</th>\n",
       "      <td>Tegal leket gi ci ndox mi, du diig.</td>\n",
       "      <td>Pose la calebasse sur l'eau, elle ne coulera pas.</td>\n",
       "      <td>On s'est passé dans la zone côtière, il s'est</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>280</th>\n",
       "      <td>Soo koy def, na doon ci sag coobare.</td>\n",
       "      <td>Si tu dois le faire, que ce soit de plein gré.</td>\n",
       "      <td>Si tu le fais, il s'en va à ta mère, il</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>281</th>\n",
       "      <td>Dañu ko déj.</td>\n",
       "      <td>On lui a jeté un sort pour le sédentariser.</td>\n",
       "      <td>Ils se sont fatigués.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>282</th>\n",
       "      <td>Gis naa góor gi doon wax te toogoon ci suuf.</td>\n",
       "      <td>J'ai vu cet homme qui parlait et qui était ass...</td>\n",
       "      <td>J'ai vu l'homme qui est venu à l'homme qui</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>283</th>\n",
       "      <td>Di noyyi xet gu bon gi ci ban bi.</td>\n",
       "      <td>Humant l'odeur fétide de la boue.</td>\n",
       "      <td>Avec l'air de la paix pour l'autre endroit qui s</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>284</th>\n",
       "      <td>Def ci xobi bàkkis!</td>\n",
       "      <td>Mets-y des feuilles de Tinospora!</td>\n",
       "      <td>Mets-y trois pinc pour l'ur!</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>285</th>\n",
       "      <td>Nit ki ci sama wet ak nit kooku mbokk la ñu.</td>\n",
       "      <td>Cet homme près de moi et celui là près de toi ...</td>\n",
       "      <td>L'homme qui a dit qu'ils ne sont pas l'autre</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>286</th>\n",
       "      <td>Nit ki baaxkoon la!</td>\n",
       "      <td>C'est cet homme qui eût été bon!</td>\n",
       "      <td>C'est l'homme qui a dit que c'est l'</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>287</th>\n",
       "      <td>Dafa di taseeguloo ak gaynde.</td>\n",
       "      <td>Le fait est que tu n'as pas encore rencontré d...</td>\n",
       "      <td>Il a remplir une Ze de la provi de la provi</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>288</th>\n",
       "      <td>Mu ngay dukkat di dem.</td>\n",
       "      <td>Il s'en va d'un pas lourd et disgracieux.</td>\n",
       "      <td>Il est parti.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>289</th>\n",
       "      <td>Ku am-aajo la ci ay dëkkandoom.</td>\n",
       "      <td>Il a beaucoup d'égards pour ses voisins.</td>\n",
       "      <td>Il a une affaire de l'a fait de l'argent des</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>290</th>\n",
       "      <td>Bi mu agsee, ñépp jog.</td>\n",
       "      <td>Quand il arriva, tous se levèrent.</td>\n",
       "      <td>Lorsqu'il a dit que ce soit là, il s'est répandu</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>291</th>\n",
       "      <td>Moom daal, léegi addina dafa soppeeku.</td>\n",
       "      <td>En fait, le monde a changé de nos jours.</td>\n",
       "      <td>Il n'a pas de l'habitude de l'eau, c</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>292</th>\n",
       "      <td>Mënumaa gapparu lu yàgg.</td>\n",
       "      <td>Je ne peux pas m'asseoir longtemps sur mes jam...</td>\n",
       "      <td>Je ne peux pas piler tout ce qui n'est pas un...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>293</th>\n",
       "      <td>Dafa bëtal ngénte li.</td>\n",
       "      <td>Il a reporté le baptême à une date ultérieure.</td>\n",
       "      <td>Il a une belle voix de la peur du baptêmee de...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>294</th>\n",
       "      <td>Dama dem.</td>\n",
       "      <td>C'est que j'ai effectivement été.</td>\n",
       "      <td>J'ai été pour le moment qu'il ne veut pas.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>295</th>\n",
       "      <td>Bàq bi dina yomba gas.</td>\n",
       "      <td>Le sol détrempé par la pluie sera facile à cre...</td>\n",
       "      <td>Il n'a pas de l'enfant qui n'a pas de</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>296</th>\n",
       "      <td>Soo ma dóoree, ma feyyu kay.</td>\n",
       "      <td>Si tu me frappes, je me vengerai bien sûr.</td>\n",
       "      <td>Si tu m'as offert, je me l'image de l'</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>297</th>\n",
       "      <td>Ŋga dem la ñooñu bëgg.</td>\n",
       "      <td>Que tu partes, c'est ce que ceux-là veulent.</td>\n",
       "      <td>C'est à l'intérieur que je veux que je veux.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>298</th>\n",
       "      <td>Góor gii la xale yi wax.</td>\n",
       "      <td>C'est de l'homme que parlent les enfants.</td>\n",
       "      <td>Cet homme qui a dit que les enfants sont les ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>299</th>\n",
       "      <td>Góor gi dem la, ma defe!</td>\n",
       "      <td>C'est l'homme qui part, je crois!</td>\n",
       "      <td>L'homme qui a été pour que je crois, je crois...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>300</th>\n",
       "      <td>Benn rekk laa ci gisagum.</td>\n",
       "      <td>Je n'en ai vu qu'un pour l'instant.</td>\n",
       "      <td>C'est à l'intérieur que je n'ai pas été.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>301</th>\n",
       "      <td>Bësal picc bi, dëtt ji génn!</td>\n",
       "      <td>Appuie sur le bouton pour que le pus sorte!</td>\n",
       "      <td>C'est le singe qui a dit qu'il n'est pas</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>302</th>\n",
       "      <td>Naan na ba furi.</td>\n",
       "      <td>Il a tellement bu qu'il est devenu blême.</td>\n",
       "      <td>Il a encore aujourd'hui de encore.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>303</th>\n",
       "      <td>Léegi ñu wara tàmbali bal-balal.</td>\n",
       "      <td>Bientôt on va devoir commencer à désherber.</td>\n",
       "      <td>Maintenant le pièges de l'air de la fin de l'air</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>304</th>\n",
       "      <td>Dafa ko galajaane, toog ci kowam.</td>\n",
       "      <td>Il l'a renversé et s'est assis sur lui.</td>\n",
       "      <td>Il l'a fait partie de l'eau, il est en train</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>305</th>\n",
       "      <td>Nanga bomb biir beek biti bi.</td>\n",
       "      <td>Tu frotteras l'intérieur et l'extérieur.</td>\n",
       "      <td>Tu n'es pas de la parole à l'intérieur de la ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>306</th>\n",
       "      <td>Nit, gayndé, nag... àndoon nañu fi.</td>\n",
       "      <td>Homme, lion, boeuf... allaient de concert.</td>\n",
       "      <td>Les gens n'a pas été, ils ont été, ils</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>307</th>\n",
       "      <td>Soo ñëwóon, kon, demkoon naa!</td>\n",
       "      <td>Si tu étais venu, dans ce cas, je serais parti!</td>\n",
       "      <td>Si tu veux la fille, je crois que c'est à l'</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>308</th>\n",
       "      <td>Fase na ; mu nga kër baayam.</td>\n",
       "      <td>Elle est divorcée ; elle est chez son père.</td>\n",
       "      <td>Il a le cheval qui a mangé le même ; il a dit qu</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>309</th>\n",
       "      <td>Seeti ko, sa aajo faju.</td>\n",
       "      <td>Va le voir, tes besoins seront satisfaits.</td>\n",
       "      <td>Je le singe, c'est ton ami qui l'acc du</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>310</th>\n",
       "      <td>Am na nit ñuy lekk caaxoñ.</td>\n",
       "      <td>Il y a des gens qui mangent les branchies.</td>\n",
       "      <td>Il y a beaucoup d'antennes qui ont passé en b...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>311</th>\n",
       "      <td>I janax a ngi lëňbati say butit.</td>\n",
       "      <td>Des souris fouillent tes entrailles.</td>\n",
       "      <td>Ils ont deux fois en hauts d'avoir été pour a...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>312</th>\n",
       "      <td>Séen naa as lëf.</td>\n",
       "      <td>J'ai aperçu une chose, une portion.</td>\n",
       "      <td>J'ai aperçu une autre chose de prendre que la...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>313</th>\n",
       "      <td>Génneel nañu porose bi xaalis bu takku.</td>\n",
       "      <td>Une somme d'argent considérable a été alloué a...</td>\n",
       "      <td>On a fait une bonne chose qui n'est pas encor...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>314</th>\n",
       "      <td>Ceeb bi dafa forox, bu ko lekk.</td>\n",
       "      <td>Le riz est fermenté, ne le mange pas.</td>\n",
       "      <td>Le riz n'a pas de riz, le riz n'a pas</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>315</th>\n",
       "      <td>Demuloowoon ca biir.</td>\n",
       "      <td>Tu n'avais pas été à l'intérieur.</td>\n",
       "      <td>Tu ne vas nulle que tu ne te dis pas que le t...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>316</th>\n",
       "      <td>Soo ko gisee, mu ngi firiku bu baax léegi.</td>\n",
       "      <td>Si tu le vois, il est bien épanoui maintenant.</td>\n",
       "      <td>Si tu le beau, il est beau, il y avait une chose</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>317</th>\n",
       "      <td>Dama ko fekk muy waxaale ab daar.</td>\n",
       "      <td>Je l'ai trouvé qui marchandait un veau.</td>\n",
       "      <td>Je l'ai trouvé une affaire de protection de c...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>318</th>\n",
       "      <td>Ku dëggu lañu ko wara dénk.</td>\n",
       "      <td>On doit le confier à une personne honnête.</td>\n",
       "      <td>On l'a fait de l'a fait de l'argent de</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>319</th>\n",
       "      <td>Kër gi dafa bare cambar-cambar.</td>\n",
       "      <td>Il y a trop de laisser-aller dans la maison.</td>\n",
       "      <td>La maison n'a pas encore été à la maison de l...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>320</th>\n",
       "      <td>Baase laay togg tey.</td>\n",
       "      <td>Je prépare du baase aujourd'hui.</td>\n",
       "      <td>J'ai préparé du couscous que je ne peux pas p...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>321</th>\n",
       "      <td>Ndaje mi, gaawu biy ñów la.</td>\n",
       "      <td>La réunion se tiendra samedi prochain.</td>\n",
       "      <td>La réunion ce qu'il n'a pas de l'air,</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>322</th>\n",
       "      <td>Ci sa peggi dex yi.</td>\n",
       "      <td>Sur les rives de tes cours d'eau.</td>\n",
       "      <td>C'est à l'intérieur qui a fait la maladie et ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>323</th>\n",
       "      <td>Nu ngi gëje ca takk ga.</td>\n",
       "      <td>Nous ne nous sommes pas revus depuis les noces.</td>\n",
       "      <td>Comment vais-je monter de la saison des pluie...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>324</th>\n",
       "      <td>Sëpp-jaleñ du la fasale ak say teeñ.</td>\n",
       "      <td>Une culbute ne te débarrassera pas de tes poux.</td>\n",
       "      <td>C'est la même occasion de l'eau du cheval qui n'</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>325</th>\n",
       "      <td>Nanu roñu bala nu fi taw bee fekk!</td>\n",
       "      <td>Plions bagage avant que la pluie ne nous trouv...</td>\n",
       "      <td>C'est le singe qui n'a pas de faire le creux</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>326</th>\n",
       "      <td>Maa ngi ko fekk, rongoñ yiy bas-basi.</td>\n",
       "      <td>Je l'ai trouvé en larmes.</td>\n",
       "      <td>Je l'ai trouvé, je suis dans le même d'un</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>327</th>\n",
       "      <td>Dafa dikkale xaritam fan yii.</td>\n",
       "      <td>Il bat froid à son ami ces temps-ci.</td>\n",
       "      <td>Il a dit de l'air de l'air de l'air</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>328</th>\n",
       "      <td>Sag càggan rekk a tax xale bi daanu.</td>\n",
       "      <td>C'est par ta négligence que l'enfant est tombé.</td>\n",
       "      <td>Ta de la fin de l'eau que le fleuve de la fin de</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>329</th>\n",
       "      <td>Gisoon naa xar yi yépp.</td>\n",
       "      <td>J'ai déjà vu tous ces moutons-ci.</td>\n",
       "      <td>J'ai vu les moutons de tous les moutons.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>330</th>\n",
       "      <td>Ferñent wee ko lakk.</td>\n",
       "      <td>C'est l'étincelle qui l'a brûlé.</td>\n",
       "      <td>C'est à l'intérieur qui l'a fait de l'</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>331</th>\n",
       "      <td>Kenn du paas ; ku bëgg dugg.</td>\n",
       "      <td>Personne ne paye ; entre qui veut.</td>\n",
       "      <td>C'est à la fin de l'affaire n'a pas de</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>332</th>\n",
       "      <td>Góor gi dem te xale yi dugg!</td>\n",
       "      <td>Que l'homme parte et que les enfants entrent!</td>\n",
       "      <td>L'homme enfant que les enfants ne peut pas qu...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>333</th>\n",
       "      <td>Waaw, waaw ndax dem na?</td>\n",
       "      <td>Eh bien, est-ce qu'il est parti?</td>\n",
       "      <td>Oui, il est parti?</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>334</th>\n",
       "      <td>Tool bii, ku am-kàttan rekk a ko mëna bey.</td>\n",
       "      <td>Ce champ, seul quelqu'un de fort peut le culti...</td>\n",
       "      <td>C'est une photo sur laquelle il y a une chose</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>335</th>\n",
       "      <td>Ñun ñi nga doomoo, bu ñu fàtte!</td>\n",
       "      <td>Nous dont tu as fait tes enfants, ne nous oubl...</td>\n",
       "      <td>C'est à l'intérieur que tu vas, qu'on appelle</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>336</th>\n",
       "      <td>Mbëggéelam googee woon a ŋgi fi ba tay.</td>\n",
       "      <td>C'est son amour d'antan qui survit encore.</td>\n",
       "      <td>L'oiseau est de l'impression d'être aujourd'</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>337</th>\n",
       "      <td>Dugg nañu doonte bunt yi dañoo tëju woon.</td>\n",
       "      <td>Nous sommes entrés même si les portes étaient ...</td>\n",
       "      <td>C'était une bonne chose qui n'est pas une bonne</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>338</th>\n",
       "      <td>Li wóor ba wóor moo di ne nuni neen la woon ke...</td>\n",
       "      <td>Mais personne ne nous accompagnait.</td>\n",
       "      <td>Ce que ce qui s'est passé dans cette photo ci...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>339</th>\n",
       "      <td>Lu tax?</td>\n",
       "      <td>Qu'est-ce, qui est la cause?</td>\n",
       "      <td>Qu'est-ce qui a dit que ce soit-il?</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>340</th>\n",
       "      <td>Noonu, man, më dem.</td>\n",
       "      <td>Sur ces entrefaites, moi, je quittais.</td>\n",
       "      <td>Mais, je n'ai pas été, je n'ai pas été</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>341</th>\n",
       "      <td>Dafa ne dëndiix soow ma, di xaar leneen.</td>\n",
       "      <td>Il avala le lait d'un trait et attendit la suite.</td>\n",
       "      <td>Il a dit que j'ai ressenti de l'eau, s'</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>342</th>\n",
       "      <td>Ku doon laajte Faali, Ngoy a ngi.</td>\n",
       "      <td>Qui demandait Fali, voici Ngoye.</td>\n",
       "      <td>Qui a dit, ils sont en désunion, ils sontnt</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>343</th>\n",
       "      <td>Séy bu ànd ak basiira gu mat sëkk.</td>\n",
       "      <td>Une union où il y a une grande maturité.</td>\n",
       "      <td>Une deux grand-mère avec une bonne chose de a...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>344</th>\n",
       "      <td>Xaaral ba mu géex, nga sog koo tëral.</td>\n",
       "      <td>Attends qu'il fasse son rot avant de le coucher.</td>\n",
       "      <td>Attends que ça soit tout à fait de l'hôpital,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>345</th>\n",
       "      <td>jigeen ňaak góor ňa donte dañu tëdd.</td>\n",
       "      <td>où femmes et hommes même couchés.</td>\n",
       "      <td>L'homme qui a dit que c'est à l'intérieur que</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>346</th>\n",
       "      <td>Duy def suukar ci kafeem.</td>\n",
       "      <td>Habituellement, il ne met pas de sucre dans so...</td>\n",
       "      <td>Ce n'est pas pour le moment qu'il y a de l</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>347</th>\n",
       "      <td>Noonu, mu binni daldi wuyuji Boromam.</td>\n",
       "      <td>Alors, il soupira puis rendit l'âme.</td>\n",
       "      <td>Une deuxième, il s'est immobilt à la tête</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>348</th>\n",
       "      <td>Dafa bëgg xaalis.</td>\n",
       "      <td>C'est qu'il veut de l'argent.</td>\n",
       "      <td>Il a remplie de l'argent de l'argent de l'</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>349</th>\n",
       "      <td>Maamam garmi la woon.</td>\n",
       "      <td>Son grand-père faisait partie des autorités en...</td>\n",
       "      <td>C'est moi qui a été médecin de la fin de l'ha...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>350</th>\n",
       "      <td>Ndaje maa ngay ame ca bayaal ba.</td>\n",
       "      <td>La réunion aura lieu sur le grand espace vide.</td>\n",
       "      <td>La réunion ce n'est pas encore guérié à l'int...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>351</th>\n",
       "      <td>Baaxoñ bee ko yee.</td>\n",
       "      <td>C'est le corbeau qui l'a réveillé.</td>\n",
       "      <td>C'est à cause de l'imp qui l'a mis à</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>352</th>\n",
       "      <td>Ku duppati mburu xale bi?</td>\n",
       "      <td>Qui a coupé un morceau du pain de l'enfant?</td>\n",
       "      <td>Qui a dit lavé le carrelage de la voiture?</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>353</th>\n",
       "      <td>Bu sedd bi dikkee, sama tànk yi dañuy didim.</td>\n",
       "      <td>Quand le froid arrive, mes pieds gercent.</td>\n",
       "      <td>Quand ils ont fini de l'eau, j'ai vu</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>354</th>\n",
       "      <td>Der gi ñu ko liggéeye, deru mbëtt la.</td>\n",
       "      <td>Le cuir dont il est fait est une peau de varan.</td>\n",
       "      <td>La seule le tué, il est en colère, il est en ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>355</th>\n",
       "      <td>Maa ngiy gubali samaw fas.</td>\n",
       "      <td>Je m'en vais faucher de l'herbe pour mon cheval.</td>\n",
       "      <td>Je suis à l'instant de l'eau de mon père.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>356</th>\n",
       "      <td>Indeegul dundu weer wi.</td>\n",
       "      <td>Il n'a pas encore apporté les vivres du mois.</td>\n",
       "      <td>Apporte la nuite de l'heure de la brousse du ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>357</th>\n",
       "      <td>Indil bàccu yi ; nit ñi ñów nañu.</td>\n",
       "      <td>Apporte les fléaux ; les gens sont venus.</td>\n",
       "      <td>Apporte le sacs de l'herbe qui ne sont pas le...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>358</th>\n",
       "      <td>Góor gaa ŋgi, nitu Ndar la.</td>\n",
       "      <td>L'homme le voici, il est Saint-Louisien.</td>\n",
       "      <td>L'homme, il est une chambre, c'est une</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>359</th>\n",
       "      <td>Buy, balaa jariñ, toj.</td>\n",
       "      <td>Le fruit du baobab ne sert que s'il est cassé.</td>\n",
       "      <td>Quand ils sont des femmes, qui ne l'a pas de</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>360</th>\n",
       "      <td>Gàddaay dëggal sa géntu yaay.</td>\n",
       "      <td>S'exiler pour réaliser les rêves de ta mère.</td>\n",
       "      <td>S'exiler pour avoir ton père et ma mère a mis...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>361</th>\n",
       "      <td>Danga ma bëgga gëlëmal.</td>\n",
       "      <td>Tu veux me jeter de la poudre aux yeux.</td>\n",
       "      <td>Tu ne m'as pas encore vraiment à l'intérieur ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>362</th>\n",
       "      <td>Nit ñaa ngi ko ëw, muy mbeleleli.</td>\n",
       "      <td>Les gens l'entourent et il parle sans arrêt.</td>\n",
       "      <td>Les gens n'a pas un homme de gens, il s'est</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>363</th>\n",
       "      <td>Defe naa dana ñëw tày!</td>\n",
       "      <td>Je crois qu'il viendra aujourd'hui!</td>\n",
       "      <td>Je crois que ce soit, je n'ai pas été!</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>364</th>\n",
       "      <td>Li mu ma may, duubal bi la.</td>\n",
       "      <td>Ce qu'il m'a donné c'est le double.</td>\n",
       "      <td>Ce qu'il m'a dit qu'il m'a dit</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>365</th>\n",
       "      <td>Dafa ko tere wax baat bi, waaye mu dellu waxaa...</td>\n",
       "      <td>Il lui a défendu de dire ce mot mais il le redit.</td>\n",
       "      <td>Il lui a donné une belle de l'Ouest, mais il est</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>366</th>\n",
       "      <td>Sama càkkar ba dey, bëgg naa koo xadd.</td>\n",
       "      <td>Je voudrais bien couvrir la toiture de ma case.</td>\n",
       "      <td>Ma mère sont debout de l'eau, je ne peux pas ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>367</th>\n",
       "      <td>Maa demul.</td>\n",
       "      <td>C'est moi qui n'ai pas été.</td>\n",
       "      <td>C'est moi qui n'a été.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>368</th>\n",
       "      <td>Lu ay, di njàmbat.</td>\n",
       "      <td>Quand une chose a lieu, on en parle.</td>\n",
       "      <td>Qu'est-ce qui a été, c'est à la manière</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>369</th>\n",
       "      <td>Ndax réew mi am na alal?</td>\n",
       "      <td>Est-ce que le pays a des richesses?</td>\n",
       "      <td>Est-ce que le pays a dit qu'il a dit?</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>370</th>\n",
       "      <td>Xoolal meew mi bala muy fuur!</td>\n",
       "      <td>Surveille le lait de peur qu'il ne monte!</td>\n",
       "      <td>Regarde ce que j'ai acheté le vent!</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>371</th>\n",
       "      <td>Ay àddu-kalpe ñoo ko bóom.</td>\n",
       "      <td>Ce sont des coupeurs de route qui l'ont assass...</td>\n",
       "      <td>C'est la brise de l'amour qu'il n'</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>372</th>\n",
       "      <td>Baabun gi dafa këf banaanaam.</td>\n",
       "      <td>Le chimpanzé lui a arraché sa banane.</td>\n",
       "      <td>C'est l'homme qui avaient la même occasion qu...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>373</th>\n",
       "      <td>Cobteg doomam a tax.</td>\n",
       "      <td>C'est à cause de la turbulence de son enfant.</td>\n",
       "      <td>C'est la raison de l'eau qui a été la tête de</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>374</th>\n",
       "      <td>Dend bi laay daw, dafa bare wax.</td>\n",
       "      <td>Je fuis la promiscuité, il est bavard.</td>\n",
       "      <td>J'ai visé le vent, il s'est pas le</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>375</th>\n",
       "      <td>Doxandéem nelawul ba yàndoor.</td>\n",
       "      <td>L'étranger ne dort pas à poings fermés.</td>\n",
       "      <td>Il n'a pas de l'habitude de faire du mariager de</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>376</th>\n",
       "      <td>Ñaar ñii, bëggante nañu ci dëgg-dëgg.</td>\n",
       "      <td>Ils sont amoureux.</td>\n",
       "      <td>À l'intérieur, c'est à l'intérieur de l'</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>377</th>\n",
       "      <td>Damay waaja ballarñi.</td>\n",
       "      <td>Je m'apprête à faire le troisième binage.</td>\n",
       "      <td>Je me masser de l'eau de l'eau de l'</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>378</th>\n",
       "      <td>Demal cay ceeb bi, dese na ndox!</td>\n",
       "      <td>Va asperger le riz, il manque d'eau!</td>\n",
       "      <td>Va riz, il s'est immobile au riz, il s'</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>379</th>\n",
       "      <td>Ëlbatil furno bi ci ngelaw li.</td>\n",
       "      <td>Tourne le fourneau au vent!</td>\n",
       "      <td>La corde n'a pas de la saison des pluies qui ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>380</th>\n",
       "      <td>Gall na ñaari yoon, dafa nàmp lu ëpp.</td>\n",
       "      <td>Il a régurgité deux fois, il a trop tété.</td>\n",
       "      <td>Il a deux deux enfants, il a fait une deux an...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>381</th>\n",
       "      <td>Gisul yeneen.</td>\n",
       "      <td>Il n'en a pas vu d'autres.</td>\n",
       "      <td>Il n'a pas vu le fils de la dame.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>382</th>\n",
       "      <td>Xawma lan lay nurool ci yeen.</td>\n",
       "      <td>Je ne sais pas à quoi cela ressemble pour vous.</td>\n",
       "      <td>Je suis dans la même conf de l'eau en paix de l</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>383</th>\n",
       "      <td>Borom woto yépp a ngiy jooytu tali yu yàqu yi.</td>\n",
       "      <td>Tous les automobilistes se plaignent des route...</td>\n",
       "      <td>Mon père avaient dans les montagnes d'herbes ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>384</th>\n",
       "      <td>Ba ñu ko falee ministar la wat faasam ba.</td>\n",
       "      <td>C'est quand on l'a nommé ministre qu'il s'est ...</td>\n",
       "      <td>Quand on l'a dit de la forêt de l'eau en Fran...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>385</th>\n",
       "      <td>Ku fóon màngo ji xam ne neex na.</td>\n",
       "      <td>Rien qu'à sentir les mangues, on sait qu'elles...</td>\n",
       "      <td>Qui a dit que ce qui a dit qu'il a dit que le...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>386</th>\n",
       "      <td>Dàqal weñ yiiy biiw yàpp wi rekk.</td>\n",
       "      <td>Chasse ces mouches qui n'arrêtent pas de voler...</td>\n",
       "      <td>C'est la même occasion qui avaient appris à l...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>387</th>\n",
       "      <td>Lu mu for, fejat ko, jox i cuujam.</td>\n",
       "      <td>Tout ce qu'elle ramasse, elle l'émiette et le ...</td>\n",
       "      <td>Qu'est-ce qu'il a dit, il est mettre au momen...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>388</th>\n",
       "      <td>Ren la xale yi di xaraf.</td>\n",
       "      <td>C'est cette année que les enfants vont se fair...</td>\n",
       "      <td>C'est pour cette année que les enfants qui av...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>389</th>\n",
       "      <td>Fanaan bee ma jaaxal.</td>\n",
       "      <td>C'est le fait de rester la nuit qui m'ennuie.</td>\n",
       "      <td>C'est le bâtiment sur l'eau qui a fait quelqu...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>390</th>\n",
       "      <td>Su yaboo sax ma ni ca la fàttee naka la nit di...</td>\n",
       "      <td>Un tel besoin de me mesurer, de dominer.</td>\n",
       "      <td>Si nos parolesais, même, je ne peux pas que j...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>391</th>\n",
       "      <td>Léegi de, yaay tegoo njaboot gu sew gi fi sa b...</td>\n",
       "      <td>Maintenant, c'est toi qui seras chargé de la j...</td>\n",
       "      <td>Maintenant, ils ont été, ils ont été une zone...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>392</th>\n",
       "      <td>Bëy, bu àndul ak bëy ya, ànd ak cere ja.</td>\n",
       "      <td>De deux choses l'une : ou la chèvre est avec l...</td>\n",
       "      <td>Une personne, ils sont fatigués, ils sontnt à...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>393</th>\n",
       "      <td>Afrig ci boppam, boole ràŋŋatikook xol bu rafe...</td>\n",
       "      <td>L'Afrique à la fois sauvage et très humaine es...</td>\n",
       "      <td>L'Afrique, c'est le fruit de l'image presque ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>394</th>\n",
       "      <td>Daj naa lu ne laata may agsi fii.</td>\n",
       "      <td>J'en ai vu de toutes les couleurs avant d'en a...</td>\n",
       "      <td>Je voudrais qu'il a dit que j'ai acheté l'enf...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>395</th>\n",
       "      <td>Xon wi lal ci gàncax gi.</td>\n",
       "      <td>De l'arc-en-ciel sur la verdure.</td>\n",
       "      <td>Les enfants qui avaient la zone large la vian...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>396</th>\n",
       "      <td>Xale bu góor bu tollu ci diggu dooleem, ànd ak...</td>\n",
       "      <td>Un jeune homme dans la force de l'âge, d'une b...</td>\n",
       "      <td>Un enfant ne vont pas de même d'un enfant, il...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>397</th>\n",
       "      <td>Dafa ko dëkke woon dóor, moo tax mu fàq.</td>\n",
       "      <td>C'est parce qu'il passait son temps à le battr...</td>\n",
       "      <td>Il lui a donné un homme, c'est pour cela qu'i...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>398</th>\n",
       "      <td>Bu nu déggee sab dënnu, gaajo ga dina tas.</td>\n",
       "      <td>Quand nous entendrons ton rugissement, la part...</td>\n",
       "      <td>Quand nous allions une zone, une zonement de ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>399</th>\n",
       "      <td>Danga goreedi, moo tax séddalewoo ci yoon.</td>\n",
       "      <td>Tu es malhonnête, c'est pour cela que tu n'as ...</td>\n",
       "      <td>Tu n'as pas de l'image, c'est pour cela que c...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>400</th>\n",
       "      <td>Tey, fàww ma seeti Omar.</td>\n",
       "      <td>Aujourd'hui, il faut absolument que j'aille vo...</td>\n",
       "      <td>Aujourd'hui, je ne peux pas Omar n'ai pas été.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>401</th>\n",
       "      <td>Tamxaritu daaw la woon.</td>\n",
       "      <td>C'était l'an dernier au jour de l'an musulman.</td>\n",
       "      <td>C'est la route de l'Afrique de l'Afrique de l...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>402</th>\n",
       "      <td>Mënuloo woon jénd meew, xanaa furóoñu cere ji?</td>\n",
       "      <td>Tu ne pouvais pas acheter du lait, au lieu de ...</td>\n",
       "      <td>Ne peux-tu pas passé, ils sontnt, le manger, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>403</th>\n",
       "      <td>Findifeer lañu yeewe sàkket wi.</td>\n",
       "      <td>C'est avec du fil de fer qu'on a attaché la pa...</td>\n",
       "      <td>C'est le singe qui avaient sèche la même occa...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>404</th>\n",
       "      <td>Moom de toogu fi, cuub-cupet la fi def.</td>\n",
       "      <td>Il n'est pas resté ici, il n'a fait qu'un pass...</td>\n",
       "      <td>Ils ont été fi, ils sont fatigués ici, ils so...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>405</th>\n",
       "      <td>Làmmiñ jigul bant, astemaak nit.</td>\n",
       "      <td>La langue est mauvaise pour le bout de bois, à...</td>\n",
       "      <td>Le long de l'Ouest, il n'a jamais passé de l'...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>406</th>\n",
       "      <td>Aw faaraar ci diggu bopp : kooku Jóob la sant.</td>\n",
       "      <td>Une bande de cheveux au milieu de la tête : ce...</td>\n",
       "      <td>Une deuxie de l'eau qui se passent le temps d...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>407</th>\n",
       "      <td>Su ma yewwoo rekk, sama bët yi dañuy tappaloo.</td>\n",
       "      <td>Chaque fois que je me réveille, j'ai les yeux ...</td>\n",
       "      <td>Si je me souviens de l'air de moi, je ne peux...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>408</th>\n",
       "      <td>Dafa yëngu woon té beg lool ba mu daanal bunt ...</td>\n",
       "      <td>Il était tellement excité et débordant de joie...</td>\n",
       "      <td>Il y a beaucoup d'herbes à l'entrée de l'Oues...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>409</th>\n",
       "      <td>Ki ko bóom, ca ña ko dànd bi muy wax la wara b...</td>\n",
       "      <td>Celui qui l'a assassiné doit être l'un de ceux...</td>\n",
       "      <td>Celui-là, il le lui a bóom, il le lui a certa...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>410</th>\n",
       "      <td>Sa jarbaat yi, jenn gënu kë ci tayel.</td>\n",
       "      <td>Aucun de tes neveux n'est aussi paresseux que ...</td>\n",
       "      <td>Ton père a fait une tasse, ils sont en France...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>411</th>\n",
       "      <td>Waaye dina fàttaliwaale yit njàqareg Baay, raw...</td>\n",
       "      <td>La mémoire des espérances et des angoisses de ...</td>\n",
       "      <td>Mais la savane qui s'est senti proche en ce q...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>412</th>\n",
       "      <td>Dina la jéndal woto bu ganaar saxee béjjén.</td>\n",
       "      <td>Il t'achètera une voiture quand les poules aur...</td>\n",
       "      <td>Ils ont fait une simple de l'argent qui s'est...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>413</th>\n",
       "      <td>Gàcce ak xamadi, laajul a ko indi.</td>\n",
       "      <td>Le manque d'information provoque la honte et l...</td>\n",
       "      <td>Faire la retraite et moi, il n'a pas de la re...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>414</th>\n",
       "      <td>Loolu moo aju woon ci ndaje moomu.</td>\n",
       "      <td>C'est de cela qu'il était question à cette réu...</td>\n",
       "      <td>C'est le même vent qui se sont celle de l'eau...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>415</th>\n",
       "      <td>Fii, Tubaab bi jotu fee am doole noonu.</td>\n",
       "      <td>La colonisation européenne en fin de compte a ...</td>\n",
       "      <td>Ici, c'est l'extérieur, c'est le corps s'est ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>416</th>\n",
       "      <td>Bët du yenu waaye xam na lu bopp àttan.</td>\n",
       "      <td>L'oeil ne porte pas mais il sait ce que la têt...</td>\n",
       "      <td>L'oeil qui ne peut pas pour qu'il ne fait pas...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>417</th>\n",
       "      <td>Kolkolaat yëgul ne yamb baax na.</td>\n",
       "      <td>La mouche n'a cure de savoir si l'abeille est ...</td>\n",
       "      <td>Celui qui n'a pas de l'âne est beau.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>418</th>\n",
       "      <td>Ndax mën naa jariñoo masin bi su ma amee lu ma...</td>\n",
       "      <td>Est-ce que je peux me servir de la machine si ...</td>\n",
       "      <td>Est-ce que je suis resté sur le point de l'ea...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>419</th>\n",
       "      <td>Soq bi metti na ; yóbbul dugub ji ca masin ba!</td>\n",
       "      <td>Le pilage est pénible ; porte le mil à la mach...</td>\n",
       "      <td>Il a tant que le feu ; je suis dans le même d...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>420</th>\n",
       "      <td>Taxawaayu kooka sax taxul nëw.</td>\n",
       "      <td>La présence de celui-là même ne justifie pas q...</td>\n",
       "      <td>C'est à l'intérieur qui se sont réunis et qui...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>421</th>\n",
       "      <td>Soo dee doxandéem cim réew, say kayit dañu war...</td>\n",
       "      <td>Si tu es étranger dans un pays, tes papiers do...</td>\n",
       "      <td>Si tu ne peux pas, ils sont en avoir une bell...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>422</th>\n",
       "      <td>Ekoolu xale yu doyadi yi la.</td>\n",
       "      <td>C'est l'école des enfants handicapés mentaux.</td>\n",
       "      <td>Ils ont commencé à la manière de l'eau de l'e...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>423</th>\n",
       "      <td>Sa mbind mi, danga ko.</td>\n",
       "      <td>Ñaral ce que tu as écrit, tu ne l'as pas appris.</td>\n",
       "      <td>C'est à moi qu'il n'a pas été, il le lui a fa...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>424</th>\n",
       "      <td>Kenn faalewul dara, lu neex waay def.</td>\n",
       "      <td>Personne ne se soucie de rien, chacun fait ce ...</td>\n",
       "      <td>C'est à la fin de l'habitude de l'air qui ava...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>425</th>\n",
       "      <td>Li nga ko wax a tax mu gedd.</td>\n",
       "      <td>C'est à cause de ce que tu lui as dit qu'il bo...</td>\n",
       "      <td>Ce que tu le lui a dit que tu le lui a dit qu...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>426</th>\n",
       "      <td>Xéy-na àgg na ba tatawu Mak-Mawoŋ, ca El Goleyaa.</td>\n",
       "      <td>Peut-être qu'il atteint le fort Mac-Mahon, à E...</td>\n",
       "      <td>Il y avait une bonne pension de l'Ouest, ils ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>427</th>\n",
       "      <td>Amuma doomu-ndey.</td>\n",
       "      <td>Je n'ai pas de frères et soeurs utérins, cousi...</td>\n",
       "      <td>Je n'ai pas de bagages de la saison des pluie...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>428</th>\n",
       "      <td>Ciñaag li baaxul ci sa sibiru bi.</td>\n",
       "      <td>Ce n'est pas bien de te réchauffer au soleil a...</td>\n",
       "      <td>C'est à l'intérieur qu'il a dit qu'il a dit q...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>429</th>\n",
       "      <td>Ca bëj-saalum ba, am mbartal mooy jëme ca benn...</td>\n",
       "      <td>Au Sud, la pente conduisait à un affluent de l...</td>\n",
       "      <td>Une deuxième avec sa mort qu'il n'a pas de l'...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>430</th>\n",
       "      <td>Bëggul ku ko ne sa bët bi suuf a ngi ci.</td>\n",
       "      <td>Il ne veut pas qu'on lui fasse le moindre repr...</td>\n",
       "      <td>Il l'a fait tomber en avoir fait une bonne ch...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>431</th>\n",
       "      <td>Borom bi, duma la bàyyi ba abada.</td>\n",
       "      <td>Seigneur, je ne te quitterai pas jusqu'à l'éte...</td>\n",
       "      <td>Mon Dieu, c'est une simple, une simple de l'e...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>432</th>\n",
       "      <td>Lu-tax muy darngu mel ni mënul dox?</td>\n",
       "      <td>Pourquoi traîne-t-il par terre comme s'il ne p...</td>\n",
       "      <td>Pourquoi prendrai-tu à l'intérieur que j'ai d...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>433</th>\n",
       "      <td>Géwél, ne ma naam, bëj-géwél, ne ma naam!</td>\n",
       "      <td>Griot, réponds-moi, tambour-major, réponds-moi!</td>\n",
       "      <td>Est-ce que je n'ai pas plus de voix, je me su...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>434</th>\n",
       "      <td>Masin dafay gar rekk, bu ko defee, nga wolaat ...</td>\n",
       "      <td>La machine donne une farine grossière, alors t...</td>\n",
       "      <td>Une deuxième, quand il s'est immobilt à la fi...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>435</th>\n",
       "      <td>Yaw, jàllal, dama ni siiwa mucc ci yaw, demal!</td>\n",
       "      <td>Toi, passe, je ne veux qu'être préservé de toi...</td>\n",
       "      <td>Toi, je vais me dégoût, je vais préparer, je ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>436</th>\n",
       "      <td>Dafa laabir ba ku dikk ci dëkk bi rekk, mu boo...</td>\n",
       "      <td>Il est si charitable qu'il prend en charge tou...</td>\n",
       "      <td>Il a dit que ce qui s'est échappé dans la zon...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>437</th>\n",
       "      <td>Bëccëg googu yépp, naaj wi lakk na seen yaram,...</td>\n",
       "      <td>Tout le jour le soleil a brûlé leur corps, ils...</td>\n",
       "      <td>En amen, qu'il y avait quelque chose de l'Oue...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>438</th>\n",
       "      <td>Nég ba jaan dem, nga topp ca watit wa di dóor.</td>\n",
       "      <td>Attendre que le serpent soit parti pour se met...</td>\n",
       "      <td>L'homme qui avait été dans une sorte de route...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>439</th>\n",
       "      <td>Waxam ji dafa bar ; xanaa danga ko def dara?</td>\n",
       "      <td>Ses propos ont été brefs ; lui aurais-tu fait ...</td>\n",
       "      <td>L'individu n'est pas mauvaise personne qui l'...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>440</th>\n",
       "      <td>Boo xamoon caay-caay gi ci gone gii!</td>\n",
       "      <td>Si tu connaissais l'espièglerie de cet enfant!</td>\n",
       "      <td>Si tu veux cette époque que l'enfant!</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>441</th>\n",
       "      <td>Ni mu beroo noonu ci àll bi, tax koo gise Afri...</td>\n",
       "      <td>Pour lui, isolé dans la brousse, l'Afrique est...</td>\n",
       "      <td>Une force électrique qu'il avait connu, dans ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>442</th>\n",
       "      <td>Amu ci woon benn bu ma xam naka la ko nëwu biy...</td>\n",
       "      <td>Je suis en ce temps-là très loin des adjectifs...</td>\n",
       "      <td>C'est à l'intérieur que j'ai été pour moi que...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>443</th>\n",
       "      <td>Su ma ragalul woon fen, dinaa ni toog naa ay a...</td>\n",
       "      <td>Pendant des années, je crois que je ne l'ai ja...</td>\n",
       "      <td>Si je lis Wi mal, je ne peux pas oublier que ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>444</th>\n",
       "      <td>Samay way-jur yëg ñañu fi jàmmu yaram ju ñu ma...</td>\n",
       "      <td>Mon père et ma mère y ressentent une liberté q...</td>\n",
       "      <td>Mon père est agricul qu'il faut écouter, ils ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>445</th>\n",
       "      <td>Soo demee góor gee ni kookule la!</td>\n",
       "      <td>Peut-être l'homme a-t-il dit que c'est celui-là!</td>\n",
       "      <td>C'est l'homme qui a dit que c'est l'homme qui...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>446</th>\n",
       "      <td>Cokkeer, mënta réer morom ma ci dédd.</td>\n",
       "      <td>Un francolin ne peut pas en semer un autre dan...</td>\n",
       "      <td>Le francolin ne peut pas s'est égarée!</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>447</th>\n",
       "      <td>Masuma woon a xalaat ne dinaa dund ba bés ñëw ...</td>\n",
       "      <td>Je n'avais jamais imaginé goûter à une telle i...</td>\n",
       "      <td>Nous n'a pas de l'Afrique qui m'a dit que je ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>448</th>\n",
       "      <td>Jaxaaw la defe dëj bi.</td>\n",
       "      <td>C'est à Diakhaw qu'il a fait les funérailles.</td>\n",
       "      <td>C'est à l'inste de l'eau que le ge de l'eau d...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>449</th>\n",
       "      <td>Caax moo baax ci tàngoor wi.</td>\n",
       "      <td>C'est un sous-vêtement en mailles qu'il faut p...</td>\n",
       "      <td>C'est l'autre qui a fait la maison qui avaien...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>450</th>\n",
       "      <td>Duma wax ci taariixu dëkk bi ndax dafa gudd lool.</td>\n",
       "      <td>Je ne parlerai pas de l'histoire de la ville c...</td>\n",
       "      <td>Je ne sais pas de l'argent à l'intérieur de l...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>451</th>\n",
       "      <td>Su nu delloo ëllëg sa, fekk ay gunóor fatt faw...</td>\n",
       "      <td>Le jour suivant, les ouvrières avaient colmaté...</td>\n",
       "      <td>Si nous parlaitions une simples où ils nous p...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>452</th>\n",
       "      <td>Réew mu rëb la, waaso yi dëkkee xeex ci seen b...</td>\n",
       "      <td>Le pays est troublé par les guerres tribales, ...</td>\n",
       "      <td>Les guérisses de la foudre, ils vont à l'inst...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>453</th>\n",
       "      <td>Bàyyil jën yi, dangay fot de.</td>\n",
       "      <td>Laisse les poissons, tu vas prendre une arête ...</td>\n",
       "      <td>Arrête de l'eau, ils sont en désunion, ils so...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>454</th>\n",
       "      <td>Ñuy bëkk di dem, wuti géej.</td>\n",
       "      <td>Ils marchèrent d'un pas décidé, se dirigeant v...</td>\n",
       "      <td>Nous avons reparlé, ils sontnt, ils sontnt, i...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>455</th>\n",
       "      <td>Gëje nañu ca Aadama ak Awa.</td>\n",
       "      <td>Nos liens de parenté s'arrêtent à Adam et Ève.</td>\n",
       "      <td>Ils ont apporté la même de l'eau de ses enfan...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>456</th>\n",
       "      <td>Ñaari Korite du guléet ci réew mi.</td>\n",
       "      <td>Ce n'est pas la première fois dans le pays que...</td>\n",
       "      <td>L'oiseau est en train de l'eau du corps dans ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>457</th>\n",
       "      <td>Soo bareetee liggéey, danga koy cër-cëre ci bé...</td>\n",
       "      <td>S'il t'arrive encore d'avoir beaucoup de trava...</td>\n",
       "      <td>Si tu n'y a pas de la p spéci, il le allumere...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>458</th>\n",
       "      <td>Ngir gën a gëmloo loolu sama bopp, ma daldi ko...</td>\n",
       "      <td>En l'affublant de lorgnons, je justifiais mon ...</td>\n",
       "      <td>Le voilà, je ne peux pas oublierr par l'avoir...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>459</th>\n",
       "      <td>Gone, du jëfandikooy baat. Rax-ci-dolli, baat ...</td>\n",
       "      <td>Quand on est enfant, on n'use pas de mots (et ...</td>\n",
       "      <td>Une deuxième avec sa touffe, avec la voix de ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>460</th>\n",
       "      <td>Kuy xalam di ci jaayu.</td>\n",
       "      <td>Quand on fait quelque chose, il est normal d'e...</td>\n",
       "      <td>Qui a dit en Afrique de la fin de l'argent de...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>461</th>\n",
       "      <td>Te sax, lépp li jiitu saa soosu ñu ma sàkk lañ...</td>\n",
       "      <td>Et, avant même l'instant de la conception, tou...</td>\n",
       "      <td>Et pourtant, ils ne rient pas à la manière du...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>462</th>\n",
       "      <td>Ñawoon la ba bi mu fiy joge.</td>\n",
       "      <td>C'était ceux-là même jusqu'à ce qu'il parte.</td>\n",
       "      <td>C'est une photo sur le point de l'enfant qui ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>463</th>\n",
       "      <td>Dànkaafu naa ko ba tàyyi ; ma ba ko.</td>\n",
       "      <td>Je l'ai mis en garde ; de guerre lasse, je l'a...</td>\n",
       "      <td>J'ai eu une jeune fille qui l'a mis à la main...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>464</th>\n",
       "      <td>Béjjénu nag jégeñe na lool, waaye bu ñu laalee...</td>\n",
       "      <td>S'il ne tenait qu'à nous, certaines choses ne ...</td>\n",
       "      <td>Une deuxième, mais ils sont en France, mais i...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>465</th>\n",
       "      <td>Nee ñu ndaw soosu ab dëmm la.</td>\n",
       "      <td>On dit que cette dame est une « sorcière ».</td>\n",
       "      <td>Ils ont fait une nappeée de la ennemis.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>466</th>\n",
       "      <td>Bamendaa la Baay yóbbu sama yaay ba mu séysee....</td>\n",
       "      <td>C'est à Bamenda que mon père emmène ma mère ap...</td>\n",
       "      <td>C'était une maison qui est à une maison de ba...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>467</th>\n",
       "      <td>Baadoolo bu ragal naaj ragal na njariñam.</td>\n",
       "      <td>L'homme de condition modeste qui craint le sol...</td>\n",
       "      <td>C'est à cause de la même occasion qui avaient...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>468</th>\n",
       "      <td>Borom ŋàdd yëgul borom pañe.</td>\n",
       "      <td>Celui qui a un morceau de noix n'a cure de cel...</td>\n",
       "      <td>Mon père a fait une jeune fille qui ne s'est ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>469</th>\n",
       "      <td>Amul werante, mbaa yéexantu mba jàdd koñ. Àndu...</td>\n",
       "      <td>C'était une règle de vie, un code de conduite.</td>\n",
       "      <td>Il n'a pas de l'Afrique, c'est le singe qui l...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>470</th>\n",
       "      <td>Dama ko botti, ñu ànd daanu.</td>\n",
       "      <td>Je lui ai fait une prise de ceinture, nous som...</td>\n",
       "      <td>Je l'ai trouvé en train de l'eau, ils lui met...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>471</th>\n",
       "      <td>Lu defu waxu.</td>\n",
       "      <td>Dès lors qu'une chose est commise, elle prête ...</td>\n",
       "      <td>C'est à l'intérieur que le s'est pas le vent ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>472</th>\n",
       "      <td>Mu ne feek amul liggéey, du takk jabar.</td>\n",
       "      <td>Il dit que tant qu'il n'aura pas d'emploi, il ...</td>\n",
       "      <td>Il dit qu'il n'a pas pu durer, il s'en va à l...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>473</th>\n",
       "      <td>Bëgg në ŋga ñëw, mu bañ dem su nëjul!</td>\n",
       "      <td>Je veux que tu viennes, qu'il refuse de partir...</td>\n",
       "      <td>Je veux que tu viennes et que tu viennes, il ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>474</th>\n",
       "      <td>Dama la bëgga biral sama dikk, moo tax ma tele...</td>\n",
       "      <td>Je veux t'assurer de ma venue, c'est pour cett...</td>\n",
       "      <td>Je t'ai vu, cela me semble que c'est cela que...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>475</th>\n",
       "      <td>Maa ngi lay dénku : bul dem te dellusiwuma!</td>\n",
       "      <td>Je te mets en garde : ne t'en va pas avant mon...</td>\n",
       "      <td>Je te remercie beaucoup de guerre, c'est moi ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>476</th>\n",
       "      <td>Colal ci mbedd, jekkul ci mag.</td>\n",
       "      <td>C'est indécent pour un adulte d'uriner debout ...</td>\n",
       "      <td>Nous avons fait de la rue, ils sont en bandou...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>477</th>\n",
       "      <td>Noonu laa ko tàmme woon a gis ci mujjug dundam.</td>\n",
       "      <td>C'est ainsi que je le vois à la fin de sa vie.</td>\n",
       "      <td>Je n'ai pas de ce qu'il ne l'a pas vu à l'int...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>478</th>\n",
       "      <td>Boo amaan bu-tuut, mën nga cee jénd paketu mbi...</td>\n",
       "      <td>Quand tu avais un bu-tuut, tu pouvais acheter ...</td>\n",
       "      <td>Quand tu sauras, tu sauras, tu sauras de puis...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>479</th>\n",
       "      <td>Bu pënd wuree, fattu bare.</td>\n",
       "      <td>Quand la poussière monte, beaucoup d'yeux en p...</td>\n",
       "      <td>Quand ils se sont propos, ils sontnt le « cas...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>480</th>\n",
       "      <td>Dafa koy cuuj dëgël bi, fas wiy tëbbantu mel n...</td>\n",
       "      <td>Il le chatouille avec l'éperon, le cheval saut...</td>\n",
       "      <td>Il l'a fait tomber, il s'est sentie dans la c...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>481</th>\n",
       "      <td>Tey faaru xar yu ñu wàjj laa buun.</td>\n",
       "      <td>Aujourd'hui, j'ai une forte envie de côtelette...</td>\n",
       "      <td>Aujourd'hui, le mouton de la forêt qu'il faut...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>482</th>\n",
       "      <td>Soo xoromee, sa tànk yi dañuy galt.</td>\n",
       "      <td>Un taux élevé d'albumine peut provoquer une en...</td>\n",
       "      <td>Si tu n'as pas été, ils sontnt, ils sontnt à ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>483</th>\n",
       "      <td>Li ma ko yàqal mooy : àbbaatu.</td>\n",
       "      <td>Ce qui me gêne en lui, c'est qu'il emprunte.</td>\n",
       "      <td>Ce que je le dis qu'il m'a dit que je le dis ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>484</th>\n",
       "      <td>Ma barkeelu ci kaalag sëriñ bi.</td>\n",
       "      <td>Que le turban du marabout m'apporte un peu de ...</td>\n",
       "      <td>Mes de l'eau de l'eau de l'eau du puits qui f...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>485</th>\n",
       "      <td>Puróx du gërëm ñamu daaw.</td>\n",
       "      <td>L'oesophage ne remercie pas les mets de l'an d...</td>\n",
       "      <td>Une deuxièmee de l'eau du fleuve au-dessus de...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>486</th>\n",
       "      <td>Ku am-jom la ; su ko baay ji xulee, mu jàng bu...</td>\n",
       "      <td>Il a de l'amour-propre ; si son père le sermon...</td>\n",
       "      <td>C'est une raison que j'ai découvert, il est ;...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>487</th>\n",
       "      <td>« dévaluation » bi kaay, mook nemmali ñoo yem.</td>\n",
       "      <td>La dévaluation revient assurément à nous achever.</td>\n",
       "      <td>L'inter, c'est le corps que j'ai vu, c'est qu...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>488</th>\n",
       "      <td>Barkeel bi moo wutale ndoxu batise bi ak ndox ...</td>\n",
       "      <td>C'est la bénédiction qui différencie l'eau du ...</td>\n",
       "      <td>Il n'a pas de la même grande vu du scorpion d...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>489</th>\n",
       "      <td>Raxasal coowu li bala nga ciy sotti meew mi.</td>\n",
       "      <td>Lave le récipient réservé à la préparation du ...</td>\n",
       "      <td>Les temps de leurs yeux de leurs yeux de leur...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>490</th>\n",
       "      <td>Gajj ndox, sa dënn a ciy metti, ndege du ko ta...</td>\n",
       "      <td>Griffer l'eau, c'est ta poitrine qui en pâtira...</td>\n",
       "      <td>Les patientss de l'eau, c'est la fin de l'eau...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>491</th>\n",
       "      <td>Dama mas a jàpp ni kenn mënumaa ëpple ci bant ...</td>\n",
       "      <td>Pour moi, ces objets, ces bois sculptés et ces...</td>\n",
       "      <td>Je vais me souviens de l'orage avec les enfan...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>492</th>\n",
       "      <td>Turu góor la am ndax maamam ju góor lañu ko di...</td>\n",
       "      <td>Elle porte un nom d'homme car on lui a donné l...</td>\n",
       "      <td>C'est une maison de ses enfants qu'il a cousu...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>493</th>\n",
       "      <td>Su déggoo amee ci seen biir, lépp dina dox.</td>\n",
       "      <td>S'il y a une bonne entente entre vous, tout ma...</td>\n",
       "      <td>Si cela ne veux pas, c'était le temps de l'or...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>494</th>\n",
       "      <td>Tey, ma ngiy fàttaliku bu ci nekk ci ñoom ak i...</td>\n",
       "      <td>Je me souviens de chacun d'eux, de leurs noms,...</td>\n",
       "      <td>Aujourd'hui, je suis dans les rues de l'Ouest...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>495</th>\n",
       "      <td>Damay fajaru ; nanga teela yewwu.</td>\n",
       "      <td>Je pars à l'aube ; il faudra te réveiller tôt.</td>\n",
       "      <td>J'ai l'air de la brousse ; il a aller qu'il n...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>496</th>\n",
       "      <td>Benn séex bi, gàjj bi ci lex bi laa koy xàmmee.</td>\n",
       "      <td>Je reconnais l'un des jumeaux à la scarificati...</td>\n",
       "      <td>Je le jure le gàjj sur le plan, il le travail...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>497</th>\n",
       "      <td>Lépp lu may yëg walla ma koy jëf saxe fa, meññ...</td>\n",
       "      <td>À la source de mes sentiments et de mes déterm...</td>\n",
       "      <td>Je me suis resté sur le corps qui me harceler...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>498</th>\n",
       "      <td>Ab cuuñ a ngi ci guy gi.</td>\n",
       "      <td>Il y a un essaim d'abeilles dans l'arbre.</td>\n",
       "      <td>Une deux fois en beauté dans la maison.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>499</th>\n",
       "      <td>Déj bi dafay tax duñu màgg ni mu ware.</td>\n",
       "      <td>Si le semis est dense, cela empêche une croiss...</td>\n",
       "      <td>L'enfant n'a pas de l'eau que le arbre qu'il ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>500</th>\n",
       "      <td>Ku dem ba xamatuloo fa nga jëm, nga dellu fa n...</td>\n",
       "      <td>Celui qui va jusqu'à ne plus savoir où il va, ...</td>\n",
       "      <td>Qui a dit que tu cachess, tu vas à la tête, t...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>501</th>\n",
       "      <td>Gis ŋga nag yii yépp, woowuu moo ci gën.</td>\n",
       "      <td>Des boeufs que tu vois, celui-là tout près est...</td>\n",
       "      <td>Il a vu tous ces autres femmes, celui-là est ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>502</th>\n",
       "      <td>Doxatu mbaam wee ko yee.</td>\n",
       "      <td>C'est le pet de l'âne qui l'a réveillé.</td>\n",
       "      <td>C'est à l'intérieur que le feu qui l'a mis à ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>503</th>\n",
       "      <td>Lëg gi dafa fëll rekk, ma soqi.</td>\n",
       "      <td>Le lapin déboucha et, à l'instant-même, je déc...</td>\n",
       "      <td>La bouteille est de l'argent, je n'a pas de l...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>504</th>\n",
       "      <td>Lérén dina fanqal liir yaram wu ñaawlu.</td>\n",
       "      <td>Le colostrum épargne au nourrisson une constit...</td>\n",
       "      <td>Ils ont apporté en boubou en boubou de l'autr...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>505</th>\n",
       "      <td>Tool bu réy bii dey, soo ko bayee, Yàlla na ng...</td>\n",
       "      <td>Ce grand champ, si tu le cultives, puisses-tu ...</td>\n",
       "      <td>Cette petite table, si tu le fais, tu le fais...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>506</th>\n",
       "      <td>Fas nañu ko yéene lëkkale ak anam googu.</td>\n",
       "      <td>Nous avons la volonté de le faire coïncider av...</td>\n",
       "      <td>On lui a donné le « hameaux – par l'a fait le...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>507</th>\n",
       "      <td>Yee téere la ma jóox.</td>\n",
       "      <td>C'est ces livres-là qu'il m'a donnés.</td>\n",
       "      <td>C'est à partir de l'autre endroit qui avaient...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>508</th>\n",
       "      <td>Mu ngi ci gall bi ci ndeyjoor.</td>\n",
       "      <td>C'est dans le sac qui est sur le flanc droit d...</td>\n",
       "      <td>Il est dans la chambre dans l'enfant au-dessu...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>509</th>\n",
       "      <td>Nanga ma jéndal gom, damay fóot sama mbubb mi.</td>\n",
       "      <td>Tu m'achèteras de l'amidon, je dois laver mon ...</td>\n",
       "      <td>Tu me guigne la tête, je me souviens de l'eau...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>510</th>\n",
       "      <td>Kawlax bare woon na béntéñe, waaye gor nañu le...</td>\n",
       "      <td>Il y avait beaucoup de fromagers à Kaolack mai...</td>\n",
       "      <td>Il y avait une belle qui ont commencé à l'ext...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>511</th>\n",
       "      <td>Ay xalaat yu, su ñu leen boolantee, dina baax.</td>\n",
       "      <td>Des idées qui seraient bien si on les mettait ...</td>\n",
       "      <td>De ma mère, ils sont en petits, ils sont peti...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>512</th>\n",
       "      <td>Miir yeek taaxi kow yi ňu tiim day nañ, xanaa,...</td>\n",
       "      <td>Ces murailles occupaient une superficie aussi ...</td>\n",
       "      <td>Une deuxie avec ma mère, ils sontnt à la fin ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>513</th>\n",
       "      <td>Ci fànweeri atam la Baay jël gaal, jóge Saawus...</td>\n",
       "      <td>À l'âge de trente ans, mon père quitte Southam...</td>\n",
       "      <td>Mon père les deux domestiques sur les volets ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>514</th>\n",
       "      <td>Ni ko waa Bansoo aadawoo, Buur baa ngi def yar...</td>\n",
       "      <td>Selon la tradition, le roi est nu jusqu'à la c...</td>\n",
       "      <td>Une deuxiAprès, ils s'agitent dans les rues, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>515</th>\n",
       "      <td>Jigéen ñi tàmbali nañoo fecc, daanaka dañu ne ...</td>\n",
       "      <td>Les femmes ont commencé à danser, elles sont c...</td>\n",
       "      <td>Les femmes et les femmes, près de la savane, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>516</th>\n",
       "      <td>Biñ yeggee ci diggu joor gi, sori lool sunu kë...</td>\n",
       "      <td>Au milieu de la plaine, à une distance suffisa...</td>\n",
       "      <td>Quand ils ont été en train de la maison, ils ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>517</th>\n",
       "      <td>Puuriti ginnax yaa ngi muur garabi àll bi, jax...</td>\n",
       "      <td>Les embruns apportés par le vent recouvrent le...</td>\n",
       "      <td>Les bananes par la service, dans la même excr...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>518</th>\n",
       "      <td>Teewul nataal yiy dekkali bànneex beek tawféex...</td>\n",
       "      <td>Malgré tout cela, à cause de tout cela, ces im...</td>\n",
       "      <td>L'arbre m'a fait une route qui n'est pas plus...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>519</th>\n",
       "      <td>Ma daaneele xanaa móobalam ya. Soppul woon tab...</td>\n",
       "      <td>Les meubles enfin, non pas ces fameux tabouret...</td>\n",
       "      <td>Mese mon père et ma mère sont une hutte de la...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>520</th>\n",
       "      <td>Dama doon jàpp, ndox mi ne carax dugg ; ndax l...</td>\n",
       "      <td>Je faisais mes ablutions et tout d'un coup, l'...</td>\n",
       "      <td>Je viens de l'eau qui s'en va à l'eau qui s'e...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>521</th>\n",
       "      <td>Nataal bi ma jot de gis naa ni gejje la, waaw ...</td>\n",
       "      <td>Sur la photo que j'ai eue a voir il y a assuré...</td>\n",
       "      <td>Sur la photo que j'ai vu une sorte dessage. C...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>522</th>\n",
       "      <td>Lu naqari la mu ma doon def, di ma ko yéjje ak...</td>\n",
       "      <td>J'ai appelé ce village Njar-Meew à cause des c...</td>\n",
       "      <td>C'est ce qu'il me fait que je ne me sab pas p...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>523</th>\n",
       "      <td>Ñu topp biddiw ba feq ba àgg ca bërëb ba liir ...</td>\n",
       "      <td>Ils suivirent l'étoile qui s'était levée et pa...</td>\n",
       "      <td>Les femmes et les femmes qui ont commencé à l...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>524</th>\n",
       "      <td>Ki la bëgg, mooy ki lay wax dëgg ; bokkul ak k...</td>\n",
       "      <td>Celui qui t'aime, c'est celui qui te dit la vé...</td>\n",
       "      <td>C'est celui-là qui ne peut pas se laver à l'i...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>525</th>\n",
       "      <td>Su dee mën na ñoo yëkkati kàddu yu rafet yooyu...</td>\n",
       "      <td>La réputation de douceur des gens de la région...</td>\n",
       "      <td>Si le roi de l'homme de la guerre, le roi de ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>526</th>\n",
       "      <td>Gone, lawtanu yomb la ; boo walbatiwul, mu law...</td>\n",
       "      <td>L'enfant, c'est un plant de courge qui rampe ;...</td>\n",
       "      <td>C'est une guerre d'un coup ; il ne te parle p...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>527</th>\n",
       "      <td>Naka lanu def ba xam ko ci saa si ci lu amul b...</td>\n",
       "      <td>Comment l'avons-nous su? Peut-être par mon pèr...</td>\n",
       "      <td>Comment s'est arrêtéé dans le plus qu'il en s...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>528</th>\n",
       "      <td>Sindax yaa nga ni noŋŋ ci kow miir yi, saa yu ...</td>\n",
       "      <td>Aux murs, là où la lumière se reflétait, les m...</td>\n",
       "      <td>Si cela n'ai pas de plaine d'herbes, le déser...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>529</th>\n",
       "      <td>Fi ñu nekk dafa kowe ba mu mel ni asamaan saa ...</td>\n",
       "      <td>Ils sont si haut que le ciel brumeux semble s'...</td>\n",
       "      <td>Actuellement les dés avaients deuxis deux fau...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>530</th>\n",
       "      <td>Tugal soof ma. Ogosaa jommal ma, daanaka am ci...</td>\n",
       "      <td>Alors les jours d'Ogoja, contraitrement à ceux...</td>\n",
       "      <td>Mais je me suis en paix de l'eau de moi, je m...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>531</th>\n",
       "      <td>Àddinay dox ba Baay tollu ci noppalug liggéey,...</td>\n",
       "      <td>Puis j'ai découvert, lorsque mon père, à l'âge...</td>\n",
       "      <td>À trois moment de la chemise en Afrique, il n...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>532</th>\n",
       "      <td>Man mii, ni ma nekkee gone gu ndaw ba jóge fa ...</td>\n",
       "      <td>J'ai passé une grande partie de mon enfance et...</td>\n",
       "      <td>Moi-moi des torches de l'Afrique de moi, je v...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>533</th>\n",
       "      <td>Aw faaraar wër booru nopp bi, mel ni'cercle' :...</td>\n",
       "      <td>Une bande de cheveux le long des oreilles, fai...</td>\n",
       "      <td>Le monsieur est de l'eau, c'est une bonne cho...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>534</th>\n",
       "      <td>Yàggul dara laata Baay di jóge Afrig, Sefiriis...</td>\n",
       "      <td>Un peu avant le départ de mon père, Jeffries t...</td>\n",
       "      <td>Une deuxième avec sa voix, en paix d'avoir à ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>535</th>\n",
       "      <td>Li nga ma yónnee ba tey, li ma ci gis ay jëfan...</td>\n",
       "      <td>Ce que tu m'as envoyé jusqu'à maintenant, la p...</td>\n",
       "      <td>Ce que je suis face de l'eau, j'ai vu une bon...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>536</th>\n",
       "      <td>Moonte, sañumaa weddi ne ni Baay mujjee doon d...</td>\n",
       "      <td>Mais ces manières africaines qui étaient deven...</td>\n",
       "      <td>Ils ont deuxieurs fois que mon père avaient a...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>537</th>\n",
       "      <td>Ak lii, di li gën a xëtt xolu Baay : ni ñuy se...</td>\n",
       "      <td>Et surtout l'abandon de l'Afrique à ses vieux ...</td>\n",
       "      <td>Avec ce qui portait la fin de l'orage sur la ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>538</th>\n",
       "      <td>Dañ doon jékkee-jékki sànneeku nig fett, daw s...</td>\n",
       "      <td>S'ils chapardaient, ce ne pouvait être que des...</td>\n",
       "      <td>Ils ont deux fois, ils sont fatigués, ils son...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>539</th>\n",
       "      <td>Fowe naay nit ak i mala yu ñu yatte dàttu aloo...</td>\n",
       "      <td>J'ai joué avec les statues d'ébène, avec les s...</td>\n",
       "      <td>Je me souviens de l'orages de guerre, la fin ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>540</th>\n",
       "      <td>Nataal bii ngeen ma yónnee gis naa ci biir nat...</td>\n",
       "      <td>Sur la photo que vous m'avez envoyée j'y ai vu...</td>\n",
       "      <td>Sur la photo que vous m'avez envoyée, j'y ai ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>541</th>\n",
       "      <td>Waaw lii de ab nataal la boog. Maa ngi gis ñaa...</td>\n",
       "      <td>Cela est une photo. Je vois, dessus, deux femm...</td>\n",
       "      <td>Sur cette photo il y a deux bancs. J'y vois a...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>542</th>\n",
       "      <td>Su weesoo ñaari goney Enjeŋ yi, du leneen lu B...</td>\n",
       "      <td>De son séjour en Guyane, mon père ne rapporter...</td>\n",
       "      <td>Si mon père découvre, ils ont passé dans les ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>543</th>\n",
       "      <td>Ci weeru mars 1932 la samay way-jur jóge Fores...</td>\n",
       "      <td>À partir de mars 1932, mon père et ma mère qui...</td>\n",
       "      <td>C'est à la manière du soir de la même maison ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>544</th>\n",
       "      <td>Waaw lii de, dafa mel ni ab fulëer la, waaye f...</td>\n",
       "      <td>Oui ça, il semble que ce soit une fleur, mais ...</td>\n",
       "      <td>Ceci est une deuxulëer qui s'agît de ces coul...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>545</th>\n",
       "      <td>Looloo ko dal ba xare ba tàkkee ca Alseeri. Mo...</td>\n",
       "      <td>Et la guerre qui venait d'éclater en Algérie, ...</td>\n",
       "      <td>Je le contact avec l'ai à l'intérieur de la g...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>546</th>\n",
       "      <td>Ay caisse yu bari yuñ tegale la, caisse yi dañ...</td>\n",
       "      <td>Il y a beaucoup de caisses empilées, vraiment ...</td>\n",
       "      <td>Les go entre sont des garçons de l'autre qui ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>547</th>\n",
       "      <td>Sama yaay ak Baay a nga jaaxaan ca seen néeg b...</td>\n",
       "      <td>Mon père et ma mère sont couchés dans leur lit...</td>\n",
       "      <td>Ma mère sont debout sur le pont de la côte, e...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>548</th>\n",
       "      <td>Tàmbalee ko ci lekk gi. Lekk gu bon a bon – mb...</td>\n",
       "      <td>La nourriture désastreuse – ce pain noir, dont...</td>\n",
       "      <td>Une deuxième giclée en France, dans l'image d...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>549</th>\n",
       "      <td>Sunu po mi, nii la daan tëdde : ci kow « taraa...</td>\n",
       "      <td>Le jeu consistait, du haut du trapèze, à taqui...</td>\n",
       "      <td>Si tu veux « taraapees », il s'est senti dans...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>550</th>\n",
       "      <td>Waaw nataal bii tamit de gis naa ci benn bool ...</td>\n",
       "      <td>Oui sur cette photo ci aussi, j'y ai vu un bol...</td>\n",
       "      <td>Sur cette photo il semble qu'on appelle'a con...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>551</th>\n",
       "      <td>Bariwu fa ñu mas a teg bët Tubaab. Waaye mag ñ...</td>\n",
       "      <td>Les gens qui y vivent pour la plupart n'ont ja...</td>\n",
       "      <td>Au point de la France, ils sontnt l'a fait pa...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>552</th>\n",
       "      <td>Lii duggukaay bu Luga la frontière bu Luga fii...</td>\n",
       "      <td>C'est l'entrée de Louga. C'est à la frontière ...</td>\n",
       "      <td>Ceci ressemble à une bonne chose de Luga ou u...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>553</th>\n",
       "      <td>Amaana mu yaakaar, ni ñu bare ci jamono jooju,...</td>\n",
       "      <td>Peut-être espère-t-il, comme beaucoup de gens ...</td>\n",
       "      <td>Peut-être qu'on laisse au bout de sa vie, dan...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>554</th>\n",
       "      <td>Soppiw askan yombul, rawati-na bu dee yow mi k...</td>\n",
       "      <td>Cette leçon, mon père l'a sans doute apprise d...</td>\n",
       "      <td>Pour une sorte de milliers de milliers d'un a...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>555</th>\n",
       "      <td>Lii ab néeg la, néeg bi dañ kaa xadde ñax. Biñ...</td>\n",
       "      <td>Ceci est une chambre couverte d'herbe. En le c...</td>\n",
       "      <td>Ceci est une chambre. C'est une chambre. Une ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>556</th>\n",
       "      <td>Mayleen ma, ma sóorale ci nettali bi doomi niw...</td>\n",
       "      <td>Et le goût de la quinine dans la bouche, cette...</td>\n",
       "      <td>D'où me guignet à la discipline, dans la même...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>557</th>\n",
       "      <td>Liw bu mettee-metti ba ma daan dugg ba nu dëkk...</td>\n",
       "      <td>Je me souviens du froid de l'hiver, à Nice, ou...</td>\n",
       "      <td>Une deuxième giclée de la côte, dans la côte ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>558</th>\n",
       "      <td>Waaw mbokk mi, gis naa ni lii de ay potu Nesca...</td>\n",
       "      <td>Oui mon parent, je vois que ça c'est des pots ...</td>\n",
       "      <td>Sur cette photo il semble qu'on appelle'a mon...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>559</th>\n",
       "      <td>Ñàkkul su ma ko jeexoon ci xeetu nataal yu sii...</td>\n",
       "      <td>Image caractéristique de la Colonie : des voya...</td>\n",
       "      <td>Sans doute est-ce que je ne sais rien de l'ar...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>560</th>\n",
       "      <td>Daan na yónnee léeg-léeg ay bataaxal, di ci jo...</td>\n",
       "      <td>Il envoyait des nouvelles de temps à autre, so...</td>\n",
       "      <td>Il s'est senti proche dans la savane, ils ne ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>561</th>\n",
       "      <td>Joor gaa ngi mel ni amul àpp, ma seetlu ay gar...</td>\n",
       "      <td>La plaine, de chaque côté de la rivière, parai...</td>\n",
       "      <td>Je n'ai pas de jeunes gens de jeunes gens son...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>562</th>\n",
       "      <td>Lu yàgg déggu ñu, déggunu ko, muy wéy di liggé...</td>\n",
       "      <td>De longues années d'éloignement et de silence,...</td>\n",
       "      <td>À l'intérieur de la tête, la photo que j'ai g...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>563</th>\n",
       "      <td>Waaye li ma ëppal solo ci nataal bi, mooy ñaar...</td>\n",
       "      <td>Et sur une plage, où viennent mourir les vague...</td>\n",
       "      <td>Mais ce qui s'est passé dans laquelle je n'ai...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>564</th>\n",
       "      <td>Xamul woon waxtaan ba déggoo ciy kàddu. Déedée...</td>\n",
       "      <td>Il était inflexible, autoritaire, en même temp...</td>\n",
       "      <td>Mais la fin de l'orage sur le grand fleuve. M...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>565</th>\n",
       "      <td>Nataal bii nag moom benn pañe la. Lu ëpp liñ k...</td>\n",
       "      <td>Sur cette photo ci cependant, il y a un panier...</td>\n",
       "      <td>Sur cette photo il y a une chose. Lu, il y a ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>566</th>\n",
       "      <td>Ma seetlu ci mbooloo mi benn jigéen ju màgget....</td>\n",
       "      <td>Parmi tous ceux qui se pressent autour de moi,...</td>\n",
       "      <td>Les hommes sont les hommes sont de l'arrivée ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>567</th>\n",
       "      <td>Gone gu naqadi deret laa woon. Saa yu ma bëgga...</td>\n",
       "      <td>Je me souviens aussi d'avoir été pris par des ...</td>\n",
       "      <td>J'ai rencontré une vieux fauteuil de l'Afriqu...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>568</th>\n",
       "      <td>Nataal bii maa ngi ciy gis ku yor raaya Senega...</td>\n",
       "      <td>Sur cette photo ci, j'y vois quelqu'un portant...</td>\n",
       "      <td>Sur cette photo il y a une chose de service. ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>569</th>\n",
       "      <td>Yaakaar naa ni Ogosaa lanu xame ni Peer Nowel ...</td>\n",
       "      <td>Je suppose que c'est en arrivant à Ogoja que n...</td>\n",
       "      <td>Je ne peux pas l'ai reconnu de trois personne...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>570</th>\n",
       "      <td>Lii ak kër. Kër gi nag ni lu bindoo tegale gi ...</td>\n",
       "      <td>Ceci est une maison. La maison est, cependant,...</td>\n",
       "      <td>Ceci est une maison de maison qui s'y coucher...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>571</th>\n",
       "      <td>Dafa fa yegg rekk, luye gaal ànd ak fajkat ya ...</td>\n",
       "      <td>Dès qu'il arrive, il affrète une pirogue munie...</td>\n",
       "      <td>Il a l'habitude de l'air de la fin de mon pèr...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>572</th>\n",
       "      <td>Nataal bii mu ngi nekk ak nit, nit ku xees, ku...</td>\n",
       "      <td>Sur cette photo, c'est une personne de couleur...</td>\n",
       "      <td>Sur cette photo il y a une chose qui ressembl...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>573</th>\n",
       "      <td>Ni réewi Tubaab yi sàccee alalu Afrig ba desee...</td>\n",
       "      <td>Déjà, il avait perçu l'oubli tactique dans leq...</td>\n",
       "      <td>Les hommes sont des femmess passés de la soci...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>574</th>\n",
       "      <td>Yaakaar naa ne ba ma nee cëpp Niseryaa – nu ja...</td>\n",
       "      <td>Je crois que dans les premières heures qui ont...</td>\n",
       "      <td>Je ne peux pas pour qu'il me souviens de l'ar...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>575</th>\n",
       "      <td>Ca bëj-gànnaar ak penku, mënoon naa séen i jàn...</td>\n",
       "      <td>Vers le Nord et l'est, je pouvais voir la gran...</td>\n",
       "      <td>Les fourmiss de la Nice des années en Afrique...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>576</th>\n",
       "      <td>Waa lii, ci wanag lay nekk. Ndee lu weex lii d...</td>\n",
       "      <td>Ceci se trouve dans les toilettes.  cette chos...</td>\n",
       "      <td>Une large chambre dans la terre qui est en la...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>577</th>\n",
       "      <td>Moonte loolu taxul ñu mën ni yoon wi ñépp daan...</td>\n",
       "      <td>Les jeux, les discussions et les menus travaux...</td>\n",
       "      <td>Ils ont été nuages en Afrique de la société a...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>578</th>\n",
       "      <td>Xanaa liñ fi tudde woon Xare bu Mag bi. Sunu k...</td>\n",
       "      <td>La guerre, le confinement dans l'appartement d...</td>\n",
       "      <td>Une grand-mère qui avait parcouru les fleuves...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>579</th>\n",
       "      <td>Bi sama yaay dellusee – xéy-na ndaje mi dalul ...</td>\n",
       "      <td>Quand ma mère revient (peut-être vaguement inq...</td>\n",
       "      <td>Alors ma mère ses deux enfants âgés sur le gr...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>580</th>\n",
       "      <td>Li koy yëngal noonu, xam naa ko. Lan moo ko dà...</td>\n",
       "      <td>Je peux ressentir l'émotion qu'il éprouve à tr...</td>\n",
       "      <td>J'ai travaillé en avaient deuxies en Afrique ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>581</th>\n",
       "      <td>Walla ma ni boog loolu la doon gént, laata xar...</td>\n",
       "      <td>C'était avant la guerre, avant la solitude et ...</td>\n",
       "      <td>Je me souviens de la guerre – à la guerre pou...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>582</th>\n",
       "      <td>Walla boog mu mas maa may ci jaloorey ponkali ...</td>\n",
       "      <td>Ou bien les récits de grands Blancs qui voyage...</td>\n",
       "      <td>D'où venait de personnes personnes britanniqu...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>583</th>\n",
       "      <td>Xam naa damaa waroon a yuuxu keroog ndax sama ...</td>\n",
       "      <td>Je ne m'en souviens pas, mais j'ai dû crier, h...</td>\n",
       "      <td>Je me souviens de la savane de la voiture dan...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>584</th>\n",
       "      <td>Ñu ngi dëkkoon ci benn néegu-ňax. Baatu “néegu...</td>\n",
       "      <td>Dans la case que nous habitions (le mot case a...</td>\n",
       "      <td>Les érus ne sont pasinéen de douze ou le pidg...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                    original_sentences  \\\n",
       "0                              Jéndal gejju Kaasamaas.   \n",
       "1                                    Bët yu réy la am.   \n",
       "2                              Asamaan saa ngiy dënnu.   \n",
       "3                                         Mi xar mépp.   \n",
       "4                                   Kër gu jëkk gi la.   \n",
       "5                                        Ndax kan dem?   \n",
       "6                                           Jamma rek.   \n",
       "7                                             Ana ŋga?   \n",
       "8                                                Naan?   \n",
       "9                                    Yaw xamuloo kenn.   \n",
       "10                             Ba tey maa ngi liggéey.   \n",
       "11                               Ci seen baax la bokk.   \n",
       "12                                   Fanweer yi ñaata?   \n",
       "13                                             Ku dem?   \n",
       "14                             Ñaari xew yi dañu daje.   \n",
       "15                                    Yàlla, bàjjo bi.   \n",
       "16                                  Ana njëgu guró gi?   \n",
       "17                                        Ban nga wax?   \n",
       "18                                      Ba nga xam ne.   \n",
       "19                                         Keneen ŋga.   \n",
       "20                                           Dafay fo.   \n",
       "21                                    Séen naa am xar.   \n",
       "22                                     Ana waa kër gi?   \n",
       "23                                            Du kenn.   \n",
       "24                                      Gis ŋga kooku?   \n",
       "25                                        Deemal waay!   \n",
       "26                                        Jaam nga am?   \n",
       "27                                   Man xar moo réer?   \n",
       "28                                     Bi ŋga dee dem.   \n",
       "29                                     Bi ŋga dee dem.   \n",
       "30                                      Lu am, Yàllaa!   \n",
       "31                                 Léegi dafa bejjaaw.   \n",
       "32                                        Dama càqaar.   \n",
       "33                                          Danu sonn.   \n",
       "34                                           Jaam rek.   \n",
       "35                                     Ab juló dàkkat.   \n",
       "36                                      La ŋga wax la.   \n",
       "37                                         Xolal fépp!   \n",
       "38                                       Moo doon wax.   \n",
       "39                                          Benn lañu.   \n",
       "40                                      Naka-nga sant?   \n",
       "41                                      Ñaata lay jar?   \n",
       "42                                    Dëddu na àdduna.   \n",
       "43                                          Nanga def?   \n",
       "44                                    Giskóonuma leen.   \n",
       "45                                Waxtaan ak kenn kan?   \n",
       "46                                            Kile la.   \n",
       "47                                 Dangeen di waxtaan?   \n",
       "48                             Jigéen ñi danañu naŋgu.   \n",
       "49                                     Naka la bindoo?   \n",
       "50                                         Booy, kaay!   \n",
       "51                                    Mayal benn ñépp!   \n",
       "52                                        Samay xarit!   \n",
       "53                                         Su dee dem.   \n",
       "54                          Nit ag gaynde duñu dëkkóo.   \n",
       "55                                   Xar man moo réer?   \n",
       "56                                  Xaj bi dafa gaañu.   \n",
       "57                                        Ñaata ŋgeen?   \n",
       "58                                     Góor gi di dem.   \n",
       "59                                 Seetal néeg beneen!   \n",
       "60                                        Aw lawbe la.   \n",
       "61                                  Poos bi dafa bënn.   \n",
       "62                                         Cig gàttal.   \n",
       "63                                   Xale buu laa wax.   \n",
       "64                                      Deg nga Wolof?   \n",
       "65                                  Fàttali ma bés bi!   \n",
       "66                                           Sama bos.   \n",
       "67                                     Xar man a réer?   \n",
       "68                                          Ana kooku.   \n",
       "69                                 Ak nu mu mëna deme.   \n",
       "70                             Négël xale bu rafet bi.   \n",
       "71                                     Dafa dàŋŋaaral.   \n",
       "72                                 Kees bi dafa dëppu.   \n",
       "73                                        Omar a ngii.   \n",
       "74                                  Dafa doon liggéey.   \n",
       "75                                         Xolal fépp!   \n",
       "76                                              Fu mu?   \n",
       "77                             Degg naa tuuti Faranse.   \n",
       "78                                           Dem nañu.   \n",
       "79                                        Dem fa suba.   \n",
       "80                                    Séen naa aw fas.   \n",
       "81                                             Fee la.   \n",
       "82                                 Góor gi deekoon na.   \n",
       "83                                      Sama xarit la!   \n",
       "84                                 Su góor gi dee ñëw.   \n",
       "85                                           Ñunga fa.   \n",
       "86                                             Ku ñëw?   \n",
       "87                                     Góor gi du bày.   \n",
       "88                                          Moom daal!   \n",
       "89                                  Sax yee koy mujje.   \n",
       "90                                Mu ngi toog ne cell.   \n",
       "91                                Ngoon saangi fi rek.   \n",
       "92                                  Ma ŋgoogule foofu.   \n",
       "93                                   Ya ŋgi te mi ŋgi!   \n",
       "94                                          Danaa dem.   \n",
       "95                                            Benn la.   \n",
       "96                              Baal naa la sama wàll.   \n",
       "97                            Bul fëggës basaŋ gi fii.   \n",
       "98                                   Yaw doŋŋ laa wax.   \n",
       "99                                           Nekku fi.   \n",
       "100                               Mu ngi ci sa càmmoñ.   \n",
       "101                                 Xale yi agsi nañu.   \n",
       "102                                            Am lii!   \n",
       "103                               Duggal mba nga génn!   \n",
       "104                                    Wekkil gëñ yii!   \n",
       "105                                           Dem naa.   \n",
       "106                                     Naka ngoon si?   \n",
       "107                                     Dawal ma dara!   \n",
       "108                                         Keneen la?   \n",
       "109                                  Li ŋga wax loolu.   \n",
       "110                                        Fu mu bëgg.   \n",
       "111                         Aw gaynde ci biir néeg bi!   \n",
       "112                                    Mi ŋgi foofule!   \n",
       "113                                  Xarit yan ño ñëw?   \n",
       "114                                     Naka-nga sant?   \n",
       "115                                  Jiit dafay fette.   \n",
       "116                               Kaay gunge ma marse.   \n",
       "117                                    Dinaa leen dab.   \n",
       "118                                      Soo dee góor.   \n",
       "119                                 Góor gi demoon na.   \n",
       "120                                            Dem na.   \n",
       "121                                  Duŋgeen woon ñëw.   \n",
       "122                                        Noonee nan?   \n",
       "123                                            Guy la.   \n",
       "124                                         Dafa tóol.   \n",
       "125                              Wax ma say bëgg-bëgg!   \n",
       "126                                     Buuse la woon.   \n",
       "127                                Fi ŋga dem foofile.   \n",
       "128                           Gannax yee këpp gaal gi.   \n",
       "129                     Nanga dàkkaande sama mbubb mi.   \n",
       "130                           Fépp foo dem, mu nga fa.   \n",
       "131                                         Lu la dal?   \n",
       "132                              Jënd ñaa menn xar mi.   \n",
       "133                        Waaye man sax dama dul ñëw!   \n",
       "134                                  Suba lañuy feyyu.   \n",
       "135                             Buleen aw ci yoon wii!   \n",
       "136                              Cangat li laay defsi.   \n",
       "137                                Jox na nit ki dara.   \n",
       "138                    Bar ca bar ca ; bar ca ñaar ca.   \n",
       "139                                Sama caq dafa dagg.   \n",
       "140                            Borom dëkk bi dëddu na.   \n",
       "141                                   Conkom du màndi.   \n",
       "142                            Jaaykat bu am-bayre la.   \n",
       "143                    Coow laa ngiy awu ci sama kaaŋ.   \n",
       "144                          Damay dooleel li nga wax.   \n",
       "145                                  Meew mi dafa rax.   \n",
       "146                         Yobul na ka ŋgooñ tool ya.   \n",
       "147                                    Doo dem, xanaa?   \n",
       "148                           Safara sa gandeer na ko.   \n",
       "149                             Gor gii di Lawbe Ndar.   \n",
       "150                  Bindal, ndax nga baña fàtte dara.   \n",
       "151                     Sama gannaaw gi yépp ay metti.   \n",
       "152                                       Ma ngii dem.   \n",
       "153                            Nit keneen kaa laa wax!   \n",
       "154                         Jile jigéen jan ŋgeen wax?   \n",
       "155                                 Bul guddal wax ji!   \n",
       "156                                  Fenn laa dul dem.   \n",
       "157                            Gis naa la yaw ak moom!   \n",
       "158                   Sàngara si dafa leen daan fital.   \n",
       "159                                  Mbaa kenn génnul?   \n",
       "160                         Mu nga ne faax ca saal ba.   \n",
       "161                                  Waxul cu li dara.   \n",
       "162                      Góor gi ak xale bi génn nañu.   \n",
       "163                                   Yaa ŋgi demuloo.   \n",
       "164                 jëme ko ci dëkki Afrig yu ŋiis yi.   \n",
       "165                                Gaw na ay ndomboom.   \n",
       "166                              Day na ni Omar léegi.   \n",
       "167                          Loolu jar naa deñi-kumpa.   \n",
       "168                                    Kañ ŋga ka gis?   \n",
       "169                        Ca gannaaw-ëllëg sa la dem.   \n",
       "170                                      Abdoo nu doy.   \n",
       "171                            Dégg ngeen jóolóoli bi?   \n",
       "172                                Amul bàmmeelu biir.   \n",
       "173                                  Tàggool ba-noppi.   \n",
       "174                         Am nàa mbekte ci guiss la.   \n",
       "175                    Ñoo leen daan yenul seeni gaal.   \n",
       "176                                   Naka waa kër ga?   \n",
       "177                             Gor gii di Lawbe Ndar.   \n",
       "178                    Dinga xam ne man nga digaaleel.   \n",
       "179                        Demleen futbal ca mbedd ma!   \n",
       "180                                 Ana doomu néeg bi?   \n",
       "181                      Awma dencukaay bu mënti nekk.   \n",
       "182                                   Xale bu baax la.   \n",
       "183                                  Jaama nga fanaan?   \n",
       "184                            Mu ngiy bëgga ëpp loxo.   \n",
       "185                    Yéesu dundalaat na ku dee woon.   \n",
       "186                                  Góor gi doon dem.   \n",
       "187                         Loolu génn na ci sama xel.   \n",
       "188                       Wooyil Musaa moom mi di dem.   \n",
       "189                            Yég na ko ba bëgga dee.   \n",
       "190                            Yég na ko ba bëgga dee.   \n",
       "191                         Mu nga ne déjj di la xaar.   \n",
       "192                           Basaŋ gu ñu ràbbe barax.   \n",
       "193                           Mën na def ñaari liitar.   \n",
       "194                      Sa bopp baa ngi fees ak dëññ.   \n",
       "195                                        Fase na ko.   \n",
       "196                             Dafa foye palaasam bi.   \n",
       "197                            Weneen fas wan ŋga gis?   \n",
       "198               Takkal jaaro bi ci baaraamu-digg bi!   \n",
       "199                              Demal góor gi gis la!   \n",
       "200                         Su ma la jàppee, aaxajala!   \n",
       "201                         Doxatu gëléem, kow la jëm.   \n",
       "202                                 Nit ku baaxoon la!   \n",
       "203                                Benn ban ŋga gisul?   \n",
       "204                               Jambaar du bare wax.   \n",
       "205                      Nit ñooñii génn ay mbër lanu.   \n",
       "206                   Sa yay nee na ci ŋgoon dana ñëw.   \n",
       "207                                 Dem ŋga te ñëw na.   \n",
       "208                                 Yaa doonkoon falu.   \n",
       "209                                Bumi bi du fi buur.   \n",
       "210                         Góor gi waxtaan na ag yaw.   \n",
       "211                                  Kooku rekk ñëwul.   \n",
       "212                                   Ni ka bu mu ñëw.   \n",
       "213                       Am na ab banaana ci pañe bi.   \n",
       "214                                    Keneen, ki ñëw!   \n",
       "215                       Bul diri xale bi ci suuf si!   \n",
       "216                  Sàmmoon neñu ci kaaraange bëy yi.   \n",
       "217                              Tey, danga am-wërsëg.   \n",
       "218                             Xalamkat bu aaytal la.   \n",
       "219                           Wekkiwuloo, dangay daaj.   \n",
       "220                                     Bi mu dee dem.   \n",
       "221                          Dëkk bi am na ag coppite.   \n",
       "222                         Suuf si nu meňň ëmb wurus.   \n",
       "223                             Joxu la woon juuti bi.   \n",
       "224                          Màgget ñu bare dañu dëpp.   \n",
       "225                                           Demagul.   \n",
       "226                          Mu ngay ciñaag ca ëtt ba.   \n",
       "227                              Wax-dëgg jaambaar la.   \n",
       "228                                   Yoon daan na ko.   \n",
       "229                                  Dajaleel call yi.   \n",
       "230                                          Loolu la.   \n",
       "231                                       Góor gi dem.   \n",
       "232                                        Man demuma.   \n",
       "233            Joxal téere bi doomu nit ku yaru kooku!   \n",
       "234                                 Wax ko ni mu deme.   \n",
       "235                               Jëkkëru yoonu Yàlla.   \n",
       "236                              Ñii dañu demul xanaa!   \n",
       "237                                      Gisu la woon.   \n",
       "238                      Ŋga dem ŋga bañ dem dana ñëw!   \n",
       "239                   Dafa ko doon dóor suba ak ngoon.   \n",
       "240                                          Maay dem.   \n",
       "241                            Dund gi metti na foofa.   \n",
       "242                                   Omar dinay bàcc.   \n",
       "243                                 Mësumaa dem foofa.   \n",
       "244                               Demal ba mu dellusi.   \n",
       "245                                           Nit laa!   \n",
       "246                                     Kooku ci biir.   \n",
       "247                        Dafa ko bojj paaka ci biir.   \n",
       "248                      Làq ci seen biir xal yu yànj.   \n",
       "249                               Jambaar du bare wax.   \n",
       "250                               Lem ko ndax mu dëll.   \n",
       "251                                         Nit ŋgeen!   \n",
       "252                                    Nag woowa wépp.   \n",
       "253                                   Mbaa kenn demul.   \n",
       "254                                Man la góor gi woo?   \n",
       "255                                   Birig yi doyuñu.   \n",
       "256                     Fas yéene naa dem Kawlax suba.   \n",
       "257                Sa taal bi bërëx na ; kaay tàmbali.   \n",
       "258                             Jeneen jabar laa bëgg!   \n",
       "259                 Boo wàcce yoonu Bamako jëm Arundu.   \n",
       "260  Ogosaa, xaritoo woon nan faak yeneen gunóor : ...   \n",
       "261                                Fas yéenen la bëgg!   \n",
       "262                           Toogal ci fépp, fu leer!   \n",
       "263                                 Nit ñeñeen la gis!   \n",
       "264                               Yaw la ndaw si sopp.   \n",
       "265                       Gàddaay ba mana daw kàdd gi.   \n",
       "266                              Omar moo donn kër gi.   \n",
       "267                       Daŋga tayal naka keneen kee.   \n",
       "268                           Gisuma ko, dama gëmmoon.   \n",
       "269                Soo yaree sa doom, du la teg gàcce.   \n",
       "270                          Dana tukki tay mbaa ëlëk.   \n",
       "271                         Demkoon doon na ma fi war.   \n",
       "272                    Du bés bu nekk baay rey gaynde.   \n",
       "273                     Nit ki dóor na nag wi ak bant.   \n",
       "274              Fètt bi amul woon benn wersseku fàcc.   \n",
       "275   Dafa doon nëbb biiram bi wànte léegi dey fés na.   \n",
       "276               Faraas dóor na Berezil ñett ci dara.   \n",
       "277        Waxtu wi dafa jot rekk, mu daldi gaawa dem.   \n",
       "278                                    Demoon ba Ndar.   \n",
       "279                Tegal leket gi ci ndox mi, du diig.   \n",
       "280               Soo koy def, na doon ci sag coobare.   \n",
       "281                                       Dañu ko déj.   \n",
       "282       Gis naa góor gi doon wax te toogoon ci suuf.   \n",
       "283                  Di noyyi xet gu bon gi ci ban bi.   \n",
       "284                                Def ci xobi bàkkis!   \n",
       "285       Nit ki ci sama wet ak nit kooku mbokk la ñu.   \n",
       "286                                Nit ki baaxkoon la!   \n",
       "287                      Dafa di taseeguloo ak gaynde.   \n",
       "288                             Mu ngay dukkat di dem.   \n",
       "289                    Ku am-aajo la ci ay dëkkandoom.   \n",
       "290                             Bi mu agsee, ñépp jog.   \n",
       "291             Moom daal, léegi addina dafa soppeeku.   \n",
       "292                           Mënumaa gapparu lu yàgg.   \n",
       "293                              Dafa bëtal ngénte li.   \n",
       "294                                          Dama dem.   \n",
       "295                             Bàq bi dina yomba gas.   \n",
       "296                       Soo ma dóoree, ma feyyu kay.   \n",
       "297                             Ŋga dem la ñooñu bëgg.   \n",
       "298                           Góor gii la xale yi wax.   \n",
       "299                           Góor gi dem la, ma defe!   \n",
       "300                          Benn rekk laa ci gisagum.   \n",
       "301                       Bësal picc bi, dëtt ji génn!   \n",
       "302                                   Naan na ba furi.   \n",
       "303                   Léegi ñu wara tàmbali bal-balal.   \n",
       "304                  Dafa ko galajaane, toog ci kowam.   \n",
       "305                      Nanga bomb biir beek biti bi.   \n",
       "306                Nit, gayndé, nag... àndoon nañu fi.   \n",
       "307                      Soo ñëwóon, kon, demkoon naa!   \n",
       "308                       Fase na ; mu nga kër baayam.   \n",
       "309                            Seeti ko, sa aajo faju.   \n",
       "310                         Am na nit ñuy lekk caaxoñ.   \n",
       "311                   I janax a ngi lëňbati say butit.   \n",
       "312                                   Séen naa as lëf.   \n",
       "313            Génneel nañu porose bi xaalis bu takku.   \n",
       "314                    Ceeb bi dafa forox, bu ko lekk.   \n",
       "315                               Demuloowoon ca biir.   \n",
       "316         Soo ko gisee, mu ngi firiku bu baax léegi.   \n",
       "317                  Dama ko fekk muy waxaale ab daar.   \n",
       "318                        Ku dëggu lañu ko wara dénk.   \n",
       "319                    Kër gi dafa bare cambar-cambar.   \n",
       "320                               Baase laay togg tey.   \n",
       "321                        Ndaje mi, gaawu biy ñów la.   \n",
       "322                                Ci sa peggi dex yi.   \n",
       "323                            Nu ngi gëje ca takk ga.   \n",
       "324               Sëpp-jaleñ du la fasale ak say teeñ.   \n",
       "325                 Nanu roñu bala nu fi taw bee fekk!   \n",
       "326              Maa ngi ko fekk, rongoñ yiy bas-basi.   \n",
       "327                      Dafa dikkale xaritam fan yii.   \n",
       "328               Sag càggan rekk a tax xale bi daanu.   \n",
       "329                            Gisoon naa xar yi yépp.   \n",
       "330                               Ferñent wee ko lakk.   \n",
       "331                       Kenn du paas ; ku bëgg dugg.   \n",
       "332                       Góor gi dem te xale yi dugg!   \n",
       "333                            Waaw, waaw ndax dem na?   \n",
       "334         Tool bii, ku am-kàttan rekk a ko mëna bey.   \n",
       "335                    Ñun ñi nga doomoo, bu ñu fàtte!   \n",
       "336            Mbëggéelam googee woon a ŋgi fi ba tay.   \n",
       "337          Dugg nañu doonte bunt yi dañoo tëju woon.   \n",
       "338  Li wóor ba wóor moo di ne nuni neen la woon ke...   \n",
       "339                                            Lu tax?   \n",
       "340                                Noonu, man, më dem.   \n",
       "341           Dafa ne dëndiix soow ma, di xaar leneen.   \n",
       "342                  Ku doon laajte Faali, Ngoy a ngi.   \n",
       "343                 Séy bu ànd ak basiira gu mat sëkk.   \n",
       "344              Xaaral ba mu géex, nga sog koo tëral.   \n",
       "345               jigeen ňaak góor ňa donte dañu tëdd.   \n",
       "346                          Duy def suukar ci kafeem.   \n",
       "347              Noonu, mu binni daldi wuyuji Boromam.   \n",
       "348                                  Dafa bëgg xaalis.   \n",
       "349                              Maamam garmi la woon.   \n",
       "350                   Ndaje maa ngay ame ca bayaal ba.   \n",
       "351                                 Baaxoñ bee ko yee.   \n",
       "352                          Ku duppati mburu xale bi?   \n",
       "353       Bu sedd bi dikkee, sama tànk yi dañuy didim.   \n",
       "354              Der gi ñu ko liggéeye, deru mbëtt la.   \n",
       "355                         Maa ngiy gubali samaw fas.   \n",
       "356                            Indeegul dundu weer wi.   \n",
       "357                  Indil bàccu yi ; nit ñi ñów nañu.   \n",
       "358                        Góor gaa ŋgi, nitu Ndar la.   \n",
       "359                             Buy, balaa jariñ, toj.   \n",
       "360                      Gàddaay dëggal sa géntu yaay.   \n",
       "361                            Danga ma bëgga gëlëmal.   \n",
       "362                  Nit ñaa ngi ko ëw, muy mbeleleli.   \n",
       "363                             Defe naa dana ñëw tày!   \n",
       "364                        Li mu ma may, duubal bi la.   \n",
       "365  Dafa ko tere wax baat bi, waaye mu dellu waxaa...   \n",
       "366             Sama càkkar ba dey, bëgg naa koo xadd.   \n",
       "367                                         Maa demul.   \n",
       "368                                 Lu ay, di njàmbat.   \n",
       "369                           Ndax réew mi am na alal?   \n",
       "370                      Xoolal meew mi bala muy fuur!   \n",
       "371                         Ay àddu-kalpe ñoo ko bóom.   \n",
       "372                      Baabun gi dafa këf banaanaam.   \n",
       "373                               Cobteg doomam a tax.   \n",
       "374                   Dend bi laay daw, dafa bare wax.   \n",
       "375                      Doxandéem nelawul ba yàndoor.   \n",
       "376              Ñaar ñii, bëggante nañu ci dëgg-dëgg.   \n",
       "377                              Damay waaja ballarñi.   \n",
       "378                   Demal cay ceeb bi, dese na ndox!   \n",
       "379                     Ëlbatil furno bi ci ngelaw li.   \n",
       "380              Gall na ñaari yoon, dafa nàmp lu ëpp.   \n",
       "381                                      Gisul yeneen.   \n",
       "382                      Xawma lan lay nurool ci yeen.   \n",
       "383     Borom woto yépp a ngiy jooytu tali yu yàqu yi.   \n",
       "384          Ba ñu ko falee ministar la wat faasam ba.   \n",
       "385                   Ku fóon màngo ji xam ne neex na.   \n",
       "386                  Dàqal weñ yiiy biiw yàpp wi rekk.   \n",
       "387                 Lu mu for, fejat ko, jox i cuujam.   \n",
       "388                           Ren la xale yi di xaraf.   \n",
       "389                              Fanaan bee ma jaaxal.   \n",
       "390  Su yaboo sax ma ni ca la fàttee naka la nit di...   \n",
       "391  Léegi de, yaay tegoo njaboot gu sew gi fi sa b...   \n",
       "392           Bëy, bu àndul ak bëy ya, ànd ak cere ja.   \n",
       "393  Afrig ci boppam, boole ràŋŋatikook xol bu rafe...   \n",
       "394                  Daj naa lu ne laata may agsi fii.   \n",
       "395                           Xon wi lal ci gàncax gi.   \n",
       "396  Xale bu góor bu tollu ci diggu dooleem, ànd ak...   \n",
       "397           Dafa ko dëkke woon dóor, moo tax mu fàq.   \n",
       "398         Bu nu déggee sab dënnu, gaajo ga dina tas.   \n",
       "399         Danga goreedi, moo tax séddalewoo ci yoon.   \n",
       "400                           Tey, fàww ma seeti Omar.   \n",
       "401                            Tamxaritu daaw la woon.   \n",
       "402     Mënuloo woon jénd meew, xanaa furóoñu cere ji?   \n",
       "403                    Findifeer lañu yeewe sàkket wi.   \n",
       "404            Moom de toogu fi, cuub-cupet la fi def.   \n",
       "405                   Làmmiñ jigul bant, astemaak nit.   \n",
       "406     Aw faaraar ci diggu bopp : kooku Jóob la sant.   \n",
       "407     Su ma yewwoo rekk, sama bët yi dañuy tappaloo.   \n",
       "408  Dafa yëngu woon té beg lool ba mu daanal bunt ...   \n",
       "409  Ki ko bóom, ca ña ko dànd bi muy wax la wara b...   \n",
       "410              Sa jarbaat yi, jenn gënu kë ci tayel.   \n",
       "411  Waaye dina fàttaliwaale yit njàqareg Baay, raw...   \n",
       "412        Dina la jéndal woto bu ganaar saxee béjjén.   \n",
       "413                 Gàcce ak xamadi, laajul a ko indi.   \n",
       "414                 Loolu moo aju woon ci ndaje moomu.   \n",
       "415            Fii, Tubaab bi jotu fee am doole noonu.   \n",
       "416            Bët du yenu waaye xam na lu bopp àttan.   \n",
       "417                   Kolkolaat yëgul ne yamb baax na.   \n",
       "418  Ndax mën naa jariñoo masin bi su ma amee lu ma...   \n",
       "419     Soq bi metti na ; yóbbul dugub ji ca masin ba!   \n",
       "420                     Taxawaayu kooka sax taxul nëw.   \n",
       "421  Soo dee doxandéem cim réew, say kayit dañu war...   \n",
       "422                       Ekoolu xale yu doyadi yi la.   \n",
       "423                             Sa mbind mi, danga ko.   \n",
       "424              Kenn faalewul dara, lu neex waay def.   \n",
       "425                       Li nga ko wax a tax mu gedd.   \n",
       "426  Xéy-na àgg na ba tatawu Mak-Mawoŋ, ca El Goleyaa.   \n",
       "427                                  Amuma doomu-ndey.   \n",
       "428                  Ciñaag li baaxul ci sa sibiru bi.   \n",
       "429  Ca bëj-saalum ba, am mbartal mooy jëme ca benn...   \n",
       "430           Bëggul ku ko ne sa bët bi suuf a ngi ci.   \n",
       "431                  Borom bi, duma la bàyyi ba abada.   \n",
       "432                Lu-tax muy darngu mel ni mënul dox?   \n",
       "433          Géwél, ne ma naam, bëj-géwél, ne ma naam!   \n",
       "434  Masin dafay gar rekk, bu ko defee, nga wolaat ...   \n",
       "435     Yaw, jàllal, dama ni siiwa mucc ci yaw, demal!   \n",
       "436  Dafa laabir ba ku dikk ci dëkk bi rekk, mu boo...   \n",
       "437  Bëccëg googu yépp, naaj wi lakk na seen yaram,...   \n",
       "438     Nég ba jaan dem, nga topp ca watit wa di dóor.   \n",
       "439       Waxam ji dafa bar ; xanaa danga ko def dara?   \n",
       "440               Boo xamoon caay-caay gi ci gone gii!   \n",
       "441  Ni mu beroo noonu ci àll bi, tax koo gise Afri...   \n",
       "442  Amu ci woon benn bu ma xam naka la ko nëwu biy...   \n",
       "443  Su ma ragalul woon fen, dinaa ni toog naa ay a...   \n",
       "444  Samay way-jur yëg ñañu fi jàmmu yaram ju ñu ma...   \n",
       "445                  Soo demee góor gee ni kookule la!   \n",
       "446              Cokkeer, mënta réer morom ma ci dédd.   \n",
       "447  Masuma woon a xalaat ne dinaa dund ba bés ñëw ...   \n",
       "448                             Jaxaaw la defe dëj bi.   \n",
       "449                       Caax moo baax ci tàngoor wi.   \n",
       "450  Duma wax ci taariixu dëkk bi ndax dafa gudd lool.   \n",
       "451  Su nu delloo ëllëg sa, fekk ay gunóor fatt faw...   \n",
       "452  Réew mu rëb la, waaso yi dëkkee xeex ci seen b...   \n",
       "453                      Bàyyil jën yi, dangay fot de.   \n",
       "454                        Ñuy bëkk di dem, wuti géej.   \n",
       "455                        Gëje nañu ca Aadama ak Awa.   \n",
       "456                 Ñaari Korite du guléet ci réew mi.   \n",
       "457  Soo bareetee liggéey, danga koy cër-cëre ci bé...   \n",
       "458  Ngir gën a gëmloo loolu sama bopp, ma daldi ko...   \n",
       "459  Gone, du jëfandikooy baat. Rax-ci-dolli, baat ...   \n",
       "460                             Kuy xalam di ci jaayu.   \n",
       "461  Te sax, lépp li jiitu saa soosu ñu ma sàkk lañ...   \n",
       "462                       Ñawoon la ba bi mu fiy joge.   \n",
       "463               Dànkaafu naa ko ba tàyyi ; ma ba ko.   \n",
       "464  Béjjénu nag jégeñe na lool, waaye bu ñu laalee...   \n",
       "465                      Nee ñu ndaw soosu ab dëmm la.   \n",
       "466  Bamendaa la Baay yóbbu sama yaay ba mu séysee....   \n",
       "467          Baadoolo bu ragal naaj ragal na njariñam.   \n",
       "468                       Borom ŋàdd yëgul borom pañe.   \n",
       "469  Amul werante, mbaa yéexantu mba jàdd koñ. Àndu...   \n",
       "470                       Dama ko botti, ñu ànd daanu.   \n",
       "471                                      Lu defu waxu.   \n",
       "472            Mu ne feek amul liggéey, du takk jabar.   \n",
       "473              Bëgg në ŋga ñëw, mu bañ dem su nëjul!   \n",
       "474  Dama la bëgga biral sama dikk, moo tax ma tele...   \n",
       "475        Maa ngi lay dénku : bul dem te dellusiwuma!   \n",
       "476                     Colal ci mbedd, jekkul ci mag.   \n",
       "477    Noonu laa ko tàmme woon a gis ci mujjug dundam.   \n",
       "478  Boo amaan bu-tuut, mën nga cee jénd paketu mbi...   \n",
       "479                         Bu pënd wuree, fattu bare.   \n",
       "480  Dafa koy cuuj dëgël bi, fas wiy tëbbantu mel n...   \n",
       "481                 Tey faaru xar yu ñu wàjj laa buun.   \n",
       "482                Soo xoromee, sa tànk yi dañuy galt.   \n",
       "483                     Li ma ko yàqal mooy : àbbaatu.   \n",
       "484                    Ma barkeelu ci kaalag sëriñ bi.   \n",
       "485                          Puróx du gërëm ñamu daaw.   \n",
       "486  Ku am-jom la ; su ko baay ji xulee, mu jàng bu...   \n",
       "487     « dévaluation » bi kaay, mook nemmali ñoo yem.   \n",
       "488  Barkeel bi moo wutale ndoxu batise bi ak ndox ...   \n",
       "489       Raxasal coowu li bala nga ciy sotti meew mi.   \n",
       "490  Gajj ndox, sa dënn a ciy metti, ndege du ko ta...   \n",
       "491  Dama mas a jàpp ni kenn mënumaa ëpple ci bant ...   \n",
       "492  Turu góor la am ndax maamam ju góor lañu ko di...   \n",
       "493        Su déggoo amee ci seen biir, lépp dina dox.   \n",
       "494  Tey, ma ngiy fàttaliku bu ci nekk ci ñoom ak i...   \n",
       "495                  Damay fajaru ; nanga teela yewwu.   \n",
       "496    Benn séex bi, gàjj bi ci lex bi laa koy xàmmee.   \n",
       "497  Lépp lu may yëg walla ma koy jëf saxe fa, meññ...   \n",
       "498                           Ab cuuñ a ngi ci guy gi.   \n",
       "499             Déj bi dafay tax duñu màgg ni mu ware.   \n",
       "500  Ku dem ba xamatuloo fa nga jëm, nga dellu fa n...   \n",
       "501           Gis ŋga nag yii yépp, woowuu moo ci gën.   \n",
       "502                           Doxatu mbaam wee ko yee.   \n",
       "503                    Lëg gi dafa fëll rekk, ma soqi.   \n",
       "504            Lérén dina fanqal liir yaram wu ñaawlu.   \n",
       "505  Tool bu réy bii dey, soo ko bayee, Yàlla na ng...   \n",
       "506           Fas nañu ko yéene lëkkale ak anam googu.   \n",
       "507                              Yee téere la ma jóox.   \n",
       "508                     Mu ngi ci gall bi ci ndeyjoor.   \n",
       "509     Nanga ma jéndal gom, damay fóot sama mbubb mi.   \n",
       "510  Kawlax bare woon na béntéñe, waaye gor nañu le...   \n",
       "511     Ay xalaat yu, su ñu leen boolantee, dina baax.   \n",
       "512  Miir yeek taaxi kow yi ňu tiim day nañ, xanaa,...   \n",
       "513  Ci fànweeri atam la Baay jël gaal, jóge Saawus...   \n",
       "514  Ni ko waa Bansoo aadawoo, Buur baa ngi def yar...   \n",
       "515  Jigéen ñi tàmbali nañoo fecc, daanaka dañu ne ...   \n",
       "516  Biñ yeggee ci diggu joor gi, sori lool sunu kë...   \n",
       "517  Puuriti ginnax yaa ngi muur garabi àll bi, jax...   \n",
       "518  Teewul nataal yiy dekkali bànneex beek tawféex...   \n",
       "519  Ma daaneele xanaa móobalam ya. Soppul woon tab...   \n",
       "520  Dama doon jàpp, ndox mi ne carax dugg ; ndax l...   \n",
       "521  Nataal bi ma jot de gis naa ni gejje la, waaw ...   \n",
       "522  Lu naqari la mu ma doon def, di ma ko yéjje ak...   \n",
       "523  Ñu topp biddiw ba feq ba àgg ca bërëb ba liir ...   \n",
       "524  Ki la bëgg, mooy ki lay wax dëgg ; bokkul ak k...   \n",
       "525  Su dee mën na ñoo yëkkati kàddu yu rafet yooyu...   \n",
       "526  Gone, lawtanu yomb la ; boo walbatiwul, mu law...   \n",
       "527  Naka lanu def ba xam ko ci saa si ci lu amul b...   \n",
       "528  Sindax yaa nga ni noŋŋ ci kow miir yi, saa yu ...   \n",
       "529  Fi ñu nekk dafa kowe ba mu mel ni asamaan saa ...   \n",
       "530  Tugal soof ma. Ogosaa jommal ma, daanaka am ci...   \n",
       "531  Àddinay dox ba Baay tollu ci noppalug liggéey,...   \n",
       "532  Man mii, ni ma nekkee gone gu ndaw ba jóge fa ...   \n",
       "533  Aw faaraar wër booru nopp bi, mel ni'cercle' :...   \n",
       "534  Yàggul dara laata Baay di jóge Afrig, Sefiriis...   \n",
       "535  Li nga ma yónnee ba tey, li ma ci gis ay jëfan...   \n",
       "536  Moonte, sañumaa weddi ne ni Baay mujjee doon d...   \n",
       "537  Ak lii, di li gën a xëtt xolu Baay : ni ñuy se...   \n",
       "538  Dañ doon jékkee-jékki sànneeku nig fett, daw s...   \n",
       "539  Fowe naay nit ak i mala yu ñu yatte dàttu aloo...   \n",
       "540  Nataal bii ngeen ma yónnee gis naa ci biir nat...   \n",
       "541  Waaw lii de ab nataal la boog. Maa ngi gis ñaa...   \n",
       "542  Su weesoo ñaari goney Enjeŋ yi, du leneen lu B...   \n",
       "543  Ci weeru mars 1932 la samay way-jur jóge Fores...   \n",
       "544  Waaw lii de, dafa mel ni ab fulëer la, waaye f...   \n",
       "545  Looloo ko dal ba xare ba tàkkee ca Alseeri. Mo...   \n",
       "546  Ay caisse yu bari yuñ tegale la, caisse yi dañ...   \n",
       "547  Sama yaay ak Baay a nga jaaxaan ca seen néeg b...   \n",
       "548  Tàmbalee ko ci lekk gi. Lekk gu bon a bon – mb...   \n",
       "549  Sunu po mi, nii la daan tëdde : ci kow « taraa...   \n",
       "550  Waaw nataal bii tamit de gis naa ci benn bool ...   \n",
       "551  Bariwu fa ñu mas a teg bët Tubaab. Waaye mag ñ...   \n",
       "552  Lii duggukaay bu Luga la frontière bu Luga fii...   \n",
       "553  Amaana mu yaakaar, ni ñu bare ci jamono jooju,...   \n",
       "554  Soppiw askan yombul, rawati-na bu dee yow mi k...   \n",
       "555  Lii ab néeg la, néeg bi dañ kaa xadde ñax. Biñ...   \n",
       "556  Mayleen ma, ma sóorale ci nettali bi doomi niw...   \n",
       "557  Liw bu mettee-metti ba ma daan dugg ba nu dëkk...   \n",
       "558  Waaw mbokk mi, gis naa ni lii de ay potu Nesca...   \n",
       "559  Ñàkkul su ma ko jeexoon ci xeetu nataal yu sii...   \n",
       "560  Daan na yónnee léeg-léeg ay bataaxal, di ci jo...   \n",
       "561  Joor gaa ngi mel ni amul àpp, ma seetlu ay gar...   \n",
       "562  Lu yàgg déggu ñu, déggunu ko, muy wéy di liggé...   \n",
       "563  Waaye li ma ëppal solo ci nataal bi, mooy ñaar...   \n",
       "564  Xamul woon waxtaan ba déggoo ciy kàddu. Déedée...   \n",
       "565  Nataal bii nag moom benn pañe la. Lu ëpp liñ k...   \n",
       "566  Ma seetlu ci mbooloo mi benn jigéen ju màgget....   \n",
       "567  Gone gu naqadi deret laa woon. Saa yu ma bëgga...   \n",
       "568  Nataal bii maa ngi ciy gis ku yor raaya Senega...   \n",
       "569  Yaakaar naa ni Ogosaa lanu xame ni Peer Nowel ...   \n",
       "570  Lii ak kër. Kër gi nag ni lu bindoo tegale gi ...   \n",
       "571  Dafa fa yegg rekk, luye gaal ànd ak fajkat ya ...   \n",
       "572  Nataal bii mu ngi nekk ak nit, nit ku xees, ku...   \n",
       "573  Ni réewi Tubaab yi sàccee alalu Afrig ba desee...   \n",
       "574  Yaakaar naa ne ba ma nee cëpp Niseryaa – nu ja...   \n",
       "575  Ca bëj-gànnaar ak penku, mënoon naa séen i jàn...   \n",
       "576  Waa lii, ci wanag lay nekk. Ndee lu weex lii d...   \n",
       "577  Moonte loolu taxul ñu mën ni yoon wi ñépp daan...   \n",
       "578  Xanaa liñ fi tudde woon Xare bu Mag bi. Sunu k...   \n",
       "579  Bi sama yaay dellusee – xéy-na ndaje mi dalul ...   \n",
       "580  Li koy yëngal noonu, xam naa ko. Lan moo ko dà...   \n",
       "581  Walla ma ni boog loolu la doon gént, laata xar...   \n",
       "582  Walla boog mu mas maa may ci jaloorey ponkali ...   \n",
       "583  Xam naa damaa waroon a yuuxu keroog ndax sama ...   \n",
       "584  Ñu ngi dëkkoon ci benn néegu-ňax. Baatu “néegu...   \n",
       "\n",
       "                                          translations  \\\n",
       "0                Achète du poisson séché de Casamance.   \n",
       "1                                   Il a de gros yeux.   \n",
       "2                                            Il tonne.   \n",
       "3                                 Ce mouton en entier.   \n",
       "4                            C'est la première maison.   \n",
       "5                                      Afin que parte?   \n",
       "6                                      Paix seulement.   \n",
       "7                                            Où es-tu?   \n",
       "8                                   De quelle manière?   \n",
       "9                             Toi tu connais personne.   \n",
       "10                      Je travaille encore à ce jour.   \n",
       "11                 Cela fait partie de leur tradition.   \n",
       "12                                 Combien les trente?   \n",
       "13                                          Qui a été?   \n",
       "14                          Les deux fêtes coïncident.   \n",
       "15                                     Dieu, l'unique.   \n",
       "16                               Où est la récompense?   \n",
       "17                                      Lequel dis-tu?   \n",
       "18                                      Tel point que.   \n",
       "19                                    Un autre, tu es.   \n",
       "20                                            Il joue.   \n",
       "21                              J'ai aperçu un mouton.   \n",
       "22                              Comment va ta famille?   \n",
       "23                                  Ce n'est personne.   \n",
       "24                                  Tu as vu celui-ci?   \n",
       "25                                           Vas t'en!   \n",
       "26                                      Es-tu en paix?   \n",
       "27                             Quel moutons est égaré?   \n",
       "28                              Du moment que tu pars.   \n",
       "29                            Au moment où tu partais.   \n",
       "30                                Advienne que pourra!   \n",
       "31                 Maintenant il a des cheveux blancs.   \n",
       "32                                 J'ai des ganglions.   \n",
       "33                               Nous sommes fatigués.   \n",
       "34                                    Je suis en paix.   \n",
       "35                            Un commerçant grossiste.   \n",
       "36                                   Ce que tu as dit.   \n",
       "37                             Fouille toute la place!   \n",
       "38                              C'est lui qui parlait.   \n",
       "39                                Ils sont semblables.   \n",
       "40                              Comment t'appelles-tu?   \n",
       "41                                   Combien ça coute?   \n",
       "42                               Il a quitté le monde.   \n",
       "43                                     Comment vas-tu?   \n",
       "44                           Je ne vous eusse pas vus.   \n",
       "45                              Converser avec lequel?   \n",
       "46                                     C'est celui-là.   \n",
       "47                                        Vous causez?   \n",
       "48                             Les femmes accepteront.   \n",
       "49                             Quelle en est la forme?   \n",
       "50                                 Jeune homme, viens!   \n",
       "51                                  Donne un à chacun!   \n",
       "52                                           Mes amis!   \n",
       "53                                          S'il part.   \n",
       "54                        Homme et lion ne cohabitent.   \n",
       "55                             Quel moutons est égaré?   \n",
       "56                                Le chien est blessé.   \n",
       "57                                  Combien êtes-vous?   \n",
       "58                                       Góor giy dem.   \n",
       "59                           Cherche une autre maison!   \n",
       "60                                  C'est un bûcheron.   \n",
       "61                                La poche est trouée.   \n",
       "62                                            En bref.   \n",
       "63                               Je parle de ce petit.   \n",
       "64                                    Parles-tu Wolof?   \n",
       "65                               Rappelle-moi le jour!   \n",
       "66                                            Le mien.   \n",
       "67                            Quel mouton s'est égaré?   \n",
       "68                                    Où est celui-là?   \n",
       "69                                     De toute façon.   \n",
       "70                       Attends la belle jeune fille.   \n",
       "71                                     Il est hautain.   \n",
       "72                            La caisse est renversée.   \n",
       "73                                         Voici Omar.   \n",
       "74                                     Il travaillait.   \n",
       "75                               Fouille tout endroit!   \n",
       "76                                          Où est-ce?   \n",
       "77                            Je parle un peu anglais.   \n",
       "78                                       Nous partons.   \n",
       "79                                       Vas-y demain.   \n",
       "80                              J'ai aperçu un cheval.   \n",
       "81                                       C'est là-bas.   \n",
       "82                                L'homme serait mort.   \n",
       "83                                      C'est mon ami!   \n",
       "84                              Quand l'homme viendra.   \n",
       "85                                    Ils sont là-bas.   \n",
       "86                                       Qui est venu?   \n",
       "87                           L'homme ne cultivera pas.   \n",
       "88                                         Lui, aussi!   \n",
       "89                             Il finira par les vers.   \n",
       "90                        Il est assis et reste calme.   \n",
       "91                            La soirée se passe bien.   \n",
       "92                                          Il est là.   \n",
       "93                               Le voilà et le voilà!   \n",
       "94                                             J'irai.   \n",
       "95                                       C'est unique.   \n",
       "96                                 Je te cède ma part.   \n",
       "97                         Ne secoue pas la natte ici.   \n",
       "98                             Je ne parle que de toi.   \n",
       "99                                   Il n'est pas ici.   \n",
       "100                                 C'est à ta gauche.   \n",
       "101                          Les enfants sont arrivés.   \n",
       "102                                          Tiens ça!   \n",
       "103                                     entre ou sors!   \n",
       "104                              Arrache ces dents-là!   \n",
       "105                                     Je suis parti.   \n",
       "106                       Comment se passes la soirée?   \n",
       "107                               Fais-moi un acompte!   \n",
       "108                                    C'est un autre?   \n",
       "109                                  Ce que tu as dit.   \n",
       "110                                        Où veut-il.   \n",
       "111                           Un lion dans la chambre!   \n",
       "112                                     Il est par-là!   \n",
       "113                           Quels amis sont arrivés?   \n",
       "114                             C'est comment ton nom?   \n",
       "115                                 Le scorpion pique.   \n",
       "116                     Viens m'accompagner au marché.   \n",
       "117                                 Je les rejoindrai.   \n",
       "118                                 Si tu es un homme.   \n",
       "119                                 L'homme avait été.   \n",
       "120                                      Il est parti.   \n",
       "121                          Vous ne seriez pas venus.   \n",
       "122                                           Comment?   \n",
       "123                                   C'est un boabab.   \n",
       "124                                      Il en manque.   \n",
       "125                                Dis-moi tes désirs!   \n",
       "126                                  Il était boucher.   \n",
       "127                                   Là où tu as été.   \n",
       "128    Ce sont les vagues qui ont renversé la pirogue.   \n",
       "129                           Tu empèseras mon boubou.   \n",
       "130                       Partout où tu vas, il y est.   \n",
       "131                            Qu'est-ce qui t'arrive?   \n",
       "132                             J'ai acheté le mouton.   \n",
       "133                Mais, même moi, je ne viendrai pas!   \n",
       "134                    On perçoit les salaires demain.   \n",
       "135                       Ne passez pas par ce chemin!   \n",
       "136                    Je viens faire le bain lustral.   \n",
       "137            Il a donné à la personne quelque chose.   \n",
       "138                                        Une mesure.   \n",
       "139                   Mon collier de perles est cassé.   \n",
       "140                   Le chef du village s'est éteint.   \n",
       "141                Le vin de palme frais n'enivre pas.   \n",
       "142                        C'est un commerçant en vue.   \n",
       "143                  Le bruit retentit dans mon crâne.   \n",
       "144                      Je renforce ce que tu as dit.   \n",
       "145                             Le lait n'est pas pur.   \n",
       "146               Il lui a amené du fourrage au champ.   \n",
       "147                          Donc, tu ne partiras pas?   \n",
       "148                      La chaleur du feu l'a envahi.   \n",
       "149                Cet homme est Laobe de Saint-Louis.   \n",
       "150                 Écris, de peur de ne rien oublier.   \n",
       "151                         J'ai mal dans tout le dos.   \n",
       "152                             Voilà qu'il est parti.   \n",
       "153             Je parle de cet autre homme que voilà!   \n",
       "154                        Vous parlez de quelle dame?   \n",
       "155                   Ne fais pas durer la discussion!   \n",
       "156                              Je n'irai nulle part.   \n",
       "157                               Je t'ai vu avec lui!   \n",
       "158                       L'alcool les rendait braves.   \n",
       "159                      J'espère que nul n'est sorti?   \n",
       "160             Il est assis tout à son aise au salon.   \n",
       "161                            Il ne dit rien de cela.   \n",
       "162                   L'homme et l'enfant sont partis.   \n",
       "163                    Toi que voilà, tu n'as pas été.   \n",
       "164                    ces villages d'Afrique anémiés.   \n",
       "165              Il a attaché ses ceintures talismans.   \n",
       "166             Il est aussi grand qu'Omar maintenant.   \n",
       "167                  Ça vaut la peine d'aller voir ça.   \n",
       "168                                  Quand l'as-tu vu?   \n",
       "169                         Il est parti le lendemain.   \n",
       "170                    C'est Abdou qui nous satisfait.   \n",
       "171                       Avez-vous entendu la cloche?   \n",
       "172                   Il ne sait pas garder un secret.   \n",
       "173                    Demande d'abord l'autorisation.   \n",
       "174                          Content de te rencontrer.   \n",
       "175       Ce sont eux qui leur portaient leurs malles.   \n",
       "176                Comment vont les gens de la maison?   \n",
       "177            Cet homme qui est Laobe de Saint-Louis.   \n",
       "178                 Tu sauras que tu as affaire à moi.   \n",
       "179               Allez jouer au football dans la rue!   \n",
       "180                       Où est la clé de la chambre?   \n",
       "181             Je n'ai pas de magasin en particulier.   \n",
       "182                      C'est un enfant qui est bien.   \n",
       "183                       As-tu passé la nuit en paix?   \n",
       "184               Ça commence à être hors de contrôle.   \n",
       "185       Jésus a ressuscité quelqu'un qui était mort.   \n",
       "186                         L'homme qui devait partir.   \n",
       "187                         Je n'en ai aucun souvenir.   \n",
       "188                       Rappelle Moussa qui s'en va.   \n",
       "189                     Il est bel et bien au courant.   \n",
       "190                          Il l'a bel et bien senti.   \n",
       "191             Elle est assise fermement et t'attend.   \n",
       "192                   Une natte tissée avec du roseau.   \n",
       "193                      Ça peut contenir deux litres.   \n",
       "194                      Ta tête est pleine de lentes.   \n",
       "195                                   Il l'a répudiée.   \n",
       "196                  Il a pris son emploi à la légère.   \n",
       "197                        Quel autre cheval as-tu vu?   \n",
       "198                           Mets la bague au majeur!   \n",
       "199                            Va que l'homme te voie!   \n",
       "200                             Si je te prends, gare!   \n",
       "201                Le pet du dromadaire va en montant.   \n",
       "202                        C'est un homme qui fut bon!   \n",
       "203                             Lequel n'as-tu pas vu?   \n",
       "204             Homme de courage n'abonde pas paroles.   \n",
       "205        Ces gens qui sont sortis sont des lutteurs.   \n",
       "206               Ta mère dit qu'elle viendra ce soir.   \n",
       "207                          Tu as été et il est venu.   \n",
       "208                      C'est toi qui eusses été élu.   \n",
       "209                     Le prétendant n'y est pas roi.   \n",
       "210                          L'homme a parlé avec toi.   \n",
       "211                      Celui-là n'est pas venu seul.   \n",
       "212                       Dis-lui qu'il ne vienne pas.   \n",
       "213                  Il y a une banane dans le panier.   \n",
       "214                     L'autre, celui qui est arrivé!   \n",
       "215                  Ne traîne pas l'enfant par terre!   \n",
       "216           Les chèvres ont été gardées en sécurité.   \n",
       "217                       Aujourd'hui, tu es chanceux.   \n",
       "218                    C'est un guitariste talentueux.   \n",
       "219                          Tu aggraves la situation.   \n",
       "220                         Du fait qu'il doit partir.   \n",
       "221                Il y a un changement dans la ville.   \n",
       "222                   Terre-mère au ventre gorgé d'or.   \n",
       "223                   Il ne t'avait pas remis la taxe.   \n",
       "224                     Beaucoup de vieux sont voûtés.   \n",
       "225                             Il n'a pas encore été.   \n",
       "226            Il se réchauffe au soleil dans la cour.   \n",
       "227                        Franchement c'est un brave.   \n",
       "228                       La loi l'a reconnu coupable.   \n",
       "229              Rassemble les petits tas d'arachides.   \n",
       "230                                Voilà ce qu'il est.   \n",
       "231                             L'homme qui s'en alla.   \n",
       "232                               Moi je n'ai pas été.   \n",
       "233              Donnes le livre à ce fils bien élevé!   \n",
       "234                    Dis-lui comment ça s'est passé.   \n",
       "235                       Fils de la tante paternelle.   \n",
       "236                  Ceux-ci ne partent peut-être pas!   \n",
       "237                              Il ne t'avait pas vu.   \n",
       "238                 Il viendra que tu veuilles ou non!   \n",
       "239                       Il le battait matin et soir.   \n",
       "240                                C'est moi qui pars.   \n",
       "241                            La vie est dure là-bas.   \n",
       "242               Omar élimine les parasites des yeux.   \n",
       "243                         Je n'ai jamais été là-bas.   \n",
       "244                      Va jusqu'à ce qu'il revienne.   \n",
       "245                                J'ai été quelqu'un!   \n",
       "246                            Celui-là à l'intérieur.   \n",
       "247        Il lui a enfoncé un couteau dans le ventre.   \n",
       "248                      Couvant des braises ardentes.   \n",
       "249          Homme de courage n'abonde pas en paroles.   \n",
       "250              Plie-le pour que cela devienne épais.   \n",
       "251                           Vous avez été quelqu'un!   \n",
       "252                          Ce boeuf-là, en totalité.   \n",
       "253                 J'espère que personne n'est parti.   \n",
       "254                     C'est moi que l'homme appelle?   \n",
       "255                      Les briques ne suffisent pas.   \n",
       "256  J'ai pris la résolution d'aller à Kaolack demain.   \n",
       "257           Ton feu est bien pris ; viens commencer.   \n",
       "258                C'est une autre épouse que je veux!   \n",
       "259       En contrebas du chemin de Bamako à Aroundou.   \n",
       "260  Nous avions découvert d'autres compagnons de j...   \n",
       "261                C'est d'autres chevaux que je veux!   \n",
       "262           Mets-toi n'importe où, où il fait clair!   \n",
       "263                  C'est d'autres gens que j'ai vus!   \n",
       "264                C'est toi qui aimes la jeune femme.   \n",
       "265              S'exiler pour s'éloigner de l'acacia.   \n",
       "266              C'est Omar qui a hérité de la maison.   \n",
       "267               Tu es paresseuse comme cet autre-là.   \n",
       "268         Je ne l'ai pas vu, j'avais fermé les yeux.   \n",
       "269  Si tu éduques ton enfant, il ne te fera pas ho...   \n",
       "270                 Il voyagera aujourd'hui ou demain.   \n",
       "271      J'ai failli être dans l'obligation d'y aller.   \n",
       "272     Ce n'est pas chaque jour que père tue un lion.   \n",
       "273           L'homme a frappé la vache avec un bâton.   \n",
       "274         La bombe n'avait aucune chance d'exploser.   \n",
       "275  Elle cachait sa grossesse, mais maintenant c'e...   \n",
       "276          La France a battu le Brésil trois à zéro.   \n",
       "277          Le moment venu, rapidement, il s'en alla.   \n",
       "278                Avoir pu aller jusqu'à Saint-Louis.   \n",
       "279  Pose la calebasse sur l'eau, elle ne coulera pas.   \n",
       "280     Si tu dois le faire, que ce soit de plein gré.   \n",
       "281        On lui a jeté un sort pour le sédentariser.   \n",
       "282  J'ai vu cet homme qui parlait et qui était ass...   \n",
       "283                  Humant l'odeur fétide de la boue.   \n",
       "284                  Mets-y des feuilles de Tinospora!   \n",
       "285  Cet homme près de moi et celui là près de toi ...   \n",
       "286                   C'est cet homme qui eût été bon!   \n",
       "287  Le fait est que tu n'as pas encore rencontré d...   \n",
       "288          Il s'en va d'un pas lourd et disgracieux.   \n",
       "289           Il a beaucoup d'égards pour ses voisins.   \n",
       "290                 Quand il arriva, tous se levèrent.   \n",
       "291           En fait, le monde a changé de nos jours.   \n",
       "292  Je ne peux pas m'asseoir longtemps sur mes jam...   \n",
       "293     Il a reporté le baptême à une date ultérieure.   \n",
       "294                  C'est que j'ai effectivement été.   \n",
       "295  Le sol détrempé par la pluie sera facile à cre...   \n",
       "296         Si tu me frappes, je me vengerai bien sûr.   \n",
       "297       Que tu partes, c'est ce que ceux-là veulent.   \n",
       "298          C'est de l'homme que parlent les enfants.   \n",
       "299                  C'est l'homme qui part, je crois!   \n",
       "300                Je n'en ai vu qu'un pour l'instant.   \n",
       "301        Appuie sur le bouton pour que le pus sorte!   \n",
       "302          Il a tellement bu qu'il est devenu blême.   \n",
       "303        Bientôt on va devoir commencer à désherber.   \n",
       "304            Il l'a renversé et s'est assis sur lui.   \n",
       "305           Tu frotteras l'intérieur et l'extérieur.   \n",
       "306         Homme, lion, boeuf... allaient de concert.   \n",
       "307    Si tu étais venu, dans ce cas, je serais parti!   \n",
       "308        Elle est divorcée ; elle est chez son père.   \n",
       "309         Va le voir, tes besoins seront satisfaits.   \n",
       "310         Il y a des gens qui mangent les branchies.   \n",
       "311               Des souris fouillent tes entrailles.   \n",
       "312                J'ai aperçu une chose, une portion.   \n",
       "313  Une somme d'argent considérable a été alloué a...   \n",
       "314              Le riz est fermenté, ne le mange pas.   \n",
       "315                  Tu n'avais pas été à l'intérieur.   \n",
       "316     Si tu le vois, il est bien épanoui maintenant.   \n",
       "317            Je l'ai trouvé qui marchandait un veau.   \n",
       "318         On doit le confier à une personne honnête.   \n",
       "319       Il y a trop de laisser-aller dans la maison.   \n",
       "320                   Je prépare du baase aujourd'hui.   \n",
       "321             La réunion se tiendra samedi prochain.   \n",
       "322                  Sur les rives de tes cours d'eau.   \n",
       "323    Nous ne nous sommes pas revus depuis les noces.   \n",
       "324    Une culbute ne te débarrassera pas de tes poux.   \n",
       "325  Plions bagage avant que la pluie ne nous trouv...   \n",
       "326                          Je l'ai trouvé en larmes.   \n",
       "327               Il bat froid à son ami ces temps-ci.   \n",
       "328    C'est par ta négligence que l'enfant est tombé.   \n",
       "329                  J'ai déjà vu tous ces moutons-ci.   \n",
       "330                   C'est l'étincelle qui l'a brûlé.   \n",
       "331                 Personne ne paye ; entre qui veut.   \n",
       "332      Que l'homme parte et que les enfants entrent!   \n",
       "333                   Eh bien, est-ce qu'il est parti?   \n",
       "334  Ce champ, seul quelqu'un de fort peut le culti...   \n",
       "335  Nous dont tu as fait tes enfants, ne nous oubl...   \n",
       "336         C'est son amour d'antan qui survit encore.   \n",
       "337  Nous sommes entrés même si les portes étaient ...   \n",
       "338                Mais personne ne nous accompagnait.   \n",
       "339                       Qu'est-ce, qui est la cause?   \n",
       "340             Sur ces entrefaites, moi, je quittais.   \n",
       "341  Il avala le lait d'un trait et attendit la suite.   \n",
       "342                   Qui demandait Fali, voici Ngoye.   \n",
       "343           Une union où il y a une grande maturité.   \n",
       "344   Attends qu'il fasse son rot avant de le coucher.   \n",
       "345                  où femmes et hommes même couchés.   \n",
       "346  Habituellement, il ne met pas de sucre dans so...   \n",
       "347               Alors, il soupira puis rendit l'âme.   \n",
       "348                      C'est qu'il veut de l'argent.   \n",
       "349  Son grand-père faisait partie des autorités en...   \n",
       "350     La réunion aura lieu sur le grand espace vide.   \n",
       "351                 C'est le corbeau qui l'a réveillé.   \n",
       "352        Qui a coupé un morceau du pain de l'enfant?   \n",
       "353          Quand le froid arrive, mes pieds gercent.   \n",
       "354    Le cuir dont il est fait est une peau de varan.   \n",
       "355   Je m'en vais faucher de l'herbe pour mon cheval.   \n",
       "356      Il n'a pas encore apporté les vivres du mois.   \n",
       "357          Apporte les fléaux ; les gens sont venus.   \n",
       "358           L'homme le voici, il est Saint-Louisien.   \n",
       "359     Le fruit du baobab ne sert que s'il est cassé.   \n",
       "360       S'exiler pour réaliser les rêves de ta mère.   \n",
       "361            Tu veux me jeter de la poudre aux yeux.   \n",
       "362       Les gens l'entourent et il parle sans arrêt.   \n",
       "363                Je crois qu'il viendra aujourd'hui!   \n",
       "364                Ce qu'il m'a donné c'est le double.   \n",
       "365  Il lui a défendu de dire ce mot mais il le redit.   \n",
       "366    Je voudrais bien couvrir la toiture de ma case.   \n",
       "367                        C'est moi qui n'ai pas été.   \n",
       "368               Quand une chose a lieu, on en parle.   \n",
       "369                Est-ce que le pays a des richesses?   \n",
       "370          Surveille le lait de peur qu'il ne monte!   \n",
       "371  Ce sont des coupeurs de route qui l'ont assass...   \n",
       "372              Le chimpanzé lui a arraché sa banane.   \n",
       "373      C'est à cause de la turbulence de son enfant.   \n",
       "374             Je fuis la promiscuité, il est bavard.   \n",
       "375            L'étranger ne dort pas à poings fermés.   \n",
       "376                                 Ils sont amoureux.   \n",
       "377          Je m'apprête à faire le troisième binage.   \n",
       "378               Va asperger le riz, il manque d'eau!   \n",
       "379                        Tourne le fourneau au vent!   \n",
       "380          Il a régurgité deux fois, il a trop tété.   \n",
       "381                         Il n'en a pas vu d'autres.   \n",
       "382    Je ne sais pas à quoi cela ressemble pour vous.   \n",
       "383  Tous les automobilistes se plaignent des route...   \n",
       "384  C'est quand on l'a nommé ministre qu'il s'est ...   \n",
       "385  Rien qu'à sentir les mangues, on sait qu'elles...   \n",
       "386  Chasse ces mouches qui n'arrêtent pas de voler...   \n",
       "387  Tout ce qu'elle ramasse, elle l'émiette et le ...   \n",
       "388  C'est cette année que les enfants vont se fair...   \n",
       "389      C'est le fait de rester la nuit qui m'ennuie.   \n",
       "390           Un tel besoin de me mesurer, de dominer.   \n",
       "391  Maintenant, c'est toi qui seras chargé de la j...   \n",
       "392  De deux choses l'une : ou la chèvre est avec l...   \n",
       "393  L'Afrique à la fois sauvage et très humaine es...   \n",
       "394  J'en ai vu de toutes les couleurs avant d'en a...   \n",
       "395                   De l'arc-en-ciel sur la verdure.   \n",
       "396  Un jeune homme dans la force de l'âge, d'une b...   \n",
       "397  C'est parce qu'il passait son temps à le battr...   \n",
       "398  Quand nous entendrons ton rugissement, la part...   \n",
       "399  Tu es malhonnête, c'est pour cela que tu n'as ...   \n",
       "400  Aujourd'hui, il faut absolument que j'aille vo...   \n",
       "401     C'était l'an dernier au jour de l'an musulman.   \n",
       "402  Tu ne pouvais pas acheter du lait, au lieu de ...   \n",
       "403  C'est avec du fil de fer qu'on a attaché la pa...   \n",
       "404  Il n'est pas resté ici, il n'a fait qu'un pass...   \n",
       "405  La langue est mauvaise pour le bout de bois, à...   \n",
       "406  Une bande de cheveux au milieu de la tête : ce...   \n",
       "407  Chaque fois que je me réveille, j'ai les yeux ...   \n",
       "408  Il était tellement excité et débordant de joie...   \n",
       "409  Celui qui l'a assassiné doit être l'un de ceux...   \n",
       "410  Aucun de tes neveux n'est aussi paresseux que ...   \n",
       "411  La mémoire des espérances et des angoisses de ...   \n",
       "412  Il t'achètera une voiture quand les poules aur...   \n",
       "413  Le manque d'information provoque la honte et l...   \n",
       "414  C'est de cela qu'il était question à cette réu...   \n",
       "415  La colonisation européenne en fin de compte a ...   \n",
       "416  L'oeil ne porte pas mais il sait ce que la têt...   \n",
       "417  La mouche n'a cure de savoir si l'abeille est ...   \n",
       "418  Est-ce que je peux me servir de la machine si ...   \n",
       "419  Le pilage est pénible ; porte le mil à la mach...   \n",
       "420  La présence de celui-là même ne justifie pas q...   \n",
       "421  Si tu es étranger dans un pays, tes papiers do...   \n",
       "422      C'est l'école des enfants handicapés mentaux.   \n",
       "423   Ñaral ce que tu as écrit, tu ne l'as pas appris.   \n",
       "424  Personne ne se soucie de rien, chacun fait ce ...   \n",
       "425  C'est à cause de ce que tu lui as dit qu'il bo...   \n",
       "426  Peut-être qu'il atteint le fort Mac-Mahon, à E...   \n",
       "427  Je n'ai pas de frères et soeurs utérins, cousi...   \n",
       "428  Ce n'est pas bien de te réchauffer au soleil a...   \n",
       "429  Au Sud, la pente conduisait à un affluent de l...   \n",
       "430  Il ne veut pas qu'on lui fasse le moindre repr...   \n",
       "431  Seigneur, je ne te quitterai pas jusqu'à l'éte...   \n",
       "432  Pourquoi traîne-t-il par terre comme s'il ne p...   \n",
       "433    Griot, réponds-moi, tambour-major, réponds-moi!   \n",
       "434  La machine donne une farine grossière, alors t...   \n",
       "435  Toi, passe, je ne veux qu'être préservé de toi...   \n",
       "436  Il est si charitable qu'il prend en charge tou...   \n",
       "437  Tout le jour le soleil a brûlé leur corps, ils...   \n",
       "438  Attendre que le serpent soit parti pour se met...   \n",
       "439  Ses propos ont été brefs ; lui aurais-tu fait ...   \n",
       "440     Si tu connaissais l'espièglerie de cet enfant!   \n",
       "441  Pour lui, isolé dans la brousse, l'Afrique est...   \n",
       "442  Je suis en ce temps-là très loin des adjectifs...   \n",
       "443  Pendant des années, je crois que je ne l'ai ja...   \n",
       "444  Mon père et ma mère y ressentent une liberté q...   \n",
       "445   Peut-être l'homme a-t-il dit que c'est celui-là!   \n",
       "446  Un francolin ne peut pas en semer un autre dan...   \n",
       "447  Je n'avais jamais imaginé goûter à une telle i...   \n",
       "448      C'est à Diakhaw qu'il a fait les funérailles.   \n",
       "449  C'est un sous-vêtement en mailles qu'il faut p...   \n",
       "450  Je ne parlerai pas de l'histoire de la ville c...   \n",
       "451  Le jour suivant, les ouvrières avaient colmaté...   \n",
       "452  Le pays est troublé par les guerres tribales, ...   \n",
       "453  Laisse les poissons, tu vas prendre une arête ...   \n",
       "454  Ils marchèrent d'un pas décidé, se dirigeant v...   \n",
       "455     Nos liens de parenté s'arrêtent à Adam et Ève.   \n",
       "456  Ce n'est pas la première fois dans le pays que...   \n",
       "457  S'il t'arrive encore d'avoir beaucoup de trava...   \n",
       "458  En l'affublant de lorgnons, je justifiais mon ...   \n",
       "459  Quand on est enfant, on n'use pas de mots (et ...   \n",
       "460  Quand on fait quelque chose, il est normal d'e...   \n",
       "461  Et, avant même l'instant de la conception, tou...   \n",
       "462       C'était ceux-là même jusqu'à ce qu'il parte.   \n",
       "463  Je l'ai mis en garde ; de guerre lasse, je l'a...   \n",
       "464  S'il ne tenait qu'à nous, certaines choses ne ...   \n",
       "465        On dit que cette dame est une « sorcière ».   \n",
       "466  C'est à Bamenda que mon père emmène ma mère ap...   \n",
       "467  L'homme de condition modeste qui craint le sol...   \n",
       "468  Celui qui a un morceau de noix n'a cure de cel...   \n",
       "469     C'était une règle de vie, un code de conduite.   \n",
       "470  Je lui ai fait une prise de ceinture, nous som...   \n",
       "471  Dès lors qu'une chose est commise, elle prête ...   \n",
       "472  Il dit que tant qu'il n'aura pas d'emploi, il ...   \n",
       "473  Je veux que tu viennes, qu'il refuse de partir...   \n",
       "474  Je veux t'assurer de ma venue, c'est pour cett...   \n",
       "475  Je te mets en garde : ne t'en va pas avant mon...   \n",
       "476  C'est indécent pour un adulte d'uriner debout ...   \n",
       "477     C'est ainsi que je le vois à la fin de sa vie.   \n",
       "478  Quand tu avais un bu-tuut, tu pouvais acheter ...   \n",
       "479  Quand la poussière monte, beaucoup d'yeux en p...   \n",
       "480  Il le chatouille avec l'éperon, le cheval saut...   \n",
       "481  Aujourd'hui, j'ai une forte envie de côtelette...   \n",
       "482  Un taux élevé d'albumine peut provoquer une en...   \n",
       "483       Ce qui me gêne en lui, c'est qu'il emprunte.   \n",
       "484  Que le turban du marabout m'apporte un peu de ...   \n",
       "485  L'oesophage ne remercie pas les mets de l'an d...   \n",
       "486  Il a de l'amour-propre ; si son père le sermon...   \n",
       "487  La dévaluation revient assurément à nous achever.   \n",
       "488  C'est la bénédiction qui différencie l'eau du ...   \n",
       "489  Lave le récipient réservé à la préparation du ...   \n",
       "490  Griffer l'eau, c'est ta poitrine qui en pâtira...   \n",
       "491  Pour moi, ces objets, ces bois sculptés et ces...   \n",
       "492  Elle porte un nom d'homme car on lui a donné l...   \n",
       "493  S'il y a une bonne entente entre vous, tout ma...   \n",
       "494  Je me souviens de chacun d'eux, de leurs noms,...   \n",
       "495     Je pars à l'aube ; il faudra te réveiller tôt.   \n",
       "496  Je reconnais l'un des jumeaux à la scarificati...   \n",
       "497  À la source de mes sentiments et de mes déterm...   \n",
       "498          Il y a un essaim d'abeilles dans l'arbre.   \n",
       "499  Si le semis est dense, cela empêche une croiss...   \n",
       "500  Celui qui va jusqu'à ne plus savoir où il va, ...   \n",
       "501  Des boeufs que tu vois, celui-là tout près est...   \n",
       "502            C'est le pet de l'âne qui l'a réveillé.   \n",
       "503  Le lapin déboucha et, à l'instant-même, je déc...   \n",
       "504  Le colostrum épargne au nourrisson une constit...   \n",
       "505  Ce grand champ, si tu le cultives, puisses-tu ...   \n",
       "506  Nous avons la volonté de le faire coïncider av...   \n",
       "507              C'est ces livres-là qu'il m'a donnés.   \n",
       "508  C'est dans le sac qui est sur le flanc droit d...   \n",
       "509  Tu m'achèteras de l'amidon, je dois laver mon ...   \n",
       "510  Il y avait beaucoup de fromagers à Kaolack mai...   \n",
       "511  Des idées qui seraient bien si on les mettait ...   \n",
       "512  Ces murailles occupaient une superficie aussi ...   \n",
       "513  À l'âge de trente ans, mon père quitte Southam...   \n",
       "514  Selon la tradition, le roi est nu jusqu'à la c...   \n",
       "515  Les femmes ont commencé à danser, elles sont c...   \n",
       "516  Au milieu de la plaine, à une distance suffisa...   \n",
       "517  Les embruns apportés par le vent recouvrent le...   \n",
       "518  Malgré tout cela, à cause de tout cela, ces im...   \n",
       "519  Les meubles enfin, non pas ces fameux tabouret...   \n",
       "520  Je faisais mes ablutions et tout d'un coup, l'...   \n",
       "521  Sur la photo que j'ai eue a voir il y a assuré...   \n",
       "522  J'ai appelé ce village Njar-Meew à cause des c...   \n",
       "523  Ils suivirent l'étoile qui s'était levée et pa...   \n",
       "524  Celui qui t'aime, c'est celui qui te dit la vé...   \n",
       "525  La réputation de douceur des gens de la région...   \n",
       "526  L'enfant, c'est un plant de courge qui rampe ;...   \n",
       "527  Comment l'avons-nous su? Peut-être par mon pèr...   \n",
       "528  Aux murs, là où la lumière se reflétait, les m...   \n",
       "529  Ils sont si haut que le ciel brumeux semble s'...   \n",
       "530  Alors les jours d'Ogoja, contraitrement à ceux...   \n",
       "531  Puis j'ai découvert, lorsque mon père, à l'âge...   \n",
       "532  J'ai passé une grande partie de mon enfance et...   \n",
       "533  Une bande de cheveux le long des oreilles, fai...   \n",
       "534  Un peu avant le départ de mon père, Jeffries t...   \n",
       "535  Ce que tu m'as envoyé jusqu'à maintenant, la p...   \n",
       "536  Mais ces manières africaines qui étaient deven...   \n",
       "537  Et surtout l'abandon de l'Afrique à ses vieux ...   \n",
       "538  S'ils chapardaient, ce ne pouvait être que des...   \n",
       "539  J'ai joué avec les statues d'ébène, avec les s...   \n",
       "540  Sur la photo que vous m'avez envoyée j'y ai vu...   \n",
       "541  Cela est une photo. Je vois, dessus, deux femm...   \n",
       "542  De son séjour en Guyane, mon père ne rapporter...   \n",
       "543  À partir de mars 1932, mon père et ma mère qui...   \n",
       "544  Oui ça, il semble que ce soit une fleur, mais ...   \n",
       "545  Et la guerre qui venait d'éclater en Algérie, ...   \n",
       "546  Il y a beaucoup de caisses empilées, vraiment ...   \n",
       "547  Mon père et ma mère sont couchés dans leur lit...   \n",
       "548  La nourriture désastreuse – ce pain noir, dont...   \n",
       "549  Le jeu consistait, du haut du trapèze, à taqui...   \n",
       "550  Oui sur cette photo ci aussi, j'y ai vu un bol...   \n",
       "551  Les gens qui y vivent pour la plupart n'ont ja...   \n",
       "552  C'est l'entrée de Louga. C'est à la frontière ...   \n",
       "553  Peut-être espère-t-il, comme beaucoup de gens ...   \n",
       "554  Cette leçon, mon père l'a sans doute apprise d...   \n",
       "555  Ceci est une chambre couverte d'herbe. En le c...   \n",
       "556  Et le goût de la quinine dans la bouche, cette...   \n",
       "557  Je me souviens du froid de l'hiver, à Nice, ou...   \n",
       "558  Oui mon parent, je vois que ça c'est des pots ...   \n",
       "559  Image caractéristique de la Colonie : des voya...   \n",
       "560  Il envoyait des nouvelles de temps à autre, so...   \n",
       "561  La plaine, de chaque côté de la rivière, parai...   \n",
       "562  De longues années d'éloignement et de silence,...   \n",
       "563  Et sur une plage, où viennent mourir les vague...   \n",
       "564  Il était inflexible, autoritaire, en même temp...   \n",
       "565  Sur cette photo ci cependant, il y a un panier...   \n",
       "566  Parmi tous ceux qui se pressent autour de moi,...   \n",
       "567  Je me souviens aussi d'avoir été pris par des ...   \n",
       "568  Sur cette photo ci, j'y vois quelqu'un portant...   \n",
       "569  Je suppose que c'est en arrivant à Ogoja que n...   \n",
       "570  Ceci est une maison. La maison est, cependant,...   \n",
       "571  Dès qu'il arrive, il affrète une pirogue munie...   \n",
       "572  Sur cette photo, c'est une personne de couleur...   \n",
       "573  Déjà, il avait perçu l'oubli tactique dans leq...   \n",
       "574  Je crois que dans les premières heures qui ont...   \n",
       "575  Vers le Nord et l'est, je pouvais voir la gran...   \n",
       "576  Ceci se trouve dans les toilettes.  cette chos...   \n",
       "577  Les jeux, les discussions et les menus travaux...   \n",
       "578  La guerre, le confinement dans l'appartement d...   \n",
       "579  Quand ma mère revient (peut-être vaguement inq...   \n",
       "580  Je peux ressentir l'émotion qu'il éprouve à tr...   \n",
       "581  C'était avant la guerre, avant la solitude et ...   \n",
       "582  Ou bien les récits de grands Blancs qui voyage...   \n",
       "583  Je ne m'en souviens pas, mais j'ai dû crier, h...   \n",
       "584  Dans la case que nous habitions (le mot case a...   \n",
       "\n",
       "                                           predictions  \n",
       "0                          Achète une autre chose de l  \n",
       "1                                 C'est cette femme qu  \n",
       "2                               On y a des choses qui   \n",
       "3                            Le mouton n'est pas celui  \n",
       "4                                C'est la maison de la  \n",
       "5                                  Afin que qui parte?  \n",
       "6                                    Je n'ai pas été à  \n",
       "7                                      Où est l'homme?  \n",
       "8                                                 Pai?  \n",
       "9                      Tois ne sont pas encore arrivée  \n",
       "10                                 Jusqu'à présent, il  \n",
       "11                                 C'est à l'intérieur  \n",
       "12               Les enfants sont des enfants qui sont  \n",
       "13                                      Qui est parti?  \n",
       "14                      Les femmes ont les deux mainss  \n",
       "15                            Dieu, nous avons fait le  \n",
       "16                            Où est le Monsieur de la  \n",
       "17                        Tu as dit cela qui tellement  \n",
       "18                             Depuis que tu ne peux t  \n",
       "19                            Tu as discuté avec qui n  \n",
       "20                            Il a fait une cravate de  \n",
       "21                            J'ai aperçu un mouton de  \n",
       "22                              Où sont les membres de  \n",
       "23                               Ce n'est pas le nuque  \n",
       "24                               Tu as vu celui-là que  \n",
       "25                                 Va voir le travail!  \n",
       "26                                      Es-tu en paix?  \n",
       "27                              Quel mouton en entier?  \n",
       "28                         Alors que tu partais pour l  \n",
       "29                         Alors que tu partais pour l  \n",
       "30                                 C'est à l'intérieur  \n",
       "31                          Maintenant l'air de la mer  \n",
       "32                            Je veux que tu ne peux t  \n",
       "33                                Nous avons faiton n'  \n",
       "34                                 C'est à l'intérieur  \n",
       "35                         Un commerçant détaillant du  \n",
       "36                                     Tu as dit cela.  \n",
       "37                                        Prends l'un!  \n",
       "38                                 C'est lui qui a dit  \n",
       "39                                 C'est à l'intérieur  \n",
       "40                                Comment s'est-ce que  \n",
       "41                              Combien pour te faire?  \n",
       "42                               Il a tué une placarde  \n",
       "43                                    Tu n'es pas de l  \n",
       "44                              Je n'ai pas de bagages  \n",
       "45                                 C'est à l'intérieur  \n",
       "46                                  C'est celui-là qui  \n",
       "47                                  Vous ne t'a pas de  \n",
       "48                                     C'est ce que j'  \n",
       "49                                Comment s'est-ce que  \n",
       "50                                 Quand tu vas, tu ne  \n",
       "51                              Donne-lui une vache de  \n",
       "52                                       Un ami à moi!  \n",
       "53                        Une fois en chemin est parti  \n",
       "54                    Homme et lion ne sont pas encore  \n",
       "55                           C'est cette petite mouton  \n",
       "56                              Les chiens le pri de l  \n",
       "57                                  Combien pour quoi?  \n",
       "58                                   L'homme qui part.  \n",
       "59                            Cherche une autre chose!  \n",
       "60                                  C'est une mauvaise  \n",
       "61                                    Le Est-ce que le  \n",
       "62                                 C'est à l'intérieur  \n",
       "63                          Je parle de cet enfant là-  \n",
       "64                                 Parles-tu Français?  \n",
       "65                             Mon Dieu-moi le lait de  \n",
       "66                                     C'est à moi qu'  \n",
       "67                                        Quel mouton?  \n",
       "68                             Où est le tambour de la  \n",
       "69                                     Avec ce qu'il a  \n",
       "70                                   C'est à moi que j  \n",
       "71                             Il a une bonne chose de  \n",
       "72                                        Le ége de l'  \n",
       "73                                 Voilà Omar aujourd'  \n",
       "74                         Il a une graisse de graisse  \n",
       "75                                        Prends l'un!  \n",
       "76                                          Où va-t-il  \n",
       "77                               J'ai aperçu une jeune  \n",
       "78                                    Ils sont partis.  \n",
       "79                               Ils sont partiss de l  \n",
       "80                               J'ai aperçu une jeune  \n",
       "81                               C'est une ceinture de  \n",
       "82                                     L'homme n'a pas  \n",
       "83                                     C'est une ami à  \n",
       "84                                 Si l'homme vient, l  \n",
       "85                         Ils se sont rassemblés dans  \n",
       "86                                        Qui a parlé?  \n",
       "87                                     L'homme n'a pas  \n",
       "88                                 C'est à l'intérieur  \n",
       "89                                     C'est à moi qu'  \n",
       "90                       Il porte son enfant qui a dit  \n",
       "91                                 C'est à l'intérieur  \n",
       "92                                Le voilà, là-bas que  \n",
       "93                             Tu aurais été le voilà!  \n",
       "94                                 C'est lui qui a été  \n",
       "95                       C'est une personne malhonnête  \n",
       "96                              Je t'aimes de mon père  \n",
       "97                         Ne me tourmente pas l'homme  \n",
       "98                            Je parle de toi-même que  \n",
       "99                                     Il n'a pas de l  \n",
       "100                           Il est gonflé de ton ami  \n",
       "101              Les enfants sont des enfants qui sont  \n",
       "102                               Dans ce n'est pas un  \n",
       "103                             Tu as péché le riz au-  \n",
       "104                             Il a mis à l'intérieur  \n",
       "105                                          J'ai été.  \n",
       "106                                    Comment va-ils?  \n",
       "107                                   Con-moi la paix!  \n",
       "108                             C'est un autre qui est  \n",
       "109                            Ce que tu as dit que tu  \n",
       "110                                 Où avait-il que tu  \n",
       "111                         Adresse-le dans la chambre  \n",
       "112                                  Le voilà, là, là,  \n",
       "113                                C'est à l'intérieur  \n",
       "114                               Comment s'est-ce que  \n",
       "115                                 C'est à cause de l  \n",
       "116                                    Il n'a pas de l  \n",
       "117                                 J'ai le singe de l  \n",
       "118                              Si tu ne veux pas, tu  \n",
       "119                      L'homme a donné quelque chose  \n",
       "120                             Il est parti il y a un  \n",
       "121                             Vous ne mangez pas, ne  \n",
       "122                                  C'est à partir-tu  \n",
       "123                               C'est une personne n  \n",
       "124                          Il a fait une bonne chose  \n",
       "125                                   Dis-moi de l'eau  \n",
       "126                      C'est une personne malhonnête  \n",
       "127                             Tu ne vas nulle que tu  \n",
       "128            C'est la voiture qui avaient appris à l  \n",
       "129              Tu n'es ni roi ni en brousse pour que  \n",
       "130              Alors, tu partes, tu ne dois rien, tu  \n",
       "131                          Qu'est-ce qui a dit qu'il  \n",
       "132                 J'ai acheté ce que tu ne te pas le  \n",
       "133                        Mais, moi, moi, je n'ai pas  \n",
       "134                     S'il n'était pas l'habitude de  \n",
       "135               Quand il ne mange pas à l'intérieur,  \n",
       "136                   C'est à moi que je veux ce qui a  \n",
       "137            Il a donné à l'homme qui a fait quelque  \n",
       "138           Ils ont deux enfants qui s'est en proche  \n",
       "139   Mon épouse est en train de la ceinture de la ...  \n",
       "140               Le veau a fait une ville de l'argent  \n",
       "141                  Je ne sais pas de ce qu'il ne pas  \n",
       "142                     C'est une ceinture de ce qu'il  \n",
       "143                Je Coows dans les encenss de la awu  \n",
       "144                      J'ai dit de ce qu'on parle de  \n",
       "145                 C'est à l'intérieur que le feu qui  \n",
       "146                  Il s'est arrêtée dans une chose à  \n",
       "147                   Tu ne partiras donc pas, je n'ai  \n",
       "148                       C'est ton ami qu'il a héberg  \n",
       "149             Cet homme qui a dit que c'est un Laobe  \n",
       "150                       C'est à cause de l'eau qui a  \n",
       "151                   Mon père a dit que l'homme n'est  \n",
       "152                              Le voilà qui s'en va.  \n",
       "153            Je parle de l'autre homme de cette dame  \n",
       "154              C'est à l'intérieur que vous ne parle  \n",
       "155                Ne peux-tu pas à l'intérieur que le  \n",
       "156                  C'est l'habitude de partir qui n'  \n",
       "157                   J'ai vu l'homme que toi avec lui  \n",
       "158               C'est la nuit qui avaient appris qu'  \n",
       "159                     Que personne n'est pas bon-il?  \n",
       "160                   Il dit que tu ne peux t'en a dit  \n",
       "161                Tu n'avais rien dit de l'autre côté  \n",
       "162                L'homme que l'homme que le faire de  \n",
       "163                          Te voilà, là-bas, là, là,  \n",
       "164                Tu le souvenir de l'Afrique qui l'a  \n",
       "165                      Il a des Gaws qu'il a des Gaw  \n",
       "166                        Il a tué une aujourd'hui de  \n",
       "167                             C'est là qu'il n'a pas  \n",
       "168                        Quand tu as vu le as-tu vu?  \n",
       "169            C'est l'autre endroit qui a été médecin  \n",
       "170                       Une deuxième, on a une bonne  \n",
       "171           C'est vous-ce que vous vous réconciliiez  \n",
       "172                       Il n'a pas de l'enfant qui a  \n",
       "173             Ils ont été tellement de la paix de la  \n",
       "174                     Je n'ai pas de l'autre bout de  \n",
       "175        Tu ne peux pas pour que les défaire de leur  \n",
       "176                      Comment se passe à la maison?  \n",
       "177             Cet homme qui a dit que c'est un Laobe  \n",
       "178            Tu as dit cette année de noir que tu ne  \n",
       "179                  Allez-moi la paix de la rue qu'il  \n",
       "180                 Où est le Monsieur à la chose à l'  \n",
       "181                      C'est à une ceinture de l'eau  \n",
       "182                     C'est un enfant qu'il est bon!  \n",
       "183            Es-tu passé la nuit fois que tu ne peux  \n",
       "184                                    Il est « « « «   \n",
       "185           Jésus a ressuscité un mort qu'il a cousu  \n",
       "186                                 L'homme qui a été.  \n",
       "187           Cela a dit que ce n'est pas encore mieux  \n",
       "188                    C'est la vieille de l'eau qui a  \n",
       "189                         Il l'a fait une chose à l'  \n",
       "190                         Il l'a fait une chose à l'  \n",
       "191          Il est debout sur le point de l'herbe qui  \n",
       "192                  C'est à l'extérieur qu'on appelle  \n",
       "193   Il peut le faire aussi deux deux fois en fair...  \n",
       "194              Ton père a dit de Bansos de Bansos de  \n",
       "195                  Il l'a terrasséé à l'intérieur de  \n",
       "196              Il a fait une bonne chose de l'argent  \n",
       "197                            Quel cheval veux-tu vu?  \n",
       "198                 Il n'a pas de la foudre du Baol de  \n",
       "199                       Va te l'homme qui te c'est l  \n",
       "200                Si je me souviens de l'argent, je n  \n",
       "201                            C'est l'air qui n'a pas  \n",
       "202                  C'est un homme gentil qui n'a pas  \n",
       "203                   Tu as dit ces femmes-tu achetés?  \n",
       "204                     C'est le nuque qui a dit qu'il  \n",
       "205            Les gens n'a rien dit de ces femmes ont  \n",
       "206                Ta mère dit qu'il y avait une bonne  \n",
       "207                    Tu as été et tu as été et qu'il  \n",
       "208                        C'est toi qui n'a pas été à  \n",
       "209                 Les Bum de l'argent du fi où il ne  \n",
       "210              L'homme a donné à cause de toi et moi  \n",
       "211                   Celui-là ne peut pas aujourd'hui  \n",
       "212             Un enfant n'est pas une bonne chose qu  \n",
       "213                  Il y a une chose à l'intérieur de  \n",
       "214          Cet autre qui est arrivé, qui est arrivé!  \n",
       "215   Ne fais pas le couscous le couscous dans la m...  \n",
       "216   Les hommes sont des choses qui sont en colère...  \n",
       "217                       Aujourd'hui, ils ne sont pas  \n",
       "218               C'est une mauvaise mauvaise fois qui  \n",
       "219            Ils ont fait une cravate, tu ne pourrai  \n",
       "220                 Du moment qu'il part pour qu'il ne  \n",
       "221             Il a fait une affaire de l'habitude de  \n",
       "222         Quand tu as été si nous allions le fatigue  \n",
       "223             Il t'a donné le morceau de la suite de  \n",
       "224   Les enfants sont assiss les enfants sont assi...  \n",
       "225           Il n'a pas encore vraiment à l'intérieur  \n",
       "226         Il est debout sur le point de la cour de l  \n",
       "227                      C'est à l'intérieur que c'est  \n",
       "228                  Il l'a fait une belle parole de l  \n",
       "229     Les enfants sont arrivéss de l'eau les enfants  \n",
       "230                    C'est une personne n'est pas un  \n",
       "231                           L'homme qui n'a pas été.  \n",
       "232                               Moi je n'ai pas été.  \n",
       "233           Donne le travail à un autre qui n'as pas  \n",
       "234                        C'est une chose de ce qu'il  \n",
       "235                  C'est à l'intérieur qui a fait un  \n",
       "236                        Ceux-ci ne sont pas partis!  \n",
       "237                     Il n'a pas une belle belleé de  \n",
       "238         Tu ne vas pas que tu ne viennes pas que je  \n",
       "239                Il l'a fait tomber sur le sol et qu  \n",
       "240                         C'est moi qui vais partir.  \n",
       "241            L'homme a donné quelque chose à l'homme  \n",
       "242                   C'est Omar qui a fait une qui ne  \n",
       "243                  Je n'ai pas été à une autre jeune  \n",
       "244                   Va jusqu'à ce qu'il soit fait de  \n",
       "245                  C'est un homme pour l'homme qui a  \n",
       "246                           Celui-là, il n'y a pas à  \n",
       "247                             Il l'a l'a grifféé à l  \n",
       "248                       C'est à l'intérieur de l'eau  \n",
       "249                     C'est le nuque qui a dit qu'il  \n",
       "250                           C'est le champ qu'il l'a  \n",
       "251                 L'homme n'est pas un homme de paix  \n",
       "252                          Il n'a pas de l'air de la  \n",
       "253                          Que personne n'est parti.  \n",
       "254                          C'est à l'homme que c'est  \n",
       "255   Les enfants ses enfants sont assiss les arach...  \n",
       "256         J'ai été chez un autre qui a mangé le riz.  \n",
       "257        Ta de la ville ; il a fait une simple de l'  \n",
       "258                   C'est à l'intérieur que je veux!  \n",
       "259   Quand tu ne peux pas de Bamakor leundu de la ...  \n",
       "260   Ogoja, les insectess xaritoos xaritoos dans l...  \n",
       "261            C'est à l'autre que c'est à l'intérieur  \n",
       "262        Mets-toi en garçon, il y a une bonne chose!  \n",
       "263   C'est un homme pour que je te dit que je te dit!  \n",
       "264             C'est toi qu'il est en l'eau qui a été  \n",
       "265   S'exiler pour être loin l'enfant que l'enfant...  \n",
       "266      C'est Omar qui a dit que l'homme qui a dit qu  \n",
       "267   Il n'a pas de la suite de la voiture de la vo...  \n",
       "268                    Je n'ai pas de l'ai vu, je n'ai  \n",
       "269      Si tu ne veux pas, il te fait sous le répara.  \n",
       "270                 C'est à l'inste de l'eau qui a été  \n",
       "271                     Il m'a dit qu'il m'a dit qu'il  \n",
       "272        C'était une chose qui n'a pas été de la mer  \n",
       "273   L'homme a donné quelque chose à l'intérieur d...  \n",
       "274          Le lest est en train de l'eau de l'eau du  \n",
       "275                 Il a l'a d'un coup de l'argent à l  \n",
       "276    Il a dit qu'il y a quelque chose de l'argent de  \n",
       "277    Une fois en chemin, il ne peut pas s'en aller à  \n",
       "278     Ils ont été tellement toute cette Saint-Louis.  \n",
       "279      On s'est passé dans la zone côtière, il s'est  \n",
       "280            Si tu le fais, il s'en va à ta mère, il  \n",
       "281                              Ils se sont fatigués.  \n",
       "282         J'ai vu l'homme qui est venu à l'homme qui  \n",
       "283   Avec l'air de la paix pour l'autre endroit qui s  \n",
       "284                       Mets-y trois pinc pour l'ur!  \n",
       "285       L'homme qui a dit qu'ils ne sont pas l'autre  \n",
       "286               C'est l'homme qui a dit que c'est l'  \n",
       "287        Il a remplir une Ze de la provi de la provi  \n",
       "288                                      Il est parti.  \n",
       "289       Il a une affaire de l'a fait de l'argent des  \n",
       "290   Lorsqu'il a dit que ce soit là, il s'est répandu  \n",
       "291               Il n'a pas de l'habitude de l'eau, c  \n",
       "292   Je ne peux pas piler tout ce qui n'est pas un...  \n",
       "293   Il a une belle voix de la peur du baptêmee de...  \n",
       "294         J'ai été pour le moment qu'il ne veut pas.  \n",
       "295              Il n'a pas de l'enfant qui n'a pas de  \n",
       "296             Si tu m'as offert, je me l'image de l'  \n",
       "297       C'est à l'intérieur que je veux que je veux.  \n",
       "298   Cet homme qui a dit que les enfants sont les ...  \n",
       "299   L'homme qui a été pour que je crois, je crois...  \n",
       "300           C'est à l'intérieur que je n'ai pas été.  \n",
       "301           C'est le singe qui a dit qu'il n'est pas  \n",
       "302                 Il a encore aujourd'hui de encore.  \n",
       "303   Maintenant le pièges de l'air de la fin de l'air  \n",
       "304       Il l'a fait partie de l'eau, il est en train  \n",
       "305   Tu n'es pas de la parole à l'intérieur de la ...  \n",
       "306             Les gens n'a pas été, ils ont été, ils  \n",
       "307       Si tu veux la fille, je crois que c'est à l'  \n",
       "308   Il a le cheval qui a mangé le même ; il a dit qu  \n",
       "309            Je le singe, c'est ton ami qui l'acc du  \n",
       "310   Il y a beaucoup d'antennes qui ont passé en b...  \n",
       "311   Ils ont deux fois en hauts d'avoir été pour a...  \n",
       "312   J'ai aperçu une autre chose de prendre que la...  \n",
       "313   On a fait une bonne chose qui n'est pas encor...  \n",
       "314              Le riz n'a pas de riz, le riz n'a pas  \n",
       "315   Tu ne vas nulle que tu ne te dis pas que le t...  \n",
       "316   Si tu le beau, il est beau, il y avait une chose  \n",
       "317   Je l'ai trouvé une affaire de protection de c...  \n",
       "318             On l'a fait de l'a fait de l'argent de  \n",
       "319   La maison n'a pas encore été à la maison de l...  \n",
       "320   J'ai préparé du couscous que je ne peux pas p...  \n",
       "321              La réunion ce qu'il n'a pas de l'air,  \n",
       "322   C'est à l'intérieur qui a fait la maladie et ...  \n",
       "323   Comment vais-je monter de la saison des pluie...  \n",
       "324   C'est la même occasion de l'eau du cheval qui n'  \n",
       "325       C'est le singe qui n'a pas de faire le creux  \n",
       "326          Je l'ai trouvé, je suis dans le même d'un  \n",
       "327                Il a dit de l'air de l'air de l'air  \n",
       "328   Ta de la fin de l'eau que le fleuve de la fin de  \n",
       "329           J'ai vu les moutons de tous les moutons.  \n",
       "330             C'est à l'intérieur qui l'a fait de l'  \n",
       "331             C'est à la fin de l'affaire n'a pas de  \n",
       "332   L'homme enfant que les enfants ne peut pas qu...  \n",
       "333                                 Oui, il est parti?  \n",
       "334      C'est une photo sur laquelle il y a une chose  \n",
       "335      C'est à l'intérieur que tu vas, qu'on appelle  \n",
       "336       L'oiseau est de l'impression d'être aujourd'  \n",
       "337    C'était une bonne chose qui n'est pas une bonne  \n",
       "338   Ce que ce qui s'est passé dans cette photo ci...  \n",
       "339                Qu'est-ce qui a dit que ce soit-il?  \n",
       "340             Mais, je n'ai pas été, je n'ai pas été  \n",
       "341            Il a dit que j'ai ressenti de l'eau, s'  \n",
       "342        Qui a dit, ils sont en désunion, ils sontnt  \n",
       "343   Une deux grand-mère avec une bonne chose de a...  \n",
       "344   Attends que ça soit tout à fait de l'hôpital,...  \n",
       "345      L'homme qui a dit que c'est à l'intérieur que  \n",
       "346         Ce n'est pas pour le moment qu'il y a de l  \n",
       "347          Une deuxième, il s'est immobilt à la tête  \n",
       "348         Il a remplie de l'argent de l'argent de l'  \n",
       "349   C'est moi qui a été médecin de la fin de l'ha...  \n",
       "350   La réunion ce n'est pas encore guérié à l'int...  \n",
       "351               C'est à cause de l'imp qui l'a mis à  \n",
       "352         Qui a dit lavé le carrelage de la voiture?  \n",
       "353               Quand ils ont fini de l'eau, j'ai vu  \n",
       "354   La seule le tué, il est en colère, il est en ...  \n",
       "355          Je suis à l'instant de l'eau de mon père.  \n",
       "356   Apporte la nuite de l'heure de la brousse du ...  \n",
       "357   Apporte le sacs de l'herbe qui ne sont pas le...  \n",
       "358             L'homme, il est une chambre, c'est une  \n",
       "359       Quand ils sont des femmes, qui ne l'a pas de  \n",
       "360   S'exiler pour avoir ton père et ma mère a mis...  \n",
       "361   Tu ne m'as pas encore vraiment à l'intérieur ...  \n",
       "362        Les gens n'a pas un homme de gens, il s'est  \n",
       "363             Je crois que ce soit, je n'ai pas été!  \n",
       "364                     Ce qu'il m'a dit qu'il m'a dit  \n",
       "365   Il lui a donné une belle de l'Ouest, mais il est  \n",
       "366   Ma mère sont debout de l'eau, je ne peux pas ...  \n",
       "367                             C'est moi qui n'a été.  \n",
       "368            Qu'est-ce qui a été, c'est à la manière  \n",
       "369              Est-ce que le pays a dit qu'il a dit?  \n",
       "370                Regarde ce que j'ai acheté le vent!  \n",
       "371                 C'est la brise de l'amour qu'il n'  \n",
       "372   C'est l'homme qui avaient la même occasion qu...  \n",
       "373      C'est la raison de l'eau qui a été la tête de  \n",
       "374                 J'ai visé le vent, il s'est pas le  \n",
       "375   Il n'a pas de l'habitude de faire du mariager de  \n",
       "376           À l'intérieur, c'est à l'intérieur de l'  \n",
       "377               Je me masser de l'eau de l'eau de l'  \n",
       "378            Va riz, il s'est immobile au riz, il s'  \n",
       "379   La corde n'a pas de la saison des pluies qui ...  \n",
       "380   Il a deux deux enfants, il a fait une deux an...  \n",
       "381                  Il n'a pas vu le fils de la dame.  \n",
       "382    Je suis dans la même conf de l'eau en paix de l  \n",
       "383   Mon père avaient dans les montagnes d'herbes ...  \n",
       "384   Quand on l'a dit de la forêt de l'eau en Fran...  \n",
       "385   Qui a dit que ce qui a dit qu'il a dit que le...  \n",
       "386   C'est la même occasion qui avaient appris à l...  \n",
       "387   Qu'est-ce qu'il a dit, il est mettre au momen...  \n",
       "388   C'est pour cette année que les enfants qui av...  \n",
       "389   C'est le bâtiment sur l'eau qui a fait quelqu...  \n",
       "390   Si nos parolesais, même, je ne peux pas que j...  \n",
       "391   Maintenant, ils ont été, ils ont été une zone...  \n",
       "392   Une personne, ils sont fatigués, ils sontnt à...  \n",
       "393   L'Afrique, c'est le fruit de l'image presque ...  \n",
       "394   Je voudrais qu'il a dit que j'ai acheté l'enf...  \n",
       "395   Les enfants qui avaient la zone large la vian...  \n",
       "396   Un enfant ne vont pas de même d'un enfant, il...  \n",
       "397   Il lui a donné un homme, c'est pour cela qu'i...  \n",
       "398   Quand nous allions une zone, une zonement de ...  \n",
       "399   Tu n'as pas de l'image, c'est pour cela que c...  \n",
       "400     Aujourd'hui, je ne peux pas Omar n'ai pas été.  \n",
       "401   C'est la route de l'Afrique de l'Afrique de l...  \n",
       "402   Ne peux-tu pas passé, ils sontnt, le manger, ...  \n",
       "403   C'est le singe qui avaient sèche la même occa...  \n",
       "404   Ils ont été fi, ils sont fatigués ici, ils so...  \n",
       "405   Le long de l'Ouest, il n'a jamais passé de l'...  \n",
       "406   Une deuxie de l'eau qui se passent le temps d...  \n",
       "407   Si je me souviens de l'air de moi, je ne peux...  \n",
       "408   Il y a beaucoup d'herbes à l'entrée de l'Oues...  \n",
       "409   Celui-là, il le lui a bóom, il le lui a certa...  \n",
       "410   Ton père a fait une tasse, ils sont en France...  \n",
       "411   Mais la savane qui s'est senti proche en ce q...  \n",
       "412   Ils ont fait une simple de l'argent qui s'est...  \n",
       "413   Faire la retraite et moi, il n'a pas de la re...  \n",
       "414   C'est le même vent qui se sont celle de l'eau...  \n",
       "415   Ici, c'est l'extérieur, c'est le corps s'est ...  \n",
       "416   L'oeil qui ne peut pas pour qu'il ne fait pas...  \n",
       "417               Celui qui n'a pas de l'âne est beau.  \n",
       "418   Est-ce que je suis resté sur le point de l'ea...  \n",
       "419   Il a tant que le feu ; je suis dans le même d...  \n",
       "420   C'est à l'intérieur qui se sont réunis et qui...  \n",
       "421   Si tu ne peux pas, ils sont en avoir une bell...  \n",
       "422   Ils ont commencé à la manière de l'eau de l'e...  \n",
       "423   C'est à moi qu'il n'a pas été, il le lui a fa...  \n",
       "424   C'est à la fin de l'habitude de l'air qui ava...  \n",
       "425   Ce que tu le lui a dit que tu le lui a dit qu...  \n",
       "426   Il y avait une bonne pension de l'Ouest, ils ...  \n",
       "427   Je n'ai pas de bagages de la saison des pluie...  \n",
       "428   C'est à l'intérieur qu'il a dit qu'il a dit q...  \n",
       "429   Une deuxième avec sa mort qu'il n'a pas de l'...  \n",
       "430   Il l'a fait tomber en avoir fait une bonne ch...  \n",
       "431   Mon Dieu, c'est une simple, une simple de l'e...  \n",
       "432   Pourquoi prendrai-tu à l'intérieur que j'ai d...  \n",
       "433   Est-ce que je n'ai pas plus de voix, je me su...  \n",
       "434   Une deuxième, quand il s'est immobilt à la fi...  \n",
       "435   Toi, je vais me dégoût, je vais préparer, je ...  \n",
       "436   Il a dit que ce qui s'est échappé dans la zon...  \n",
       "437   En amen, qu'il y avait quelque chose de l'Oue...  \n",
       "438   L'homme qui avait été dans une sorte de route...  \n",
       "439   L'individu n'est pas mauvaise personne qui l'...  \n",
       "440              Si tu veux cette époque que l'enfant!  \n",
       "441   Une force électrique qu'il avait connu, dans ...  \n",
       "442   C'est à l'intérieur que j'ai été pour moi que...  \n",
       "443   Si je lis Wi mal, je ne peux pas oublier que ...  \n",
       "444   Mon père est agricul qu'il faut écouter, ils ...  \n",
       "445   C'est l'homme qui a dit que c'est l'homme qui...  \n",
       "446             Le francolin ne peut pas s'est égarée!  \n",
       "447   Nous n'a pas de l'Afrique qui m'a dit que je ...  \n",
       "448   C'est à l'inste de l'eau que le ge de l'eau d...  \n",
       "449   C'est l'autre qui a fait la maison qui avaien...  \n",
       "450   Je ne sais pas de l'argent à l'intérieur de l...  \n",
       "451   Si nous parlaitions une simples où ils nous p...  \n",
       "452   Les guérisses de la foudre, ils vont à l'inst...  \n",
       "453   Arrête de l'eau, ils sont en désunion, ils so...  \n",
       "454   Nous avons reparlé, ils sontnt, ils sontnt, i...  \n",
       "455   Ils ont apporté la même de l'eau de ses enfan...  \n",
       "456   L'oiseau est en train de l'eau du corps dans ...  \n",
       "457   Si tu n'y a pas de la p spéci, il le allumere...  \n",
       "458   Le voilà, je ne peux pas oublierr par l'avoir...  \n",
       "459   Une deuxième avec sa touffe, avec la voix de ...  \n",
       "460   Qui a dit en Afrique de la fin de l'argent de...  \n",
       "461   Et pourtant, ils ne rient pas à la manière du...  \n",
       "462   C'est une photo sur le point de l'enfant qui ...  \n",
       "463   J'ai eu une jeune fille qui l'a mis à la main...  \n",
       "464   Une deuxième, mais ils sont en France, mais i...  \n",
       "465            Ils ont fait une nappeée de la ennemis.  \n",
       "466   C'était une maison qui est à une maison de ba...  \n",
       "467   C'est à cause de la même occasion qui avaient...  \n",
       "468   Mon père a fait une jeune fille qui ne s'est ...  \n",
       "469   Il n'a pas de l'Afrique, c'est le singe qui l...  \n",
       "470   Je l'ai trouvé en train de l'eau, ils lui met...  \n",
       "471   C'est à l'intérieur que le s'est pas le vent ...  \n",
       "472   Il dit qu'il n'a pas pu durer, il s'en va à l...  \n",
       "473   Je veux que tu viennes et que tu viennes, il ...  \n",
       "474   Je t'ai vu, cela me semble que c'est cela que...  \n",
       "475   Je te remercie beaucoup de guerre, c'est moi ...  \n",
       "476   Nous avons fait de la rue, ils sont en bandou...  \n",
       "477   Je n'ai pas de ce qu'il ne l'a pas vu à l'int...  \n",
       "478   Quand tu sauras, tu sauras, tu sauras de puis...  \n",
       "479   Quand ils se sont propos, ils sontnt le « cas...  \n",
       "480   Il l'a fait tomber, il s'est sentie dans la c...  \n",
       "481   Aujourd'hui, le mouton de la forêt qu'il faut...  \n",
       "482   Si tu n'as pas été, ils sontnt, ils sontnt à ...  \n",
       "483   Ce que je le dis qu'il m'a dit que je le dis ...  \n",
       "484   Mes de l'eau de l'eau de l'eau du puits qui f...  \n",
       "485   Une deuxièmee de l'eau du fleuve au-dessus de...  \n",
       "486   C'est une raison que j'ai découvert, il est ;...  \n",
       "487   L'inter, c'est le corps que j'ai vu, c'est qu...  \n",
       "488   Il n'a pas de la même grande vu du scorpion d...  \n",
       "489   Les temps de leurs yeux de leurs yeux de leur...  \n",
       "490   Les patientss de l'eau, c'est la fin de l'eau...  \n",
       "491   Je vais me souviens de l'orage avec les enfan...  \n",
       "492   C'est une maison de ses enfants qu'il a cousu...  \n",
       "493   Si cela ne veux pas, c'était le temps de l'or...  \n",
       "494   Aujourd'hui, je suis dans les rues de l'Ouest...  \n",
       "495   J'ai l'air de la brousse ; il a aller qu'il n...  \n",
       "496   Je le jure le gàjj sur le plan, il le travail...  \n",
       "497   Je me suis resté sur le corps qui me harceler...  \n",
       "498            Une deux fois en beauté dans la maison.  \n",
       "499   L'enfant n'a pas de l'eau que le arbre qu'il ...  \n",
       "500   Qui a dit que tu cachess, tu vas à la tête, t...  \n",
       "501   Il a vu tous ces autres femmes, celui-là est ...  \n",
       "502   C'est à l'intérieur que le feu qui l'a mis à ...  \n",
       "503   La bouteille est de l'argent, je n'a pas de l...  \n",
       "504   Ils ont apporté en boubou en boubou de l'autr...  \n",
       "505   Cette petite table, si tu le fais, tu le fais...  \n",
       "506   On lui a donné le « hameaux – par l'a fait le...  \n",
       "507   C'est à partir de l'autre endroit qui avaient...  \n",
       "508   Il est dans la chambre dans l'enfant au-dessu...  \n",
       "509   Tu me guigne la tête, je me souviens de l'eau...  \n",
       "510   Il y avait une belle qui ont commencé à l'ext...  \n",
       "511   De ma mère, ils sont en petits, ils sont peti...  \n",
       "512   Une deuxie avec ma mère, ils sontnt à la fin ...  \n",
       "513   Mon père les deux domestiques sur les volets ...  \n",
       "514   Une deuxiAprès, ils s'agitent dans les rues, ...  \n",
       "515   Les femmes et les femmes, près de la savane, ...  \n",
       "516   Quand ils ont été en train de la maison, ils ...  \n",
       "517   Les bananes par la service, dans la même excr...  \n",
       "518   L'arbre m'a fait une route qui n'est pas plus...  \n",
       "519   Mese mon père et ma mère sont une hutte de la...  \n",
       "520   Je viens de l'eau qui s'en va à l'eau qui s'e...  \n",
       "521   Sur la photo que j'ai vu une sorte dessage. C...  \n",
       "522   C'est ce qu'il me fait que je ne me sab pas p...  \n",
       "523   Les femmes et les femmes qui ont commencé à l...  \n",
       "524   C'est celui-là qui ne peut pas se laver à l'i...  \n",
       "525   Si le roi de l'homme de la guerre, le roi de ...  \n",
       "526   C'est une guerre d'un coup ; il ne te parle p...  \n",
       "527   Comment s'est arrêtéé dans le plus qu'il en s...  \n",
       "528   Si cela n'ai pas de plaine d'herbes, le déser...  \n",
       "529   Actuellement les dés avaients deuxis deux fau...  \n",
       "530   Mais je me suis en paix de l'eau de moi, je m...  \n",
       "531   À trois moment de la chemise en Afrique, il n...  \n",
       "532   Moi-moi des torches de l'Afrique de moi, je v...  \n",
       "533   Le monsieur est de l'eau, c'est une bonne cho...  \n",
       "534   Une deuxième avec sa voix, en paix d'avoir à ...  \n",
       "535   Ce que je suis face de l'eau, j'ai vu une bon...  \n",
       "536   Ils ont deuxieurs fois que mon père avaient a...  \n",
       "537   Avec ce qui portait la fin de l'orage sur la ...  \n",
       "538   Ils ont deux fois, ils sont fatigués, ils son...  \n",
       "539   Je me souviens de l'orages de guerre, la fin ...  \n",
       "540   Sur la photo que vous m'avez envoyée, j'y ai ...  \n",
       "541   Sur cette photo il y a deux bancs. J'y vois a...  \n",
       "542   Si mon père découvre, ils ont passé dans les ...  \n",
       "543   C'est à la manière du soir de la même maison ...  \n",
       "544   Ceci est une deuxulëer qui s'agît de ces coul...  \n",
       "545   Je le contact avec l'ai à l'intérieur de la g...  \n",
       "546   Les go entre sont des garçons de l'autre qui ...  \n",
       "547   Ma mère sont debout sur le pont de la côte, e...  \n",
       "548   Une deuxième giclée en France, dans l'image d...  \n",
       "549   Si tu veux « taraapees », il s'est senti dans...  \n",
       "550   Sur cette photo il semble qu'on appelle'a con...  \n",
       "551   Au point de la France, ils sontnt l'a fait pa...  \n",
       "552   Ceci ressemble à une bonne chose de Luga ou u...  \n",
       "553   Peut-être qu'on laisse au bout de sa vie, dan...  \n",
       "554   Pour une sorte de milliers de milliers d'un a...  \n",
       "555   Ceci est une chambre. C'est une chambre. Une ...  \n",
       "556   D'où me guignet à la discipline, dans la même...  \n",
       "557   Une deuxième giclée de la côte, dans la côte ...  \n",
       "558   Sur cette photo il semble qu'on appelle'a mon...  \n",
       "559   Sans doute est-ce que je ne sais rien de l'ar...  \n",
       "560   Il s'est senti proche dans la savane, ils ne ...  \n",
       "561   Je n'ai pas de jeunes gens de jeunes gens son...  \n",
       "562   À l'intérieur de la tête, la photo que j'ai g...  \n",
       "563   Mais ce qui s'est passé dans laquelle je n'ai...  \n",
       "564   Mais la fin de l'orage sur le grand fleuve. M...  \n",
       "565   Sur cette photo il y a une chose. Lu, il y a ...  \n",
       "566   Les hommes sont les hommes sont de l'arrivée ...  \n",
       "567   J'ai rencontré une vieux fauteuil de l'Afriqu...  \n",
       "568   Sur cette photo il y a une chose de service. ...  \n",
       "569   Je ne peux pas l'ai reconnu de trois personne...  \n",
       "570   Ceci est une maison de maison qui s'y coucher...  \n",
       "571   Il a l'habitude de l'air de la fin de mon pèr...  \n",
       "572   Sur cette photo il y a une chose qui ressembl...  \n",
       "573   Les hommes sont des femmess passés de la soci...  \n",
       "574   Je ne peux pas pour qu'il me souviens de l'ar...  \n",
       "575   Les fourmiss de la Nice des années en Afrique...  \n",
       "576   Une large chambre dans la terre qui est en la...  \n",
       "577   Ils ont été nuages en Afrique de la société a...  \n",
       "578   Une grand-mère qui avait parcouru les fleuves...  \n",
       "579   Alors ma mère ses deux enfants âgés sur le gr...  \n",
       "580   J'ai travaillé en avaient deuxies en Afrique ...  \n",
       "581   Je me souviens de la guerre – à la guerre pou...  \n",
       "582   D'où venait de personnes personnes britanniqu...  \n",
       "583   Je me souviens de la savane de la voiture dan...  \n",
       "584   Les érus ne sont pasinéen de douze ou le pidg...  "
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.options.display.max_rows = 1000\n",
    "prediction.head(1000)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_pytorch_p310",
   "language": "python",
   "name": "conda_pytorch_p310"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
