{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Training the best custom Transformer ü§ñ\n",
    "-----------------------------------\n",
    "\n",
    "In this notebook, we will continue the training of the best custom transformer on the new extracted sentences from the book **Grammaire de Wolof Moderne**. We provide, bellow, the main evaluation figures, obtained from the hyperparameter search step. We will evaluate the training on the validation dataset.\n",
    "\n",
    "- Parallel coordinates:\n",
    "\n",
    "- Parameter importance (from [panel]()):\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let us add some libraries bellow:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Oumar Kane\\AppData\\Local\\pypoetry\\Cache\\virtualenvs\\pytorch1-HleOW5am-py3.10\\lib\\site-packages\\tqdm\\auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "# let us import all necessary libraries\n",
    "from transformers import AutoModelForSeq2SeqLM, Seq2SeqTrainingArguments, Seq2SeqTrainer, T5TokenizerFast, set_seed, AdamW, get_linear_schedule_with_warmup, T5ForConditionalGeneration,\\\n",
    "    get_cosine_schedule_with_warmup, Adafactor\n",
    "from wolof_translate.utils.sent_transformers import TransformerSequences\n",
    "from torch.nn import TransformerEncoderLayer, TransformerDecoderLayer\n",
    "from torch.utils.data import Dataset, DataLoader, random_split\n",
    "from wolof_translate.data.dataset_v2 import SentenceDataset\n",
    "from wolof_translate.utils.sent_corrections import *\n",
    "from sklearn.model_selection import train_test_split\n",
    "from torch.optim.lr_scheduler import _LRScheduler\n",
    "# from custom_rnn.utils.kwargs import Kwargs\n",
    "from torch.nn.utils.rnn import pad_sequence\n",
    "from plotly.subplots import make_subplots\n",
    "from nlpaug.augmenter import char as nac\n",
    "from torch.utils.data import DataLoader\n",
    "# from datasets  import load_metric # make pip install evaluate instead\n",
    "# and pip install sacrebleu for instance\n",
    "from torch.nn import functional as F\n",
    "import plotly.graph_objects as go\n",
    "from tokenizers import Tokenizer\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm import tqdm, trange\n",
    "from functools import partial\n",
    "from torch.nn import utils\n",
    "from copy import deepcopy\n",
    "from torch import optim\n",
    "from typing import *\n",
    "from torch import nn\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import itertools\n",
    "import evaluate\n",
    "import random\n",
    "import string\n",
    "import shutil\n",
    "import wandb\n",
    "import torch\n",
    "import json\n",
    "import copy\n",
    "import os\n",
    "\n",
    "os.environ[\"WANDB_DISABLED\"] = \"true\""
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Steps"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We must add some classes that we implemented when making the hyperparameter search including:\n",
    "- The custom Sinusoidal-based encoder\n",
    "- The custom Size prediction module\n",
    "- The custom Transformer requiring the `pytorch encoder and decoder stacked layers`\n",
    "- The custom Transformer' learning rate scheduler\n",
    "- The custom Trainer\n",
    "\n",
    "And include them in our `wolof-translate` package.\n",
    "\n",
    "-------------------\n",
    "\n",
    "After that we will continue the training of the custom Transformer, for which we will resume its parameters from the saved checkpoints.\n",
    "\n",
    "-------------------\n",
    "\n",
    "The last part is to evaluate the model on the test set."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let us go into our pipeline üëå"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Add custom modules"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Custom Positional Encoder"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let us add bellow the positional encoder module which will permit us to put the positions of the sequence elements on the embedding vector."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting wolof-translate/wolof_translate/models/transformers/position.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile wolof-translate/wolof_translate/models/transformers/position.py\n",
    "\n",
    "from torch import nn\n",
    "import numpy as np\n",
    "import torch\n",
    "\n",
    "class PositionalEncoding(nn.Module):\n",
    "\n",
    "    def __init__(self, n_poses_max: int = 500, d_model: int = 512):\n",
    "        super(PositionalEncoding, self).__init__()    \n",
    "        \n",
    "        self.n_poses = n_poses_max\n",
    "        \n",
    "        self.n_dims = d_model\n",
    "        \n",
    "        # the angle is calculated as following\n",
    "        angle = lambda pos, i: pos / 10000 ** (i / self.n_dims)\n",
    "\n",
    "        # let's initialize the different token positions\n",
    "        poses = np.arange(0, self.n_poses)\n",
    "\n",
    "        # let's initialize also the different dimension indexes\n",
    "        dims = np.arange(0, self.n_dims)\n",
    "\n",
    "        # let's initialize the index of the different positional vector values\n",
    "        circle_index = np.arange(0, self.n_dims / 2)\n",
    "\n",
    "        # let's create the possible combinations between a position and a dimension index\n",
    "        xv, yv = np.meshgrid(poses, circle_index)\n",
    "\n",
    "        # let's create a matrix which will contain all the different points initialized\n",
    "        points = np.zeros((self.n_poses, self.n_dims))\n",
    "\n",
    "        # let's calculate the circle y axis coordinates\n",
    "        points[:, ::2] = np.sin(angle(xv.T, yv.T))\n",
    "\n",
    "        # let's calculate the circle x axis coordinates\n",
    "        points[:, 1::2] = np.cos(angle(xv.T, yv.T))\n",
    "        \n",
    "        self.register_buffer('pe', torch.from_numpy(points).unsqueeze(0))\n",
    "    \n",
    "    def forward(self, input_: torch.Tensor):\n",
    "        \n",
    "        # let's scale the input\n",
    "        input_ = input_ * torch.sqrt(torch.tensor(self.n_dims))\n",
    "        \n",
    "        # let's recuperate the result of the sum between the input and the positional encoding vectors\n",
    "        return input_ + self.pe[:, :input_.size(1), :].type_as(input_)\n",
    "    "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Size Prediction module"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let us define bellow the Size Prediction's module. It is a multi layer perceptron with multiple layers of `linear + relu activation + drop out + layer normalization`. `The number of features`, `the number of layers`, `the layer normalization' activation function` and `the drop out rate` are given as parameters to the module.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting wolof-translate/wolof_translate/models/transformers/size.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile wolof-translate/wolof_translate/models/transformers/size.py\n",
    "\n",
    "from torch import nn\n",
    "import torch\n",
    "class SizePredict(nn.Module):\n",
    "    \n",
    "    def __init__(self, input_size: int, target_size: int = 1, n_features: int = 100, n_layers: int = 1, normalization: bool = True, drop_out: float = 0.1):\n",
    "        super(SizePredict, self).__init__()\n",
    "        \n",
    "        self.layers = nn.ModuleList([])\n",
    "        \n",
    "        for l in range(n_layers):\n",
    "            \n",
    "            # we have to add batch normalization and drop_out if their are specified\n",
    "            self.layers.append(\n",
    "                nn.Sequential(\n",
    "                    nn.Linear(input_size if l == 0 else n_features, n_features),\n",
    "                    nn.LayerNorm(n_features) if normalization else nn.Identity(),\n",
    "                    nn.ReLU(),\n",
    "                    nn.Dropout(drop_out),\n",
    "                )\n",
    "            )\n",
    "        \n",
    "        # Initiate the last linear layer\n",
    "        self.output_layer = nn.Linear(n_features, target_size)\n",
    "    \n",
    "    def forward(self, input_: torch.Tensor):\n",
    "        \n",
    "        # let's pass the input into the different sequences\n",
    "        out = input_\n",
    "        \n",
    "        for layer in self.layers:\n",
    "            \n",
    "            out = layer(out)\n",
    "        \n",
    "        # return the final result (you have to take the absolute value of the result to make the number positive)\n",
    "        return self.output_layer(out)\n",
    "        \n",
    "        "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Transformer"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following module is the primary transformer model. It takes as argument:\n",
    "- a pytorch encoder and a pytorch decoder (they are defined outside of the module)\n",
    "- the input size or vocabulary size\n",
    "- the class criterion or loss function of the predict labels (as default to None but can be `nn.CrossEntropyLoss`, which apply the softmax transformation is made on the logits before calculation. label_smoothing can be added to the loss to prevent the model to over-fit according to the prediction values.)\n",
    "- the size criterion (`Mean Squared Error` $\\frac{1}{n}\\sum_{i = 1}^n (y_i - \\hat{y}_i)^2$ where $n$ is the batch size, $y_i$ is the true label and $\\hat{y}_i$ is the predicted label)\n",
    "- the number of features and the number of layers of the size prediction module\n",
    "- the max number of positions (it must be the max number of tokens defined when creating the pytorch dataset or the tokenizer)\n",
    "- the projection type (can be 'embedding' for 2-dimensional data containing integers as we are using or 'linear' for any other type of data different from the sequence of integers).\n",
    "\n",
    "For the `forward` method we have the following arguments:\n",
    "\n",
    "- the input sequence\n",
    "- the input padding mask\n",
    "- the target sequence or labels\n",
    "- the target padding mask\n",
    "- the padding token id.\n",
    "\n",
    "For the `generate` method we have the following arguments:\n",
    "\n",
    "- the input sequence\n",
    "- the input padding mask\n",
    "- the temperature\n",
    "- the padding token id.\n",
    "\n",
    "We added also two exception modules to handle errors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting wolof-translate/wolof_translate/models/transformers/main.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile wolof-translate/wolof_translate/models/transformers/main.py\n",
    "\n",
    "from wolof_translate.models.transformers.position import PositionalEncoding\n",
    "from wolof_translate.models.transformers.size import SizePredict\n",
    "from torch.nn.utils.rnn import pad_sequence\n",
    "from torch import nn\n",
    "from typing import *\n",
    "import torch\n",
    "import copy\n",
    "# new Exception for that transformer\n",
    "class TargetException(Exception):\n",
    "    \n",
    "    def __init__(self, error):\n",
    "        \n",
    "        print(error)\n",
    "\n",
    "class GenerationException(Exception):\n",
    "\n",
    "    def __init__(self, error):\n",
    "\n",
    "        print(error)\n",
    "\n",
    "class Transformer(nn.Module):\n",
    "    \n",
    "    def __init__(self, \n",
    "                 vocab_size: int,\n",
    "                 encoder,\n",
    "                 decoder,\n",
    "                 class_criterion = nn.CrossEntropyLoss(label_smoothing=0.1),\n",
    "                 size_criterion = nn.MSELoss(),\n",
    "                 n_features: int = 100,\n",
    "                 n_layers: int = 2,\n",
    "                 n_poses_max: int = 500,\n",
    "                 projection_type: str = \"embedding\",\n",
    "                 max_len: Union[int, None] = None):\n",
    "        \n",
    "        super(Transformer, self).__init__()\n",
    "        \n",
    "        assert len(encoder.layers) > 0 and len(decoder.layers) > 0\n",
    "    \n",
    "        self.dropout = encoder.layers._modules['0'].dropout.p\n",
    "        \n",
    "        self.enc_embed_dim = encoder.layers._modules['0'].linear1.in_features\n",
    "        \n",
    "        self.dec_embed_dim = decoder.layers._modules['0'].linear1.in_features\n",
    "        \n",
    "        # we can initiate the positional encoding model\n",
    "        self.pe = PositionalEncoding(n_poses_max, self.enc_embed_dim)\n",
    "        \n",
    "        if projection_type == \"embedding\":\n",
    "            \n",
    "            self.embedding_layer = nn.Embedding(vocab_size, self.enc_embed_dim)\n",
    "        \n",
    "        elif projection_type == \"linear\":\n",
    "            \n",
    "            self.embedding_layer = nn.Linear(vocab_size, self.enc_embed_dim)\n",
    "        \n",
    "        # initialize the first encoder and decoder\n",
    "        self.encoder = encoder\n",
    "        \n",
    "        self.decoder = decoder\n",
    "        \n",
    "        self.class_criterion = class_criterion\n",
    "        \n",
    "        self.size_criterion = size_criterion\n",
    "        \n",
    "        # let's initiate the mlp for predicting the target size\n",
    "        self.size_prediction = SizePredict(\n",
    "            self.enc_embed_dim,\n",
    "            n_features=n_features,\n",
    "            n_layers=n_layers,\n",
    "            normalization=True, # we always use normalization\n",
    "            drop_out=self.dropout\n",
    "            )\n",
    "\n",
    "        self.classifier = nn.Linear(self.dec_embed_dim, vocab_size)\n",
    "\n",
    "        # let us share the weights between the embedding layer and classification\n",
    "        # linear layer\n",
    "        self.classifier.weight.data = self.embedding_layer.weight.data\n",
    "\n",
    "        self.max_len = max_len\n",
    "        \n",
    "        \n",
    "    def forward(self, input_, input_mask = None, target = None, target_mask = None, \n",
    "                pad_token_id:int = 3):\n",
    "\n",
    "        # ---> Encoder prediction\n",
    "        input_embed = self.embedding_layer(input_)\n",
    "        \n",
    "        # recuperate the last input (before position)\n",
    "        last_input = input_embed[:, -1:]\n",
    "       \n",
    "        # add position to input_embedding\n",
    "        input_embed = self.pe(input_embed)\n",
    "        \n",
    "        # recuperate the input mask for pytorch encoder\n",
    "        pad_mask1 = (input_mask == 0).to(next(self.parameters()).device, dtype = torch.bool) if not input_mask is None else None\n",
    "        \n",
    "        # let us compute the states\n",
    "        input_embed = input_embed.type_as(next(self.encoder.parameters()))\n",
    "        \n",
    "        states = self.encoder(input_embed, src_key_padding_mask = pad_mask1)\n",
    "   \n",
    "        # ---> Decoder prediction\n",
    "        # let's predict the size of the target \n",
    "        target_size = self.size_prediction(states).mean(axis = 1)\n",
    "        \n",
    "        target_embed = self.embedding_layer(target)\n",
    "        \n",
    "        # recuperate target mask for pytorch decoder            \n",
    "        pad_mask2 = (target_mask == 0).to(next(self.parameters()).device, dtype = torch.bool) if not target_mask is None else None\n",
    "        \n",
    "        # define the attention mask\n",
    "        targ_mask = self.get_target_mask(target_embed.size(1))\n",
    "\n",
    "        # let's concatenate the last input and the target shifted from one position to the right (new seq dim = target seq dim)\n",
    "        target_embed = torch.cat((last_input, target_embed[:, :-1]), dim = 1)\n",
    "        \n",
    "        # add position to target embed\n",
    "        target_embed = self.pe(target_embed)\n",
    "        \n",
    "        # we pass all of the shifted target sequence to the decoder if training mode\n",
    "        if self.training:\n",
    "            \n",
    "            target_embed = target_embed.type_as(next(self.encoder.parameters()))\n",
    "            \n",
    "            outputs = self.decoder(target_embed, states, tgt_mask = targ_mask, tgt_key_padding_mask = pad_mask2)\n",
    "            \n",
    "        else: ## This part was understand with the help of the professor Bousso.\n",
    "            \n",
    "            # if we are in evaluation mode we will not use the target but the outputs to make prediction and it is\n",
    "            # sequentially done (see comments)\n",
    "            \n",
    "            # let us recuperate the last input as the current outputs\n",
    "            outputs = last_input.type_as(next(self.encoder.parameters()))\n",
    "            \n",
    "            # for each target that we want to predict\n",
    "            for t in range(target.size(1)):\n",
    "                \n",
    "                # recuperate the target mask of the current decoder input\n",
    "                current_targ_mask = targ_mask[:t+1, :t+1] # all attentions between the elements before the last target\n",
    "                \n",
    "                # we do the same for the padding mask\n",
    "                current_pad_mask = None\n",
    "                \n",
    "                if not pad_mask2 is None:\n",
    "                    \n",
    "                    current_pad_mask = pad_mask2[:, :t+1]\n",
    "                \n",
    "                # make new predictions\n",
    "                out = self.decoder(outputs, states, tgt_mask = current_targ_mask, tgt_key_padding_mask = current_pad_mask) \n",
    "                \n",
    "                # add the last new prediction to the decoder inputs\n",
    "                outputs = torch.cat((outputs, out[:, -1:]), dim = 1) # the prediction of the last output is the last to add (!)\n",
    "            \n",
    "            # let's take only the predictions (the last input will not be taken)\n",
    "            outputs = outputs[:, 1:]\n",
    "        \n",
    "        # let us add padding index to the outputs\n",
    "        if not target_mask is None: \n",
    "          target = copy.deepcopy(target.cpu())\n",
    "          target = target.to(target_mask.device).masked_fill_(target_mask == 0, -100)\n",
    "\n",
    "        # ---> Loss Calculation\n",
    "        # let us calculate the loss of the size prediction\n",
    "        size_loss = 0\n",
    "        if not self.size_criterion is None:\n",
    "            \n",
    "            size_loss = self.size_criterion(target_size, target_mask.sum(axis = -1).unsqueeze(1).type_as(next(self.parameters())))\n",
    "            \n",
    "        outputs = self.classifier(outputs)\n",
    "        \n",
    "        # let us permute the two last dimensions of the outputs\n",
    "        outputs_ = outputs.permute(0, -1, -2)\n",
    "\n",
    "        # calculate the loss\n",
    "        loss = self.class_criterion(outputs_, target)\n",
    "\n",
    "        outputs = torch.softmax(outputs, dim = -1)\n",
    "\n",
    "        # calculate the predictionos\n",
    "        outputs = copy.deepcopy(outputs.detach().cpu())\n",
    "        predictions = torch.argmax(outputs, dim = -1).to(target_mask.device).masked_fill_(target_mask == 0, pad_token_id)\n",
    "\n",
    "        return {'loss': loss + size_loss, 'preds': predictions}\n",
    "    \n",
    "    def generate(self, input_, input_mask = None, temperature: float = 0, pad_token_id:int = 3):\n",
    "\n",
    "        if self.training:\n",
    "\n",
    "          raise GenerationException(\"You cannot generate when the model is on training mode!\")\n",
    "\n",
    "        # ---> Encoder prediction\n",
    "        input_embed = self.embedding_layer(input_)\n",
    "        \n",
    "        # recuperate the last input (before position)\n",
    "        last_input = input_embed[:, -1:]\n",
    "       \n",
    "        # add position to input_embedding\n",
    "        input_embed = self.pe(input_embed)\n",
    "        \n",
    "        # recuperate the input mask for pytorch encoder\n",
    "        pad_mask1 = (input_mask == 0).bool().to(next(self.parameters()).device) if not input_mask is None else None\n",
    "        \n",
    "        # let us compute the states\n",
    "        input_embed = input_embed.type_as(next(self.encoder.parameters()))\n",
    "        \n",
    "        states = self.encoder(input_embed, src_key_padding_mask = pad_mask1)\n",
    "\n",
    "        # ---> Decoder prediction\n",
    "        # let us recuperate the maximum length\n",
    "        max_len = self.max_len if not self.max_len is None else 0\n",
    "\n",
    "        # let's predict the size of the target and the target mask\n",
    "        if max_len > 0:\n",
    "\n",
    "          target_size = self.size_prediction(states).mean(axis = 1).round().clip(1, max_len)\n",
    "        \n",
    "        else:\n",
    "\n",
    "          target_size = torch.max(self.size_prediction(states).mean(axis = 1).round(), torch.tensor(1.0))\n",
    "\n",
    "        target_ = copy.deepcopy(target_size.cpu())\n",
    "\n",
    "        target_mask = [torch.tensor(int(size[0])*[1] + [0] * max(max_len - int(size[0]), 0)) for size in target_.tolist()]\n",
    "\n",
    "        if max_len > 0:\n",
    "\n",
    "          target_mask = torch.stack(target_mask).to(next(self.parameters()).device, dtype = torch.bool)\n",
    "\n",
    "        else:\n",
    "\n",
    "          target_mask = pad_sequence(target_, batch_first = True).to(next(self.parameters()).device, dtype = torch.bool)\n",
    "      \n",
    "        # recuperate target mask for pytorch decoder            \n",
    "        pad_mask2 = (target_mask == 0).to(next(self.parameters()).device, dtype = torch.bool) if not target_mask is None else None\n",
    "        \n",
    "        # define the attention mask\n",
    "        targ_mask = self.get_target_mask(target_mask.size(1))\n",
    "            \n",
    "        # if we are in evaluation mode we will not use the target but the outputs to make prediction and it is\n",
    "        # sequentially done (see comments)\n",
    "        \n",
    "        # let us recuperate the last input as the current outputs\n",
    "        outputs = last_input.type_as(next(self.encoder.parameters()))\n",
    "        \n",
    "        # for each target that we want to predict\n",
    "        for t in range(target_mask.size(1)):\n",
    "            \n",
    "            # recuperate the target mask of the current decoder input\n",
    "            current_targ_mask = targ_mask[:t+1, :t+1] # all attentions between the elements before the last target\n",
    "            \n",
    "            # we do the same for the padding mask\n",
    "            current_pad_mask = None\n",
    "            \n",
    "            if not pad_mask2 is None:\n",
    "                \n",
    "                current_pad_mask = pad_mask2[:, :t+1]\n",
    "            \n",
    "            # make new predictions\n",
    "            out = self.decoder(outputs, states, tgt_mask = current_targ_mask, tgt_key_padding_mask = current_pad_mask) \n",
    "            \n",
    "            # add the last new prediction to the decoder inputs\n",
    "            outputs = torch.cat((outputs, out[:, -1:]), dim = 1) # the prediction of the last output is the last to add (!)\n",
    "        \n",
    "        # let's take only the predictions (the last input will not be taken)\n",
    "        outputs = outputs[:, 1:]\n",
    "\n",
    "        # ---> Predictions\n",
    "        outputs = self.classifier(outputs)\n",
    "\n",
    "        # calculate the resulted outputs with temperature\n",
    "        if temperature > 0:\n",
    "\n",
    "          outputs = torch.softmax(outputs / temperature, dim = -1)\n",
    "        \n",
    "        else:\n",
    "\n",
    "          outputs = torch.softmax(outputs, dim = -1)\n",
    "\n",
    "        # calculate the predictionos\n",
    "        outputs = copy.deepcopy(outputs.detach().cpu())\n",
    "        predictions = torch.argmax(outputs, dim = -1).to(target_mask.device).masked_fill_(target_mask == 0, pad_token_id)\n",
    "\n",
    "        return predictions\n",
    "    \n",
    "\n",
    "    def get_target_mask(self, attention_size: int):\n",
    "        \n",
    "        return torch.triu(torch.ones((attention_size, attention_size)), diagonal = 1).to(next(self.parameters()).device, dtype = torch.bool)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Learning scheduler"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let us create our own learning rate scheduler according to the paper [Attention Is All You Need](https://proceedings.neurips.cc/paper/2017/file/3f5ee243547dee91fbd053c1c4a845aa-Paper.pdf) paper."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![scheduler_transformer](https://i.stack.imgur.com/GQurA.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting wolof-translate/wolof_translate/models/transformers/optimization.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile wolof-translate/wolof_translate/models/transformers/optimization.py\n",
    "\n",
    "from torch.optim.lr_scheduler import _LRScheduler\n",
    "from torch import optim\n",
    "from typing import *\n",
    "\n",
    "class TransformerScheduler(_LRScheduler):\n",
    "    \n",
    "    def __init__(self, optimizer: Union[optim.AdamW, optim.Adam], d_model = 512, lr_warmup_step = 100, **kwargs):\n",
    "        \n",
    "        self._optimizer = optimizer\n",
    "        \n",
    "        self._dmodel = d_model\n",
    "        \n",
    "        self._lr_warmup = lr_warmup_step\n",
    "\n",
    "        # get the number of parameters\n",
    "        self.len_param_groups = len(self._optimizer.param_groups)\n",
    "\n",
    "        # provide the LRScheduler parameters\n",
    "        super().__init__(self._optimizer, **kwargs)\n",
    "        \n",
    "    def get_lr(self):\n",
    "        \n",
    "        # recuperate the step number\n",
    "        _step_num = self._step_count\n",
    "        \n",
    "        # calculate the learning rate\n",
    "        lr = self._dmodel ** -0.5 * min(_step_num ** -0.5, \n",
    "                                              _step_num * self._lr_warmup ** -1.5)\n",
    "        # provide the corresponding learning rate of each parameter vector\n",
    "        # for updating\n",
    "        return [lr] * self.len_param_groups\n",
    "\n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %%writefile wolof-translate/wolof_translate/models/transformers/optimization.py\n",
    "\n",
    "# from torch.optim.lr_scheduler import _LRScheduler\n",
    "# from torch import optim\n",
    "# from typing import *\n",
    "# class TransformerScheduler(_LRScheduler):\n",
    "    \n",
    "#     def __init__(self, optimizer: Union[optim.AdamW, optim.Adam], scale_factor = 1.0, lr_warmup_step = 100, **kwargs):\n",
    "\n",
    "#         self._optimizer = optimizer\n",
    "\n",
    "#         self._scale_factor = scale_factor\n",
    "        \n",
    "#         self._lr_warmup = lr_warmup_step\n",
    "\n",
    "#         # get the number of parameters\n",
    "#         self.len_param_groups = len(self._optimizer.param_groups)\n",
    "\n",
    "#         # provide the LRScheduler parameters\n",
    "#         super().__init__(self._optimizer, **kwargs)\n",
    "        \n",
    "#     def get_lr(self):\n",
    "        \n",
    "#         # recuperate the step number\n",
    "#         _step_num = self._step_count\n",
    "        \n",
    "#         # calculate the learning rate\n",
    "#         lr = self._scale_factor * min(_step_num ** -0.5, \n",
    "#                                               _step_num * self._lr_warmup ** -1.5)\n",
    "#         # provide the corresponding learning rate of each parameter vector\n",
    "#         # for updating\n",
    "#         return [lr] * self.len_param_groups\n",
    "\n",
    "        \n",
    "        "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Trainer"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let us define bellow a part of our long training class that we create and which is available in github. But the lines are commented in French."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting wolof-translate/wolof_translate/trainers/transformer_trainer.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile wolof-translate/wolof_translate/trainers/transformer_trainer.py\n",
    "\"\"\"Nouvelle classe d'entra√Ænement. On la fournit un mod√®le et des hyperparam√®tres en entr√©e.\n",
    "Nous allons cr√©er des classes suppl√©mentaire qui vont supporter la classe d'entra√Ænement\n",
    "\"\"\"\n",
    "\n",
    "from wolof_translate.utils.evaluation import TranslationEvaluation\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from tokenizers import Tokenizer\n",
    "from tqdm import tqdm, trange\n",
    "from torch.nn import utils\n",
    "from torch import optim\n",
    "from typing import *\n",
    "from torch import nn\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import string\n",
    "import torch\n",
    "import json\n",
    "import copy\n",
    "import os\n",
    "\n",
    "# choose letters for random words\n",
    "letters = string.ascii_lowercase\n",
    "\n",
    "class PredictionError(Exception):\n",
    "    \n",
    "    def __init__(self, error: Union[str, None] = None):\n",
    "\n",
    "        if not error is None:\n",
    "            \n",
    "            print(error)\n",
    "        \n",
    "        else:\n",
    "            \n",
    "            print(\"You cannot with this type of data! Provide a list of tensors, a list of numpy arrays, a numpy array or a torch tensor.\")\n",
    "\n",
    "class LossError(Exception):\n",
    "    \n",
    "    def __init__(self, error: Union[str, None] = None):\n",
    "\n",
    "        if not error is None:\n",
    "            \n",
    "            print(error)\n",
    "        \n",
    "        else:\n",
    "            \n",
    "            print(\"A list of losses is provided for multiple outputs.\")\n",
    "        \n",
    "class ModelRunner:\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        model: nn.Module,\n",
    "        optimizer = optim.AdamW,\n",
    "        seed: Union[int, None] = None, \n",
    "        evaluation: Union[TranslationEvaluation, None] = None,\n",
    "        version: int = 1\n",
    "    ):\n",
    "\n",
    "        # Initialisation de la graine du g√©n√©rateur\n",
    "        self.seed = seed\n",
    "        \n",
    "        # Initialisation de la version\n",
    "        self.version = version\n",
    "\n",
    "        # Recuperate the evaluation metric\n",
    "        self.evaluation = evaluation\n",
    "\n",
    "        # Initialisation du g√©n√©rateur\n",
    "        if self.seed:\n",
    "            torch.manual_seed(self.seed)\n",
    "\n",
    "        # Le mod√®le √† utiliser pour les diff√©rents entra√Ænements\n",
    "        self.orig_model = model\n",
    "\n",
    "        # L'optimiseur √† utiliser pour les diff√©rentes mises √† jour du mod√®le\n",
    "        self.orig_optimizer = optimizer\n",
    "\n",
    "        # R√©cup√©ration du type de 'device'\n",
    "        self.device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "        self.compilation = False\n",
    "\n",
    "    # ------------------------------ Training staffs (Partie entra√Ænement et compilation) --------------------------\n",
    "    \n",
    "    def batch_train(self, input_: torch.Tensor, input_mask: torch.Tensor,\n",
    "                    labels: torch.Tensor, labels_mask: torch.Tensor, pad_token_id: int = 3):\n",
    "        if self.hugging_face: # Nous allons utilise un mod√®le text to text de hugging face (but only for fine-tuning)\n",
    "\n",
    "          # effectuons un passage vers l'avant\n",
    "          outputs = self.model(input_ids = input_, attention_mask = input_mask, \n",
    "                               labels = labels)\n",
    "          \n",
    "          # recuperate the predictions and the loss\n",
    "          preds, loss = outputs.logits, outputs.loss\n",
    "        \n",
    "        else:\n",
    "\n",
    "          # effectuons un passage vers l'avant\n",
    "          outputs = self.model(input_, input_mask, labels, labels_mask, pad_token_id = pad_token_id)\n",
    "\n",
    "          # recuperate the predictions and the loss\n",
    "          preds, loss = outputs['preds'], outputs['loss']\n",
    "\n",
    "        # effectuons un passage vers l'arri√®re\n",
    "        loss.backward()\n",
    "\n",
    "        # forcons les valeurs des gradients √† se tenir dans un certain interval si n√©cessaire\n",
    "        if not self.clipping_value is None:\n",
    "\n",
    "            utils.clip_grad_value_(\n",
    "                self.model.parameters(), clip_value=self.clipping_value\n",
    "            )\n",
    "\n",
    "        # mettons √† jour les param√®tres\n",
    "        self.optimizer.step()\n",
    "\n",
    "        # R√©duction du taux d'apprentissage √† chaque it√©ration si n√©cessaire\n",
    "        if not self.lr_scheduling is None:\n",
    "\n",
    "            self.lr_scheduling.step()\n",
    "\n",
    "        # reinitialisation des gradients\n",
    "        self.optimizer.zero_grad()\n",
    "\n",
    "        return preds, loss\n",
    "\n",
    "    def batch_eval(self, input_: torch.Tensor, input_mask: torch.Tensor,\n",
    "                    labels: torch.Tensor, labels_mask: torch.Tensor, pad_token_id: int = 3):\n",
    "\n",
    "        if self.hugging_face: # Nous allons utilise un mod√®le text to text de hugging face (but only for fine-tuning)\n",
    "\n",
    "          # effectuons un passage vers l'avant\n",
    "          outputs = self.model(input_ids = input_, attention_mask = input_mask, \n",
    "                               labels = labels)\n",
    "          # recuperate the predictions and the loss\n",
    "          preds, loss = outputs.logits, outputs.loss\n",
    "        \n",
    "        else:\n",
    "\n",
    "          # effectuons un passage vers l'avant\n",
    "          outputs = self.model(input_, input_mask, labels, labels_mask, pad_token_id = pad_token_id)\n",
    "\n",
    "          # recuperate the predictions and the loss\n",
    "          preds, loss = outputs['preds'], outputs['loss']\n",
    "\n",
    "        return preds, loss\n",
    "\n",
    "    # On a d√©cid√© d'ajouter quelques param√®tres qui ont √©t√© utiles au niveau des enciennes classes d'entra√Ænement\n",
    "    def compile(\n",
    "        self,\n",
    "        train_dataset: Dataset,\n",
    "        test_dataset: Union[Dataset, None] = None,\n",
    "        tokenizer: Union[Tokenizer, None] = None,\n",
    "        train_loader_kwargs: dict = {\"batch_size\": 16},\n",
    "        test_loader_kwargs: dict = {\"batch_size\": 16},\n",
    "        optimizer_kwargs: dict = {\"lr\": 1e-4, \"weight_decay\": 0.4},\n",
    "        model_kwargs: dict = {'class_criterion': nn.CrossEntropyLoss(label_smoothing=0.1)},\n",
    "        lr_scheduler_kwargs: dict = {'d_model': 512, 'lr_warmup_step': 100},\n",
    "        lr_scheduler = None,\n",
    "        gradient_clipping_value: Union[float, torch.Tensor, None] = None,\n",
    "        predict_with_generate: bool = False,\n",
    "        logging_dir: Union[str, None] = None,\n",
    "        hugging_face: bool = False,\n",
    "    ):\n",
    "\n",
    "        if self.seed:\n",
    "            torch.manual_seed(self.seed)\n",
    "\n",
    "        # On devra utiliser la m√©thode 'spread' car on ne connait pas les param√®tres du mod√®le\n",
    "        if isinstance(self.orig_model, nn.Module): # si c'est une instance d'un mod√®le alors pas de param√®tres requis\n",
    "            \n",
    "            self.model = copy.deepcopy(self.orig_model).to(self.device)\n",
    "        \n",
    "        else: # sinon on fournit les param√®tres\n",
    "        \n",
    "            self.model = copy.deepcopy(self.orig_model(**model_kwargs)).to(self.device)\n",
    "\n",
    "        # Initialisation des param√®tres de l'optimiseur\n",
    "        self.optimizer = self.orig_optimizer(\n",
    "            self.model.parameters(), **optimizer_kwargs\n",
    "        )\n",
    "        \n",
    "        # On ajoute un r√©ducteur de taux d'apprentissage si n√©cessaire\n",
    "        self.lr_scheduling = None\n",
    "\n",
    "        if not lr_scheduler is None and self.lr_scheduling is None:\n",
    "\n",
    "            self.lr_scheduling = lr_scheduler(self.optimizer, **lr_scheduler_kwargs)\n",
    "\n",
    "        self.train_loader = DataLoader(\n",
    "            train_dataset,\n",
    "            shuffle=True,\n",
    "            **train_loader_kwargs,\n",
    "        )\n",
    "        \n",
    "        if test_dataset:\n",
    "          self.test_loader = DataLoader(\n",
    "              test_dataset,\n",
    "              shuffle=False,\n",
    "              **test_loader_kwargs,\n",
    "          )\n",
    "        \n",
    "        else:\n",
    "          self.test_loader = None\n",
    "        \n",
    "        # Let us initialize the clipping value to make gradient clipping\n",
    "        self.clipping_value = gradient_clipping_value\n",
    "\n",
    "        # Other parameters for step tracking and metrics\n",
    "        self.compilation = True\n",
    "\n",
    "        self.current_epoch = None\n",
    "\n",
    "        self.best_score = None\n",
    "\n",
    "        self.best_epoch = self.current_epoch\n",
    "\n",
    "        # Recuperate some boolean attributes\n",
    "        self.predict_with_generate = predict_with_generate\n",
    "\n",
    "        # Recuperate tokenizer\n",
    "        self.tokenizer = tokenizer\n",
    "        \n",
    "        # Recuperate the logging directory\n",
    "        self.logging_dir = logging_dir\n",
    "        \n",
    "        # Initialize the metrics\n",
    "        self.metrics = {}\n",
    "\n",
    "        # Initialize the attribute which indicate if the model is from huggingface\n",
    "        self.hugging_face = hugging_face\n",
    "        \n",
    "\n",
    "    def train(\n",
    "        self,\n",
    "        epochs: int = 100,\n",
    "        auto_save: bool = False,\n",
    "        log_step: Union[int, None] = None,\n",
    "        saving_directory: str = \"data/checkpoints/last_checkpoints\",\n",
    "        file_name: str = \"checkpoints\",\n",
    "        save_best: bool = True,\n",
    "        metric_for_best_model: str = 'test_loss',\n",
    "        metric_objective: str = 'minimize'\n",
    "    ):\n",
    "        \"\"\"Entra√Ænement du mod√®le\n",
    "\n",
    "        Args:\n",
    "            epochs (int, optional): Le nombre d'it√©rations. Defaults to 100.\n",
    "            auto_save (bool, optional): Auto-sauvegarde du mod√®le. Defaults to False.\n",
    "            log_step (int, optional): Le nombre d'it√©ration avant d'afficher les performances. Defaults to 1.\n",
    "            saving_directory (str, optional): Le dossier de sauvegarde du mod√®le. Defaults to \"inception_package/storage\".\n",
    "            file_name (str, optional): Le nom du fichier de sauvegarde. Defaults to \"checkpoints\".\n",
    "            save_best (bool): Une varible bool√©enne indiquant si l'on souhaite sauvegarder le meilleur mod√®le. Defaults to True.\n",
    "            metric_for_best_model (str): Le nom de la m√©trique qui permet de choisir le meilleur mod√®le. Defaults to 'eval_loss'.\n",
    "            metric_objective (str): Indique si la m√©trique doit √™tre maximis√©e 'maximize' ou minimis√©e 'minimize'. Defaults to 'minimize'.\n",
    "\n",
    "        Raises:\n",
    "            Exception: L'entra√Ænement implique d'avoir d√©ja initialis√© les param√®tres\n",
    "        \"\"\"\n",
    "\n",
    "        # the file name cannot be \"best_checkpoints\"\n",
    "        assert file_name != \"best_checkpoints\"\n",
    "        \n",
    "        ##################### Error Handling ##################################################\n",
    "        if not self.compilation:\n",
    "            raise Exception(\"You must initialize datasets and\\\n",
    "                            parameters with `compile` method. Make sure you don't forget any of them before \\n \\\n",
    "                                training the model\"\n",
    "            )\n",
    "\n",
    "        ##################### Initializations #################################################\n",
    "\n",
    "        if metric_objective in ['maximize', 'minimize']:\n",
    "\n",
    "          best_score = float('-inf') if metric_objective == 'maximize' else float('inf')\n",
    "\n",
    "        else:\n",
    "\n",
    "          raise ValueError(\"The metric objective can only between 'maximize' or minimize!\")\n",
    "\n",
    "        if not self.best_score is None:\n",
    "\n",
    "          best_score = self.best_score\n",
    "\n",
    "        start_epoch = self.current_epoch if not self.current_epoch is None else 0\n",
    "\n",
    "        ##################### Training ########################################################\n",
    "\n",
    "        modes = ['train', 'test'] \n",
    "        \n",
    "        if self.test_loader is None: modes = ['train']\n",
    "\n",
    "        for epoch in tqdm(range(start_epoch, start_epoch + epochs)):\n",
    "\n",
    "            # Print the actual learning rate\n",
    "            print(f\"For epoch {epoch + 1}: {{Learning rate: {self.lr_scheduling.get_lr()}}}\")\n",
    "            \n",
    "            self.metrics = {}\n",
    "        \n",
    "            for mode in modes:\n",
    "            \n",
    "                with torch.set_grad_enabled(mode == \"train\"):\n",
    "                  \n",
    "                    # Initialize the loss of the current mode\n",
    "                    self.metrics[f'{mode}_loss'] = 0\n",
    "\n",
    "                    # Let us initialize the predictions\n",
    "                    predictions_ = []\n",
    "\n",
    "                    # Let us initialize the labels\n",
    "                    labels_ = []\n",
    "\n",
    "                    if mode == \"train\":\n",
    "\n",
    "                        self.model.train()\n",
    "\n",
    "                        loader = list(iter(self.train_loader))\n",
    "\n",
    "                    else:\n",
    "\n",
    "                        self.model.eval()\n",
    "\n",
    "                        loader = list(iter(self.test_loader))\n",
    "                    \n",
    "                    with trange(len(loader), unit = \"batches\", position = 0, leave = True) as pbar:\n",
    "                      \n",
    "                      for i in pbar:\n",
    "                        \n",
    "                        pbar.set_description(f\"{mode[0].upper() + mode[1:]} batch number {i + 1}\")\n",
    "                        \n",
    "                        data = loader[i]\n",
    "\n",
    "                        input_ = data[0].long().to(self.device)\n",
    "                        \n",
    "                        input_mask = data[1].to(self.device)\n",
    "\n",
    "                        labels = data[2].long().to(self.device)\n",
    "\n",
    "                        if self.hugging_face:\n",
    "\n",
    "                          labels[labels == self.tokenizer.pad_token_id] == -100\n",
    "\n",
    "                        labels_mask = data[3].to(self.device)\n",
    "                        \n",
    "                        # R√©cup√©ration de identifiant token du padding (par d√©faut = 3)\n",
    "                        pad_token_id = 3 if self.tokenizer is None else self.tokenizer.pad_token_id\n",
    "\n",
    "                        preds, loss = (\n",
    "                            self.batch_train(input_, input_mask, labels, labels_mask, pad_token_id)\n",
    "                            if mode == \"train\"\n",
    "                            else self.batch_eval(input_, input_mask, labels, labels_mask, pad_token_id)\n",
    "                        )\n",
    "\n",
    "                        self.metrics[f\"{mode}_loss\"] += loss.item()\n",
    "                        \n",
    "                        # let us add the predictions and labels in the list of predictions and labels after their determinations\n",
    "                        if mode == \"test\":\n",
    "\n",
    "                            if self.predict_with_generate:\n",
    "\n",
    "                              if self.hugging_face:\n",
    "\n",
    "                                  preds = self.model.generate(input_, attention_mask = input_mask)\n",
    "\n",
    "                              else:\n",
    "\n",
    "                                  preds = self.model.generate(input_, input_mask, pad_token_id = pad_token_id)\n",
    "\n",
    "                                  labels = labels.masked_fill_(labels_mask == 0, -100)\n",
    "\n",
    "                            else:\n",
    "\n",
    "                              if self.hugging_face:\n",
    "\n",
    "                                  preds = torch.argmax(preds, dim = -1)\n",
    "\n",
    "                              else:\n",
    "\n",
    "                                  labels = labels.masked_fill_(labels_mask == 0, -100)\n",
    "\n",
    "                            predictions_.extend(preds.detach().cpu().tolist())\n",
    "\n",
    "                            labels_.extend(labels.detach().cpu().tolist())\n",
    "                      \n",
    "            if not self.evaluation is None and mode == 'test':\n",
    "              \n",
    "              self.metrics.update(self.evaluation.compute_metrics((np.array(predictions_), np.array(labels_))))\n",
    "\n",
    "            self.metrics[f\"train_loss\"] = self.metrics[f\"train_loss\"] / len(self.train_loader)\n",
    "            \n",
    "            if not self.test_loader is None:\n",
    "            \n",
    "                self.metrics[f\"test_loss\"] = self.metrics[f\"test_loss\"] / len(self.test_loader)\n",
    "\n",
    "            # for metric in self.metrics:\n",
    "\n",
    "            #    if metric != 'train_loss':\n",
    "\n",
    "            #     self.metrics[metric] = self.metrics[metric] / len(self.test_loader)\n",
    "\n",
    "            # Affichage des m√©triques\n",
    "            if not log_step is None and (epoch + 1) % log_step == 0:\n",
    "\n",
    "              print(f\"\\nMetrics: {self.metrics}\")\n",
    "              \n",
    "              if not self.logging_dir is None:\n",
    "                  \n",
    "                  with SummaryWriter(os.path.join(self.logging_dir, f'version_{self.version}')) as writer:\n",
    "                      \n",
    "                      for metric in self.metrics:\n",
    "                          \n",
    "                        writer.add_scalar(metric, self.metrics[metric], global_step = epoch)\n",
    "                        \n",
    "                        writer.add_scalar(\"global_step\", epoch)\n",
    "\n",
    "            print(\"\\n=============================\\n\")\n",
    "\n",
    "            ##################### Model saving #########################################################\n",
    "\n",
    "            # Save the model in the end of the current epoch. Sauvegarde du mod√®le √† la fin d'une it√©ration\n",
    "            if auto_save:\n",
    "\n",
    "                self.current_epoch = epoch + 1\n",
    "                \n",
    "                if save_best:\n",
    "\n",
    "                  # verify if the current score is best and recuperate it if yes\n",
    "                  if metric_objective == 'maximize':\n",
    "                    \n",
    "                    last_score = best_score < self.metrics[metric_for_best_model]\n",
    "                  \n",
    "                  elif metric_objective == 'minimize':\n",
    "\n",
    "                    last_score = best_score > self.metrics[metric_for_best_model]\n",
    "                  \n",
    "                  else:\n",
    "                      \n",
    "                      raise ValueError(\"The metric objective can only be in ['maximize', 'minimize'] !\")\n",
    "                  \n",
    "                  # recuperate the best score\n",
    "                  if last_score: \n",
    "\n",
    "                    best_score = self.metrics[metric_for_best_model]\n",
    "\n",
    "                    self.best_epoch = self.current_epoch + 1\n",
    "                    \n",
    "                    self.best_score = best_score \n",
    "                    \n",
    "                    self.save(saving_directory, \"best_checkpoints\")\n",
    "                             \n",
    "                self.save(saving_directory, file_name)\n",
    "\n",
    "    # Pour la m√©thode nous allons nous inspirer sur la m√©thode save de l'agent ddpg (RL) que l'on avait cr√©√©e\n",
    "    def save(\n",
    "        self,\n",
    "        directory: str = \"data/checkpoints/last_checkpoints\",\n",
    "        file_name: str = \"checkpoints\"\n",
    "    ):\n",
    "\n",
    "          if not os.path.exists(directory):\n",
    "              os.makedirs(directory)\n",
    "\n",
    "          file_path = os.path.join(directory, f\"{file_name}.pth\")\n",
    "\n",
    "          checkpoints = {\n",
    "              \"model_state_dict\": self.model.state_dict(),\n",
    "              \"optimizer_state_dict\": self.optimizer.state_dict(),\n",
    "              \"current_epoch\": self.current_epoch,\n",
    "              \"metrics\": self.metrics,\n",
    "              \"best_score\": self.best_score,\n",
    "              \"best_epoch\": self.best_epoch,\n",
    "              \"lr_scheduler_state_dict\": self.lr_scheduling.state_dict() if not self.lr_scheduling is None else None\n",
    "          }\n",
    "\n",
    "          torch.save(checkpoints, file_path)\n",
    "\n",
    "          # update metrics and the best score dict\n",
    "          self.metrics['current_epoch'] = self.current_epoch + 1 if not self.current_epoch is None else self.current_epoch\n",
    "\n",
    "          best_score_dict = {\"best_score\": self.best_score, \"best_epoch\": self.best_epoch}\n",
    "\n",
    "          # save the metrics as json file\n",
    "          metrics = json.dumps({'metrics': self.metrics, \"best_performance\": best_score_dict}, indent=4)\n",
    "\n",
    "          with open(os.path.join(directory, f'{file_name}.json'), 'w') as f:\n",
    "\n",
    "            f.write(metrics)   \n",
    "          \n",
    "    # Ainsi que pour la m√©thode load\n",
    "    def load(\n",
    "        self,\n",
    "        directory: str = \"data/checkpoints/last_checkpoints\",\n",
    "        file_name: str = \"checkpoints\",\n",
    "        load_best: bool = False\n",
    "    ):\n",
    "\n",
    "        if load_best: file_name = \"best_checkpoints\"\n",
    "        \n",
    "        file_path = os.path.join(\n",
    "            directory, \n",
    "            f\"{file_name}.pth\"\n",
    "        )\n",
    "\n",
    "        if os.path.exists(file_path):\n",
    "\n",
    "            checkpoints = torch.load(file_path)\n",
    "\n",
    "            self.model.load_state_dict(checkpoints[\"model_state_dict\"])\n",
    "\n",
    "            self.optimizer.load_state_dict(checkpoints[\"optimizer_state_dict\"])\n",
    "\n",
    "            self.current_epoch = checkpoints[\"current_epoch\"]\n",
    "\n",
    "            self.best_score = checkpoints[\"best_score\"]\n",
    "\n",
    "            self.best_epoch = checkpoints[\"best_epoch\"]\n",
    "\n",
    "            if not self.lr_scheduling is None:\n",
    "                \n",
    "                self.lr_scheduling.load_state_dict(checkpoints[\"lr_scheduler_state_dict\"])\n",
    "\n",
    "        else:\n",
    "\n",
    "            raise OSError(\n",
    "                f\"Le fichier {file_path} est introuvable. V√©rifiez si le chemin fourni est correct!\"\n",
    "            )\n",
    "    \n",
    "    def evaluate(self, test_dataset, batch_size: int = 16, loader_kwargs: dict = {}):\n",
    "        \n",
    "        self.model.eval()\n",
    "        \n",
    "        test_loader = list(iter(DataLoader(\n",
    "            test_dataset,\n",
    "            batch_size,\n",
    "            shuffle=False,\n",
    "            **loader_kwargs,\n",
    "        )))\n",
    "        \n",
    "        # Let us initialize the predictions\n",
    "        predictions_ = []\n",
    "\n",
    "        # Let us initialize the labels\n",
    "        labels_ = []\n",
    "        \n",
    "        metrics = {'test_loss': 0.0}\n",
    "\n",
    "        results = {'original_sentences': [], 'translations': [], 'predictions': []}\n",
    "\n",
    "        with torch.no_grad():\n",
    "\n",
    "            with trange(len(test_loader), unit = \"batches\", position = 0, leave = True) as pbar:\n",
    "\n",
    "                for i in pbar:\n",
    "                \n",
    "                    pbar.set_description(f\"Evaluation batch number {i + 1}\")\n",
    "                    \n",
    "                    data = test_loader[i]\n",
    "                                \n",
    "                    input_ = data[0].long().to(self.device)\n",
    "                        \n",
    "                    input_mask = data[1].to(self.device)\n",
    "\n",
    "                    labels = data[2].long().to(self.device)\n",
    "\n",
    "                    if self.hugging_face:\n",
    "\n",
    "                        labels[labels == self.tokenizer.pad_token_id] == -100\n",
    "\n",
    "                    labels_mask = data[3].to(self.device)\n",
    "\n",
    "                    preds, loss = self.batch_eval(input_, input_mask, labels, labels_mask, test_dataset.tokenizer.pad_token_id)\n",
    "\n",
    "                    metrics[f\"test_loss\"] += loss.item()\n",
    "                \n",
    "                    if self.hugging_face:\n",
    "\n",
    "                        preds = self.model.generate(input_, attention_mask = input_mask)\n",
    "                        \n",
    "                        labels_.extend(labels.detach().cpu().tolist())\n",
    "\n",
    "                    else:\n",
    "\n",
    "                        preds = self.model.generate(input_, input_mask, pad_token_id = pad_token_id)\n",
    "\n",
    "                        labels__ = labels.masked_fill_(labels_mask == 0, -100)\n",
    "                        \n",
    "                        labels_.extend(labels__.detach().cpu().tolist())\n",
    "                    \n",
    "                    predictions_.extend(preds.detach().cpu().tolist())\n",
    "                    \n",
    "                    # let us recuperate the original sentences\n",
    "                    results['original_sentences'].extend(test_dataset.tokenizer.batch_decode(input_, skip_special_tokens = True))\n",
    "\n",
    "                    results['translations'].extend(test_dataset.tokenizer.batch_decode(labels, skip_special_tokens = True))\n",
    "\n",
    "                    results['predictions'].extend(test_dataset.tokenizer.batch_decode(preds, skip_special_tokens = True))\n",
    "\n",
    "            if not self.evaluation is None:\n",
    "              \n",
    "                metrics.update(self.evaluation.compute_metrics((np.array(predictions_), np.array(labels_))))\n",
    "\n",
    "            metrics[\"test_loss\"] = metrics[\"test_loss\"] / len(test_loader)\n",
    "\n",
    "            return metrics, pd.DataFrame(results)\n",
    "        \n",
    "            \n",
    "            "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## French to wolof"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Configure dataset üî†"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting wolof-translate/wolof_translate/utils/split_with_valid.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile wolof-translate/wolof_translate/utils/split_with_valid.py\n",
    "\"\"\" This module contains a function which split the data. It will consider adding the validation set\n",
    "\"\"\"\n",
    "from sklearn.model_selection import train_test_split\n",
    "import pandas as pd\n",
    "import os\n",
    "\n",
    "def split_data(random_state: int = 50, data_directory: str = \"data/extractions/new_data\", csv_file: str = \"sentences.csv\"):\n",
    "  \"\"\"Split data between train, validation and test sets\n",
    "\n",
    "  Args:\n",
    "    random_state (int): the seed of the splitting generator. Defaults to 50\n",
    "  \"\"\"\n",
    "  # load the corpora and split into train and test sets\n",
    "  corpora = pd.read_csv(os.path.join(data_directory, csv_file))\n",
    "\n",
    "  train_set, test_set = train_test_split(corpora, test_size=0.1, random_state=random_state)\n",
    "\n",
    "  # let us save the final training set when performing\n",
    "\n",
    "  train_set, valid_set = train_test_split(train_set, test_size=0.1, random_state=random_state)\n",
    "\n",
    "  train_set.to_csv(os.path.join(data_directory, \"final_train_set.csv\"), index=False)\n",
    "\n",
    "  # let us save the sets\n",
    "  train_set.to_csv(os.path.join(data_directory, \"train_set.csv\"), index=False)\n",
    "\n",
    "  valid_set.to_csv(os.path.join(data_directory, \"valid_set.csv\"), index=False)\n",
    "\n",
    "  test_set.to_csv(os.path.join(data_directory, \"test_set.csv\"), index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# recuperate the tokenizer from a json file\n",
    "tokenizer = T5TokenizerFast(tokenizer_file=f\"wolof-translate/wolof_translate/tokenizers/t5_tokenizers/tokenizer_v3.json\")\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following function is used to recuperate the datasets from csv files. The test test is not anymore the validation set which is now part of the train set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def recuperate_datasets(fr_char_p: float, fr_word_p: float):\n",
    "\n",
    "  # Create augmentation to add on French sentences\n",
    "  fr_augmentation = TransformerSequences(nac.KeyboardAug(aug_char_p=fr_char_p, aug_word_p=fr_word_p),\n",
    "                                        remove_mark_space, delete_guillemet_space)\n",
    "\n",
    "  # Recuperate the train dataset\n",
    "  train_dataset_aug = SentenceDataset(f\"data/extractions/new_data/train_set.csv\",\n",
    "                                        tokenizer,\n",
    "                                        truncation = True,\n",
    "                                        cp1_transformer = fr_augmentation)\n",
    "\n",
    "  # Recuperate the validation dataset\n",
    "  valid_dataset = SentenceDataset(f\"data/extractions/new_data/valid_set.csv\",\n",
    "                                        tokenizer,\n",
    "                                        truncation = True)\n",
    "  \n",
    "  # Return the datasets\n",
    "  return train_dataset_aug, valid_dataset"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Configure the evaluation class ‚öôÔ∏è"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will evaluate the predictions with the `bleu` metric. The predictions will be generated like we did when making hyperparameter search."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting wolof-translate/wolof_translate/utils/evaluation.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile wolof-translate/wolof_translate/utils/evaluation.py\n",
    "from tokenizers import Tokenizer\n",
    "from typing import *\n",
    "import numpy as np\n",
    "import evaluate\n",
    "\n",
    "class TranslationEvaluation:\n",
    "    \n",
    "    def __init__(self, \n",
    "                 tokenizer: Tokenizer,\n",
    "                 decoder: Union[Callable, None] = None,\n",
    "                 metric = evaluate.load('sacrebleu'),\n",
    "                 ):\n",
    "        \n",
    "        self.tokenizer = tokenizer\n",
    "        \n",
    "        self.decoder = decoder\n",
    "        \n",
    "        self.metric = metric\n",
    "    \n",
    "    def postprocess_text(self, preds, labels):\n",
    "        \n",
    "        preds = [pred.strip() for pred in preds]\n",
    "        \n",
    "        labels = [[label.strip()] for label in labels]\n",
    "        \n",
    "        return preds, labels\n",
    "\n",
    "    def compute_metrics(self, eval_preds):\n",
    "\n",
    "        preds, labels = eval_preds\n",
    "\n",
    "        if isinstance(preds, tuple):\n",
    "        \n",
    "            preds = preds[0]\n",
    "        \n",
    "        decoded_preds = self.tokenizer.batch_decode(preds, skip_special_tokens=True)\n",
    "\n",
    "        labels = np.where(labels != -100, labels, self.tokenizer.pad_token_id)\n",
    "        \n",
    "        decoded_labels = self.tokenizer.batch_decode(labels, skip_special_tokens=True)\n",
    "\n",
    "        decoded_preds, decoded_labels = self.postprocess_text(decoded_preds, decoded_labels)\n",
    "\n",
    "        result = self.metric.compute(predictions=decoded_preds, references=decoded_labels)\n",
    "        \n",
    "        result = {\"bleu\": result[\"score\"]}\n",
    "\n",
    "        prediction_lens = [np.count_nonzero(pred != self.tokenizer.pad_token_id) for pred in preds]\n",
    "        \n",
    "        result[\"gen_len\"] = np.mean(prediction_lens)\n",
    "        \n",
    "        result = {k: round(v, 4) for k, v in result.items()}\n",
    "        \n",
    "        return result"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let us import the transformer, the data splitter, the learning rate scheduler and the evaluation function, bellow:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from wolof_translate.models.transformers.optimization import TransformerScheduler\n",
    "from wolof_translate.trainers.transformer_trainer import ModelRunner\n",
    "from wolof_translate.utils.evaluation import TranslationEvaluation\n",
    "from wolof_translate.models.transformers.main import Transformer\n",
    "from wolof_translate.utils.split_with_valid import split_data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ---"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let us configure the parameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# let us initialize the hyperparameter configuration\n",
    "config = {\n",
    "    'random_state': 0,\n",
    "    'fr_char_p': 0.02586151531081308,\n",
    "    'fr_word_p': 0.8713619950477987,\n",
    "    'dim_ff': 2092,\n",
    "    'drop_out_rate': 0.1892588927795259,\n",
    "    'label_smoothing': 0.1,\n",
    "    'n_layers': 11,\n",
    "    'n_features': 186,\n",
    "    'learning_rate': 0.004094000716163921,\n",
    "    'weight_decay': 0.5218045932883156,\n",
    "    'batch_size': 8,\n",
    "    'max_epoch': 7663,\n",
    "    'warmup_ratio': 0.05133210495607013,\n",
    "    'bleu': 0.9865,\n",
    "    'model_dir': 'data/checkpoints/fw_custom_v3_checkpoints/',\n",
    "    'new_model_dir': 'data/checkpoints/custom_results_fw_v3/'\n",
    "}\n",
    "\n",
    "# let us initialize the evaluation class\n",
    "evaluation = TranslationEvaluation(tokenizer)\n",
    "\n",
    "# let us initialize the trainer\n",
    "trainer = ModelRunner(model = Transformer, seed = 0, evaluation = evaluation)\n",
    "\n",
    "# split the data\n",
    "split_data(config['random_state'])\n",
    "\n",
    "# recuperate train and test set\n",
    "train_dataset, test_dataset = recuperate_datasets(config['fr_char_p'], \n",
    "                                                    config['fr_word_p'])\n",
    "\n",
    "# initialize the encoder and the decoder layers\n",
    "encoder_layer = nn.TransformerEncoderLayer(512, \n",
    "                                            8,\n",
    "                                            config['dim_ff'],\n",
    "                                            config['drop_out_rate'], batch_first = True)\n",
    "\n",
    "decoder_layer = nn.TransformerDecoderLayer(512, \n",
    "                                            8,\n",
    "                                            config['dim_ff'],\n",
    "                                            config['drop_out_rate'], batch_first = True)\n",
    "\n",
    "# let us initialize the encoder and the decoder\n",
    "encoder = nn.TransformerEncoder(encoder_layer, 6)\n",
    "\n",
    "decoder = nn.TransformerDecoder(decoder_layer, 6)\n",
    "\n",
    "# let us calculate the appropriate warmup steps (let us take a max epoch of 100)\n",
    "length = len(train_dataset)\n",
    "\n",
    "n_steps = length // config['batch_size']\n",
    "\n",
    "num_steps = config['max_epoch'] * n_steps\n",
    "\n",
    "warmup_steps = (config['max_epoch'] * n_steps) * config['warmup_ratio']\n",
    "\n",
    "# Initialize the scheduler parameters\n",
    "scheduler_args = {'num_warmup_steps': warmup_steps, 'num_training_steps': num_steps}\n",
    "\n",
    "# Initialize the transformer parameters\n",
    "model_args = {\n",
    "    'vocab_size': len(tokenizer),\n",
    "    'encoder': encoder,\n",
    "    'decoder': decoder,\n",
    "    'class_criterion': nn.CrossEntropyLoss(label_smoothing = config['label_smoothing']),\n",
    "    'n_poses_max': train_dataset.max_len,\n",
    "    'n_layers': config['n_layers'],\n",
    "    'n_features': config['n_features'],\n",
    "    'max_len': test_dataset.max_len\n",
    "}\n",
    "\n",
    "# Initialize the optimizer parameters\n",
    "optimizer_args = {\n",
    "    'lr': config['learning_rate'],\n",
    "    'weight_decay': config['weight_decay'],\n",
    "    'betas': (0.9, 0.98),\n",
    "}\n",
    "\n",
    "# Initialize the loaders parameters\n",
    "train_loader_args = {'batch_size': config['batch_size']}\n",
    "\n",
    "# Add the datasets and hyperparameters to trainer\n",
    "trainer.compile(train_dataset, test_dataset, tokenizer, train_loader_args,\n",
    "                optimizer_kwargs = optimizer_args, model_kwargs = model_args,\n",
    "                lr_scheduler=get_linear_schedule_with_warmup,\n",
    "                lr_scheduler_kwargs=scheduler_args, \n",
    "                predict_with_generate = True,\n",
    "                logging_dir=\"data/logs/custom_fw_v3\"\n",
    "                )\n",
    "\n",
    "# We will from checkpoints so let us the model\n",
    "trainer.load(config['model_dir'], load_best=True) # Only for the first loading\n",
    "# trainer.load(config['new_model_dir'])\n",
    "\n",
    "        "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let us train the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/7658 [00:00<?, ?it/s]c:\\Users\\Oumar Kane\\AppData\\Local\\pypoetry\\Cache\\virtualenvs\\pytorch1-HleOW5am-py3.10\\lib\\site-packages\\torch\\optim\\lr_scheduler.py:257: UserWarning: To get the last learning rate computed by the scheduler, please use `get_last_lr()`.\n",
      "  warnings.warn(\"To get the last learning rate computed by the scheduler, \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "For epoch 6: {Learning rate: [5.235838752223153e-05]}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train batch number 164: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 164/164 [00:42<00:00,  3.85batches/s]\n",
      "Test batch number 10: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 10/10 [00:18<00:00,  1.84s/batches]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Metrics: {'train_loss': 25.17254326983196, 'test_loss': 22.810916709899903, 'bleu': 0.9354, 'gen_len': 7.0}\n",
      "\n",
      "=============================\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 1/7658 [01:04<137:18:32, 64.56s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "For epoch 7: {Learning rate: [6.283006502667784e-05]}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train batch number 164: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 164/164 [00:41<00:00,  3.94batches/s]\n",
      "Test batch number 10: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 10/10 [00:19<00:00,  1.90s/batches]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Metrics: {'train_loss': 22.694559748579817, 'test_loss': 21.488267707824708, 'bleu': 0.6816, 'gen_len': 7.0}\n",
      "\n",
      "=============================\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 2/7658 [02:07<135:22:03, 63.65s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "For epoch 8: {Learning rate: [7.330174253112416e-05]}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train batch number 164: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 164/164 [00:42<00:00,  3.85batches/s]\n",
      "Test batch number 10: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 10/10 [00:18<00:00,  1.87s/batches]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Metrics: {'train_loss': 21.025487684622043, 'test_loss': 19.922970581054688, 'bleu': 0.8345, 'gen_len': 7.0}\n",
      "\n",
      "=============================\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 3/7658 [03:10<135:07:07, 63.54s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "For epoch 9: {Learning rate: [8.377342003557045e-05]}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train batch number 164: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 164/164 [00:42<00:00,  3.83batches/s]\n",
      "Test batch number 10: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 10/10 [00:19<00:00,  1.95s/batches]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Metrics: {'train_loss': 19.763988721661452, 'test_loss': 20.067963218688966, 'bleu': 0.3727, 'gen_len': 7.0}\n",
      "\n",
      "=============================\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 4/7658 [04:15<135:53:24, 63.91s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "For epoch 10: {Learning rate: [9.424509754001677e-05]}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train batch number 164: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 164/164 [00:48<00:00,  3.41batches/s]\n",
      "Test batch number 10: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 10/10 [00:22<00:00,  2.28s/batches]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Metrics: {'train_loss': 18.89585805520779, 'test_loss': 18.763123416900633, 'bleu': 0.6127, 'gen_len': 8.0}\n",
      "\n",
      "=============================\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 5/7658 [05:28<142:49:56, 67.19s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "For epoch 11: {Learning rate: [0.00010471677504446306]}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train batch number 164: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 164/164 [00:49<00:00,  3.29batches/s]\n",
      "Test batch number 10: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 10/10 [00:22<00:00,  2.22s/batches]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Metrics: {'train_loss': 18.004612713325315, 'test_loss': 17.962421703338624, 'bleu': 0.8481, 'gen_len': 7.0}\n",
      "\n",
      "=============================\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 6/7658 [06:42<147:54:03, 69.58s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "For epoch 12: {Learning rate: [0.00011518845254890938]}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train batch number 164: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 164/164 [00:42<00:00,  3.85batches/s]\n",
      "Test batch number 10: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 10/10 [00:22<00:00,  2.25s/batches]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Metrics: {'train_loss': 17.304877121274064, 'test_loss': 19.071235752105714, 'bleu': 0.75, 'gen_len': 7.0}\n",
      "\n",
      "=============================\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 7/7658 [07:50<146:27:12, 68.91s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "For epoch 13: {Learning rate: [0.00012566013005335568]}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train batch number 164: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 164/164 [00:43<00:00,  3.78batches/s]\n",
      "Test batch number 10: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 10/10 [00:19<00:00,  1.96s/batches]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Metrics: {'train_loss': 16.503681883579347, 'test_loss': 17.05331382751465, 'bleu': 0.63, 'gen_len': 7.0}\n",
      "\n",
      "=============================\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 8/7658 [08:59<146:57:27, 69.16s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "For epoch 14: {Learning rate: [0.000136131807557802]}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train batch number 164: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 164/164 [00:43<00:00,  3.74batches/s]\n",
      "Test batch number 10: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 10/10 [00:19<00:00,  1.91s/batches]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Metrics: {'train_loss': 15.823595762252808, 'test_loss': 18.452527236938476, 'bleu': 0.7102, 'gen_len': 7.0}\n",
      "\n",
      "=============================\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 9/7658 [10:04<144:07:04, 67.83s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "For epoch 15: {Learning rate: [0.00014660348506224832]}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train batch number 164: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 164/164 [00:42<00:00,  3.86batches/s]\n",
      "Test batch number 10: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 10/10 [00:19<00:00,  1.97s/batches]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Metrics: {'train_loss': 15.43495674540357, 'test_loss': 16.518132495880128, 'bleu': 0.431, 'gen_len': 8.0}\n",
      "\n",
      "=============================\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 10/7658 [11:08<141:40:45, 66.69s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "For epoch 16: {Learning rate: [0.00015707516256669463]}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train batch number 164: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 164/164 [00:44<00:00,  3.67batches/s]\n",
      "Test batch number 10: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 10/10 [00:19<00:00,  1.96s/batches]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Metrics: {'train_loss': 14.792796794961138, 'test_loss': 17.613335800170898, 'bleu': 0.7231, 'gen_len': 8.0}\n",
      "\n",
      "=============================\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 11/7658 [12:15<141:32:59, 66.64s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "For epoch 17: {Learning rate: [0.0001675468400711409]}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train batch number 164: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 164/164 [00:45<00:00,  3.63batches/s]\n",
      "Test batch number 10: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 10/10 [00:19<00:00,  1.95s/batches]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Metrics: {'train_loss': 14.484055321391036, 'test_loss': 18.315938663482665, 'bleu': 0.5124, 'gen_len': 7.0}\n",
      "\n",
      "=============================\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 12/7658 [13:22<141:32:44, 66.64s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "For epoch 18: {Learning rate: [0.0001780185175755872]}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train batch number 164: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 164/164 [00:41<00:00,  3.97batches/s]\n",
      "Test batch number 10: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 10/10 [00:21<00:00,  2.18s/batches]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Metrics: {'train_loss': 13.276074848523955, 'test_loss': 16.692030334472655, 'bleu': 0.3781, 'gen_len': 7.7877}\n",
      "\n",
      "=============================\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 13/7658 [14:27<140:29:11, 66.15s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "For epoch 19: {Learning rate: [0.00018849019508003354]}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train batch number 164: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 164/164 [00:43<00:00,  3.80batches/s]\n",
      "Test batch number 10: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 10/10 [00:21<00:00,  2.10s/batches]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Metrics: {'train_loss': 11.67305976879306, 'test_loss': 16.31410150527954, 'bleu': 0.3009, 'gen_len': 8.7055}\n",
      "\n",
      "=============================\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 14/7658 [15:33<140:32:05, 66.19s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "For epoch 20: {Learning rate: [0.00019896187258447985]}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train batch number 164: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 164/164 [00:41<00:00,  3.95batches/s]\n",
      "Test batch number 10: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 10/10 [00:23<00:00,  2.32s/batches]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Metrics: {'train_loss': 11.60483580100827, 'test_loss': 15.374696254730225, 'bleu': 0.3983, 'gen_len': 7.5959}\n",
      "\n",
      "=============================\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 15/7658 [16:41<141:27:19, 66.63s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "For epoch 21: {Learning rate: [0.00020943355008892612]}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train batch number 164: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 164/164 [00:45<00:00,  3.64batches/s]\n",
      "Test batch number 10: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 10/10 [00:21<00:00,  2.16s/batches]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Metrics: {'train_loss': 11.7785655056558, 'test_loss': 16.712066555023192, 'bleu': 0.7059, 'gen_len': 8.9726}\n",
      "\n",
      "=============================\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 16/7658 [17:49<142:34:16, 67.16s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "For epoch 22: {Learning rate: [0.00021990522759337246]}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train batch number 164: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 164/164 [00:42<00:00,  3.88batches/s]\n",
      "Test batch number 10: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 10/10 [00:20<00:00,  2.08s/batches]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Metrics: {'train_loss': 11.041598311284693, 'test_loss': 13.52727108001709, 'bleu': 0.1913, 'gen_len': 6.5959}\n",
      "\n",
      "=============================\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 17/7658 [18:54<141:09:20, 66.50s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "For epoch 23: {Learning rate: [0.00023037690509781876]}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train batch number 164: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 164/164 [00:44<00:00,  3.67batches/s]\n",
      "Test batch number 10: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 10/10 [00:21<00:00,  2.16s/batches]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Metrics: {'train_loss': 10.40609178891996, 'test_loss': 13.572932577133178, 'bleu': 0.4442, 'gen_len': 7.9315}\n",
      "\n",
      "=============================\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 18/7658 [20:02<142:20:10, 67.07s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "For epoch 24: {Learning rate: [0.00024084858260226506]}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train batch number 164: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 164/164 [00:44<00:00,  3.73batches/s]\n",
      "Test batch number 10: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 10/10 [00:23<00:00,  2.32s/batches]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Metrics: {'train_loss': 11.525304282583841, 'test_loss': 15.881868267059327, 'bleu': 0.0881, 'gen_len': 8.726}\n",
      "\n",
      "=============================\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 19/7658 [21:12<143:47:32, 67.76s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "For epoch 25: {Learning rate: [0.00025132026010671137]}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train batch number 164: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 164/164 [00:43<00:00,  3.80batches/s]\n",
      "Test batch number 10: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 10/10 [00:21<00:00,  2.19s/batches]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Metrics: {'train_loss': 10.475133506263175, 'test_loss': 14.993079662322998, 'bleu': 0.4345, 'gen_len': 7.1575}\n",
      "\n",
      "=============================\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 20/7658 [22:20<144:16:26, 68.00s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "For epoch 26: {Learning rate: [0.0002617919376111577]}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train batch number 164: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 164/164 [00:44<00:00,  3.69batches/s]\n",
      "Test batch number 10: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 10/10 [00:23<00:00,  2.31s/batches]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Metrics: {'train_loss': 10.528580456245237, 'test_loss': 14.337573146820068, 'bleu': 0.2915, 'gen_len': 7.9658}\n",
      "\n",
      "=============================\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 21/7658 [23:31<145:52:49, 68.77s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "For epoch 27: {Learning rate: [0.000272263615115604]}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train batch number 164: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 164/164 [00:49<00:00,  3.33batches/s]\n",
      "Test batch number 10: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 10/10 [00:19<00:00,  1.98s/batches]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Metrics: {'train_loss': 11.511181425757524, 'test_loss': 15.158411979675293, 'bleu': 0.3897, 'gen_len': 8.0}\n",
      "\n",
      "=============================\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 22/7658 [24:44<148:24:35, 69.97s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "For epoch 28: {Learning rate: [0.0002827352926200503]}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train batch number 164: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 164/164 [00:44<00:00,  3.68batches/s]\n",
      "Test batch number 10: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 10/10 [00:22<00:00,  2.29s/batches]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Metrics: {'train_loss': 11.783760895089406, 'test_loss': 15.13263521194458, 'bleu': 0.4599, 'gen_len': 7.0}\n",
      "\n",
      "=============================\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 23/7658 [25:53<147:59:40, 69.78s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "For epoch 29: {Learning rate: [0.00029320697012449664]}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train batch number 164: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 164/164 [00:45<00:00,  3.63batches/s]\n",
      "Test batch number 10: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 10/10 [00:22<00:00,  2.23s/batches]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Metrics: {'train_loss': 11.540030284625727, 'test_loss': 15.036835956573487, 'bleu': 0.3755, 'gen_len': 7.0}\n",
      "\n",
      "=============================\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 24/7658 [27:02<147:48:30, 69.70s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "For epoch 30: {Learning rate: [0.0003036786476289429]}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train batch number 164: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 164/164 [00:49<00:00,  3.33batches/s]\n",
      "Test batch number 10: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 10/10 [00:23<00:00,  2.34s/batches]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Metrics: {'train_loss': 10.84920192200963, 'test_loss': 15.045287132263184, 'bleu': 0.2928, 'gen_len': 8.0}\n",
      "\n",
      "=============================\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 25/7658 [28:17<151:02:08, 71.23s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "For epoch 31: {Learning rate: [0.00031415032513338925]}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train batch number 147:  89%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ | 146/164 [00:44<00:05,  3.30batches/s]\n",
      "  0%|          | 25/7658 [29:02<147:48:29, 69.71s/it]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32mc:\\Users\\Oumar Kane\\OneDrive\\Documents\\subject2\\fw_training_custom_transformer_best_model_v3.ipynb Cell 43\u001b[0m in \u001b[0;36m1\n\u001b[1;32m----> <a href='vscode-notebook-cell:/c%3A/Users/Oumar%20Kane/OneDrive/Documents/subject2/fw_training_custom_transformer_best_model_v3.ipynb#X54sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m trainer\u001b[39m.\u001b[39;49mtrain(epochs \u001b[39m=\u001b[39;49m config[\u001b[39m'\u001b[39;49m\u001b[39mmax_epoch\u001b[39;49m\u001b[39m'\u001b[39;49m] \u001b[39m-\u001b[39;49m trainer\u001b[39m.\u001b[39;49mcurrent_epoch, auto_save\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m, metric_for_best_model\u001b[39m=\u001b[39;49m\u001b[39m'\u001b[39;49m\u001b[39mbleu\u001b[39;49m\u001b[39m'\u001b[39;49m, metric_objective\u001b[39m=\u001b[39;49m\u001b[39m'\u001b[39;49m\u001b[39mmaximize\u001b[39;49m\u001b[39m'\u001b[39;49m, log_step\u001b[39m=\u001b[39;49m\u001b[39m1\u001b[39;49m,\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/Oumar%20Kane/OneDrive/Documents/subject2/fw_training_custom_transformer_best_model_v3.ipynb#X54sZmlsZQ%3D%3D?line=1'>2</a>\u001b[0m               saving_directory \u001b[39m=\u001b[39;49m config[\u001b[39m'\u001b[39;49m\u001b[39mnew_model_dir\u001b[39;49m\u001b[39m'\u001b[39;49m])\n",
      "File \u001b[1;32mc:\\users\\oumar kane\\onedrive\\documents\\subject2\\wolof-translate\\wolof_translate\\trainers\\transformer_trainer.py:349\u001b[0m, in \u001b[0;36mModelRunner.train\u001b[1;34m(self, epochs, auto_save, log_step, saving_directory, file_name, save_best, metric_for_best_model, metric_objective)\u001b[0m\n\u001b[0;32m    345\u001b[0m \u001b[39m# R√©cup√©ration de identifiant token du padding (par d√©faut = 3)\u001b[39;00m\n\u001b[0;32m    346\u001b[0m pad_token_id \u001b[39m=\u001b[39m \u001b[39m3\u001b[39m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtokenizer \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39melse\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtokenizer\u001b[39m.\u001b[39mpad_token_id\n\u001b[0;32m    348\u001b[0m preds, loss \u001b[39m=\u001b[39m (\n\u001b[1;32m--> 349\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mbatch_train(input_, input_mask, labels, labels_mask, pad_token_id)\n\u001b[0;32m    350\u001b[0m     \u001b[39mif\u001b[39;00m mode \u001b[39m==\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mtrain\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m    351\u001b[0m     \u001b[39melse\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mbatch_eval(input_, input_mask, labels, labels_mask, pad_token_id)\n\u001b[0;32m    352\u001b[0m )\n\u001b[0;32m    354\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mmetrics[\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m{\u001b[39;00mmode\u001b[39m}\u001b[39;00m\u001b[39m_loss\u001b[39m\u001b[39m\"\u001b[39m] \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m loss\u001b[39m.\u001b[39mitem()\n\u001b[0;32m    356\u001b[0m \u001b[39m# let us add the predictions and labels in the list of predictions and labels after their determinations\u001b[39;00m\n",
      "File \u001b[1;32mc:\\users\\oumar kane\\onedrive\\documents\\subject2\\wolof-translate\\wolof_translate\\trainers\\transformer_trainer.py:105\u001b[0m, in \u001b[0;36mModelRunner.batch_train\u001b[1;34m(self, input_, input_mask, labels, labels_mask, pad_token_id)\u001b[0m\n\u001b[0;32m    102\u001b[0m   preds, loss \u001b[39m=\u001b[39m outputs[\u001b[39m'\u001b[39m\u001b[39mpreds\u001b[39m\u001b[39m'\u001b[39m], outputs[\u001b[39m'\u001b[39m\u001b[39mloss\u001b[39m\u001b[39m'\u001b[39m]\n\u001b[0;32m    104\u001b[0m \u001b[39m# effectuons un passage vers l'arri√®re\u001b[39;00m\n\u001b[1;32m--> 105\u001b[0m loss\u001b[39m.\u001b[39;49mbackward()\n\u001b[0;32m    107\u001b[0m \u001b[39m# forcons les valeurs des gradients √† se tenir dans un certain interval si n√©cessaire\u001b[39;00m\n\u001b[0;32m    108\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mclipping_value \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n",
      "File \u001b[1;32mc:\\Users\\Oumar Kane\\AppData\\Local\\pypoetry\\Cache\\virtualenvs\\pytorch1-HleOW5am-py3.10\\lib\\site-packages\\torch\\_tensor.py:487\u001b[0m, in \u001b[0;36mTensor.backward\u001b[1;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[0;32m    477\u001b[0m \u001b[39mif\u001b[39;00m has_torch_function_unary(\u001b[39mself\u001b[39m):\n\u001b[0;32m    478\u001b[0m     \u001b[39mreturn\u001b[39;00m handle_torch_function(\n\u001b[0;32m    479\u001b[0m         Tensor\u001b[39m.\u001b[39mbackward,\n\u001b[0;32m    480\u001b[0m         (\u001b[39mself\u001b[39m,),\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    485\u001b[0m         inputs\u001b[39m=\u001b[39minputs,\n\u001b[0;32m    486\u001b[0m     )\n\u001b[1;32m--> 487\u001b[0m torch\u001b[39m.\u001b[39;49mautograd\u001b[39m.\u001b[39;49mbackward(\n\u001b[0;32m    488\u001b[0m     \u001b[39mself\u001b[39;49m, gradient, retain_graph, create_graph, inputs\u001b[39m=\u001b[39;49minputs\n\u001b[0;32m    489\u001b[0m )\n",
      "File \u001b[1;32mc:\\Users\\Oumar Kane\\AppData\\Local\\pypoetry\\Cache\\virtualenvs\\pytorch1-HleOW5am-py3.10\\lib\\site-packages\\torch\\autograd\\__init__.py:197\u001b[0m, in \u001b[0;36mbackward\u001b[1;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[0;32m    192\u001b[0m     retain_graph \u001b[39m=\u001b[39m create_graph\n\u001b[0;32m    194\u001b[0m \u001b[39m# The reason we repeat same the comment below is that\u001b[39;00m\n\u001b[0;32m    195\u001b[0m \u001b[39m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[0;32m    196\u001b[0m \u001b[39m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[1;32m--> 197\u001b[0m Variable\u001b[39m.\u001b[39;49m_execution_engine\u001b[39m.\u001b[39;49mrun_backward(  \u001b[39m# Calls into the C++ engine to run the backward pass\u001b[39;49;00m\n\u001b[0;32m    198\u001b[0m     tensors, grad_tensors_, retain_graph, create_graph, inputs,\n\u001b[0;32m    199\u001b[0m     allow_unreachable\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m, accumulate_grad\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m)\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "trainer.train(epochs = config['max_epoch'] - trainer.current_epoch, auto_save=True, metric_for_best_model='bleu', metric_objective='maximize', log_step=1,\n",
    "              saving_directory = config['new_model_dir'])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pytorch1-HleOW5am-py3.10",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
