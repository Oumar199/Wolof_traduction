{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Generate sentences to augment the dataset\n",
    "-------------------------------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this notebook we will try to create a generative-adversarial network which will generate for us new sentences in order to augment the corpora size. We will use the `pytorch-lightning` module to improve the training fastness. \n",
    "\n",
    "- The generative model will understand the following characteristics:\n",
    "    - we will provide the `size of the sequences` to a first model to generate a output of the same size that the given sequences\n",
    "    - the output will be rounded in order to be transmit to the discriminator\n",
    "    - we will use a transformer encoder to the generate sentence ids in place of a simple `RNN` module\n",
    "    - some rules will used on the decoded output in order to obtain the textual sentences\n",
    "\n",
    "- The discriminative model will be used to verify if the output is close to the true sentences:\n",
    "    - ~~we will use for that a pre-trained BERT Model to discriminate of the output~~\n",
    "    - A Multi-Layers Perceptron will be sufficient to discriminate the output\n",
    "    - we will tokenize the GAN inputs with a WordPiece tokenizer without normalizer because we want to generate texts\n",
    "\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Steps"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following steps will be required:\n",
    "\n",
    "- Create a custom dataset to recuperate the sentences\n",
    "- Create the generator\n",
    "- Create the discriminator\n",
    "- ~~Create the GAN~~\n",
    "- Create Trainer \n",
    "- Search for the best parameters\n",
    "- Train the model and evaluate it"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create a custom dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let us use the already trained tokenizer to recuperate the encoded sequences. Note that this dataset is different from that we want to use to train the translation model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Oumar Kane\\AppData\\Local\\pypoetry\\Cache\\virtualenvs\\pytorch1-HleOW5am-py3.10\\lib\\site-packages\\tqdm\\auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "# %%writefile wolof-translate/wolof_translate/data/gan_dataset.py\n",
    "import torch\n",
    "import pandas as pd\n",
    "from torch import nn\n",
    "from tokenizers import Tokenizer\n",
    "from torch.utils.data import Dataset\n",
    "\n",
    "class SentenceDatasetGAN(Dataset):\n",
    "    \n",
    "    def __init__(self, file_path: str, corpus_1: str = \"french_corpus\", corpus_2: str = \"wolof_corpus\",\n",
    "                 tokenizer_path: str = \"wolof-translate/wolof_translate/tokenizers/adverse_tokenizer.json\",\n",
    "                 cls_token: str = \"[CLS]\", sep_token: str = \"[SEP]\", sep: str = \",\", **kwargs):\n",
    "        \n",
    "        # let us recuperate the data frame\n",
    "        self.__sentences = pd.read_csv(file_path, sep=sep, **kwargs)\n",
    "        \n",
    "        # let us recuperate the tokenizer\n",
    "        self.tokenizer = Tokenizer.from_file(tokenizer_path)\n",
    "        \n",
    "        # recuperate the first corpus' sentences\n",
    "        self.__sentences_1 = self.__sentences[corpus_1].to_list()\n",
    "        \n",
    "        # recuperate the second corpus' sentences\n",
    "        self.__sentences_2 = self.__sentences[corpus_2].to_list()\n",
    "        \n",
    "        # recuperate the special tokens\n",
    "        self.cls_token = cls_token\n",
    "        \n",
    "        self.sep_token = sep_token\n",
    "        \n",
    "        # recuperate the length\n",
    "        self.__length = len(self.__sentences_1)\n",
    "        \n",
    "        # recuperate the max id\n",
    "        self.max_id = self.tokenizer.get_vocab_size() - 1\n",
    "        \n",
    "        # let us recuperate the max len\n",
    "        self.max_len = 0\n",
    "        \n",
    "        for i in range(self.__length):\n",
    "            \n",
    "            sentence = f\"{self.cls_token}{self.__sentences_1[i]}{self.sep_token}{self.__sentences_2[i]}{self.sep_token}\"\n",
    "            \n",
    "            encoding = self.tokenizer.encode(sentence)\n",
    "            \n",
    "            if len(encoding.ids) > self.max_len:\n",
    "                \n",
    "                self.max_len = len(encoding.ids)    \n",
    "        \n",
    "    def __getitem__(self, index):\n",
    "        \n",
    "        sentence_1 = self.__sentences_1[index]\n",
    "        \n",
    "        sentence_2 = self.__sentences_2[index]\n",
    "        \n",
    "        # let us create the sentence with special tokens\n",
    "        sentence = f\"{self.cls_token}{sentence_1}{self.sep_token}{sentence_2}{self.sep_token}\"\n",
    "        \n",
    "        # let us encode the sentence\n",
    "        encoding = self.tokenizer.encode(sentence)\n",
    "        \n",
    "        # it will return the padded ids and attention mask\n",
    "        padding = self.max_len - len(encoding.ids)\n",
    "        \n",
    "        ids = torch.tensor(encoding.ids + [0] * padding)\n",
    "        \n",
    "        return ids.float(), (ids > 0).float()\n",
    "        \n",
    "    def __len__(self):\n",
    "        \n",
    "        return self.__length"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The data loader will generate the padded sequences of ids and the attention masks. Let us test it bellow."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = SentenceDatasetGAN(\"data/extractions/new_data/sent_extraction.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ids:\n",
      "tensor([[2.0000e+00, 3.8000e+02, 1.2406e+04,  ..., 0.0000e+00, 0.0000e+00,\n",
      "         0.0000e+00],\n",
      "        [2.0000e+00, 2.1820e+03, 3.9460e+03,  ..., 0.0000e+00, 0.0000e+00,\n",
      "         0.0000e+00],\n",
      "        [2.0000e+00, 5.2900e+02, 6.2050e+03,  ..., 0.0000e+00, 0.0000e+00,\n",
      "         0.0000e+00],\n",
      "        ...,\n",
      "        [2.0000e+00, 1.9990e+03, 1.1000e+01,  ..., 0.0000e+00, 0.0000e+00,\n",
      "         0.0000e+00],\n",
      "        [2.0000e+00, 8.1000e+02, 3.2600e+02,  ..., 0.0000e+00, 0.0000e+00,\n",
      "         0.0000e+00],\n",
      "        [2.0000e+00, 5.6500e+02, 6.6000e+01,  ..., 0.0000e+00, 0.0000e+00,\n",
      "         0.0000e+00]])\n",
      "\n",
      "Mask:\n",
      "tensor([[1., 1., 1.,  ..., 0., 0., 0.],\n",
      "        [1., 1., 1.,  ..., 0., 0., 0.],\n",
      "        [1., 1., 1.,  ..., 0., 0., 0.],\n",
      "        ...,\n",
      "        [1., 1., 1.,  ..., 0., 0., 0.],\n",
      "        [1., 1., 1.,  ..., 0., 0., 0.],\n",
      "        [1., 1., 1.,  ..., 0., 0., 0.]])\n"
     ]
    }
   ],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "\n",
    "# let us generate 10 sentences\n",
    "ids, mask = next(iter(DataLoader(dataset, batch_size=10, shuffle=True)))\n",
    "\n",
    "print(\"Ids:\")\n",
    "print(ids)\n",
    "\n",
    "print(\"\\nMask:\")\n",
    "print(mask)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generator"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The generator use a transformer encoder with a d_model, a number of layers, a number of features and activation function specified as arguments. We can also specify a drop out. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %%writefile wolof-translate/wolof_translate/models/generative_model.py\n",
    "from torch.nn import functional as F\n",
    "from custom_rnn.transformers.add_position import PositionalEncoding\n",
    "from typing import *\n",
    "from torch import nn\n",
    "\n",
    "class SentenceGenerator(nn.Module):\n",
    "    \n",
    "    def __init__(self, \n",
    "                 output_size: int,\n",
    "                 d_model: int = 512,\n",
    "                 latent_dim: Union[int, None] = None,\n",
    "                 num_features: int = 2048,\n",
    "                 n_heads: int = 8,\n",
    "                 dropout: float = 0.0,\n",
    "                 activation = F.relu,\n",
    "                 num_layers: int = 6,\n",
    "                 min: int = 0, max: int = 100):\n",
    "        \n",
    "        super(SentenceGenerator, self).__init__()\n",
    "        \n",
    "        self.min, self.max = min, max\n",
    "        \n",
    "        self.d_model = d_model\n",
    "        \n",
    "        self.n_heads = n_heads\n",
    "        \n",
    "        self.dropout = dropout\n",
    "        \n",
    "        self.activation = activation\n",
    "        \n",
    "        \n",
    "        self.num_layers = num_layers\n",
    "        \n",
    "        self.num_features = num_features\n",
    "        \n",
    "        self.output_size = output_size\n",
    "        \n",
    "        self.latent_dim = latent_dim if not latent_dim is None else self.output_size\n",
    "        \n",
    "        \n",
    "        self.pe = PositionalEncoding(self.latent_dim, self.d_model)\n",
    "        \n",
    "        self.encoder_layer = nn.TransformerEncoderLayer(self.d_model,\n",
    "                                                        self.n_heads,\n",
    "                                                        self.num_features,\n",
    "                                                        self.dropout,\n",
    "                                                        self.activation,\n",
    "                                                        batch_first=True)\n",
    "        \n",
    "        self.encoder = nn.TransformerEncoder(self.encoder_layer, self.num_layers)\n",
    "        \n",
    "        self.output_layer = nn.Linear(self.d_model * self.latent_dim, output_size)\n",
    "        \n",
    "    def forward(self, input_, attention_mask):\n",
    "        \n",
    "        out = self.pe(input_).type_as(next(self.encoder.parameters()))\n",
    "        \n",
    "        out = self.encoder(out, src_key_padding_mask = attention_mask).view(-1, self.latent_dim * self.d_model)\n",
    "            \n",
    "        out = torch.clip(self.output_layer(out), self.min, self.max).round()\n",
    "        \n",
    "        return out\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let us test our generative model with dummy input."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "generative_model = SentenceGenerator(output_size=dataset.max_len)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([10, 379])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# the output must be rounded to the nearest integer and clipped between the lowest and the highest ids\n",
    "g_output = generative_model(torch.randn((10, 379, 512)), mask)\n",
    "\n",
    "g_output.size()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Discriminator"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let us create a new discriminator model different from the BERT Model. It will take output of the generator without converting it to a long tensor since doing so will make us losing the gradient."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %%writefile wolof-translate/wolof_translate/models/discriminative_model.py\n",
    "from torch.nn import functional as F\n",
    "from typing import *\n",
    "from torch import nn\n",
    "\n",
    "class DiscriminatorSequence(nn.Module):\n",
    "    \n",
    "    def __init__(self, \n",
    "                 input_dim,\n",
    "                 num_features,\n",
    "                 negative_slope: float = 0.01,\n",
    "                 drop_out: float = 0.0,\n",
    "                 eps: float = 0.00001,\n",
    "                 momentum: float = 0.1):\n",
    "        \n",
    "        super(DiscriminatorSequence, self).__init__()\n",
    "        \n",
    "        self.batch_norm = nn.BatchNorm1d(input_dim, eps, momentum)\n",
    "        \n",
    "        self.linear = nn.Linear(input_dim, num_features)\n",
    "        \n",
    "        self.drop_out = nn.Dropout1d(drop_out)\n",
    "        \n",
    "        self.activation = nn.LeakyReLU(negative_slope)\n",
    "        \n",
    "        \n",
    "    def forward(self, input_):\n",
    "        \n",
    "        out = self.batch_norm(input_)\n",
    "        \n",
    "        out = self.activation(self.drop_out(self.linear(out)))\n",
    "        \n",
    "        return out\n",
    "\n",
    "class SentenceDiscriminator(nn.Module):\n",
    "    \n",
    "    def __init__(self, \n",
    "                 input_dim: int,\n",
    "                 num_features: Union[int, List] = 300,\n",
    "                 num_layers: int = 5,\n",
    "                 negative_slope: float = 0.01,\n",
    "                 drop_out: float = 0.0,\n",
    "                 eps: float = 0.00001,\n",
    "                 momentum: float = 0.1):\n",
    "        \n",
    "        super(SentenceDiscriminator, self).__init__()\n",
    "        \n",
    "        self.input_dim = input_dim\n",
    "        \n",
    "        self.num_features = [num_features] * num_layers if type(num_features) is int else num_features\n",
    "        \n",
    "        assert len(self.num_features) == num_layers\n",
    "        \n",
    "        self.num_layers = num_layers\n",
    "        \n",
    "        self.sequences = nn.ModuleList()\n",
    "        \n",
    "        self.sequences.append(DiscriminatorSequence(input_dim, self.num_features[0], negative_slope, drop_out, eps, momentum))\n",
    "        \n",
    "        for l in range(1, num_layers):\n",
    "            \n",
    "            self.sequences.append(DiscriminatorSequence(self.num_features[l-1], self.num_features[l], negative_slope, drop_out, eps, momentum))\n",
    "        \n",
    "        self.output_layer = nn.Linear(self.num_features[-1], 1)\n",
    "        \n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "        \n",
    "    def forward(self, input_: torch.Tensor):\n",
    "        \n",
    "        out = input_\n",
    "        \n",
    "        for sequence in self.sequences:\n",
    "            \n",
    "            out = sequence(out)\n",
    "        \n",
    "        out = self.sigmoid(self.output_layer(out))\n",
    "        \n",
    "        return out\n",
    "        "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Create a new trainer in place of pytorch lightning"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We want to create a new runner class in order to make grid search and find the best hyper parameters for generating texts. Let us import the runner bellow with some other handy functions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ok_transfer_learning.utils.gan_runner1 import SentenceGANRunner\n",
    "from ok_transfer_learning.utils.find_keys import find_ghistory_key\n",
    "import numpy as np\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Hyper parameters search"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let us initialize the runner."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "gan_runner = SentenceGANRunner(SentenceGenerator(dataset.max_len, d_model=100, num_features=1024, num_layers=3, n_heads=5), \n",
    "                               SentenceDiscriminator(dataset.max_len, num_layers=3), dataset, seed=50)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let us initialize the hyper parameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "hparams = {\n",
    "    'g_lr': np.linspace(1e-5, 0.1, 10).round(5).tolist(),\n",
    "    'd_lr': np.linspace(1e-5, 0.1, 10).round(5).tolist()\n",
    "}"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let us search for the best parameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2/2 [00:39<00:00, 19.54s/it]\n",
      "100%|██████████| 2/2 [00:37<00:00, 18.93s/it]\n",
      "100%|██████████| 2/2 [00:37<00:00, 18.72s/it]\n",
      "100%|██████████| 2/2 [00:37<00:00, 18.93s/it]\n",
      "100%|██████████| 2/2 [00:37<00:00, 18.71s/it]\n",
      "100%|██████████| 2/2 [00:38<00:00, 19.10s/it]\n",
      "100%|██████████| 2/2 [00:37<00:00, 18.91s/it]\n",
      "100%|██████████| 2/2 [00:38<00:00, 19.01s/it]\n",
      "100%|██████████| 2/2 [00:37<00:00, 18.73s/it]\n",
      "100%|██████████| 2/2 [00:38<00:00, 19.37s/it]\n"
     ]
    }
   ],
   "source": [
    "gan_runner.make_grid_search(hparams, 10,\n",
    "                            2, loader_kwargs={\"batch_size\": 2}\n",
    "                            )"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let us recuperate the key for `'g_lr': 0.01112, 'd_lr': 0.00001`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "key = find_ghistory_key(gan_runner.grid_history, {'g_lr': 0.01112, 'd_lr': 0.00001})"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training the model"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let us compile with the retained model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "gan_runner.compile(loader_kwargs={\"batch_size\": 2},\n",
    "                #    grid_search_key=key\n",
    "                   )"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let us save the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# gan_runner.save(\"data/checkpoints/generator/\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let us load the last saved model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "gan_runner.load(\"data/checkpoints/generator/\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let us train the retained model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  4%|▍         | 2/50 [02:03<43:52, 54.84s/it]   "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Generated sentences at epoch 347\n",
      "Sentence 0: \n",
      "Sentence 1: \n",
      "Sentence 2: \n",
      "Sentence 3: \n",
      "Sentence 4: \n",
      "Sentence 5: \n",
      "Sentence 6: \n",
      "Sentence 7: \n",
      "Sentence 8: \n",
      "Sentence 9: \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 10%|█         | 5/50 [02:53<18:33, 24.74s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Generated sentences at epoch 350\n",
      "Sentence 0: \n",
      "Sentence 1: \n",
      "Sentence 2: \n",
      "Sentence 3: \n",
      "Sentence 4: \n",
      "Sentence 5: \n",
      "Sentence 6: \n",
      "Sentence 7: \n",
      "Sentence 8: \n",
      "Sentence 9: \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 16%|█▌        | 8/50 [03:44<13:28, 19.26s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Generated sentences at epoch 353\n",
      "Sentence 0: \n",
      "Sentence 1: \n",
      "Sentence 2: \n",
      "Sentence 3: \n",
      "Sentence 4: \n",
      "Sentence 5: \n",
      "Sentence 6: \n",
      "Sentence 7: \n",
      "Sentence 8: \n",
      "Sentence 9: \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 22%|██▏       | 11/50 [04:36<11:40, 17.97s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Generated sentences at epoch 356\n",
      "Sentence 0: \n",
      "Sentence 1: \n",
      "Sentence 2: \n",
      "Sentence 3: \n",
      "Sentence 4: \n",
      "Sentence 5: \n",
      "Sentence 6: \n",
      "Sentence 7: \n",
      "Sentence 8: \n",
      "Sentence 9: \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 28%|██▊       | 14/50 [05:27<10:28, 17.47s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Generated sentences at epoch 359\n",
      "Sentence 0: \n",
      "Sentence 1: \n",
      "Sentence 2: \n",
      "Sentence 3: \n",
      "Sentence 4: \n",
      "Sentence 5: \n",
      "Sentence 6: \n",
      "Sentence 7: \n",
      "Sentence 8: \n",
      "Sentence 9: \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 34%|███▍      | 17/50 [06:36<11:57, 21.73s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Generated sentences at epoch 362\n",
      "Sentence 0: \n",
      "Sentence 1: \n",
      "Sentence 2: \n",
      "Sentence 3: \n",
      "Sentence 4: \n",
      "Sentence 5: \n",
      "Sentence 6: \n",
      "Sentence 7: \n",
      "Sentence 8: \n",
      "Sentence 9: \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 40%|████      | 20/50 [07:52<11:48, 23.62s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Generated sentences at epoch 365\n",
      "Sentence 0: \n",
      "Sentence 1: \n",
      "Sentence 2: \n",
      "Sentence 3: \n",
      "Sentence 4: \n",
      "Sentence 5: \n",
      "Sentence 6: \n",
      "Sentence 7: \n",
      "Sentence 8: \n",
      "Sentence 9: \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 46%|████▌     | 23/50 [08:58<10:06, 22.45s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Generated sentences at epoch 368\n",
      "Sentence 0: \n",
      "Sentence 1: \n",
      "Sentence 2: \n",
      "Sentence 3: \n",
      "Sentence 4: \n",
      "Sentence 5: \n",
      "Sentence 6: \n",
      "Sentence 7: \n",
      "Sentence 8: \n",
      "Sentence 9: \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 52%|█████▏    | 26/50 [10:03<08:39, 21.63s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Generated sentences at epoch 371\n",
      "Sentence 0: \n",
      "Sentence 1: \n",
      "Sentence 2: \n",
      "Sentence 3: \n",
      "Sentence 4: \n",
      "Sentence 5: \n",
      "Sentence 6: \n",
      "Sentence 7: \n",
      "Sentence 8: \n",
      "Sentence 9: \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 58%|█████▊    | 29/50 [11:01<07:03, 20.15s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Generated sentences at epoch 374\n",
      "Sentence 0: \n",
      "Sentence 1: \n",
      "Sentence 2: \n",
      "Sentence 3: \n",
      "Sentence 4: \n",
      "Sentence 5: \n",
      "Sentence 6: \n",
      "Sentence 7: \n",
      "Sentence 8: \n",
      "Sentence 9: \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 64%|██████▍   | 32/50 [12:00<05:55, 19.76s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Generated sentences at epoch 377\n",
      "Sentence 0: \n",
      "Sentence 1: \n",
      "Sentence 2: \n",
      "Sentence 3: \n",
      "Sentence 4: \n",
      "Sentence 5: \n",
      "Sentence 6: \n",
      "Sentence 7: \n",
      "Sentence 8: \n",
      "Sentence 9: \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 70%|███████   | 35/50 [12:59<04:56, 19.74s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Generated sentences at epoch 380\n",
      "Sentence 0: \n",
      "Sentence 1: \n",
      "Sentence 2: \n",
      "Sentence 3: \n",
      "Sentence 4: \n",
      "Sentence 5: \n",
      "Sentence 6: \n",
      "Sentence 7: \n",
      "Sentence 8: \n",
      "Sentence 9: \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 76%|███████▌  | 38/50 [13:56<03:48, 19.07s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Generated sentences at epoch 383\n",
      "Sentence 0: \n",
      "Sentence 1: \n",
      "Sentence 2: \n",
      "Sentence 3: \n",
      "Sentence 4: \n",
      "Sentence 5: \n",
      "Sentence 6: \n",
      "Sentence 7: \n",
      "Sentence 8: \n",
      "Sentence 9: \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 82%|████████▏ | 41/50 [16:52<08:15, 55.06s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Generated sentences at epoch 386\n",
      "Sentence 0: \n",
      "Sentence 1: \n",
      "Sentence 2: \n",
      "Sentence 3: \n",
      "Sentence 4: \n",
      "Sentence 5: \n",
      "Sentence 6: \n",
      "Sentence 7: \n",
      "Sentence 8: \n",
      "Sentence 9: \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 88%|████████▊ | 44/50 [28:43<16:23, 163.85s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Generated sentences at epoch 389\n",
      "Sentence 0: \n",
      "Sentence 1: \n",
      "Sentence 2: \n",
      "Sentence 3: \n",
      "Sentence 4: \n",
      "Sentence 5: \n",
      "Sentence 6: \n",
      "Sentence 7: \n",
      "Sentence 8: \n",
      "Sentence 9: \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 94%|█████████▍| 47/50 [29:41<03:26, 68.86s/it] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Generated sentences at epoch 392\n",
      "Sentence 0: \n",
      "Sentence 1: \n",
      "Sentence 2: \n",
      "Sentence 3: \n",
      "Sentence 4: \n",
      "Sentence 5: \n",
      "Sentence 6: \n",
      "Sentence 7: \n",
      "Sentence 8: \n",
      "Sentence 9: \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 50/50 [30:39<00:00, 36.80s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Generated sentences at epoch 395\n",
      "Sentence 0: \n",
      "Sentence 1: \n",
      "Sentence 2: \n",
      "Sentence 3: \n",
      "Sentence 4: \n",
      "Sentence 5: \n",
      "Sentence 6: \n",
      "Sentence 7: \n",
      "Sentence 8: \n",
      "Sentence 9: \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "gan_runner.train(50, auto_save=True, saving_directory=\"data/checkpoints/generator/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "gan_runner.save(\"data/checkpoints/generator/\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pytorch1-HleOW5am-py3.10",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
