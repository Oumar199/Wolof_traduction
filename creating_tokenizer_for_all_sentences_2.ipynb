{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Creating a Unigram Tokenizer on the version 5 of the corpora\n",
    "----------------------------------\n",
    "We added new sentences extracted from the book <i style=\"color: cyan\">Grammaire de Wolof Moderne</i> by Pathé Diagne to the original corpora to obtain the fifth version of it."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The process is almost the same as in [processing_4](text_processing4.ipynb) excepted that we will create another custom dataset for the custom transformer model and identify with a box plot the range of the maximum length of the sequences in order to tune the `max_len` parameter provided to the custom dataset."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let us import the necessary libraries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Oumar Kane\\AppData\\Local\\pypoetry\\Cache\\virtualenvs\\pytorch1-HleOW5am-py3.10\\lib\\site-packages\\tqdm\\auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "# for creating the tokenizer\n",
    "from tokenizers import (\n",
    "    decoders,\n",
    "    models,\n",
    "    pre_tokenizers,\n",
    "    processors,\n",
    "    trainers,\n",
    "    Tokenizer,\n",
    "    normalizers\n",
    ")\n",
    "\n",
    "# for importing and manipulating the sentences\n",
    "import pandas as pd\n",
    "import random\n",
    "\n",
    "# for plotting the box plot of the sequence lengths\n",
    "import plotly.express as px\n",
    "\n",
    "# for loading sentences with the custom dataset\n",
    "from torch.utils.data import DataLoader"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Load dataset and create generator"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will create one tokenizer for both of the French and Wolof corpora because the `T5` model understand only one embedding layer. So we must create one generator for both of the French and Wolof corpora. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load sentences\n",
    "sentences = pd.read_csv(\"data/extractions/new_data/corpora_v5.csv\")\n",
    "\n",
    "# initialize a batch size\n",
    "BATCH_SIZE = 400\n",
    "\n",
    "# create generators (for the corpora)\n",
    "def generate_sents():\n",
    "    \n",
    "    # recuperate the sentences\n",
    "    french_sents = sentences['french'].to_list() \n",
    "    \n",
    "    wolof_sents = sentences['wolof'].to_list() \n",
    "    \n",
    "    sents = french_sents + wolof_sents\n",
    "    \n",
    "    for i in range(1, len(sents), BATCH_SIZE):\n",
    "        \n",
    "        yield sents[i:i+BATCH_SIZE]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Initialize the tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = Tokenizer(models.Unigram())"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Add normalizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer.normalizer = normalizers.Replace(\" {2,}\", \" \")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Configure the pre-tokenizers"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will use the Metaspace pre-tokenizer which separates the words considering the spaces between them. It will replace the space by a character (by default the underscore \"_\")."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer.pre_tokenizer = pre_tokenizers.Metaspace()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Initialize the trainers"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will provide all of the necessary special tokens to the Trainer. \n",
    "\n",
    "**Notice that a sentence can be a group of words separated by ending marks and not only one group of words. Then we can, for example, tokenize the following sentences**: `<sep>sentence1.sentence2.sentence3<cls>` **or** `<sep>sentence1.<sep>sentence2.<cls>`. **But, the second sentence is composed of two separate groups. Then the two sentences will have different type ids.** "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "special_tokens = [\"<cls>\", \"<sep>\", \"<unk>\", \"<pad>\", \"<mask>\", \"<s>\", \"</s>\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer = trainers.UnigramTrainer(special_tokens=special_tokens, unk_token = \"<unk>\") # let us take the default vocab size"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Train the tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer.train_from_iterator(generate_sents(), trainer)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let us print the vocab size."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of tokens: 8000\n"
     ]
    }
   ],
   "source": [
    "print(f\"Number of tokens: {tokenizer.get_vocab_size()}\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Initialize the post-processor"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can not need the TemplateProcessor to train our corpora in a Sequence To Sequence model, but we will add it to our tokenizer. We can use it for another type of model. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 1\n"
     ]
    }
   ],
   "source": [
    "# let us recuperate the sep and cls ids\n",
    "cls_token_id = tokenizer.token_to_id(\"<cls>\")\n",
    "\n",
    "sep_token_id = tokenizer.token_to_id(\"<sep>\")\n",
    "\n",
    "print(cls_token_id, sep_token_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize the post processor\n",
    "tokenizer.post_process = processors.TemplateProcessing(\n",
    "    single=\"$A:0 <sep>:0 <cls>:2\",\n",
    "    pair=\"$A:0 <sep>:0 $B:1 <sep>:1 <cls>:2\",\n",
    "    special_tokens=[(\"<sep>\", sep_token_id), (\"<cls>\", cls_token_id)]\n",
    ")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Initialize the decoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer.decoder = decoders.Metaspace()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Save the tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer.save(\"wolof-translate/wolof_translate/tokenizers/t5_tokenizers/tokenizer_v4.json\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Make a little example"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let us recuperate random sentences from the corpora and tokenize them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "random.seed(200)\n",
    "\n",
    "french_sentence = random.choice(sentences['french'])\n",
    "\n",
    "wolof_sentence = random.choice(sentences['wolof'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"Il y a dans cette photo unique quelque chose de froid, presque austère, qui évoque l'empire instauré par les hommes blancs en Afrique, mélange de camp militaire, de pelouse anglaise et de puissance naturelle que je n'ai retrouvé, longtemps après, que dans la zone du canal de Panama.\""
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# print the french sentence\n",
    "french_sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Yaakaaroon na ni, mag ñi du ñu ci agg foofu te it xale yi nekkunu fi woon.'"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# print the wolof sentence\n",
    "wolof_sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "French tokens\n",
      "['▁Il', '▁y', '▁a', '▁dans', '▁c', 'ette', '▁photo', '▁unique', '▁quelque', '▁chose', '▁de', '▁froid', ',', '▁pres', 'que', '▁au', 'stère', ',', '▁qui', '▁év', 'oque', \"▁l'\", 'empire', '▁insta', 'uré', '▁par', '▁les', '▁hommes', '▁blancs', '▁en', '▁Afrique', ',', '▁mé', 'l', 'ange', '▁de', '▁camp', '▁militaire', ',', '▁de', '▁pelouse', '▁anglaise', '▁et', '▁de', '▁puissance', '▁naturel', 'le', '▁que', '▁je', \"▁n'ai\", '▁retrouvé', ',', '▁longtemps', '▁après', ',', '▁que', '▁dans', '▁la', '▁', 'z', 'on', 'e', '▁du', '▁canal', '▁de', '▁Panama', '.']\n",
      "French ids\n",
      "[62, 211, 18, 44, 103, 168, 1077, 1801, 400, 345, 10, 1254, 7, 992, 123, 71, 6458, 7, 31, 3937, 3876, 91, 2477, 6592, 3897, 74, 20, 601, 2123, 47, 456, 7, 1950, 49, 2751, 10, 3194, 2914, 7, 10, 3020, 1527, 28, 10, 1311, 5462, 101, 32, 77, 840, 4634, 7, 1331, 455, 7, 32, 44, 12, 8, 1142, 203, 13, 33, 4018, 10, 4327, 9]\n"
     ]
    }
   ],
   "source": [
    "french_encoding = tokenizer.encode(french_sentence)\n",
    "\n",
    "print(\"French tokens\")\n",
    "print(french_encoding.tokens)\n",
    "\n",
    "print(\"French ids\")\n",
    "print(french_encoding.ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wolof tokens\n",
      "['▁Yaakaar', 'oon', '▁na', '▁ni', ',', '▁mag', '▁ñi', '▁du', '▁ñu', '▁ci', '▁agg', '▁foofu', '▁te', '▁it', '▁xale', '▁yi', '▁nekk', 'unu', '▁fi', '▁woon', '.']\n",
      "Wolof ids\n",
      "[1662, 75, 37, 26, 7, 261, 204, 33, 46, 14, 3507, 434, 55, 292, 194, 40, 171, 3817, 178, 56, 9]\n"
     ]
    }
   ],
   "source": [
    "wolof_encoding = tokenizer.encode(wolof_sentence)\n",
    "\n",
    "print(\"Wolof tokens\")\n",
    "print(wolof_encoding.tokens)\n",
    "\n",
    "print(\"Wolof ids\")\n",
    "print(wolof_encoding.ids)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Creating the T5 custom dataset for the new sentences"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have two possibilities to use the tokenizer for fine-tuning a T5 model. "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- We can use the `PreTrainedTokenizerFast` class for which we will provide the different special tokens."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import PreTrainedTokenizerFast\n",
    "\n",
    "wrapped_tokenizer1 = PreTrainedTokenizerFast(\n",
    "    tokenizer_object=tokenizer,\n",
    "    bos_token=\"<s>\",\n",
    "    eos_token=\"</s>\",\n",
    "    unk_token=\"<unk>\",\n",
    "    pad_token=\"<pad>\",\n",
    "    cls_token=\"<cls>\",\n",
    "    sep_token=\"<sep>\",\n",
    "    mask_token=\"<mask>\",\n",
    "    padding_side=\"left\",\n",
    ")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Or give directly the tokenizer to the `T5TokenizerFast` class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import T5TokenizerFast\n",
    "\n",
    "wrapped_tokenizer2 = T5TokenizerFast(\n",
    "    tokenizer_object=tokenizer\n",
    ")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let us give them the sentences that we use as example. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input_ids': [62, 211, 18, 44, 103, 168, 1077, 1801, 400, 345, 10, 1254, 7, 992, 123], 'token_type_ids': [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]}"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fr_encoding = wrapped_tokenizer1(french_sentence, max_length=15, padding='max_length', truncation=True)\n",
    "\n",
    "fr_encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input_ids': [1662, 75, 37, 26, 7, 261, 204, 33, 46, 14, 3507, 434, 55, 292, 194], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]}"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wf_encoding = wrapped_tokenizer2(wolof_sentence, max_length=15, padding='max_length', truncation=True)\n",
    "\n",
    "wf_encoding"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let us decode the wolof sentence."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Yaakaaroon na ni, mag ñi du ñu ci agg foofu te it xale'"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wrapped_tokenizer1.decode(wf_encoding.input_ids, skip_special_tokens=True)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see that the `T5Tokenizer` add padding to the right side of the sequence while the `PretrainedTokenizer` add the padding to the left side. We can change the padding side from the settings. But, for the next steps, let us directly use the `T5Tokenizer`."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Note that we can augment the sentences when generating them like we did when using the `GPT2 model`.** See the following notebook, [augmentation](text_augmentation.ipynb), for discussion on the augmentation method that we will use. And for a more clear explanation of the augmentation methods in NLP tasks and training, look at the following article from the web [augment_or_not](https://direct.mit.edu/coli/article/48/1/5/108844/To-Augment-or-Not-to-Augment-A-Comparative-Study)."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let us verify, before creating the custom dataset, the max length that we can get from the corpora' tokens without considering the augmentation. We must for that trace the box plot of the lengths and identify the range in which we will sample the max length of the sequences."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "length = []\n",
    "\n",
    "for sent in sentences['french'].to_list() + sentences['wolof'].to_list():\n",
    "    \n",
    "    len_ids = len(wrapped_tokenizer2(sent).input_ids)\n",
    "    \n",
    "    length.append(len_ids)\n",
    "\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.plotly.v1+json": {
       "config": {
        "plotlyServerURL": "https://plot.ly"
       },
       "data": [
        {
         "alignmentgroup": "True",
         "hovertemplate": "x=%{x}<extra></extra>",
         "legendgroup": "",
         "marker": {
          "color": "#636efa"
         },
         "name": "",
         "notched": false,
         "offsetgroup": "",
         "orientation": "h",
         "showlegend": false,
         "type": "box",
         "x": [
          14,
          18,
          88,
          52,
          32,
          9,
          18,
          12,
          19,
          10,
          26,
          27,
          14,
          22,
          67,
          233,
          18,
          54,
          19,
          12,
          29,
          24,
          29,
          47,
          11,
          32,
          42,
          42,
          19,
          26,
          19,
          17,
          9,
          104,
          32,
          18,
          17,
          25,
          72,
          130,
          59,
          40,
          202,
          12,
          26,
          85,
          96,
          48,
          35,
          96,
          15,
          12,
          39,
          22,
          26,
          6,
          22,
          92,
          63,
          26,
          146,
          41,
          49,
          15,
          44,
          26,
          89,
          147,
          22,
          47,
          39,
          69,
          19,
          57,
          108,
          4,
          20,
          26,
          24,
          36,
          14,
          51,
          31,
          37,
          20,
          19,
          32,
          63,
          243,
          48,
          22,
          94,
          28,
          67,
          25,
          21,
          30,
          119,
          30,
          112,
          23,
          12,
          15,
          85,
          115,
          17,
          21,
          62,
          41,
          102,
          13,
          19,
          19,
          15,
          14,
          29,
          90,
          42,
          6,
          30,
          5,
          33,
          35,
          7,
          8,
          58,
          13,
          50,
          21,
          18,
          41,
          24,
          24,
          32,
          65,
          24,
          41,
          67,
          31,
          76,
          30,
          38,
          12,
          25,
          6,
          11,
          36,
          43,
          25,
          24,
          34,
          125,
          29,
          21,
          74,
          8,
          49,
          24,
          111,
          31,
          34,
          35,
          29,
          53,
          111,
          33,
          65,
          20,
          69,
          15,
          14,
          33,
          36,
          32,
          13,
          32,
          54,
          13,
          22,
          19,
          63,
          58,
          27,
          9,
          6,
          65,
          58,
          100,
          57,
          24,
          23,
          20,
          108,
          32,
          218,
          43,
          13,
          117,
          40,
          47,
          50,
          28,
          13,
          19,
          72,
          9,
          28,
          73,
          30,
          6,
          34,
          25,
          51,
          29,
          24,
          15,
          14,
          49,
          86,
          23,
          49,
          49,
          30,
          43,
          16,
          29,
          39,
          7,
          29,
          31,
          21,
          15,
          42,
          84,
          128,
          30,
          17,
          38,
          9,
          29,
          46,
          23,
          35,
          98,
          105,
          53,
          28,
          57,
          24,
          34,
          58,
          113,
          48,
          18,
          39,
          73,
          7,
          22,
          59,
          35,
          76,
          91,
          52,
          84,
          16,
          46,
          203,
          70,
          17,
          45,
          91,
          17,
          23,
          127,
          165,
          42,
          53,
          23,
          9,
          17,
          65,
          82,
          89,
          84,
          27,
          13,
          30,
          56,
          78,
          17,
          16,
          13,
          23,
          104,
          37,
          74,
          12,
          80,
          42,
          9,
          28,
          42,
          112,
          37,
          11,
          81,
          31,
          17,
          26,
          48,
          32,
          75,
          17,
          21,
          9,
          6,
          45,
          28,
          20,
          12,
          10,
          19,
          35,
          37,
          24,
          56,
          61,
          25,
          14,
          21,
          79,
          48,
          12,
          33,
          18,
          27,
          38,
          31,
          44,
          71,
          36,
          28,
          70,
          25,
          79,
          40,
          73,
          18,
          17,
          31,
          35,
          22,
          89,
          73,
          32,
          27,
          69,
          50,
          13,
          48,
          61,
          30,
          34,
          68,
          100,
          40,
          32,
          76,
          15,
          28,
          22,
          16,
          31,
          24,
          34,
          20,
          36,
          25,
          36,
          24,
          102,
          75,
          58,
          96,
          42,
          26,
          113,
          10,
          106,
          10,
          42,
          132,
          32,
          21,
          41,
          14,
          10,
          20,
          20,
          53,
          65,
          35,
          27,
          96,
          45,
          61,
          64,
          12,
          194,
          30,
          78,
          199,
          53,
          29,
          137,
          15,
          18,
          134,
          46,
          17,
          45,
          28,
          22,
          35,
          18,
          32,
          22,
          26,
          33,
          59,
          47,
          10,
          48,
          37,
          29,
          30,
          41,
          17,
          43,
          45,
          57,
          96,
          26,
          70,
          70,
          31,
          16,
          37,
          33,
          24,
          27,
          35,
          95,
          42,
          31,
          11,
          43,
          37,
          54,
          62,
          43,
          15,
          26,
          23,
          9,
          78,
          29,
          81,
          120,
          94,
          15,
          23,
          20,
          59,
          59,
          46,
          43,
          33,
          46,
          45,
          37,
          32,
          66,
          58,
          16,
          16,
          61,
          26,
          61,
          20,
          24,
          20,
          75,
          29,
          35,
          27,
          65,
          118,
          54,
          37,
          24,
          49,
          58,
          32,
          46,
          46,
          26,
          53,
          36,
          20,
          73,
          22,
          20,
          47,
          41,
          23,
          28,
          83,
          51,
          26,
          33,
          51,
          34,
          32,
          28,
          12,
          22,
          51,
          16,
          26,
          64,
          46,
          32,
          11,
          60,
          65,
          6,
          18,
          47,
          71,
          27,
          65,
          31,
          30,
          59,
          12,
          34,
          43,
          30,
          36,
          45,
          53,
          33,
          56,
          19,
          31,
          47,
          9,
          31,
          43,
          25,
          37,
          21,
          15,
          22,
          36,
          17,
          18,
          27,
          49,
          27,
          40,
          40,
          29,
          9,
          15,
          14,
          59,
          55,
          11,
          29,
          29,
          22,
          31,
          11,
          23,
          27,
          26,
          29,
          19,
          44,
          70,
          26,
          39,
          14,
          20,
          11,
          11,
          26,
          18,
          52,
          10,
          33,
          57,
          49,
          28,
          36,
          48,
          55,
          35,
          13,
          11,
          26,
          57,
          48,
          39,
          30,
          45,
          19,
          30,
          69,
          89,
          94,
          49,
          52,
          7,
          28,
          7,
          24,
          45,
          81,
          39,
          46,
          21,
          19,
          77,
          50,
          59,
          37,
          27,
          13,
          56,
          25,
          34,
          108,
          116,
          177,
          66,
          12,
          94,
          24,
          25,
          12,
          28,
          26,
          147,
          59,
          61,
          8,
          40,
          31,
          60,
          18,
          39,
          34,
          83,
          41,
          39,
          31,
          10,
          28,
          60,
          38,
          25,
          103,
          40,
          20,
          14,
          74,
          77,
          44,
          114,
          46,
          31,
          52,
          198,
          8,
          9,
          33,
          14,
          22,
          17,
          30,
          16,
          29,
          26,
          34,
          38,
          7,
          10,
          44,
          45,
          32,
          31,
          43,
          33,
          22,
          37,
          54,
          8,
          36,
          23,
          36,
          21,
          72,
          57,
          50,
          16,
          48,
          138,
          30,
          16,
          41,
          15,
          15,
          87,
          86,
          23,
          55,
          14,
          38,
          18,
          51,
          14,
          23,
          106,
          36,
          63,
          43,
          37,
          93,
          54,
          45,
          31,
          35,
          52,
          15,
          36,
          31,
          38,
          10,
          45,
          69,
          19,
          38,
          15,
          17,
          8,
          36,
          15,
          48,
          34,
          7,
          95,
          49,
          133,
          17,
          13,
          77,
          106,
          62,
          35,
          61,
          46,
          24,
          27,
          46,
          35,
          38,
          41,
          55,
          39,
          15,
          12,
          33,
          16,
          36,
          13,
          52,
          22,
          122,
          19,
          15,
          38,
          31,
          37,
          13,
          43,
          23,
          40,
          24,
          75,
          23,
          32,
          16,
          53,
          26,
          112,
          43,
          11,
          10,
          8,
          9,
          9,
          12,
          8,
          9,
          9,
          9,
          11,
          11,
          13,
          11,
          11,
          10,
          10,
          10,
          6,
          10,
          9,
          10,
          9,
          10,
          10,
          10,
          11,
          9,
          5,
          8,
          7,
          10,
          11,
          9,
          5,
          8,
          9,
          6,
          7,
          5,
          8,
          6,
          7,
          11,
          13,
          11,
          7,
          7,
          11,
          10,
          9,
          7,
          9,
          8,
          7,
          8,
          11,
          7,
          10,
          9,
          8,
          9,
          6,
          11,
          9,
          11,
          10,
          10,
          9,
          7,
          6,
          7,
          7,
          7,
          7,
          8,
          8,
          5,
          6,
          6,
          9,
          7,
          8,
          7,
          5,
          10,
          6,
          7,
          12,
          12,
          7,
          7,
          8,
          10,
          8,
          7,
          9,
          5,
          6,
          6,
          9,
          6,
          7,
          7,
          7,
          7,
          7,
          7,
          7,
          7,
          7,
          8,
          11,
          10,
          10,
          10,
          11,
          10,
          11,
          8,
          8,
          9,
          11,
          10,
          10,
          8,
          8,
          11,
          9,
          12,
          6,
          6,
          8,
          9,
          9,
          9,
          9,
          8,
          10,
          6,
          9,
          10,
          11,
          6,
          8,
          9,
          10,
          11,
          10,
          11,
          12,
          12,
          10,
          6,
          6,
          9,
          10,
          11,
          8,
          7,
          8,
          7,
          7,
          7,
          10,
          10,
          10,
          10,
          12,
          12,
          10,
          10,
          9,
          9,
          8,
          8,
          4,
          5,
          6,
          6,
          4,
          5,
          5,
          5,
          9,
          6,
          6,
          10,
          7,
          8,
          8,
          9,
          7,
          8,
          7,
          8,
          11,
          5,
          7,
          9,
          8,
          8,
          9,
          9,
          10,
          10,
          8,
          9,
          9,
          10,
          9,
          11,
          10,
          11,
          12,
          10,
          9,
          8,
          8,
          9,
          11,
          7,
          9,
          16,
          10,
          9,
          7,
          6,
          12,
          5,
          7,
          7,
          9,
          11,
          12,
          12,
          13,
          6,
          8,
          11,
          3,
          8,
          3,
          4,
          3,
          8,
          3,
          6,
          8,
          7,
          4,
          9,
          7,
          6,
          5,
          10,
          9,
          8,
          9,
          7,
          12,
          11,
          8,
          5,
          5,
          5,
          3,
          5,
          4,
          6,
          9,
          9,
          8,
          6,
          7,
          3,
          5,
          3,
          4,
          6,
          7,
          5,
          5,
          6,
          7,
          6,
          7,
          7,
          7,
          7,
          12,
          11,
          10,
          15,
          10,
          7,
          9,
          9,
          9,
          11,
          9,
          10,
          11,
          9,
          5,
          8,
          7,
          9,
          7,
          10,
          5,
          7,
          6,
          7,
          6,
          8,
          8,
          8,
          8,
          7,
          6,
          11,
          7,
          7,
          7,
          9,
          11,
          6,
          8,
          5,
          11,
          7,
          7,
          7,
          8,
          9,
          9,
          11,
          10,
          9,
          11,
          8,
          8,
          9,
          8,
          10,
          7,
          12,
          8,
          8,
          7,
          9,
          7,
          5,
          8,
          7,
          9,
          10,
          5,
          5,
          7,
          5,
          7,
          7,
          9,
          5,
          9,
          3,
          9,
          5,
          10,
          7,
          7,
          8,
          6,
          6,
          5,
          6,
          6,
          6,
          7,
          8,
          10,
          7,
          7,
          8,
          8,
          7,
          6,
          6,
          8,
          8,
          12,
          10,
          10,
          7,
          11,
          5,
          7,
          8,
          10,
          8,
          9,
          6,
          9,
          7,
          7,
          9,
          8,
          7,
          6,
          6,
          6,
          5,
          9,
          7,
          8,
          5,
          6,
          9,
          10,
          9,
          4,
          8,
          10,
          7,
          11,
          11,
          9,
          8,
          7,
          12,
          12,
          6,
          6,
          8,
          6,
          7,
          10,
          8,
          6,
          8,
          7,
          8,
          7,
          20,
          15,
          10,
          10,
          4,
          6,
          7,
          5,
          6,
          4,
          4,
          8,
          3,
          4,
          6,
          4,
          8,
          9,
          9,
          10,
          12,
          8,
          9,
          12,
          8,
          9,
          10,
          6,
          8,
          7,
          9,
          8,
          10,
          8,
          8,
          7,
          7,
          7,
          5,
          7,
          7,
          11,
          11,
          11,
          10,
          6,
          7,
          7,
          8,
          8,
          9,
          8,
          11,
          8,
          11,
          8,
          10,
          11,
          9,
          13,
          10,
          9,
          6,
          8,
          10,
          7,
          9,
          6,
          7,
          7,
          10,
          9,
          6,
          13,
          7,
          9,
          12,
          6,
          7,
          11,
          12,
          9,
          10,
          4,
          8,
          6,
          9,
          10,
          8,
          8,
          8,
          11,
          5,
          6,
          10,
          10,
          6,
          7,
          9,
          7,
          9,
          7,
          7,
          10,
          9,
          10,
          9,
          10,
          12,
          11,
          8,
          10,
          10,
          8,
          12,
          6,
          9,
          6,
          8,
          10,
          8,
          7,
          8,
          9,
          10,
          6,
          12,
          10,
          11,
          12,
          8,
          9,
          12,
          8,
          3,
          5,
          5,
          9,
          9,
          5,
          6,
          7,
          10,
          9,
          6,
          9,
          5,
          7,
          5,
          8,
          8,
          10,
          7,
          5,
          6,
          6,
          9,
          10,
          8,
          10,
          8,
          9,
          10,
          9,
          11,
          10,
          8,
          8,
          6,
          5,
          8,
          9,
          8,
          8,
          6,
          9,
          7,
          7,
          7,
          9,
          7,
          9,
          9,
          10,
          13,
          8,
          12,
          10,
          8,
          9,
          7,
          9,
          11,
          14,
          14,
          8,
          12,
          7,
          12,
          12,
          10,
          11,
          10,
          11,
          7,
          6,
          3,
          3,
          8,
          7,
          9,
          10,
          7,
          6,
          8,
          7,
          8,
          10,
          8,
          7,
          7,
          8,
          8,
          7,
          7,
          10,
          9,
          8,
          8,
          10,
          9,
          6,
          8,
          8,
          7,
          8,
          8,
          9,
          9,
          9,
          8,
          6,
          11,
          5,
          7,
          8,
          10,
          8,
          9,
          13,
          12,
          10,
          7,
          6,
          17,
          8,
          11,
          18,
          24,
          8,
          11,
          6,
          7,
          9,
          6,
          9,
          8,
          10,
          14,
          14,
          6,
          11,
          11,
          10,
          11,
          12,
          13,
          15,
          13,
          10,
          14,
          11,
          14,
          9,
          13,
          17,
          7,
          7,
          11,
          11,
          8,
          8,
          8,
          8,
          8,
          8,
          7,
          7,
          7,
          8,
          8,
          8,
          7,
          8,
          9,
          13,
          7,
          7,
          7,
          7,
          7,
          7,
          6,
          6,
          8,
          8,
          10,
          10,
          8,
          11,
          10,
          9,
          6,
          6,
          6,
          11,
          11,
          9,
          9,
          7,
          6,
          7,
          6,
          8,
          8,
          10,
          10,
          12,
          12,
          8,
          10,
          9,
          9,
          10,
          10,
          8,
          9,
          11,
          6,
          6,
          8,
          8,
          7,
          7,
          8,
          8,
          11,
          11,
          11,
          16,
          15,
          6,
          7,
          8,
          8,
          9,
          4,
          5,
          5,
          3,
          6,
          4,
          5,
          7,
          6,
          4,
          6,
          14,
          15,
          15,
          15,
          13,
          9,
          11,
          13,
          7,
          24,
          25,
          14,
          20,
          23,
          13,
          15,
          14,
          17,
          12,
          18,
          15,
          7,
          8,
          11,
          10,
          11,
          13,
          8,
          14,
          15,
          11,
          9,
          11,
          11,
          15,
          12,
          11,
          12,
          11,
          15,
          11,
          8,
          14,
          13,
          13,
          12,
          10,
          12,
          12,
          15,
          13,
          32,
          5,
          11,
          14,
          12,
          9,
          8,
          8,
          10,
          15,
          8,
          8,
          8,
          8,
          8,
          22,
          16,
          8,
          9,
          13,
          13,
          12,
          12,
          14,
          9,
          6,
          6,
          11,
          8,
          8,
          11,
          11,
          12,
          9,
          10,
          12,
          12,
          11,
          9,
          12,
          11,
          9,
          9,
          9,
          12,
          14,
          19,
          16,
          16,
          15,
          10,
          11,
          6,
          6,
          8,
          10,
          11,
          11,
          14,
          10,
          14,
          11,
          10,
          11,
          11,
          11,
          13,
          12,
          11,
          10,
          14,
          10,
          10,
          14,
          12,
          15,
          13,
          8,
          7,
          12,
          13,
          7,
          8,
          7,
          11,
          10,
          10,
          7,
          7,
          10,
          9,
          8,
          7,
          9,
          12,
          12,
          13,
          12,
          14,
          16,
          15,
          12,
          14,
          12,
          11,
          12,
          12,
          12,
          11,
          11,
          13,
          7,
          19,
          20,
          17,
          14,
          16,
          18,
          16,
          10,
          13,
          13,
          19,
          17,
          17,
          17,
          17,
          17,
          10,
          8,
          11,
          11,
          12,
          19,
          17,
          15,
          13,
          12,
          8,
          19,
          15,
          13,
          13,
          17,
          17,
          18,
          14,
          20,
          23,
          18,
          11,
          11,
          14,
          12,
          17,
          14,
          21,
          17,
          21,
          15,
          21,
          22,
          18,
          20,
          23,
          15,
          19,
          13,
          25,
          79,
          81,
          32,
          8,
          20,
          9,
          18,
          20,
          24,
          22,
          20,
          30,
          54,
          195,
          10,
          46,
          21,
          15,
          27,
          24,
          40,
          51,
          15,
          13,
          32,
          49,
          31,
          24,
          18,
          38,
          8,
          142,
          37,
          19,
          30,
          27,
          43,
          104,
          68,
          49,
          207,
          26,
          52,
          61,
          108,
          71,
          56,
          117,
          43,
          25,
          31,
          25,
          20,
          7,
          24,
          85,
          28,
          33,
          83,
          24,
          49,
          34,
          24,
          30,
          89,
          283,
          22,
          36,
          28,
          77,
          19,
          112,
          100,
          16,
          26,
          28,
          21,
          38,
          9,
          65,
          35,
          37,
          23,
          19,
          24,
          32,
          180,
          90,
          21,
          90,
          19,
          65,
          19,
          27,
          26,
          174,
          21,
          127,
          32,
          17,
          19,
          56,
          165,
          16,
          25,
          85,
          37,
          143,
          13,
          20,
          20,
          18,
          23,
          22,
          94,
          39,
          10,
          21,
          12,
          25,
          27,
          13,
          7,
          52,
          16,
          35,
          34,
          19,
          47,
          32,
          14,
          35,
          55,
          26,
          40,
          51,
          23,
          30,
          19,
          28,
          14,
          36,
          15,
          18,
          25,
          37,
          22,
          21,
          28,
          153,
          24,
          25,
          109,
          9,
          52,
          25,
          87,
          38,
          33,
          30,
          30,
          32,
          101,
          43,
          60,
          45,
          66,
          14,
          28,
          34,
          36,
          28,
          17,
          32,
          69,
          13,
          22,
          29,
          116,
          57,
          17,
          16,
          6,
          53,
          31,
          73,
          34,
          22,
          14,
          26,
          68,
          60,
          167,
          43,
          11,
          45,
          50,
          42,
          66,
          26,
          20,
          22,
          80,
          14,
          22,
          76,
          20,
          6,
          42,
          21,
          50,
          32,
          27,
          13,
          17,
          27,
          32,
          17,
          34,
          45,
          22,
          41,
          14,
          22,
          48,
          19,
          63,
          18,
          14,
          21,
          18,
          51,
          58,
          39,
          13,
          29,
          9,
          24,
          34,
          25,
          22,
          111,
          101,
          48,
          34,
          65,
          28,
          21,
          61,
          128,
          53,
          24,
          29,
          59,
          10,
          14,
          51,
          36,
          83,
          58,
          46,
          46,
          29,
          52,
          273,
          55,
          20,
          19,
          124,
          17,
          14,
          120,
          135,
          31,
          64,
          32,
          9,
          8,
          52,
          83,
          80,
          96,
          27,
          16,
          10,
          41,
          75,
          12,
          16,
          10,
          18,
          46,
          20,
          69,
          14,
          38,
          32,
          13,
          19,
          52,
          93,
          29,
          10,
          41,
          20,
          12,
          25,
          51,
          23,
          69,
          11,
          19,
          10,
          6,
          43,
          40,
          25,
          7,
          16,
          19,
          32,
          49,
          11,
          47,
          80,
          22,
          16,
          14,
          59,
          29,
          15,
          71,
          24,
          31,
          24,
          35,
          55,
          59,
          29,
          34,
          70,
          28,
          66,
          38,
          73,
          15,
          18,
          42,
          39,
          17,
          63,
          79,
          35,
          35,
          49,
          66,
          7,
          25,
          26,
          23,
          41,
          62,
          132,
          41,
          28,
          75,
          20,
          29,
          19,
          9,
          27,
          26,
          41,
          52,
          61,
          23,
          32,
          16,
          77,
          76,
          50,
          88,
          45,
          31,
          107,
          28,
          99,
          36,
          67,
          111,
          22,
          22,
          29,
          13,
          19,
          23,
          32,
          60,
          69,
          66,
          39,
          97,
          79,
          70,
          113,
          16,
          258,
          30,
          94,
          232,
          55,
          32,
          111,
          20,
          18,
          146,
          40,
          14,
          51,
          24,
          19,
          31,
          19,
          24,
          35,
          33,
          30,
          33,
          52,
          12,
          65,
          24,
          24,
          26,
          31,
          16,
          42,
          38,
          83,
          97,
          28,
          71,
          63,
          26,
          16,
          35,
          40,
          26,
          31,
          38,
          76,
          40,
          42,
          10,
          30,
          28,
          49,
          81,
          32,
          29,
          22,
          33,
          11,
          76,
          57,
          84,
          122,
          151,
          19,
          45,
          39,
          58,
          49,
          82,
          39,
          25,
          35,
          58,
          19,
          39,
          91,
          65,
          10,
          18,
          66,
          17,
          49,
          36,
          25,
          18,
          78,
          24,
          28,
          44,
          60,
          147,
          38,
          36,
          24,
          76,
          75,
          36,
          52,
          47,
          26,
          69,
          38,
          18,
          109,
          37,
          12,
          80,
          34,
          13,
          28,
          65,
          61,
          36,
          51,
          46,
          30,
          32,
          26,
          11,
          27,
          46,
          21,
          22,
          53,
          49,
          30,
          21,
          75,
          86,
          12,
          10,
          31,
          64,
          22,
          83,
          22,
          25,
          45,
          15,
          45,
          74,
          30,
          37,
          59,
          66,
          35,
          44,
          12,
          33,
          33,
          15,
          38,
          31,
          14,
          27,
          27,
          12,
          20,
          35,
          19,
          16,
          33,
          35,
          24,
          50,
          25,
          32,
          12,
          30,
          21,
          118,
          51,
          18,
          37,
          47,
          16,
          41,
          19,
          28,
          20,
          41,
          23,
          11,
          38,
          71,
          17,
          43,
          18,
          20,
          19,
          14,
          19,
          17,
          92,
          13,
          27,
          69,
          54,
          43,
          32,
          43,
          40,
          27,
          14,
          11,
          15,
          33,
          52,
          29,
          15,
          21,
          20,
          22,
          48,
          136,
          48,
          51,
          43,
          18,
          27,
          8,
          27,
          27,
          70,
          28,
          61,
          21,
          15,
          39,
          57,
          38,
          33,
          32,
          36,
          48,
          18,
          72,
          109,
          112,
          212,
          52,
          9,
          48,
          11,
          25,
          24,
          34,
          23,
          155,
          51,
          68,
          13,
          48,
          78,
          67,
          22,
          84,
          35,
          83,
          38,
          57,
          63,
          18,
          25,
          84,
          37,
          31,
          149,
          31,
          16,
          23,
          78,
          49,
          51,
          155,
          40,
          29,
          30,
          193,
          9,
          12,
          53,
          13,
          23,
          10,
          60,
          17,
          38,
          23,
          34,
          35,
          12,
          17,
          35,
          47,
          33,
          31,
          54,
          37,
          19,
          32,
          74,
          14,
          24,
          32,
          50,
          17,
          53,
          52,
          66,
          18,
          41,
          108,
          24,
          18,
          30,
          20,
          11,
          82,
          96,
          38,
          39,
          15,
          37,
          8,
          78,
          18,
          40,
          111,
          35,
          52,
          37,
          57,
          73,
          56,
          70,
          25,
          25,
          27,
          19,
          35,
          26,
          21,
          12,
          52,
          81,
          15,
          28,
          21,
          19,
          10,
          26,
          17,
          50,
          33,
          21,
          114,
          23,
          134,
          26,
          18,
          56,
          72,
          40,
          44,
          40,
          32,
          19,
          27,
          49,
          44,
          43,
          35,
          48,
          62,
          20,
          11,
          38,
          21,
          45,
          22,
          40,
          29,
          105,
          20,
          29,
          31,
          36,
          23,
          20,
          74,
          25,
          37,
          20,
          116,
          21,
          33,
          31,
          59,
          32,
          122,
          54,
          5,
          8,
          8,
          8,
          8,
          9,
          6,
          7,
          7,
          9,
          9,
          8,
          8,
          8,
          5,
          7,
          8,
          8,
          9,
          9,
          8,
          11,
          10,
          9,
          6,
          8,
          8,
          10,
          10,
          10,
          12,
          8,
          6,
          7,
          4,
          6,
          5,
          7,
          6,
          5,
          8,
          7,
          5,
          8,
          9,
          10,
          11,
          6,
          10,
          9,
          6,
          11,
          8,
          8,
          7,
          9,
          10,
          11,
          9,
          9,
          7,
          8,
          7,
          8,
          7,
          9,
          7,
          10,
          8,
          10,
          7,
          7,
          7,
          6,
          6,
          6,
          6,
          5,
          5,
          6,
          6,
          8,
          6,
          7,
          5,
          8,
          6,
          6,
          6,
          12,
          7,
          6,
          7,
          8,
          8,
          8,
          8,
          5,
          6,
          6,
          7,
          5,
          7,
          6,
          6,
          7,
          6,
          7,
          7,
          6,
          7,
          7,
          7,
          7,
          8,
          7,
          7,
          8,
          8,
          7,
          7,
          8,
          8,
          9,
          9,
          8,
          8,
          8,
          9,
          9,
          8,
          8,
          8,
          10,
          10,
          9,
          9,
          7,
          7,
          8,
          7,
          7,
          7,
          8,
          9,
          9,
          7,
          7,
          7,
          7,
          7,
          7,
          7,
          7,
          7,
          8,
          9,
          10,
          8,
          8,
          7,
          7,
          8,
          7,
          9,
          9,
          7,
          7,
          9,
          9,
          7,
          7,
          8,
          8,
          9,
          9,
          5,
          5,
          5,
          5,
          5,
          4,
          5,
          5,
          7,
          6,
          7,
          7,
          8,
          8,
          8,
          8,
          6,
          7,
          7,
          10,
          8,
          6,
          7,
          7,
          7,
          8,
          8,
          8,
          8,
          6,
          7,
          7,
          7,
          6,
          6,
          7,
          5,
          5,
          10,
          8,
          5,
          7,
          7,
          8,
          9,
          10,
          10,
          10,
          7,
          7,
          6,
          8,
          8,
          6,
          7,
          7,
          8,
          8,
          8,
          8,
          8,
          6,
          6,
          6,
          4,
          5,
          6,
          5,
          5,
          6,
          4,
          4,
          5,
          6,
          6,
          8,
          7,
          7,
          4,
          7,
          7,
          9,
          7,
          5,
          6,
          10,
          5,
          4,
          4,
          5,
          5,
          5,
          5,
          4,
          5,
          4,
          4,
          5,
          8,
          5,
          4,
          5,
          5,
          5,
          7,
          7,
          4,
          5,
          6,
          5,
          5,
          6,
          5,
          5,
          6,
          8,
          6,
          6,
          6,
          6,
          6,
          8,
          6,
          7,
          9,
          7,
          9,
          8,
          5,
          7,
          7,
          6,
          5,
          6,
          6,
          6,
          5,
          8,
          6,
          6,
          6,
          6,
          6,
          5,
          5,
          7,
          8,
          8,
          6,
          5,
          6,
          6,
          5,
          6,
          7,
          5,
          8,
          8,
          8,
          5,
          6,
          7,
          7,
          8,
          10,
          8,
          8,
          8,
          8,
          8,
          9,
          9,
          8,
          8,
          7,
          7,
          7,
          6,
          6,
          8,
          7,
          7,
          5,
          5,
          5,
          6,
          6,
          5,
          5,
          5,
          5,
          6,
          6,
          7,
          7,
          6,
          5,
          5,
          6,
          5,
          5,
          5,
          5,
          5,
          5,
          8,
          6,
          7,
          6,
          6,
          7,
          7,
          8,
          8,
          7,
          10,
          9,
          9,
          8,
          6,
          8,
          6,
          6,
          9,
          9,
          9,
          11,
          6,
          6,
          6,
          4,
          7,
          8,
          7,
          8,
          7,
          5,
          4,
          8,
          6,
          7,
          6,
          8,
          7,
          9,
          10,
          6,
          6,
          10,
          7,
          9,
          8,
          7,
          5,
          6,
          9,
          8,
          5,
          6,
          8,
          8,
          8,
          8,
          8,
          7,
          8,
          10,
          7,
          7,
          15,
          9,
          7,
          8,
          7,
          7,
          8,
          5,
          6,
          7,
          7,
          7,
          6,
          7,
          6,
          7,
          12,
          10,
          6,
          7,
          7,
          5,
          8,
          12,
          8,
          8,
          8,
          6,
          6,
          7,
          7,
          6,
          7,
          8,
          6,
          8,
          8,
          7,
          5,
          7,
          7,
          7,
          7,
          8,
          8,
          5,
          5,
          5,
          7,
          6,
          5,
          8,
          9,
          6,
          10,
          9,
          10,
          8,
          8,
          9,
          10,
          8,
          8,
          7,
          7,
          6,
          9,
          5,
          6,
          6,
          9,
          8,
          7,
          9,
          6,
          7,
          11,
          7,
          7,
          7,
          6,
          7,
          7,
          4,
          5,
          8,
          7,
          9,
          9,
          7,
          7,
          7,
          6,
          6,
          8,
          9,
          8,
          6,
          11,
          8,
          9,
          9,
          10,
          10,
          9,
          11,
          11,
          11,
          8,
          11,
          7,
          7,
          10,
          7,
          12,
          9,
          5,
          5,
          8,
          8,
          7,
          6,
          9,
          10,
          8,
          5,
          8,
          8,
          8,
          8,
          8,
          8,
          9,
          8,
          5,
          6,
          6,
          7,
          9,
          6,
          7,
          6,
          8,
          7,
          6,
          5,
          5,
          8,
          6,
          6,
          8,
          7,
          5,
          5,
          4,
          4,
          10,
          8,
          7,
          6,
          7,
          7,
          7,
          8,
          7,
          9,
          7,
          7,
          8,
          5,
          6,
          8,
          6,
          7,
          8,
          8,
          6,
          4,
          6,
          8,
          9,
          7,
          11,
          11,
          8,
          7,
          7,
          7,
          7,
          7,
          5,
          6,
          8,
          9,
          8,
          7,
          8,
          7,
          9,
          8,
          6,
          7,
          8,
          8,
          5,
          6,
          7,
          7,
          8,
          7,
          6,
          7,
          7,
          6,
          7,
          7,
          8,
          9,
          6,
          6,
          7,
          8,
          9,
          9,
          9,
          6,
          6,
          6,
          10,
          10,
          8,
          8,
          8,
          7,
          7,
          9,
          10,
          11,
          11,
          8,
          7,
          6,
          9,
          5,
          7,
          6,
          11,
          8,
          8,
          10,
          10,
          11,
          8,
          6,
          8,
          8,
          7,
          17,
          14,
          7,
          10,
          6,
          6,
          8,
          7,
          7,
          7,
          7,
          8,
          8,
          7,
          12,
          8,
          6,
          6,
          6,
          6,
          15,
          11,
          8,
          12,
          12,
          10,
          7,
          9,
          9,
          6,
          6,
          7,
          7,
          6,
          6,
          6,
          7,
          6,
          7,
          6,
          6,
          6,
          6,
          6,
          6,
          6,
          5,
          6,
          6,
          6,
          6,
          6,
          6,
          6,
          5,
          6,
          6,
          7,
          6,
          7,
          6,
          8,
          8,
          8,
          8,
          6,
          6,
          6,
          9,
          8,
          10,
          9,
          9,
          9,
          8,
          8,
          8,
          8,
          7,
          8,
          7,
          8,
          8,
          8,
          9,
          9,
          8,
          8,
          8,
          8,
          12,
          6,
          6,
          6,
          6,
          6,
          6,
          6,
          6,
          10,
          8,
          11,
          11,
          10,
          7,
          7,
          7,
          7,
          7,
          5,
          5,
          5,
          5,
          5,
          5,
          4,
          4,
          4,
          4,
          4,
          12,
          12,
          9,
          9,
          11,
          8,
          8,
          13,
          9,
          17,
          17,
          12,
          16,
          13,
          12,
          12,
          17,
          17,
          13,
          15,
          13,
          6,
          7,
          7,
          10,
          8,
          10,
          4,
          4,
          8,
          10,
          8,
          11,
          11,
          10,
          8,
          8,
          11,
          10,
          12,
          9,
          5,
          11,
          11,
          8,
          11,
          10,
          10,
          11,
          13,
          11,
          14,
          7,
          7,
          20,
          14,
          10,
          8,
          8,
          11,
          9,
          7,
          7,
          7,
          7,
          9,
          15,
          15,
          8,
          8,
          9,
          9,
          9,
          12,
          9,
          10,
          9,
          8,
          11,
          8,
          6,
          9,
          9,
          9,
          8,
          15,
          11,
          12,
          12,
          11,
          11,
          11,
          8,
          9,
          7,
          13,
          14,
          13,
          14,
          13,
          15,
          11,
          8,
          6,
          6,
          8,
          10,
          9,
          9,
          9,
          10,
          8,
          7,
          10,
          11,
          8,
          10,
          9,
          8,
          8,
          10,
          10,
          11,
          10,
          9,
          10,
          10,
          7,
          7,
          7,
          8,
          8,
          8,
          8,
          9,
          9,
          9,
          9,
          8,
          9,
          7,
          7,
          7,
          8,
          8,
          7,
          9,
          12,
          11,
          13,
          11,
          9,
          10,
          12,
          10,
          12,
          9,
          9,
          18,
          17,
          13,
          11,
          6,
          17,
          15,
          13,
          7,
          11,
          15,
          13,
          10,
          14,
          13,
          13,
          13,
          14,
          13,
          16,
          15,
          12,
          5,
          10,
          12,
          8,
          14,
          16,
          10,
          10,
          10,
          7,
          12,
          11,
          14,
          13,
          14,
          15,
          15,
          11,
          10,
          11,
          11,
          10,
          9,
          10,
          11,
          11,
          10,
          16,
          14,
          16,
          13,
          16,
          18,
          14,
          14,
          21,
          16,
          16
         ],
         "x0": " ",
         "xaxis": "x",
         "y0": " ",
         "yaxis": "y"
        }
       ],
       "layout": {
        "boxmode": "group",
        "legend": {
         "tracegroupgap": 0
        },
        "margin": {
         "t": 60
        },
        "template": {
         "data": {
          "bar": [
           {
            "error_x": {
             "color": "#2a3f5f"
            },
            "error_y": {
             "color": "#2a3f5f"
            },
            "marker": {
             "line": {
              "color": "#E5ECF6",
              "width": 0.5
             },
             "pattern": {
              "fillmode": "overlay",
              "size": 10,
              "solidity": 0.2
             }
            },
            "type": "bar"
           }
          ],
          "barpolar": [
           {
            "marker": {
             "line": {
              "color": "#E5ECF6",
              "width": 0.5
             },
             "pattern": {
              "fillmode": "overlay",
              "size": 10,
              "solidity": 0.2
             }
            },
            "type": "barpolar"
           }
          ],
          "carpet": [
           {
            "aaxis": {
             "endlinecolor": "#2a3f5f",
             "gridcolor": "white",
             "linecolor": "white",
             "minorgridcolor": "white",
             "startlinecolor": "#2a3f5f"
            },
            "baxis": {
             "endlinecolor": "#2a3f5f",
             "gridcolor": "white",
             "linecolor": "white",
             "minorgridcolor": "white",
             "startlinecolor": "#2a3f5f"
            },
            "type": "carpet"
           }
          ],
          "choropleth": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "type": "choropleth"
           }
          ],
          "contour": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "contour"
           }
          ],
          "contourcarpet": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "type": "contourcarpet"
           }
          ],
          "heatmap": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "heatmap"
           }
          ],
          "heatmapgl": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "heatmapgl"
           }
          ],
          "histogram": [
           {
            "marker": {
             "pattern": {
              "fillmode": "overlay",
              "size": 10,
              "solidity": 0.2
             }
            },
            "type": "histogram"
           }
          ],
          "histogram2d": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "histogram2d"
           }
          ],
          "histogram2dcontour": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "histogram2dcontour"
           }
          ],
          "mesh3d": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "type": "mesh3d"
           }
          ],
          "parcoords": [
           {
            "line": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "parcoords"
           }
          ],
          "pie": [
           {
            "automargin": true,
            "type": "pie"
           }
          ],
          "scatter": [
           {
            "fillpattern": {
             "fillmode": "overlay",
             "size": 10,
             "solidity": 0.2
            },
            "type": "scatter"
           }
          ],
          "scatter3d": [
           {
            "line": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scatter3d"
           }
          ],
          "scattercarpet": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scattercarpet"
           }
          ],
          "scattergeo": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scattergeo"
           }
          ],
          "scattergl": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scattergl"
           }
          ],
          "scattermapbox": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scattermapbox"
           }
          ],
          "scatterpolar": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scatterpolar"
           }
          ],
          "scatterpolargl": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scatterpolargl"
           }
          ],
          "scatterternary": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scatterternary"
           }
          ],
          "surface": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "surface"
           }
          ],
          "table": [
           {
            "cells": {
             "fill": {
              "color": "#EBF0F8"
             },
             "line": {
              "color": "white"
             }
            },
            "header": {
             "fill": {
              "color": "#C8D4E3"
             },
             "line": {
              "color": "white"
             }
            },
            "type": "table"
           }
          ]
         },
         "layout": {
          "annotationdefaults": {
           "arrowcolor": "#2a3f5f",
           "arrowhead": 0,
           "arrowwidth": 1
          },
          "autotypenumbers": "strict",
          "coloraxis": {
           "colorbar": {
            "outlinewidth": 0,
            "ticks": ""
           }
          },
          "colorscale": {
           "diverging": [
            [
             0,
             "#8e0152"
            ],
            [
             0.1,
             "#c51b7d"
            ],
            [
             0.2,
             "#de77ae"
            ],
            [
             0.3,
             "#f1b6da"
            ],
            [
             0.4,
             "#fde0ef"
            ],
            [
             0.5,
             "#f7f7f7"
            ],
            [
             0.6,
             "#e6f5d0"
            ],
            [
             0.7,
             "#b8e186"
            ],
            [
             0.8,
             "#7fbc41"
            ],
            [
             0.9,
             "#4d9221"
            ],
            [
             1,
             "#276419"
            ]
           ],
           "sequential": [
            [
             0,
             "#0d0887"
            ],
            [
             0.1111111111111111,
             "#46039f"
            ],
            [
             0.2222222222222222,
             "#7201a8"
            ],
            [
             0.3333333333333333,
             "#9c179e"
            ],
            [
             0.4444444444444444,
             "#bd3786"
            ],
            [
             0.5555555555555556,
             "#d8576b"
            ],
            [
             0.6666666666666666,
             "#ed7953"
            ],
            [
             0.7777777777777778,
             "#fb9f3a"
            ],
            [
             0.8888888888888888,
             "#fdca26"
            ],
            [
             1,
             "#f0f921"
            ]
           ],
           "sequentialminus": [
            [
             0,
             "#0d0887"
            ],
            [
             0.1111111111111111,
             "#46039f"
            ],
            [
             0.2222222222222222,
             "#7201a8"
            ],
            [
             0.3333333333333333,
             "#9c179e"
            ],
            [
             0.4444444444444444,
             "#bd3786"
            ],
            [
             0.5555555555555556,
             "#d8576b"
            ],
            [
             0.6666666666666666,
             "#ed7953"
            ],
            [
             0.7777777777777778,
             "#fb9f3a"
            ],
            [
             0.8888888888888888,
             "#fdca26"
            ],
            [
             1,
             "#f0f921"
            ]
           ]
          },
          "colorway": [
           "#636efa",
           "#EF553B",
           "#00cc96",
           "#ab63fa",
           "#FFA15A",
           "#19d3f3",
           "#FF6692",
           "#B6E880",
           "#FF97FF",
           "#FECB52"
          ],
          "font": {
           "color": "#2a3f5f"
          },
          "geo": {
           "bgcolor": "white",
           "lakecolor": "white",
           "landcolor": "#E5ECF6",
           "showlakes": true,
           "showland": true,
           "subunitcolor": "white"
          },
          "hoverlabel": {
           "align": "left"
          },
          "hovermode": "closest",
          "mapbox": {
           "style": "light"
          },
          "paper_bgcolor": "white",
          "plot_bgcolor": "#E5ECF6",
          "polar": {
           "angularaxis": {
            "gridcolor": "white",
            "linecolor": "white",
            "ticks": ""
           },
           "bgcolor": "#E5ECF6",
           "radialaxis": {
            "gridcolor": "white",
            "linecolor": "white",
            "ticks": ""
           }
          },
          "scene": {
           "xaxis": {
            "backgroundcolor": "#E5ECF6",
            "gridcolor": "white",
            "gridwidth": 2,
            "linecolor": "white",
            "showbackground": true,
            "ticks": "",
            "zerolinecolor": "white"
           },
           "yaxis": {
            "backgroundcolor": "#E5ECF6",
            "gridcolor": "white",
            "gridwidth": 2,
            "linecolor": "white",
            "showbackground": true,
            "ticks": "",
            "zerolinecolor": "white"
           },
           "zaxis": {
            "backgroundcolor": "#E5ECF6",
            "gridcolor": "white",
            "gridwidth": 2,
            "linecolor": "white",
            "showbackground": true,
            "ticks": "",
            "zerolinecolor": "white"
           }
          },
          "shapedefaults": {
           "line": {
            "color": "#2a3f5f"
           }
          },
          "ternary": {
           "aaxis": {
            "gridcolor": "white",
            "linecolor": "white",
            "ticks": ""
           },
           "baxis": {
            "gridcolor": "white",
            "linecolor": "white",
            "ticks": ""
           },
           "bgcolor": "#E5ECF6",
           "caxis": {
            "gridcolor": "white",
            "linecolor": "white",
            "ticks": ""
           }
          },
          "title": {
           "x": 0.05
          },
          "xaxis": {
           "automargin": true,
           "gridcolor": "white",
           "linecolor": "white",
           "ticks": "",
           "title": {
            "standoff": 15
           },
           "zerolinecolor": "white",
           "zerolinewidth": 2
          },
          "yaxis": {
           "automargin": true,
           "gridcolor": "white",
           "linecolor": "white",
           "ticks": "",
           "title": {
            "standoff": 15
           },
           "zerolinecolor": "white",
           "zerolinewidth": 2
          }
         }
        },
        "xaxis": {
         "anchor": "y",
         "domain": [
          0,
          1
         ],
         "title": {
          "text": "length"
         }
        },
        "yaxis": {
         "anchor": "x",
         "domain": [
          0,
          1
         ]
        }
       }
      }
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "fig = px.box(x = length)\n",
    "\n",
    "fig.update_layout({'xaxis': {'title': 'length'}})"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The upper fence is of **60** and the max length is equal to **283**. Then we will test any value between the two. "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "But considering the augmentation we can obtain more than the value that we will take because it will add modifications on the words and then it can recognize only parts of them and divide them in multiple other tokens. We will add to the max length the sixth of it. "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It is time to create our custom dataset.\n",
    "\n",
    "Signature:\n",
    "```python\n",
    "class T5SentenceDataset(Dataset):\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        data_path: str, \n",
    "        tokenizer: PreTrainedTokenizerFast\n",
    "        corpus_1: str = \"french\",\n",
    "        corpus_2: str = \"wolof\",\n",
    "        max_len: int = 60,\n",
    "        cp1_truncation: bool = False,\n",
    "        cp2_truncation: bool = False,\n",
    "        file_sep: str = \",\",\n",
    "        cp1_transformer: Union[TransformerSequences, None] = None,\n",
    "        cp2_transformer: Union[TransformerSequences, None] = None,\n",
    "        **kwargs):\n",
    "\n",
    "        pass\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting wolof-translate/wolof_translate/data/dataset_v3.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile wolof-translate/wolof_translate/data/dataset_v3.py\n",
    "from wolof_translate.utils.sent_transformers import TransformerSequences\n",
    "from transformers import PreTrainedTokenizerFast\n",
    "from torch.utils.data import Dataset\n",
    "from typing import *\n",
    "import pandas as pd\n",
    "import torch\n",
    "import re\n",
    "\n",
    "class T5SentenceDataset(Dataset):\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        data_path: str, \n",
    "        tokenizer: PreTrainedTokenizerFast,\n",
    "        corpus_1: str = \"french\",\n",
    "        corpus_2: str = \"wolof\",\n",
    "        max_len: int = 60,\n",
    "        truncation: bool = False,\n",
    "        file_sep: str = \",\",\n",
    "        cp1_transformer: Union[TransformerSequences, None] = None,\n",
    "        cp2_transformer: Union[TransformerSequences, None] = None,\n",
    "        **kwargs):\n",
    "        \n",
    "        # let us recuperate the data frame\n",
    "        self.__sentences = pd.read_csv(data_path, sep=file_sep, **kwargs)\n",
    "        \n",
    "        # let us recuperate the tokenizer\n",
    "        self.tokenizer = tokenizer\n",
    "        \n",
    "        # recuperate the first corpus' sentences\n",
    "        self.sentences_1 = self.__sentences[corpus_1].to_list()\n",
    "        \n",
    "        # recuperate the second corpus' sentences\n",
    "        self.sentences_2 = self.__sentences[corpus_2].to_list()\n",
    "        \n",
    "        # recuperate the length\n",
    "        self.length = len(self.sentences_1)\n",
    "        \n",
    "        # let us recuperate the max len\n",
    "        self.max_len = max_len + max_len // 6\n",
    "        \n",
    "        # let us recuperate the truncation argument\n",
    "        self.truncation = truncation\n",
    "        \n",
    "        # let us initialize the transformer\n",
    "        self.cp1_transformer = cp1_transformer\n",
    "        \n",
    "        self.cp2_transformer = cp2_transformer\n",
    "        \n",
    "    def __getitem__(self, index):\n",
    "        \"\"\"Recuperate ids and attention masks of sentences at index\n",
    "\n",
    "        Args:\n",
    "            index (int): The index of the sentences to recuperate\n",
    "\n",
    "        Returns:\n",
    "            tuple: The `sentence to translate' ids`, `the attention mask of the sentence to translate`\n",
    "            `the labels' ids`\n",
    "        \"\"\"\n",
    "        sentence_1 = self.sentences_1[index]\n",
    "        \n",
    "        sentence_2 = self.sentences_2[index]\n",
    "        \n",
    "        # apply transformers if necessary\n",
    "        if not self.cp1_transformer is None:\n",
    "            \n",
    "            sentence_1 = self.cp1_transformer(sentence_1)[0]\n",
    "        \n",
    "        if not self.cp2_transformer is None:\n",
    "            \n",
    "            sentence_2 = self.cp2_transformer(sentence_2)[0]\n",
    "        \n",
    "        sentence_1 = sentence_1 + self.tokenizer.eos_token\n",
    "        \n",
    "        sentence_2 = sentence_2 + self.tokenizer.eos_token\n",
    "        \n",
    "        # let us encode the sentences (we provide the second sentence as labels to the tokenizer)\n",
    "        data = self.tokenizer(\n",
    "            sentence_1,\n",
    "            truncation=self.truncation,\n",
    "            max_length=self.max_len, \n",
    "            padding='max_length', \n",
    "            return_tensors=\"pt\",\n",
    "            text_target=sentence_2)\n",
    "        \n",
    "        return data.input_ids.squeeze(0), data.attention_mask.squeeze(0), data.labels.squeeze(0)\n",
    "        \n",
    "    def __len__(self):\n",
    "        \n",
    "        return self.length\n",
    "    \n",
    "    def decode(self, labels: torch.Tensor):\n",
    "        \n",
    "        if labels.ndim < 2:\n",
    "            \n",
    "            labels = labels.unsqueeze(0)\n",
    "\n",
    "        sentences = self.tokenizer.batch_decode(labels, skip_special_tokens=True)\n",
    "\n",
    "        return sentences\n",
    "\n",
    "\n",
    "class SentenceDataset(T5SentenceDataset):\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        data_path: str, \n",
    "        tokenizer: PreTrainedTokenizerFast,\n",
    "        corpus_1: str = \"french\",\n",
    "        corpus_2: str = \"wolof\",\n",
    "        max_len: int = 42,\n",
    "        truncation: bool = False,\n",
    "        file_sep: str = \",\",\n",
    "        cp1_transformer: Union[TransformerSequences, None] = None,\n",
    "        cp2_transformer: Union[TransformerSequences, None] = None,\n",
    "        **kwargs):\n",
    "        \n",
    "        super().__init__(data_path, \n",
    "                        tokenizer,\n",
    "                        corpus_1,\n",
    "                        corpus_2,\n",
    "                        max_len,\n",
    "                        truncation,\n",
    "                        file_sep,\n",
    "                        cp1_transformer,\n",
    "                        cp2_transformer,\n",
    "                        **kwargs)\n",
    "        \n",
    "    def __getitem__(self, index):\n",
    "        \"\"\"Recuperate ids and attention masks of sentences at index\n",
    "\n",
    "        Args:\n",
    "            index (int): The index of the sentences to recuperate\n",
    "\n",
    "        Returns:\n",
    "            tuple: The `sentence to translate' ids`, `the attention mask of the sentence to translate`\n",
    "            `the labels' ids`\n",
    "        \"\"\"\n",
    "        sentence_1 = self.sentences_1[index]\n",
    "        \n",
    "        sentence_2 = self.sentences_2[index]\n",
    "        \n",
    "        # apply transformers if necessary\n",
    "        if not self.cp1_transformer is None:\n",
    "            \n",
    "            sentence_1 = self.cp1_transformer(sentence_1)[0] \n",
    "        \n",
    "        if not self.cp2_transformer is None:\n",
    "            \n",
    "            sentence_2 = self.cp2_transformer(sentence_2)[0]\n",
    "        \n",
    "        sentence_1 = sentence_1 + self.tokenizer.eos_token\n",
    "        \n",
    "        sentence_2 = sentence_2 + self.tokenizer.eos_token\n",
    "        \n",
    "        # let us encode the sentences (we provide the second sentence as labels to the tokenizer)\n",
    "        data = self.tokenizer(\n",
    "            sentence_1,\n",
    "            truncation=self.truncation,\n",
    "            max_length=self.max_len, \n",
    "            padding='max_length', \n",
    "            return_tensors=\"pt\")\n",
    "        \n",
    "        # let us encode the sentences (we provide the second sentence as labels to the tokenizer)\n",
    "        labels = self.tokenizer(\n",
    "            sentence_2,\n",
    "            truncation=self.truncation,\n",
    "            max_length=self.max_len, \n",
    "            padding='max_length', \n",
    "            return_tensors=\"pt\")\n",
    "        \n",
    "        return (data.input_ids.squeeze(0),\n",
    "                data.attention_mask.squeeze(0), \n",
    "                labels.input_ids.squeeze(0),\n",
    "                labels.attention_mask.squeeze(0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "%run wolof-translate/wolof_translate/data/dataset_v3.py"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let us generate some data with their masks and decode the labels.\n",
    "\n",
    "**Note that we will use, when training the `T5 model`, train and test sets and not directly the full dataset**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize our custom dataset\n",
    "dataset = SentenceDataset(\"data/extractions/new_data/corpora_v5.csv\", wrapped_tokenizer2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "generator = torch.manual_seed(5)\n",
    "input_ids, mask, labels, _ = next(iter(DataLoader(dataset, 10, shuffle=True, generator=generator))) # generate 10 sentences with shuffling"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let us print the input ids."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 127,   16,  626,   17,   41,   29, 5207,    9,    6,    3,    3,    3,\n",
       "            3,    3,    3,    3,    3,    3,    3,    3,    3,    3,    3,    3,\n",
       "            3,    3,    3,    3,    3,    3,    3,    3,    3,    3,    3,    3,\n",
       "            3,    3,    3,    3,    3,    3,    3,    3,    3,    3,    3,    3,\n",
       "            3],\n",
       "        [ 133,   32, 1114,   18, 4600,    6,    3,    3,    3,    3,    3,    3,\n",
       "            3,    3,    3,    3,    3,    3,    3,    3,    3,    3,    3,    3,\n",
       "            3,    3,    3,    3,    3,    3,    3,    3,    3,    3,    3,    3,\n",
       "            3,    3,    3,    3,    3,    3,    3,    3,    3,    3,    3,    3,\n",
       "            3],\n",
       "        [  88,   66,   41,  217,  705,   32,   77,  411,    8,   60,    6,    3,\n",
       "            3,    3,    3,    3,    3,    3,    3,    3,    3,    3,    3,    3,\n",
       "            3,    3,    3,    3,    3,    3,    3,    3,    3,    3,    3,    3,\n",
       "            3,    3,    3,    3,    3,    3,    3,    3,    3,    3,    3,    3,\n",
       "            3],\n",
       "        [  88,   66,  313,    7,  313, 1119,    8,   60,    6,    3,    3,    3,\n",
       "            3,    3,    3,    3,    3,    3,    3,    3,    3,    3,    3,    3,\n",
       "            3,    3,    3,    3,    3,    3,    3,    3,    3,    3,    3,    3,\n",
       "            3,    3,    3,    3,    3,    3,    3,    3,    3,    3,    3,    3,\n",
       "            3],\n",
       "        [ 179,   11, 1341, 3093,    7,   47, 3273,    7,   91,  169, 2168,   81,\n",
       "          301,  228,  504,    9,    6,    3,    3,    3,    3,    3,    3,    3,\n",
       "            3,    3,    3,    3,    3,    3,    3,    3,    3,    3,    3,    3,\n",
       "            3,    3,    3,    3,    3,    3,    3,    3,    3,    3,    3,    3,\n",
       "            3],\n",
       "        [ 342,  242,    7,    8,   69,  291, 3667, 1432,    7,   31, 4028,  102,\n",
       "           68, 1297, 7794,   16, 1280,   92,   61, 4011,    9,    6,    3,    3,\n",
       "            3,    3,    3,    3,    3,    3,    3,    3,    3,    3,    3,    3,\n",
       "            3,    3,    3,    3,    3,    3,    3,    3,    3,    3,    3,    3,\n",
       "            3],\n",
       "        [ 813, 1821,  102, 5775, 1797,    7,   21,   68, 2094,    7, 1700,  418,\n",
       "           10,  658, 2232,  223,   70,  298,   15,  132,   45,  110, 5596,  130,\n",
       "         6525,   71,  256,   58,  451, 1222,   10,  605,  362, 1536,    9,    6,\n",
       "            3,    3,    3,    3,    3,    3,    3,    3,    3,    3,    3,    3,\n",
       "            3],\n",
       "        [ 127,   16,   97,  358,  540,  488,    8,   60,    6,    3,    3,    3,\n",
       "            3,    3,    3,    3,    3,    3,    3,    3,    3,    3,    3,    3,\n",
       "            3,    3,    3,    3,    3,    3,    3,    3,    3,    3,    3,    3,\n",
       "            3,    3,    3,    3,    3,    3,    3,    3,    3,    3,    3,    3,\n",
       "            3],\n",
       "        [  88,   66, 1322,    7,  120, 1395,  499,   71,  855,   25,  906,    8,\n",
       "           60,    6,    3,    3,    3,    3,    3,    3,    3,    3,    3,    3,\n",
       "            3,    3,    3,    3,    3,    3,    3,    3,    3,    3,    3,    3,\n",
       "            3,    3,    3,    3,    3,    3,    3,    3,    3,    3,    3,    3,\n",
       "            3],\n",
       "        [ 364,   82,  776,  200, 2475,  162,   27,  395,    7,  301,   93,   12,\n",
       "          554,    7,  129,    8,  845,  110, 1086, 2442,   32, 3100, 3310,   39,\n",
       "          489,  526,  209, 6659, 2289,    9,    6,    3,    3,    3,    3,    3,\n",
       "            3,    3,    3,    3,    3,    3,    3,    3,    3,    3,    3,    3,\n",
       "            3]])"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input_ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0],\n",
       "        [1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0],\n",
       "        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0],\n",
       "        [1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0],\n",
       "        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0],\n",
       "        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0],\n",
       "        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0],\n",
       "        [1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0],\n",
       "        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0],\n",
       "        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0]])"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 439,   42,   57,  139, 1592,    9,    6,    3,    3,    3,    3,    3,\n",
       "            3,    3,    3,    3,    3,    3,    3,    3,    3,    3,    3,    3,\n",
       "            3,    3,    3,    3,    3,    3,    3,    3,    3,    3,    3,    3,\n",
       "            3,    3,    3,    3,    3,    3,    3,    3,    3,    3,    3,    3,\n",
       "            3],\n",
       "        [ 260, 1043,   43,  194,   38, 1363,    6,    3,    3,    3,    3,    3,\n",
       "            3,    3,    3,    3,    3,    3,    3,    3,    3,    3,    3,    3,\n",
       "            3,    3,    3,    3,    3,    3,    3,    3,    3,    3,    3,    3,\n",
       "            3,    3,    3,    3,    3,    3,    3,    3,    3,    3,    3,    3,\n",
       "            3],\n",
       "        [ 557,  206,  572,   12,  210,    8,   60,    6,    3,    3,    3,    3,\n",
       "            3,    3,    3,    3,    3,    3,    3,    3,    3,    3,    3,    3,\n",
       "            3,    3,    3,    3,    3,    3,    3,    3,    3,    3,    3,    3,\n",
       "            3,    3,    3,    3,    3,    3,    3,    3,    3,    3,    3,    3,\n",
       "            3],\n",
       "        [1728,   12,  597,  101,  503,   12,    8,   60,    6,    3,    3,    3,\n",
       "            3,    3,    3,    3,    3,    3,    3,    3,    3,    3,    3,    3,\n",
       "            3,    3,    3,    3,    3,    3,    3,    3,    3,    3,    3,    3,\n",
       "            3,    3,    3,    3,    3,    3,    3,    3,    3,    3,    3,    3,\n",
       "            3],\n",
       "        [ 536, 6587,  249,  154,    7,  100,   72,  638,  203, 1131,  564,  398,\n",
       "           36,   29,  284,    9,    6,    3,    3,    3,    3,    3,    3,    3,\n",
       "            3,    3,    3,    3,    3,    3,    3,    3,    3,    3,    3,    3,\n",
       "            3,    3,    3,    3,    3,    3,    3,    3,    3,    3,    3,    3,\n",
       "            3],\n",
       "        [2227,  140,  177,  214,   12,   56,    7,  191,   22,   84, 3710,   75,\n",
       "         1684,    9,    6,    3,    3,    3,    3,    3,    3,    3,    3,    3,\n",
       "            3,    3,    3,    3,    3,    3,    3,    3,    3,    3,    3,    3,\n",
       "            3,    3,    3,    3,    3,    3,    3,    3,    3,    3,    3,    3,\n",
       "            3],\n",
       "        [ 219,  647,  673, 1253,  409,   26,  676,   94,    7,   54,  734,  530,\n",
       "           26,  369,  909,  201,   18,  803,   14,  732, 1512,  185,   22,  207,\n",
       "         1266,   76,    9,    6,    3,    3,    3,    3,    3,    3,    3,    3,\n",
       "            3,    3,    3,    3,    3,    3,    3,    3,    3,    3,    3,    3,\n",
       "            3],\n",
       "        [ 113,   57,   52,  435,  510,  249,   17,    8,   60,    6,    3,    3,\n",
       "            3,    3,    3,    3,    3,    3,    3,    3,    3,    3,    3,    3,\n",
       "            3,    3,    3,    3,    3,    3,    3,    3,    3,    3,    3,    3,\n",
       "            3,    3,    3,    3,    3,    3,    3,    3,    3,    3,    3,    3,\n",
       "            3],\n",
       "        [ 457, 1352,   12,  306,  271,    8,   60,    6,    3,    3,    3,    3,\n",
       "            3,    3,    3,    3,    3,    3,    3,    3,    3,    3,    3,    3,\n",
       "            3,    3,    3,    3,    3,    3,    3,    3,    3,    3,    3,    3,\n",
       "            3,    3,    3,    3,    3,    3,    3,    3,    3,    3,    3,    3,\n",
       "            3],\n",
       "        [ 234,  476,  391,   65, 7485, 1572,  633,   14,  218,   43,    7, 1884,\n",
       "          602,   26, 1152,   38, 1564,  309,  139,  466,    9,    6,    3,    3,\n",
       "            3,    3,    3,    3,    3,    3,    3,    3,    3,    3,    3,    3,\n",
       "            3,    3,    3,    3,    3,    3,    3,    3,    3,    3,    3,    3,\n",
       "            3]])"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "labels"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let us decode the labels."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Séen naa am guy.',\n",
       " 'Góor gi xale bi woo',\n",
       " 'Weneen fas la bëgg!',\n",
       " 'Kookule la kookule taxaw la!',\n",
       " 'Móobal yooyu nag, ya ca ëppon maanaa topp nañ ko ba Tugal.',\n",
       " 'Jiit ju jigéen la woon, ndax mu ngi bootoon njabootam.',\n",
       " 'Dafa mujje gise boppam ni doomu Afrig, dem bay xalaat ni àll bee gën a wóor ci nun fépp fu mu mënti doon.',\n",
       " 'Gis naa sama xarit yeneen yooyuu!',\n",
       " 'Faatim la soo demee!',\n",
       " 'Waaye ginnaaw da daan gudde ñibbisi ci kër gi, danu jàpp ni tere bi taxu koo am solo.']"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset.decode(labels)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pytorch1-HleOW5am-py3.10",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
