{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YTsVWwCgLAJl"
      },
      "source": [
        "First Training with the GPT-2 decoder ü§ñ (random search)\n",
        "-----------------------------------\n",
        "\n",
        "In this notebook, we will train the pre-trained GPT-2 model provided by OPEN-AI. It only tests how the model can be accurate on the corpora we extracted. It is undoubtedly a partial model. For the prediction, we will follow the fine-tuning tutorial available at the following medium link [fine-tuning-transformers](https://medium.com/towards-data-science/guide-to-fine-tuning-text-generation-models-gpt-2-gpt-neo-and-t5-dc5de6b3bc5e) and add hyperparameter search with the `wandb` library.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LpFGxo7ULAJp"
      },
      "source": [
        "We will make this with and without augmentation and see where we obtain better results. "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "0NmODpFxX2Ik"
      },
      "outputs": [],
      "source": [
        "# let us extend the paths of the system\n",
        "import sys\n",
        "\n",
        "path = \"/content/drive/MyDrive/Memoire/subject2/\"\n",
        "\n",
        "sys.path.extend([f\"{path}new_data\", f\"{path}wolof-translate\"])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JqaQuyv3XvHl",
        "outputId": "821493d1-7ece-4cbf-e54f-1ad254d5db2d"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "env: WANDB_LOG_MODEL=true\n",
            "env: WANDB_NOTEBOOK_NAME=training_gpt2_2.ipynb\n",
            "env: WANDB_API_KEY=237a8450cd2568ea1c8e1f8e0400708e79b6b4ee\n"
          ]
        }
      ],
      "source": [
        "# define environment\n",
        "%env WANDB_LOG_MODEL=true\n",
        "%env WANDB_NOTEBOOK_NAME=training_gpt2_2.ipynb\n",
        "%env WANDB_API_KEY=237a8450cd2568ea1c8e1f8e0400708e79b6b4ee "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "rOALYu0I1th2"
      },
      "outputs": [],
      "source": [
        "!pip install -qq wandb --upgrade"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YqElKFkPLAJq",
        "outputId": "72391b9c-cd13-4a1a-83ce-fc7fffd85aa7"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "2023-04-28 18:00:08.254108: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
            "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
            "2023-04-28 18:00:10.121859: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n",
            "2023-04-28 18:00:13.504013: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:996] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
            "2023-04-28 18:00:13.504623: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:996] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
            "2023-04-28 18:00:13.504851: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:996] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting fr-core-news-lg==3.5.0\n",
            "  Downloading https://github.com/explosion/spacy-models/releases/download/fr_core_news_lg-3.5.0/fr_core_news_lg-3.5.0-py3-none-any.whl (571.8 MB)\n",
            "\u001b[2K     \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m571.8/571.8 MB\u001b[0m \u001b[31m2.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: spacy<3.6.0,>=3.5.0 in /usr/local/lib/python3.10/dist-packages (from fr-core-news-lg==3.5.0) (3.5.2)\n",
            "Requirement already satisfied: requests<3.0.0,>=2.13.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.6.0,>=3.5.0->fr-core-news-lg==3.5.0) (2.27.1)\n",
            "Requirement already satisfied: langcodes<4.0.0,>=3.2.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.6.0,>=3.5.0->fr-core-news-lg==3.5.0) (3.3.0)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from spacy<3.6.0,>=3.5.0->fr-core-news-lg==3.5.0) (3.1.2)\n",
            "Requirement already satisfied: srsly<3.0.0,>=2.4.3 in /usr/local/lib/python3.10/dist-packages (from spacy<3.6.0,>=3.5.0->fr-core-news-lg==3.5.0) (2.4.6)\n",
            "Requirement already satisfied: catalogue<2.1.0,>=2.0.6 in /usr/local/lib/python3.10/dist-packages (from spacy<3.6.0,>=3.5.0->fr-core-news-lg==3.5.0) (2.0.8)\n",
            "Requirement already satisfied: spacy-legacy<3.1.0,>=3.0.11 in /usr/local/lib/python3.10/dist-packages (from spacy<3.6.0,>=3.5.0->fr-core-news-lg==3.5.0) (3.0.12)\n",
            "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /usr/local/lib/python3.10/dist-packages (from spacy<3.6.0,>=3.5.0->fr-core-news-lg==3.5.0) (2.0.7)\n",
            "Requirement already satisfied: typer<0.8.0,>=0.3.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.6.0,>=3.5.0->fr-core-news-lg==3.5.0) (0.7.0)\n",
            "Requirement already satisfied: pathy>=0.10.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.6.0,>=3.5.0->fr-core-news-lg==3.5.0) (0.10.1)\n",
            "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.6.0,>=3.5.0->fr-core-news-lg==3.5.0) (1.0.9)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.6.0,>=3.5.0->fr-core-news-lg==3.5.0) (23.1)\n",
            "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /usr/local/lib/python3.10/dist-packages (from spacy<3.6.0,>=3.5.0->fr-core-news-lg==3.5.0) (3.0.8)\n",
            "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.6.0,>=3.5.0->fr-core-news-lg==3.5.0) (4.65.0)\n",
            "Requirement already satisfied: numpy>=1.15.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.6.0,>=3.5.0->fr-core-news-lg==3.5.0) (1.22.4)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.10/dist-packages (from spacy<3.6.0,>=3.5.0->fr-core-news-lg==3.5.0) (67.7.2)\n",
            "Requirement already satisfied: pydantic!=1.8,!=1.8.1,<1.11.0,>=1.7.4 in /usr/local/lib/python3.10/dist-packages (from spacy<3.6.0,>=3.5.0->fr-core-news-lg==3.5.0) (1.10.7)\n",
            "Requirement already satisfied: thinc<8.2.0,>=8.1.8 in /usr/local/lib/python3.10/dist-packages (from spacy<3.6.0,>=3.5.0->fr-core-news-lg==3.5.0) (8.1.9)\n",
            "Requirement already satisfied: wasabi<1.2.0,>=0.9.1 in /usr/local/lib/python3.10/dist-packages (from spacy<3.6.0,>=3.5.0->fr-core-news-lg==3.5.0) (1.1.1)\n",
            "Requirement already satisfied: smart-open<7.0.0,>=5.2.1 in /usr/local/lib/python3.10/dist-packages (from spacy<3.6.0,>=3.5.0->fr-core-news-lg==3.5.0) (6.3.0)\n",
            "Requirement already satisfied: spacy-loggers<2.0.0,>=1.0.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.6.0,>=3.5.0->fr-core-news-lg==3.5.0) (1.0.4)\n",
            "Requirement already satisfied: typing-extensions>=4.2.0 in /usr/local/lib/python3.10/dist-packages (from pydantic!=1.8,!=1.8.1,<1.11.0,>=1.7.4->spacy<3.6.0,>=3.5.0->fr-core-news-lg==3.5.0) (4.5.0)\n",
            "Requirement already satisfied: charset-normalizer~=2.0.0 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.13.0->spacy<3.6.0,>=3.5.0->fr-core-news-lg==3.5.0) (2.0.12)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.13.0->spacy<3.6.0,>=3.5.0->fr-core-news-lg==3.5.0) (2022.12.7)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.13.0->spacy<3.6.0,>=3.5.0->fr-core-news-lg==3.5.0) (3.4)\n",
            "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.13.0->spacy<3.6.0,>=3.5.0->fr-core-news-lg==3.5.0) (1.26.15)\n",
            "Requirement already satisfied: blis<0.8.0,>=0.7.8 in /usr/local/lib/python3.10/dist-packages (from thinc<8.2.0,>=8.1.8->spacy<3.6.0,>=3.5.0->fr-core-news-lg==3.5.0) (0.7.9)\n",
            "Requirement already satisfied: confection<1.0.0,>=0.0.1 in /usr/local/lib/python3.10/dist-packages (from thinc<8.2.0,>=8.1.8->spacy<3.6.0,>=3.5.0->fr-core-news-lg==3.5.0) (0.0.4)\n",
            "Requirement already satisfied: click<9.0.0,>=7.1.1 in /usr/local/lib/python3.10/dist-packages (from typer<0.8.0,>=0.3.0->spacy<3.6.0,>=3.5.0->fr-core-news-lg==3.5.0) (8.1.3)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->spacy<3.6.0,>=3.5.0->fr-core-news-lg==3.5.0) (2.1.2)\n",
            "\u001b[38;5;2m‚úî Download and installation successful\u001b[0m\n",
            "You can now load the package via spacy.load('fr_core_news_lg')\n"
          ]
        }
      ],
      "source": [
        "!pip install evaluate -qq\n",
        "!pip install sacrebleu -qq\n",
        "# !pip install optuna -qq\n",
        "!pip install transformers -qq \n",
        "!pip install tokenizers -qq\n",
        "!pip install nlpaug -qq\n",
        "!pip install ray[tune] -qq\n",
        "!python -m spacy download fr_core_news_lg "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "dF37F8_nLAJr"
      },
      "outputs": [],
      "source": [
        "# let us import all necessary libraries\n",
        "from wolof_translate.utils.sent_transformers import TransformerSequences\n",
        "from transformers import GPT2LMHeadModel, TrainingArguments, Trainer\n",
        "from wolof_translate.data.dataset_v1 import SentenceDataset\n",
        "from wolof_translate.utils.sent_corrections import *\n",
        "from sklearn.model_selection import train_test_split\n",
        "from nlpaug.augmenter import char as nac\n",
        "from torch.utils.data import DataLoader\n",
        "# from datasets  import load_metric # make pip install evaluate instead\n",
        "# and pip install sacrebleu for instance\n",
        "from functools import partial\n",
        "from tqdm import tqdm\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import evaluate\n",
        "import torch\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ypAj4KXBLAJs"
      },
      "source": [
        "We will create two models: \n",
        "\n",
        "- One translating the french corpus to a wolof corpus [french_to_wolof](#french-to-wolof)\n",
        "- One translating the wolof corpus to a french corpus [wolof_to_french](#wolof-to-french)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mtgeyZoxLAJs"
      },
      "source": [
        "--------------"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "19MVywzSLAJt"
      },
      "source": [
        "## French to wolof"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "n4tP0YGyLAJt"
      },
      "source": [
        "### Configure dataset üî†"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "e6dLQ3poLAJu"
      },
      "source": [
        "We can use the same custom dataset that we created in [text_augmentation](text_augmentation.ipynb). But we need to split the data between train and test sets and save them."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "GyCZiVSvLAJu"
      },
      "outputs": [],
      "source": [
        "def split_data(random_state: int = 50):\n",
        "\n",
        "  # load the corpora and split into train and test sets\n",
        "  corpora = pd.read_csv(f\"{path}new_data/sent_extraction.csv\")\n",
        "\n",
        "  train_set, test_set = train_test_split(corpora, test_size=0.1, random_state=random_state)\n",
        "\n",
        "  # let us save the sets\n",
        "  train_set.to_csv(f\"{path}new_data/train_set.csv\", index=False)\n",
        "\n",
        "  test_set.to_csv(f\"{path}new_data/test_set.csv\", index=False)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WahLNKJ0LAJv"
      },
      "source": [
        "Let us recuperate the datasets with and without augmentation."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "id": "BIjksuH9LAJv"
      },
      "outputs": [],
      "source": [
        "def recuperate_datasets(fr_char_p: float, wf_char_p: float, fr_word_p: float, wf_word_p):\n",
        "\n",
        "  # without augmentation\n",
        "  # train_dataset = SentenceDataset(f\"{path}new_data/train_set.csv\", \n",
        "  #                                 tokenizer_path = f\"{path}wolof-translate/wolof_translate/tokenizers/tokenizer_v1.json\")\n",
        "\n",
        "  # test_dataset = SentenceDataset(f\"{path}new_data/test_set.csv\",\n",
        "                                # tokenizer_path = f\"{path}wolof-translate/wolof_translate/tokenizers/tokenizer_v1.json\")\n",
        "\n",
        "  # with augmentation\n",
        "  fr_augmentation = TransformerSequences(nac.KeyboardAug(aug_char_p=fr_char_p, aug_word_p=fr_word_p),\n",
        "                                        remove_mark_space, delete_guillemet_space)\n",
        "\n",
        "  train_dataset_aug = SentenceDataset(f\"{path}new_data/train_set.csv\", \n",
        "                                  tokenizer_path = f\"{path}wolof-translate/wolof_translate/tokenizers/tokenizer_v1.json\",\n",
        "                                  cp1_transformer=fr_augmentation, truncation=True,\n",
        "                                  max_len=579)\n",
        "\n",
        "  test_dataset = SentenceDataset(f\"{path}new_data/test_set.csv\",\n",
        "                                tokenizer_path = f\"{path}wolof-translate/wolof_translate/tokenizers/tokenizer_v1.json\",\n",
        "                                truncation=True, max_len=579)\n",
        "  \n",
        "  return train_dataset_aug, test_dataset\n",
        "  # return {\n",
        "  #     'False': {\n",
        "  #         'train_dataset': train_dataset,\n",
        "  #         'test_dataset': test_dataset,\n",
        "  #     },\n",
        "  #     'True': {\n",
        "  #         'train_dataset': train_dataset_aug,\n",
        "  #         'test_dataset': test_dataset_aug\n",
        "  #     }\n",
        "  #     }"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eLlcsICXpOmj"
      },
      "source": [
        "### Configure hyperparameter search ‚öôÔ∏è"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JR9MwAFQppk0"
      },
      "source": [
        "We have to configure the search space and the search method (\"random\" in our case). ."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QSNJ1s_ypZWg",
        "outputId": "1b27bd14-8e35-40fa-c6db-2ae39f91e002"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m WANDB_NOTEBOOK_NAME should be a path to a notebook file, couldn't find training_gpt2_2.ipynb.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33moumar-kane\u001b[0m (\u001b[33moumar-kane-team\u001b[0m). Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m If you're specifying your api key in code, ensure this code is not shared publicly.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Consider setting the WANDB_API_KEY environment variable, or running `wandb login` from the command line.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Appending key for api.wandb.ai to your netrc file: /root/.netrc\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Create sweep with ID: 14019vvc\n",
            "Sweep URL: https://wandb.ai/oumar-kane-team/gpt2-wolof-french-translation1/sweeps/14019vvc\n"
          ]
        }
      ],
      "source": [
        "import wandb\n",
        "wandb.login(key=\"237a8450cd2568ea1c8e1f8e0400708e79b6b4ee\")\n",
        "\n",
        "# hyperparameters\n",
        "sweep_config = {\n",
        "    'method': 'random',\n",
        "    'metric':{\n",
        "          'goal': 'minimize',\n",
        "          'name': 'eval_loss'\n",
        "      },\n",
        "    'parameters':\n",
        "    {\n",
        "      'epochs': {\n",
        "          'value': 1\n",
        "      },\n",
        "      'batch_size': {\n",
        "          'values': [2, 3, 5]\n",
        "      },\n",
        "      'learning_rate': {\n",
        "          'distribution': 'log_uniform_values',\n",
        "          'min': 1e-5,\n",
        "          'max': 1e-3\n",
        "      },\n",
        "      'weight_decay': {\n",
        "          'values': [0.0, 0.1, 0.2, 0.3, 0.4, 0.5]\n",
        "      },\n",
        "     'fr_char_p': {\n",
        "         'min': 0.0,\n",
        "         'max': 0.7\n",
        "     },\n",
        "     'fr_word_p': {\n",
        "          'min': 0.0,\n",
        "          'max': 0.7\n",
        "     },\n",
        "     'wf_char_p': {\n",
        "          'min': 0.0,\n",
        "          'max': 0.7\n",
        "     },\n",
        "     'wf_word_p': {\n",
        "          'min': 0.0,\n",
        "          'max': 0.7\n",
        "     },\n",
        "     'random_state': {\n",
        "         'values': [0, 10, 20, 30, 40, 50, 60, 70, 80, 100]\n",
        "     }\n",
        "    }\n",
        "}\n",
        "\n",
        "# Initialize the hyperparameter search\n",
        "sweep_id = wandb.sweep(sweep_config, project = \"gpt2-wolof-french-translation1\")\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0vhzP3IaLAJv"
      },
      "source": [
        "### Configure the model and the evaluation function ‚öôÔ∏è"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ts_cesDLLAJw"
      },
      "source": [
        "Let us recuperate the model and resize the token embeddings."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "id": "CO1jx85eLAJw"
      },
      "outputs": [],
      "source": [
        "def gpt2_model_init(tokenizer):\n",
        "  # set the mode name\n",
        "  model_name = \"gpt2\"\n",
        "\n",
        "  # recuperate the tokenizer from the dataset\n",
        "  tokenizer = tokenizer\n",
        "\n",
        "  # configure the model\n",
        "  model = GPT2LMHeadModel.from_pretrained(model_name).cuda()\n",
        "\n",
        "  # resize the token embeddings\n",
        "  model.resize_token_embeddings(len(tokenizer))\n",
        "\n",
        "  return model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "R8I3tm4WLAJx"
      },
      "source": [
        "Let us evaluate the predictions with the `bleu` metric."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "id": "IerZolDNLAJx"
      },
      "outputs": [],
      "source": [
        "# %%writefile wolof-translate/wolof_translate/utils/evaluation.py\n",
        "from tokenizers import Tokenizer\n",
        "from typing import *\n",
        "import numpy as np\n",
        "import evaluate\n",
        "\n",
        "class TranslationEvaluation:\n",
        "    \n",
        "    def __init__(self, \n",
        "                 tokenizer: Tokenizer,\n",
        "                 decoder: Union[Callable, None] = None,\n",
        "                 metric = evaluate.load('sacrebleu'),\n",
        "                 ):\n",
        "        \n",
        "        self.tokenizer = tokenizer\n",
        "        \n",
        "        self.decoder = decoder\n",
        "        \n",
        "        self.metric = metric\n",
        "    \n",
        "    def postprocess_text(self, preds, labels):\n",
        "        \n",
        "        preds = [pred.strip() for pred in preds]\n",
        "        \n",
        "        labels = [[label.strip()] for label in labels]\n",
        "        \n",
        "        return preds, labels\n",
        "\n",
        "    def compute_metrics(self, eval_preds):\n",
        "        \n",
        "        preds, labels = eval_preds.preds.detach().cpu(), labels.detach().cpu()\n",
        "        \n",
        "        if isinstance(preds, tuple):\n",
        "            \n",
        "            preds = preds[0]\n",
        "        \n",
        "        if self.decoder is None:\n",
        "            \n",
        "            decoded_preds = self.tokenizer.batch_decode(preds, skip_special_tokens=True)\n",
        "            \n",
        "            decoded_labels = self.tokenizer.batch_decode(labels, skip_special_tokens=True)\n",
        "            \n",
        "            decoded_preds, decoded_labels = self.postprocess_text(decoded_preds, decoded_labels)\n",
        "            \n",
        "            result = self.metric.compute(predictions=decoded_preds, references=decoded_labels)\n",
        "            \n",
        "            result = {\"bleu\": result[\"score\"]}\n",
        "            \n",
        "            prediction_lens = [np.count_nonzero(pred != self.tokenizer.pad_token_id) for pred in preds]\n",
        "            \n",
        "            result[\"gen_len\"] = np.mean(prediction_lens)\n",
        "        \n",
        "        else:\n",
        "            \n",
        "            predictions = list(self.decoder(preds))\n",
        "            \n",
        "            labels = list(self.decoder(labels))\n",
        "      \n",
        "            decoded_preds, decoded_labels = self.postprocess_text(predictions, labels)\n",
        "            \n",
        "            result = self.metric.compute(predictions=predictions, references=labels)\n",
        "            \n",
        "            result = {\"bleu\": result[\"score\"]}\n",
        "        \n",
        "        result = {k:round(v, 4) for k, v in result.items()}\n",
        "\n",
        "        wandb.log(\"bleu\", result[\"bleu\"])\n",
        "            \n",
        "        return result"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "id": "OEvlO5mtLAJx"
      },
      "outputs": [],
      "source": [
        "# %run wolof-translate/wolof_translate/utils/evaluation.py"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IuppKYiyLAJx"
      },
      "source": [
        "Let us initialize the evaluation object."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "id": "a7Bpd4UPLAJy"
      },
      "outputs": [],
      "source": [
        "# translation_eval = TranslationEvaluation(test_dataset.tokenizer)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xT17hB19LAJy"
      },
      "source": [
        "### Searching for the best parameters üïñ"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XQ5evOG5LAJw"
      },
      "source": [
        "Let us define the data collator."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "id": "SgVN115tLAJw"
      },
      "outputs": [],
      "source": [
        "def data_collator(batch):\n",
        "    \"\"\"Generate a batch of data to provide to trainer\n",
        "\n",
        "    Args:\n",
        "        batch (_type_): The batch\n",
        "\n",
        "    Returns:\n",
        "        dict: A dictionary containing the ids, the attention mask and the labels\n",
        "    \"\"\"\n",
        "    input_ids = torch.stack([b[0] for b in batch])\n",
        "    \n",
        "    attention_mask = torch.stack([b[1] for b in batch])\n",
        "    \n",
        "    labels = torch.stack([b[0] for b in batch])\n",
        "    \n",
        "    return {'input_ids': input_ids, 'attention_mask': attention_mask,\n",
        "            'labels': labels}"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ry3DmkBuLAJy"
      },
      "source": [
        "Let us initialize the training arguments and make random search."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000,
          "referenced_widgets": [
            "6342d7b19cf24413a7e6abc021730a7a",
            "d540120cbcac4649a26f8197b73a673f",
            "5b9e6e1bb7194446838b1e5d8efc2874",
            "249040adcb494e02960fb0df640d935a",
            "236123f576194b84b19fe8ad0cbb9529",
            "3d16fe0f000741bd98cbc13d48359816",
            "d85038d56365464095f65a20da8b4add",
            "f2073de3f26e469593f0732e681cf8e2"
          ]
        },
        "id": "D_yP2Ny6LAJy",
        "outputId": "1393818f-f0f3-4a71-8c20-a3d0f9f2f932"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\u001b[34m\u001b[1mwandb\u001b[0m: Agent Starting Run: bsir9vds with config:\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \tbatch_size: 3\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \tepochs: 1\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \tfr_char_p: 0.6502928646092081\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \tfr_word_p: 0.6817430409598793\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \tlearning_rate: 0.0005849589572035753\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \trandom_state: 70\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \tweight_decay: 0.4\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \twf_char_p: 0.6138587279075418\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \twf_word_p: 0.5944717582493727\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m WANDB_NOTEBOOK_NAME should be a path to a notebook file, couldn't find training_gpt2_2.ipynb.\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "Tracking run with wandb version 0.15.0"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Run data is saved locally in <code>/content/wandb/run-20230428_180132-bsir9vds</code>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Syncing run <strong><a href='https://wandb.ai/oumar-kane-team/gpt2-wolof-french-translation1/runs/bsir9vds' target=\"_blank\">flowing-sweep-1</a></strong> to <a href='https://wandb.ai/oumar-kane-team/gpt2-wolof-french-translation1' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>Sweep page: <a href='https://wandb.ai/oumar-kane-team/gpt2-wolof-french-translation1/sweeps/14019vvc' target=\"_blank\">https://wandb.ai/oumar-kane-team/gpt2-wolof-french-translation1/sweeps/14019vvc</a>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              " View project at <a href='https://wandb.ai/oumar-kane-team/gpt2-wolof-french-translation1' target=\"_blank\">https://wandb.ai/oumar-kane-team/gpt2-wolof-french-translation1</a>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              " View sweep at <a href='https://wandb.ai/oumar-kane-team/gpt2-wolof-french-translation1/sweeps/14019vvc' target=\"_blank\">https://wandb.ai/oumar-kane-team/gpt2-wolof-french-translation1/sweeps/14019vvc</a>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              " View run at <a href='https://wandb.ai/oumar-kane-team/gpt2-wolof-french-translation1/runs/bsir9vds' target=\"_blank\">https://wandb.ai/oumar-kane-team/gpt2-wolof-french-translation1/runs/bsir9vds</a>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/optimization.py:391: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
            "  warnings.warn(\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'learning_rate' was locked by 'sweep' (ignored update).\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'weight_decay' was locked by 'sweep' (ignored update).\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='245' max='245' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [245/245 01:35, Epoch 1/1]\n",
              "    </div>\n",
              "    <table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              " <tr style=\"text-align: left;\">\n",
              "      <th>Epoch</th>\n",
              "      <th>Training Loss</th>\n",
              "      <th>Validation Loss</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <td>1</td>\n",
              "      <td>1.474100</td>\n",
              "      <td>0.810593</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table><p>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Waiting for W&B process to finish... <strong style=\"color:green\">(success).</strong>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "<style>\n",
              "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
              "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
              "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
              "    </style>\n",
              "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>eval/loss</td><td>‚ñÅ</td></tr><tr><td>eval/runtime</td><td>‚ñÅ</td></tr><tr><td>eval/samples_per_second</td><td>‚ñÅ</td></tr><tr><td>eval/steps_per_second</td><td>‚ñÅ</td></tr><tr><td>train/epoch</td><td>‚ñÅ‚ñÅ‚ñÅ</td></tr><tr><td>train/global_step</td><td>‚ñÅ‚ñÅ‚ñÅ</td></tr><tr><td>train/learning_rate</td><td>‚ñÅ</td></tr><tr><td>train/loss</td><td>‚ñÅ</td></tr><tr><td>train/total_flos</td><td>‚ñÅ</td></tr><tr><td>train/train_loss</td><td>‚ñÅ</td></tr><tr><td>train/train_runtime</td><td>‚ñÅ</td></tr><tr><td>train/train_samples_per_second</td><td>‚ñÅ</td></tr><tr><td>train/train_steps_per_second</td><td>‚ñÅ</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>eval/loss</td><td>0.81059</td></tr><tr><td>eval/runtime</td><td>3.3476</td></tr><tr><td>eval/samples_per_second</td><td>24.495</td></tr><tr><td>eval/steps_per_second</td><td>5.078</td></tr><tr><td>train/epoch</td><td>1.0</td></tr><tr><td>train/global_step</td><td>245</td></tr><tr><td>train/learning_rate</td><td>2e-05</td></tr><tr><td>train/loss</td><td>1.4741</td></tr><tr><td>train/total_flos</td><td>216590170752000.0</td></tr><tr><td>train/train_loss</td><td>1.47413</td></tr><tr><td>train/train_runtime</td><td>91.8484</td></tr><tr><td>train/train_samples_per_second</td><td>7.981</td></tr><tr><td>train/train_steps_per_second</td><td>2.667</td></tr></table><br/></div></div>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              " View run <strong style=\"color:#cdcd00\">flowing-sweep-1</strong> at: <a href='https://wandb.ai/oumar-kane-team/gpt2-wolof-french-translation1/runs/bsir9vds' target=\"_blank\">https://wandb.ai/oumar-kane-team/gpt2-wolof-french-translation1/runs/bsir9vds</a><br/>Synced 5 W&B file(s), 0 media file(s), 4 artifact file(s) and 0 other file(s)"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Find logs at: <code>./wandb/run-20230428_180132-bsir9vds/logs</code>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\u001b[34m\u001b[1mwandb\u001b[0m: Agent Starting Run: 13n4uo4j with config:\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \tbatch_size: 3\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \tepochs: 1\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \tfr_char_p: 0.671533610651228\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \tfr_word_p: 0.3377438777612809\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \tlearning_rate: 0.00011511089726939806\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \trandom_state: 40\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \tweight_decay: 0.4\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \twf_char_p: 0.3217907770297627\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \twf_word_p: 0.5495165396790335\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m WANDB_NOTEBOOK_NAME should be a path to a notebook file, couldn't find training_gpt2_2.ipynb.\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "Tracking run with wandb version 0.15.0"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Run data is saved locally in <code>/content/wandb/run-20230428_180356-13n4uo4j</code>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Syncing run <strong><a href='https://wandb.ai/oumar-kane-team/gpt2-wolof-french-translation1/runs/13n4uo4j' target=\"_blank\">faithful-sweep-2</a></strong> to <a href='https://wandb.ai/oumar-kane-team/gpt2-wolof-french-translation1' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>Sweep page: <a href='https://wandb.ai/oumar-kane-team/gpt2-wolof-french-translation1/sweeps/14019vvc' target=\"_blank\">https://wandb.ai/oumar-kane-team/gpt2-wolof-french-translation1/sweeps/14019vvc</a>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              " View project at <a href='https://wandb.ai/oumar-kane-team/gpt2-wolof-french-translation1' target=\"_blank\">https://wandb.ai/oumar-kane-team/gpt2-wolof-french-translation1</a>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              " View sweep at <a href='https://wandb.ai/oumar-kane-team/gpt2-wolof-french-translation1/sweeps/14019vvc' target=\"_blank\">https://wandb.ai/oumar-kane-team/gpt2-wolof-french-translation1/sweeps/14019vvc</a>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              " View run at <a href='https://wandb.ai/oumar-kane-team/gpt2-wolof-french-translation1/runs/13n4uo4j' target=\"_blank\">https://wandb.ai/oumar-kane-team/gpt2-wolof-french-translation1/runs/13n4uo4j</a>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/optimization.py:391: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
            "  warnings.warn(\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'learning_rate' was locked by 'sweep' (ignored update).\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'weight_decay' was locked by 'sweep' (ignored update).\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='245' max='245' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [245/245 01:34, Epoch 1/1]\n",
              "    </div>\n",
              "    <table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              " <tr style=\"text-align: left;\">\n",
              "      <th>Epoch</th>\n",
              "      <th>Training Loss</th>\n",
              "      <th>Validation Loss</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <td>1</td>\n",
              "      <td>1.510800</td>\n",
              "      <td>0.853371</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table><p>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Waiting for W&B process to finish... <strong style=\"color:green\">(success).</strong>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "<style>\n",
              "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
              "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
              "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
              "    </style>\n",
              "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>eval/loss</td><td>‚ñÅ</td></tr><tr><td>eval/runtime</td><td>‚ñÅ</td></tr><tr><td>eval/samples_per_second</td><td>‚ñÅ</td></tr><tr><td>eval/steps_per_second</td><td>‚ñÅ</td></tr><tr><td>train/epoch</td><td>‚ñÅ‚ñÅ‚ñÅ</td></tr><tr><td>train/global_step</td><td>‚ñÅ‚ñÅ‚ñÅ</td></tr><tr><td>train/learning_rate</td><td>‚ñÅ</td></tr><tr><td>train/loss</td><td>‚ñÅ</td></tr><tr><td>train/total_flos</td><td>‚ñÅ</td></tr><tr><td>train/train_loss</td><td>‚ñÅ</td></tr><tr><td>train/train_runtime</td><td>‚ñÅ</td></tr><tr><td>train/train_samples_per_second</td><td>‚ñÅ</td></tr><tr><td>train/train_steps_per_second</td><td>‚ñÅ</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>eval/loss</td><td>0.85337</td></tr><tr><td>eval/runtime</td><td>2.7043</td></tr><tr><td>eval/samples_per_second</td><td>30.322</td></tr><tr><td>eval/steps_per_second</td><td>6.286</td></tr><tr><td>train/epoch</td><td>1.0</td></tr><tr><td>train/global_step</td><td>245</td></tr><tr><td>train/learning_rate</td><td>0.0</td></tr><tr><td>train/loss</td><td>1.5108</td></tr><tr><td>train/total_flos</td><td>216590170752000.0</td></tr><tr><td>train/train_loss</td><td>1.51082</td></tr><tr><td>train/train_runtime</td><td>88.9753</td></tr><tr><td>train/train_samples_per_second</td><td>8.238</td></tr><tr><td>train/train_steps_per_second</td><td>2.754</td></tr></table><br/></div></div>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              " View run <strong style=\"color:#cdcd00\">faithful-sweep-2</strong> at: <a href='https://wandb.ai/oumar-kane-team/gpt2-wolof-french-translation1/runs/13n4uo4j' target=\"_blank\">https://wandb.ai/oumar-kane-team/gpt2-wolof-french-translation1/runs/13n4uo4j</a><br/>Synced 5 W&B file(s), 0 media file(s), 4 artifact file(s) and 0 other file(s)"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Find logs at: <code>./wandb/run-20230428_180356-13n4uo4j/logs</code>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\u001b[34m\u001b[1mwandb\u001b[0m: Agent Starting Run: 1uiv2ha8 with config:\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \tbatch_size: 3\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \tepochs: 1\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \tfr_char_p: 0.240477160240458\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \tfr_word_p: 0.537085624723154\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \tlearning_rate: 0.00011294168073571068\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \trandom_state: 0\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \tweight_decay: 0.2\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \twf_char_p: 0.036998228943525474\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \twf_word_p: 0.34284569105601037\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m WANDB_NOTEBOOK_NAME should be a path to a notebook file, couldn't find training_gpt2_2.ipynb.\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "Tracking run with wandb version 0.15.0"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Run data is saved locally in <code>/content/wandb/run-20230428_180559-1uiv2ha8</code>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Syncing run <strong><a href='https://wandb.ai/oumar-kane-team/gpt2-wolof-french-translation1/runs/1uiv2ha8' target=\"_blank\">solar-sweep-3</a></strong> to <a href='https://wandb.ai/oumar-kane-team/gpt2-wolof-french-translation1' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>Sweep page: <a href='https://wandb.ai/oumar-kane-team/gpt2-wolof-french-translation1/sweeps/14019vvc' target=\"_blank\">https://wandb.ai/oumar-kane-team/gpt2-wolof-french-translation1/sweeps/14019vvc</a>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              " View project at <a href='https://wandb.ai/oumar-kane-team/gpt2-wolof-french-translation1' target=\"_blank\">https://wandb.ai/oumar-kane-team/gpt2-wolof-french-translation1</a>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              " View sweep at <a href='https://wandb.ai/oumar-kane-team/gpt2-wolof-french-translation1/sweeps/14019vvc' target=\"_blank\">https://wandb.ai/oumar-kane-team/gpt2-wolof-french-translation1/sweeps/14019vvc</a>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              " View run at <a href='https://wandb.ai/oumar-kane-team/gpt2-wolof-french-translation1/runs/1uiv2ha8' target=\"_blank\">https://wandb.ai/oumar-kane-team/gpt2-wolof-french-translation1/runs/1uiv2ha8</a>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/optimization.py:391: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
            "  warnings.warn(\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'learning_rate' was locked by 'sweep' (ignored update).\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'weight_decay' was locked by 'sweep' (ignored update).\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='245' max='245' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [245/245 01:34, Epoch 1/1]\n",
              "    </div>\n",
              "    <table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              " <tr style=\"text-align: left;\">\n",
              "      <th>Epoch</th>\n",
              "      <th>Training Loss</th>\n",
              "      <th>Validation Loss</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <td>1</td>\n",
              "      <td>1.468400</td>\n",
              "      <td>0.796042</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table><p>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Waiting for W&B process to finish... <strong style=\"color:green\">(success).</strong>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "<style>\n",
              "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
              "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
              "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
              "    </style>\n",
              "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>eval/loss</td><td>‚ñÅ</td></tr><tr><td>eval/runtime</td><td>‚ñÅ</td></tr><tr><td>eval/samples_per_second</td><td>‚ñÅ</td></tr><tr><td>eval/steps_per_second</td><td>‚ñÅ</td></tr><tr><td>train/epoch</td><td>‚ñÅ‚ñÅ‚ñÅ</td></tr><tr><td>train/global_step</td><td>‚ñÅ‚ñÅ‚ñÅ</td></tr><tr><td>train/learning_rate</td><td>‚ñÅ</td></tr><tr><td>train/loss</td><td>‚ñÅ</td></tr><tr><td>train/total_flos</td><td>‚ñÅ</td></tr><tr><td>train/train_loss</td><td>‚ñÅ</td></tr><tr><td>train/train_runtime</td><td>‚ñÅ</td></tr><tr><td>train/train_samples_per_second</td><td>‚ñÅ</td></tr><tr><td>train/train_steps_per_second</td><td>‚ñÅ</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>eval/loss</td><td>0.79604</td></tr><tr><td>eval/runtime</td><td>2.7108</td></tr><tr><td>eval/samples_per_second</td><td>30.25</td></tr><tr><td>eval/steps_per_second</td><td>6.271</td></tr><tr><td>train/epoch</td><td>1.0</td></tr><tr><td>train/global_step</td><td>245</td></tr><tr><td>train/learning_rate</td><td>0.0</td></tr><tr><td>train/loss</td><td>1.4684</td></tr><tr><td>train/total_flos</td><td>216590170752000.0</td></tr><tr><td>train/train_loss</td><td>1.46838</td></tr><tr><td>train/train_runtime</td><td>88.0577</td></tr><tr><td>train/train_samples_per_second</td><td>8.324</td></tr><tr><td>train/train_steps_per_second</td><td>2.782</td></tr></table><br/></div></div>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              " View run <strong style=\"color:#cdcd00\">solar-sweep-3</strong> at: <a href='https://wandb.ai/oumar-kane-team/gpt2-wolof-french-translation1/runs/1uiv2ha8' target=\"_blank\">https://wandb.ai/oumar-kane-team/gpt2-wolof-french-translation1/runs/1uiv2ha8</a><br/>Synced 5 W&B file(s), 0 media file(s), 4 artifact file(s) and 0 other file(s)"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Find logs at: <code>./wandb/run-20230428_180559-1uiv2ha8/logs</code>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\u001b[34m\u001b[1mwandb\u001b[0m: Agent Starting Run: ezr9ubb6 with config:\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \tbatch_size: 3\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \tepochs: 1\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \tfr_char_p: 0.5254516886812962\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \tfr_word_p: 0.09464907125173204\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \tlearning_rate: 1.3111707137809985e-05\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \trandom_state: 30\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \tweight_decay: 0.5\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \twf_char_p: 0.18792842653139763\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \twf_word_p: 0.1625617142576774\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m WANDB_NOTEBOOK_NAME should be a path to a notebook file, couldn't find training_gpt2_2.ipynb.\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "Tracking run with wandb version 0.15.0"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Run data is saved locally in <code>/content/wandb/run-20230428_180801-ezr9ubb6</code>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Syncing run <strong><a href='https://wandb.ai/oumar-kane-team/gpt2-wolof-french-translation1/runs/ezr9ubb6' target=\"_blank\">lunar-sweep-4</a></strong> to <a href='https://wandb.ai/oumar-kane-team/gpt2-wolof-french-translation1' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>Sweep page: <a href='https://wandb.ai/oumar-kane-team/gpt2-wolof-french-translation1/sweeps/14019vvc' target=\"_blank\">https://wandb.ai/oumar-kane-team/gpt2-wolof-french-translation1/sweeps/14019vvc</a>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              " View project at <a href='https://wandb.ai/oumar-kane-team/gpt2-wolof-french-translation1' target=\"_blank\">https://wandb.ai/oumar-kane-team/gpt2-wolof-french-translation1</a>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              " View sweep at <a href='https://wandb.ai/oumar-kane-team/gpt2-wolof-french-translation1/sweeps/14019vvc' target=\"_blank\">https://wandb.ai/oumar-kane-team/gpt2-wolof-french-translation1/sweeps/14019vvc</a>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              " View run at <a href='https://wandb.ai/oumar-kane-team/gpt2-wolof-french-translation1/runs/ezr9ubb6' target=\"_blank\">https://wandb.ai/oumar-kane-team/gpt2-wolof-french-translation1/runs/ezr9ubb6</a>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/optimization.py:391: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
            "  warnings.warn(\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'learning_rate' was locked by 'sweep' (ignored update).\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'weight_decay' was locked by 'sweep' (ignored update).\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='245' max='245' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [245/245 01:34, Epoch 1/1]\n",
              "    </div>\n",
              "    <table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              " <tr style=\"text-align: left;\">\n",
              "      <th>Epoch</th>\n",
              "      <th>Training Loss</th>\n",
              "      <th>Validation Loss</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <td>1</td>\n",
              "      <td>1.627300</td>\n",
              "      <td>1.010671</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table><p>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Waiting for W&B process to finish... <strong style=\"color:green\">(success).</strong>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "<style>\n",
              "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
              "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
              "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
              "    </style>\n",
              "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>eval/loss</td><td>‚ñÅ</td></tr><tr><td>eval/runtime</td><td>‚ñÅ</td></tr><tr><td>eval/samples_per_second</td><td>‚ñÅ</td></tr><tr><td>eval/steps_per_second</td><td>‚ñÅ</td></tr><tr><td>train/epoch</td><td>‚ñÅ‚ñÅ‚ñÅ</td></tr><tr><td>train/global_step</td><td>‚ñÅ‚ñÅ‚ñÅ</td></tr><tr><td>train/learning_rate</td><td>‚ñÅ</td></tr><tr><td>train/loss</td><td>‚ñÅ</td></tr><tr><td>train/total_flos</td><td>‚ñÅ</td></tr><tr><td>train/train_loss</td><td>‚ñÅ</td></tr><tr><td>train/train_runtime</td><td>‚ñÅ</td></tr><tr><td>train/train_samples_per_second</td><td>‚ñÅ</td></tr><tr><td>train/train_steps_per_second</td><td>‚ñÅ</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>eval/loss</td><td>1.01067</td></tr><tr><td>eval/runtime</td><td>2.7106</td></tr><tr><td>eval/samples_per_second</td><td>30.252</td></tr><tr><td>eval/steps_per_second</td><td>6.272</td></tr><tr><td>train/epoch</td><td>1.0</td></tr><tr><td>train/global_step</td><td>245</td></tr><tr><td>train/learning_rate</td><td>0.0</td></tr><tr><td>train/loss</td><td>1.6273</td></tr><tr><td>train/total_flos</td><td>216590170752000.0</td></tr><tr><td>train/train_loss</td><td>1.62726</td></tr><tr><td>train/train_runtime</td><td>88.351</td></tr><tr><td>train/train_samples_per_second</td><td>8.296</td></tr><tr><td>train/train_steps_per_second</td><td>2.773</td></tr></table><br/></div></div>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              " View run <strong style=\"color:#cdcd00\">lunar-sweep-4</strong> at: <a href='https://wandb.ai/oumar-kane-team/gpt2-wolof-french-translation1/runs/ezr9ubb6' target=\"_blank\">https://wandb.ai/oumar-kane-team/gpt2-wolof-french-translation1/runs/ezr9ubb6</a><br/>Synced 5 W&B file(s), 0 media file(s), 4 artifact file(s) and 0 other file(s)"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Find logs at: <code>./wandb/run-20230428_180801-ezr9ubb6/logs</code>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\u001b[34m\u001b[1mwandb\u001b[0m: Agent Starting Run: d2b5l70j with config:\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \tbatch_size: 5\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \tepochs: 1\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \tfr_char_p: 0.4234154666698189\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \tfr_word_p: 0.07783532677338817\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \tlearning_rate: 1.2354743954007584e-05\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \trandom_state: 20\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \tweight_decay: 0.5\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \twf_char_p: 0.6419901404282341\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \twf_word_p: 0.5418217804360558\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m WANDB_NOTEBOOK_NAME should be a path to a notebook file, couldn't find training_gpt2_2.ipynb.\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "Tracking run with wandb version 0.15.0"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Run data is saved locally in <code>/content/wandb/run-20230428_180958-d2b5l70j</code>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Syncing run <strong><a href='https://wandb.ai/oumar-kane-team/gpt2-wolof-french-translation1/runs/d2b5l70j' target=\"_blank\">volcanic-sweep-5</a></strong> to <a href='https://wandb.ai/oumar-kane-team/gpt2-wolof-french-translation1' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>Sweep page: <a href='https://wandb.ai/oumar-kane-team/gpt2-wolof-french-translation1/sweeps/14019vvc' target=\"_blank\">https://wandb.ai/oumar-kane-team/gpt2-wolof-french-translation1/sweeps/14019vvc</a>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              " View project at <a href='https://wandb.ai/oumar-kane-team/gpt2-wolof-french-translation1' target=\"_blank\">https://wandb.ai/oumar-kane-team/gpt2-wolof-french-translation1</a>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              " View sweep at <a href='https://wandb.ai/oumar-kane-team/gpt2-wolof-french-translation1/sweeps/14019vvc' target=\"_blank\">https://wandb.ai/oumar-kane-team/gpt2-wolof-french-translation1/sweeps/14019vvc</a>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              " View run at <a href='https://wandb.ai/oumar-kane-team/gpt2-wolof-french-translation1/runs/d2b5l70j' target=\"_blank\">https://wandb.ai/oumar-kane-team/gpt2-wolof-french-translation1/runs/d2b5l70j</a>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/optimization.py:391: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
            "  warnings.warn(\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'learning_rate' was locked by 'sweep' (ignored update).\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'weight_decay' was locked by 'sweep' (ignored update).\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='147' max='147' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [147/147 01:28, Epoch 1/1]\n",
              "    </div>\n",
              "    <table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              " <tr style=\"text-align: left;\">\n",
              "      <th>Epoch</th>\n",
              "      <th>Training Loss</th>\n",
              "      <th>Validation Loss</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <td>1</td>\n",
              "      <td>1.893000</td>\n",
              "      <td>1.030977</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table><p>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Waiting for W&B process to finish... <strong style=\"color:green\">(success).</strong>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "<style>\n",
              "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
              "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
              "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
              "    </style>\n",
              "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>eval/loss</td><td>‚ñÅ</td></tr><tr><td>eval/runtime</td><td>‚ñÅ</td></tr><tr><td>eval/samples_per_second</td><td>‚ñÅ</td></tr><tr><td>eval/steps_per_second</td><td>‚ñÅ</td></tr><tr><td>train/epoch</td><td>‚ñÅ‚ñÅ‚ñÅ</td></tr><tr><td>train/global_step</td><td>‚ñÅ‚ñÅ‚ñÅ</td></tr><tr><td>train/learning_rate</td><td>‚ñÅ</td></tr><tr><td>train/loss</td><td>‚ñÅ</td></tr><tr><td>train/total_flos</td><td>‚ñÅ</td></tr><tr><td>train/train_loss</td><td>‚ñÅ</td></tr><tr><td>train/train_runtime</td><td>‚ñÅ</td></tr><tr><td>train/train_samples_per_second</td><td>‚ñÅ</td></tr><tr><td>train/train_steps_per_second</td><td>‚ñÅ</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>eval/loss</td><td>1.03098</td></tr><tr><td>eval/runtime</td><td>2.702</td></tr><tr><td>eval/samples_per_second</td><td>30.348</td></tr><tr><td>eval/steps_per_second</td><td>6.292</td></tr><tr><td>train/epoch</td><td>1.0</td></tr><tr><td>train/global_step</td><td>147</td></tr><tr><td>train/learning_rate</td><td>0.0</td></tr><tr><td>train/loss</td><td>1.893</td></tr><tr><td>train/total_flos</td><td>216590170752000.0</td></tr><tr><td>train/train_loss</td><td>1.89304</td></tr><tr><td>train/train_runtime</td><td>83.0542</td></tr><tr><td>train/train_samples_per_second</td><td>8.826</td></tr><tr><td>train/train_steps_per_second</td><td>1.77</td></tr></table><br/></div></div>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              " View run <strong style=\"color:#cdcd00\">volcanic-sweep-5</strong> at: <a href='https://wandb.ai/oumar-kane-team/gpt2-wolof-french-translation1/runs/d2b5l70j' target=\"_blank\">https://wandb.ai/oumar-kane-team/gpt2-wolof-french-translation1/runs/d2b5l70j</a><br/>Synced 5 W&B file(s), 0 media file(s), 4 artifact file(s) and 0 other file(s)"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Find logs at: <code>./wandb/run-20230428_180958-d2b5l70j/logs</code>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\u001b[34m\u001b[1mwandb\u001b[0m: Agent Starting Run: 6801a1l4 with config:\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \tbatch_size: 3\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \tepochs: 1\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \tfr_char_p: 0.4429581438929929\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \tfr_word_p: 0.3632366347028243\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \tlearning_rate: 2.6267197537308377e-05\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \trandom_state: 30\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \tweight_decay: 0.5\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \twf_char_p: 0.17395027165161914\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \twf_word_p: 0.4096146675858922\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m WANDB_NOTEBOOK_NAME should be a path to a notebook file, couldn't find training_gpt2_2.ipynb.\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "Tracking run with wandb version 0.15.0"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Run data is saved locally in <code>/content/wandb/run-20230428_181154-6801a1l4</code>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Syncing run <strong><a href='https://wandb.ai/oumar-kane-team/gpt2-wolof-french-translation1/runs/6801a1l4' target=\"_blank\">tough-sweep-6</a></strong> to <a href='https://wandb.ai/oumar-kane-team/gpt2-wolof-french-translation1' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>Sweep page: <a href='https://wandb.ai/oumar-kane-team/gpt2-wolof-french-translation1/sweeps/14019vvc' target=\"_blank\">https://wandb.ai/oumar-kane-team/gpt2-wolof-french-translation1/sweeps/14019vvc</a>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              " View project at <a href='https://wandb.ai/oumar-kane-team/gpt2-wolof-french-translation1' target=\"_blank\">https://wandb.ai/oumar-kane-team/gpt2-wolof-french-translation1</a>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              " View sweep at <a href='https://wandb.ai/oumar-kane-team/gpt2-wolof-french-translation1/sweeps/14019vvc' target=\"_blank\">https://wandb.ai/oumar-kane-team/gpt2-wolof-french-translation1/sweeps/14019vvc</a>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              " View run at <a href='https://wandb.ai/oumar-kane-team/gpt2-wolof-french-translation1/runs/6801a1l4' target=\"_blank\">https://wandb.ai/oumar-kane-team/gpt2-wolof-french-translation1/runs/6801a1l4</a>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/optimization.py:391: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
            "  warnings.warn(\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'learning_rate' was locked by 'sweep' (ignored update).\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'weight_decay' was locked by 'sweep' (ignored update).\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='245' max='245' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [245/245 01:34, Epoch 1/1]\n",
              "    </div>\n",
              "    <table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              " <tr style=\"text-align: left;\">\n",
              "      <th>Epoch</th>\n",
              "      <th>Training Loss</th>\n",
              "      <th>Validation Loss</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <td>1</td>\n",
              "      <td>1.635600</td>\n",
              "      <td>0.992037</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table><p>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Waiting for W&B process to finish... <strong style=\"color:green\">(success).</strong>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "<style>\n",
              "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
              "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
              "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
              "    </style>\n",
              "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>eval/loss</td><td>‚ñÅ</td></tr><tr><td>eval/runtime</td><td>‚ñÅ</td></tr><tr><td>eval/samples_per_second</td><td>‚ñÅ</td></tr><tr><td>eval/steps_per_second</td><td>‚ñÅ</td></tr><tr><td>train/epoch</td><td>‚ñÅ‚ñÅ‚ñÅ</td></tr><tr><td>train/global_step</td><td>‚ñÅ‚ñÅ‚ñÅ</td></tr><tr><td>train/learning_rate</td><td>‚ñÅ</td></tr><tr><td>train/loss</td><td>‚ñÅ</td></tr><tr><td>train/total_flos</td><td>‚ñÅ</td></tr><tr><td>train/train_loss</td><td>‚ñÅ</td></tr><tr><td>train/train_runtime</td><td>‚ñÅ</td></tr><tr><td>train/train_samples_per_second</td><td>‚ñÅ</td></tr><tr><td>train/train_steps_per_second</td><td>‚ñÅ</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>eval/loss</td><td>0.99204</td></tr><tr><td>eval/runtime</td><td>2.7061</td></tr><tr><td>eval/samples_per_second</td><td>30.302</td></tr><tr><td>eval/steps_per_second</td><td>6.282</td></tr><tr><td>train/epoch</td><td>1.0</td></tr><tr><td>train/global_step</td><td>245</td></tr><tr><td>train/learning_rate</td><td>0.0</td></tr><tr><td>train/loss</td><td>1.6356</td></tr><tr><td>train/total_flos</td><td>216590170752000.0</td></tr><tr><td>train/train_loss</td><td>1.63561</td></tr><tr><td>train/train_runtime</td><td>89.2108</td></tr><tr><td>train/train_samples_per_second</td><td>8.216</td></tr><tr><td>train/train_steps_per_second</td><td>2.746</td></tr></table><br/></div></div>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              " View run <strong style=\"color:#cdcd00\">tough-sweep-6</strong> at: <a href='https://wandb.ai/oumar-kane-team/gpt2-wolof-french-translation1/runs/6801a1l4' target=\"_blank\">https://wandb.ai/oumar-kane-team/gpt2-wolof-french-translation1/runs/6801a1l4</a><br/>Synced 5 W&B file(s), 0 media file(s), 4 artifact file(s) and 0 other file(s)"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Find logs at: <code>./wandb/run-20230428_181154-6801a1l4/logs</code>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\u001b[34m\u001b[1mwandb\u001b[0m: Agent Starting Run: vqouplax with config:\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \tbatch_size: 2\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \tepochs: 1\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \tfr_char_p: 0.45257388569971824\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \tfr_word_p: 0.2377921526767132\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \tlearning_rate: 0.0001388916152557796\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \trandom_state: 10\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \tweight_decay: 0.1\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \twf_char_p: 0.5470738083751061\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \twf_word_p: 0.13560745933627427\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m WANDB_NOTEBOOK_NAME should be a path to a notebook file, couldn't find training_gpt2_2.ipynb.\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "Tracking run with wandb version 0.15.0"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Run data is saved locally in <code>/content/wandb/run-20230428_181358-vqouplax</code>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Syncing run <strong><a href='https://wandb.ai/oumar-kane-team/gpt2-wolof-french-translation1/runs/vqouplax' target=\"_blank\">floral-sweep-7</a></strong> to <a href='https://wandb.ai/oumar-kane-team/gpt2-wolof-french-translation1' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>Sweep page: <a href='https://wandb.ai/oumar-kane-team/gpt2-wolof-french-translation1/sweeps/14019vvc' target=\"_blank\">https://wandb.ai/oumar-kane-team/gpt2-wolof-french-translation1/sweeps/14019vvc</a>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              " View project at <a href='https://wandb.ai/oumar-kane-team/gpt2-wolof-french-translation1' target=\"_blank\">https://wandb.ai/oumar-kane-team/gpt2-wolof-french-translation1</a>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              " View sweep at <a href='https://wandb.ai/oumar-kane-team/gpt2-wolof-french-translation1/sweeps/14019vvc' target=\"_blank\">https://wandb.ai/oumar-kane-team/gpt2-wolof-french-translation1/sweeps/14019vvc</a>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              " View run at <a href='https://wandb.ai/oumar-kane-team/gpt2-wolof-french-translation1/runs/vqouplax' target=\"_blank\">https://wandb.ai/oumar-kane-team/gpt2-wolof-french-translation1/runs/vqouplax</a>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/optimization.py:391: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
            "  warnings.warn(\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'learning_rate' was locked by 'sweep' (ignored update).\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'weight_decay' was locked by 'sweep' (ignored update).\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='367' max='367' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [367/367 01:41, Epoch 1/1]\n",
              "    </div>\n",
              "    <table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              " <tr style=\"text-align: left;\">\n",
              "      <th>Epoch</th>\n",
              "      <th>Training Loss</th>\n",
              "      <th>Validation Loss</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <td>1</td>\n",
              "      <td>1.369700</td>\n",
              "      <td>0.800643</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table><p>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Waiting for W&B process to finish... <strong style=\"color:green\">(success).</strong>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "<style>\n",
              "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
              "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
              "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
              "    </style>\n",
              "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>eval/loss</td><td>‚ñÅ</td></tr><tr><td>eval/runtime</td><td>‚ñÅ</td></tr><tr><td>eval/samples_per_second</td><td>‚ñÅ</td></tr><tr><td>eval/steps_per_second</td><td>‚ñÅ</td></tr><tr><td>train/epoch</td><td>‚ñÅ‚ñÅ‚ñÅ</td></tr><tr><td>train/global_step</td><td>‚ñÅ‚ñÅ‚ñÅ</td></tr><tr><td>train/learning_rate</td><td>‚ñÅ</td></tr><tr><td>train/loss</td><td>‚ñÅ</td></tr><tr><td>train/total_flos</td><td>‚ñÅ</td></tr><tr><td>train/train_loss</td><td>‚ñÅ</td></tr><tr><td>train/train_runtime</td><td>‚ñÅ</td></tr><tr><td>train/train_samples_per_second</td><td>‚ñÅ</td></tr><tr><td>train/train_steps_per_second</td><td>‚ñÅ</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>eval/loss</td><td>0.80064</td></tr><tr><td>eval/runtime</td><td>2.7226</td></tr><tr><td>eval/samples_per_second</td><td>30.118</td></tr><tr><td>eval/steps_per_second</td><td>6.244</td></tr><tr><td>train/epoch</td><td>1.0</td></tr><tr><td>train/global_step</td><td>367</td></tr><tr><td>train/learning_rate</td><td>0.0</td></tr><tr><td>train/loss</td><td>1.3697</td></tr><tr><td>train/total_flos</td><td>216590170752000.0</td></tr><tr><td>train/train_loss</td><td>1.3697</td></tr><tr><td>train/train_runtime</td><td>95.5691</td></tr><tr><td>train/train_samples_per_second</td><td>7.67</td></tr><tr><td>train/train_steps_per_second</td><td>3.84</td></tr></table><br/></div></div>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              " View run <strong style=\"color:#cdcd00\">floral-sweep-7</strong> at: <a href='https://wandb.ai/oumar-kane-team/gpt2-wolof-french-translation1/runs/vqouplax' target=\"_blank\">https://wandb.ai/oumar-kane-team/gpt2-wolof-french-translation1/runs/vqouplax</a><br/>Synced 5 W&B file(s), 0 media file(s), 4 artifact file(s) and 0 other file(s)"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Find logs at: <code>./wandb/run-20230428_181358-vqouplax/logs</code>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\u001b[34m\u001b[1mwandb\u001b[0m: Agent Starting Run: geo1o00w with config:\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \tbatch_size: 3\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \tepochs: 1\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \tfr_char_p: 0.1815884829234138\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \tfr_word_p: 0.6507696914179271\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \tlearning_rate: 0.000698583265792357\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \trandom_state: 100\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \tweight_decay: 0.4\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \twf_char_p: 0.5816640580970726\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \twf_word_p: 0.07847189788683867\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m WANDB_NOTEBOOK_NAME should be a path to a notebook file, couldn't find training_gpt2_2.ipynb.\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "Tracking run with wandb version 0.15.0"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Run data is saved locally in <code>/content/wandb/run-20230428_181606-geo1o00w</code>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Syncing run <strong><a href='https://wandb.ai/oumar-kane-team/gpt2-wolof-french-translation1/runs/geo1o00w' target=\"_blank\">grateful-sweep-8</a></strong> to <a href='https://wandb.ai/oumar-kane-team/gpt2-wolof-french-translation1' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>Sweep page: <a href='https://wandb.ai/oumar-kane-team/gpt2-wolof-french-translation1/sweeps/14019vvc' target=\"_blank\">https://wandb.ai/oumar-kane-team/gpt2-wolof-french-translation1/sweeps/14019vvc</a>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              " View project at <a href='https://wandb.ai/oumar-kane-team/gpt2-wolof-french-translation1' target=\"_blank\">https://wandb.ai/oumar-kane-team/gpt2-wolof-french-translation1</a>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              " View sweep at <a href='https://wandb.ai/oumar-kane-team/gpt2-wolof-french-translation1/sweeps/14019vvc' target=\"_blank\">https://wandb.ai/oumar-kane-team/gpt2-wolof-french-translation1/sweeps/14019vvc</a>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              " View run at <a href='https://wandb.ai/oumar-kane-team/gpt2-wolof-french-translation1/runs/geo1o00w' target=\"_blank\">https://wandb.ai/oumar-kane-team/gpt2-wolof-french-translation1/runs/geo1o00w</a>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/optimization.py:391: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
            "  warnings.warn(\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'learning_rate' was locked by 'sweep' (ignored update).\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'weight_decay' was locked by 'sweep' (ignored update).\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='245' max='245' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [245/245 01:34, Epoch 1/1]\n",
              "    </div>\n",
              "    <table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              " <tr style=\"text-align: left;\">\n",
              "      <th>Epoch</th>\n",
              "      <th>Training Loss</th>\n",
              "      <th>Validation Loss</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <td>1</td>\n",
              "      <td>1.427500</td>\n",
              "      <td>0.791120</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table><p>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Waiting for W&B process to finish... <strong style=\"color:green\">(success).</strong>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "<style>\n",
              "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
              "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
              "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
              "    </style>\n",
              "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>eval/loss</td><td>‚ñÅ</td></tr><tr><td>eval/runtime</td><td>‚ñÅ</td></tr><tr><td>eval/samples_per_second</td><td>‚ñÅ</td></tr><tr><td>eval/steps_per_second</td><td>‚ñÅ</td></tr><tr><td>train/epoch</td><td>‚ñÅ‚ñÅ‚ñÅ</td></tr><tr><td>train/global_step</td><td>‚ñÅ‚ñÅ‚ñÅ</td></tr><tr><td>train/learning_rate</td><td>‚ñÅ</td></tr><tr><td>train/loss</td><td>‚ñÅ</td></tr><tr><td>train/total_flos</td><td>‚ñÅ</td></tr><tr><td>train/train_loss</td><td>‚ñÅ</td></tr><tr><td>train/train_runtime</td><td>‚ñÅ</td></tr><tr><td>train/train_samples_per_second</td><td>‚ñÅ</td></tr><tr><td>train/train_steps_per_second</td><td>‚ñÅ</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>eval/loss</td><td>0.79112</td></tr><tr><td>eval/runtime</td><td>2.708</td></tr><tr><td>eval/samples_per_second</td><td>30.281</td></tr><tr><td>eval/steps_per_second</td><td>6.278</td></tr><tr><td>train/epoch</td><td>1.0</td></tr><tr><td>train/global_step</td><td>245</td></tr><tr><td>train/learning_rate</td><td>2e-05</td></tr><tr><td>train/loss</td><td>1.4275</td></tr><tr><td>train/total_flos</td><td>216590170752000.0</td></tr><tr><td>train/train_loss</td><td>1.42755</td></tr><tr><td>train/train_runtime</td><td>88.8001</td></tr><tr><td>train/train_samples_per_second</td><td>8.254</td></tr><tr><td>train/train_steps_per_second</td><td>2.759</td></tr></table><br/></div></div>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              " View run <strong style=\"color:#cdcd00\">grateful-sweep-8</strong> at: <a href='https://wandb.ai/oumar-kane-team/gpt2-wolof-french-translation1/runs/geo1o00w' target=\"_blank\">https://wandb.ai/oumar-kane-team/gpt2-wolof-french-translation1/runs/geo1o00w</a><br/>Synced 5 W&B file(s), 0 media file(s), 4 artifact file(s) and 0 other file(s)"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Find logs at: <code>./wandb/run-20230428_181606-geo1o00w/logs</code>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\u001b[34m\u001b[1mwandb\u001b[0m: Agent Starting Run: 1ccxsi4u with config:\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \tbatch_size: 3\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \tepochs: 1\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \tfr_char_p: 0.456988836641937\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \tfr_word_p: 0.3479671391940781\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \tlearning_rate: 6.6097638504684e-05\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \trandom_state: 100\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \tweight_decay: 0.3\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \twf_char_p: 0.5171411933699624\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \twf_word_p: 0.2410126560866729\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m WANDB_NOTEBOOK_NAME should be a path to a notebook file, couldn't find training_gpt2_2.ipynb.\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "Tracking run with wandb version 0.15.0"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Run data is saved locally in <code>/content/wandb/run-20230428_181809-1ccxsi4u</code>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Syncing run <strong><a href='https://wandb.ai/oumar-kane-team/gpt2-wolof-french-translation1/runs/1ccxsi4u' target=\"_blank\">golden-sweep-9</a></strong> to <a href='https://wandb.ai/oumar-kane-team/gpt2-wolof-french-translation1' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>Sweep page: <a href='https://wandb.ai/oumar-kane-team/gpt2-wolof-french-translation1/sweeps/14019vvc' target=\"_blank\">https://wandb.ai/oumar-kane-team/gpt2-wolof-french-translation1/sweeps/14019vvc</a>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              " View project at <a href='https://wandb.ai/oumar-kane-team/gpt2-wolof-french-translation1' target=\"_blank\">https://wandb.ai/oumar-kane-team/gpt2-wolof-french-translation1</a>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              " View sweep at <a href='https://wandb.ai/oumar-kane-team/gpt2-wolof-french-translation1/sweeps/14019vvc' target=\"_blank\">https://wandb.ai/oumar-kane-team/gpt2-wolof-french-translation1/sweeps/14019vvc</a>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              " View run at <a href='https://wandb.ai/oumar-kane-team/gpt2-wolof-french-translation1/runs/1ccxsi4u' target=\"_blank\">https://wandb.ai/oumar-kane-team/gpt2-wolof-french-translation1/runs/1ccxsi4u</a>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/optimization.py:391: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
            "  warnings.warn(\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'learning_rate' was locked by 'sweep' (ignored update).\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'weight_decay' was locked by 'sweep' (ignored update).\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='245' max='245' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [245/245 01:34, Epoch 1/1]\n",
              "    </div>\n",
              "    <table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              " <tr style=\"text-align: left;\">\n",
              "      <th>Epoch</th>\n",
              "      <th>Training Loss</th>\n",
              "      <th>Validation Loss</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <td>1</td>\n",
              "      <td>1.549800</td>\n",
              "      <td>0.881241</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table><p>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Waiting for W&B process to finish... <strong style=\"color:green\">(success).</strong>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "<style>\n",
              "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
              "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
              "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
              "    </style>\n",
              "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>eval/loss</td><td>‚ñÅ</td></tr><tr><td>eval/runtime</td><td>‚ñÅ</td></tr><tr><td>eval/samples_per_second</td><td>‚ñÅ</td></tr><tr><td>eval/steps_per_second</td><td>‚ñÅ</td></tr><tr><td>train/epoch</td><td>‚ñÅ‚ñÅ‚ñÅ</td></tr><tr><td>train/global_step</td><td>‚ñÅ‚ñÅ‚ñÅ</td></tr><tr><td>train/learning_rate</td><td>‚ñÅ</td></tr><tr><td>train/loss</td><td>‚ñÅ</td></tr><tr><td>train/total_flos</td><td>‚ñÅ</td></tr><tr><td>train/train_loss</td><td>‚ñÅ</td></tr><tr><td>train/train_runtime</td><td>‚ñÅ</td></tr><tr><td>train/train_samples_per_second</td><td>‚ñÅ</td></tr><tr><td>train/train_steps_per_second</td><td>‚ñÅ</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>eval/loss</td><td>0.88124</td></tr><tr><td>eval/runtime</td><td>2.7186</td></tr><tr><td>eval/samples_per_second</td><td>30.162</td></tr><tr><td>eval/steps_per_second</td><td>6.253</td></tr><tr><td>train/epoch</td><td>1.0</td></tr><tr><td>train/global_step</td><td>245</td></tr><tr><td>train/learning_rate</td><td>0.0</td></tr><tr><td>train/loss</td><td>1.5498</td></tr><tr><td>train/total_flos</td><td>216590170752000.0</td></tr><tr><td>train/train_loss</td><td>1.54979</td></tr><tr><td>train/train_runtime</td><td>89.013</td></tr><tr><td>train/train_samples_per_second</td><td>8.235</td></tr><tr><td>train/train_steps_per_second</td><td>2.752</td></tr></table><br/></div></div>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              " View run <strong style=\"color:#cdcd00\">golden-sweep-9</strong> at: <a href='https://wandb.ai/oumar-kane-team/gpt2-wolof-french-translation1/runs/1ccxsi4u' target=\"_blank\">https://wandb.ai/oumar-kane-team/gpt2-wolof-french-translation1/runs/1ccxsi4u</a><br/>Synced 5 W&B file(s), 0 media file(s), 4 artifact file(s) and 0 other file(s)"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Find logs at: <code>./wandb/run-20230428_181809-1ccxsi4u/logs</code>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\u001b[34m\u001b[1mwandb\u001b[0m: Agent Starting Run: 7hw786z1 with config:\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \tbatch_size: 2\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \tepochs: 1\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \tfr_char_p: 0.4452101340937121\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \tfr_word_p: 0.05788869827598933\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \tlearning_rate: 0.0005704647067385222\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \trandom_state: 0\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \tweight_decay: 0.5\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \twf_char_p: 0.433116386353126\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \twf_word_p: 0.1370643863720747\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m WANDB_NOTEBOOK_NAME should be a path to a notebook file, couldn't find training_gpt2_2.ipynb.\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "Tracking run with wandb version 0.15.0"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Run data is saved locally in <code>/content/wandb/run-20230428_182012-7hw786z1</code>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Syncing run <strong><a href='https://wandb.ai/oumar-kane-team/gpt2-wolof-french-translation1/runs/7hw786z1' target=\"_blank\">vivid-sweep-10</a></strong> to <a href='https://wandb.ai/oumar-kane-team/gpt2-wolof-french-translation1' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>Sweep page: <a href='https://wandb.ai/oumar-kane-team/gpt2-wolof-french-translation1/sweeps/14019vvc' target=\"_blank\">https://wandb.ai/oumar-kane-team/gpt2-wolof-french-translation1/sweeps/14019vvc</a>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              " View project at <a href='https://wandb.ai/oumar-kane-team/gpt2-wolof-french-translation1' target=\"_blank\">https://wandb.ai/oumar-kane-team/gpt2-wolof-french-translation1</a>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              " View sweep at <a href='https://wandb.ai/oumar-kane-team/gpt2-wolof-french-translation1/sweeps/14019vvc' target=\"_blank\">https://wandb.ai/oumar-kane-team/gpt2-wolof-french-translation1/sweeps/14019vvc</a>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              " View run at <a href='https://wandb.ai/oumar-kane-team/gpt2-wolof-french-translation1/runs/7hw786z1' target=\"_blank\">https://wandb.ai/oumar-kane-team/gpt2-wolof-french-translation1/runs/7hw786z1</a>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/optimization.py:391: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
            "  warnings.warn(\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'learning_rate' was locked by 'sweep' (ignored update).\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'weight_decay' was locked by 'sweep' (ignored update).\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='367' max='367' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [367/367 01:41, Epoch 1/1]\n",
              "    </div>\n",
              "    <table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              " <tr style=\"text-align: left;\">\n",
              "      <th>Epoch</th>\n",
              "      <th>Training Loss</th>\n",
              "      <th>Validation Loss</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <td>1</td>\n",
              "      <td>1.172400</td>\n",
              "      <td>0.690651</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table><p>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Waiting for W&B process to finish... <strong style=\"color:green\">(success).</strong>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "<style>\n",
              "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
              "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
              "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
              "    </style>\n",
              "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>eval/loss</td><td>‚ñÅ</td></tr><tr><td>eval/runtime</td><td>‚ñÅ</td></tr><tr><td>eval/samples_per_second</td><td>‚ñÅ</td></tr><tr><td>eval/steps_per_second</td><td>‚ñÅ</td></tr><tr><td>train/epoch</td><td>‚ñÅ‚ñÅ‚ñÅ</td></tr><tr><td>train/global_step</td><td>‚ñÅ‚ñÅ‚ñÅ</td></tr><tr><td>train/learning_rate</td><td>‚ñÅ</td></tr><tr><td>train/loss</td><td>‚ñÅ</td></tr><tr><td>train/total_flos</td><td>‚ñÅ</td></tr><tr><td>train/train_loss</td><td>‚ñÅ</td></tr><tr><td>train/train_runtime</td><td>‚ñÅ</td></tr><tr><td>train/train_samples_per_second</td><td>‚ñÅ</td></tr><tr><td>train/train_steps_per_second</td><td>‚ñÅ</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>eval/loss</td><td>0.69065</td></tr><tr><td>eval/runtime</td><td>2.7289</td></tr><tr><td>eval/samples_per_second</td><td>30.048</td></tr><tr><td>eval/steps_per_second</td><td>6.23</td></tr><tr><td>train/epoch</td><td>1.0</td></tr><tr><td>train/global_step</td><td>367</td></tr><tr><td>train/learning_rate</td><td>1e-05</td></tr><tr><td>train/loss</td><td>1.1724</td></tr><tr><td>train/total_flos</td><td>216590170752000.0</td></tr><tr><td>train/train_loss</td><td>1.17238</td></tr><tr><td>train/train_runtime</td><td>95.9141</td></tr><tr><td>train/train_samples_per_second</td><td>7.642</td></tr><tr><td>train/train_steps_per_second</td><td>3.826</td></tr></table><br/></div></div>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              " View run <strong style=\"color:#cdcd00\">vivid-sweep-10</strong> at: <a href='https://wandb.ai/oumar-kane-team/gpt2-wolof-french-translation1/runs/7hw786z1' target=\"_blank\">https://wandb.ai/oumar-kane-team/gpt2-wolof-french-translation1/runs/7hw786z1</a><br/>Synced 5 W&B file(s), 0 media file(s), 4 artifact file(s) and 0 other file(s)"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Find logs at: <code>./wandb/run-20230428_182012-7hw786z1/logs</code>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\u001b[34m\u001b[1mwandb\u001b[0m: Sweep Agent: Waiting for job.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Job received.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Agent Starting Run: cyvv3sk4 with config:\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \tbatch_size: 3\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \tepochs: 1\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \tfr_char_p: 0.1482731294957671\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \tfr_word_p: 0.5736930630644534\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \tlearning_rate: 5.668425283286384e-05\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \trandom_state: 70\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \tweight_decay: 0.1\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \twf_char_p: 0.37802382647823374\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \twf_word_p: 0.341085845908422\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m WANDB_NOTEBOOK_NAME should be a path to a notebook file, couldn't find training_gpt2_2.ipynb.\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "Tracking run with wandb version 0.15.0"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Run data is saved locally in <code>/content/wandb/run-20230428_182231-cyvv3sk4</code>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Syncing run <strong><a href='https://wandb.ai/oumar-kane-team/gpt2-wolof-french-translation1/runs/cyvv3sk4' target=\"_blank\">northern-sweep-11</a></strong> to <a href='https://wandb.ai/oumar-kane-team/gpt2-wolof-french-translation1' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>Sweep page: <a href='https://wandb.ai/oumar-kane-team/gpt2-wolof-french-translation1/sweeps/14019vvc' target=\"_blank\">https://wandb.ai/oumar-kane-team/gpt2-wolof-french-translation1/sweeps/14019vvc</a>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              " View project at <a href='https://wandb.ai/oumar-kane-team/gpt2-wolof-french-translation1' target=\"_blank\">https://wandb.ai/oumar-kane-team/gpt2-wolof-french-translation1</a>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              " View sweep at <a href='https://wandb.ai/oumar-kane-team/gpt2-wolof-french-translation1/sweeps/14019vvc' target=\"_blank\">https://wandb.ai/oumar-kane-team/gpt2-wolof-french-translation1/sweeps/14019vvc</a>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              " View run at <a href='https://wandb.ai/oumar-kane-team/gpt2-wolof-french-translation1/runs/cyvv3sk4' target=\"_blank\">https://wandb.ai/oumar-kane-team/gpt2-wolof-french-translation1/runs/cyvv3sk4</a>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/optimization.py:391: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
            "  warnings.warn(\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'learning_rate' was locked by 'sweep' (ignored update).\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'weight_decay' was locked by 'sweep' (ignored update).\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='245' max='245' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [245/245 01:35, Epoch 1/1]\n",
              "    </div>\n",
              "    <table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              " <tr style=\"text-align: left;\">\n",
              "      <th>Epoch</th>\n",
              "      <th>Training Loss</th>\n",
              "      <th>Validation Loss</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <td>1</td>\n",
              "      <td>1.505900</td>\n",
              "      <td>0.905935</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table><p>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Waiting for W&B process to finish... <strong style=\"color:green\">(success).</strong>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "<style>\n",
              "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
              "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
              "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
              "    </style>\n",
              "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>eval/loss</td><td>‚ñÅ</td></tr><tr><td>eval/runtime</td><td>‚ñÅ</td></tr><tr><td>eval/samples_per_second</td><td>‚ñÅ</td></tr><tr><td>eval/steps_per_second</td><td>‚ñÅ</td></tr><tr><td>train/epoch</td><td>‚ñÅ‚ñÅ‚ñÅ</td></tr><tr><td>train/global_step</td><td>‚ñÅ‚ñÅ‚ñÅ</td></tr><tr><td>train/learning_rate</td><td>‚ñÅ</td></tr><tr><td>train/loss</td><td>‚ñÅ</td></tr><tr><td>train/total_flos</td><td>‚ñÅ</td></tr><tr><td>train/train_loss</td><td>‚ñÅ</td></tr><tr><td>train/train_runtime</td><td>‚ñÅ</td></tr><tr><td>train/train_samples_per_second</td><td>‚ñÅ</td></tr><tr><td>train/train_steps_per_second</td><td>‚ñÅ</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>eval/loss</td><td>0.90593</td></tr><tr><td>eval/runtime</td><td>2.7145</td></tr><tr><td>eval/samples_per_second</td><td>30.209</td></tr><tr><td>eval/steps_per_second</td><td>6.263</td></tr><tr><td>train/epoch</td><td>1.0</td></tr><tr><td>train/global_step</td><td>245</td></tr><tr><td>train/learning_rate</td><td>0.0</td></tr><tr><td>train/loss</td><td>1.5059</td></tr><tr><td>train/total_flos</td><td>216590170752000.0</td></tr><tr><td>train/train_loss</td><td>1.50591</td></tr><tr><td>train/train_runtime</td><td>89.1447</td></tr><tr><td>train/train_samples_per_second</td><td>8.223</td></tr><tr><td>train/train_steps_per_second</td><td>2.748</td></tr></table><br/></div></div>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              " View run <strong style=\"color:#cdcd00\">northern-sweep-11</strong> at: <a href='https://wandb.ai/oumar-kane-team/gpt2-wolof-french-translation1/runs/cyvv3sk4' target=\"_blank\">https://wandb.ai/oumar-kane-team/gpt2-wolof-french-translation1/runs/cyvv3sk4</a><br/>Synced 5 W&B file(s), 0 media file(s), 4 artifact file(s) and 0 other file(s)"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Find logs at: <code>./wandb/run-20230428_182231-cyvv3sk4/logs</code>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\u001b[34m\u001b[1mwandb\u001b[0m: Sweep Agent: Waiting for job.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Job received.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Agent Starting Run: 8le52wm6 with config:\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \tbatch_size: 3\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \tepochs: 1\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \tfr_char_p: 0.6187226168209458\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \tfr_word_p: 0.06710867937445074\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \tlearning_rate: 0.0004360074099168583\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \trandom_state: 40\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \tweight_decay: 0.1\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \twf_char_p: 0.2799773684911801\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \twf_word_p: 0.4995626384671348\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m WANDB_NOTEBOOK_NAME should be a path to a notebook file, couldn't find training_gpt2_2.ipynb.\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "Tracking run with wandb version 0.15.0"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Run data is saved locally in <code>/content/wandb/run-20230428_182443-8le52wm6</code>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Syncing run <strong><a href='https://wandb.ai/oumar-kane-team/gpt2-wolof-french-translation1/runs/8le52wm6' target=\"_blank\">whole-sweep-12</a></strong> to <a href='https://wandb.ai/oumar-kane-team/gpt2-wolof-french-translation1' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>Sweep page: <a href='https://wandb.ai/oumar-kane-team/gpt2-wolof-french-translation1/sweeps/14019vvc' target=\"_blank\">https://wandb.ai/oumar-kane-team/gpt2-wolof-french-translation1/sweeps/14019vvc</a>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              " View project at <a href='https://wandb.ai/oumar-kane-team/gpt2-wolof-french-translation1' target=\"_blank\">https://wandb.ai/oumar-kane-team/gpt2-wolof-french-translation1</a>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              " View sweep at <a href='https://wandb.ai/oumar-kane-team/gpt2-wolof-french-translation1/sweeps/14019vvc' target=\"_blank\">https://wandb.ai/oumar-kane-team/gpt2-wolof-french-translation1/sweeps/14019vvc</a>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              " View run at <a href='https://wandb.ai/oumar-kane-team/gpt2-wolof-french-translation1/runs/8le52wm6' target=\"_blank\">https://wandb.ai/oumar-kane-team/gpt2-wolof-french-translation1/runs/8le52wm6</a>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/optimization.py:391: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
            "  warnings.warn(\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'learning_rate' was locked by 'sweep' (ignored update).\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'weight_decay' was locked by 'sweep' (ignored update).\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='245' max='245' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [245/245 01:34, Epoch 1/1]\n",
              "    </div>\n",
              "    <table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              " <tr style=\"text-align: left;\">\n",
              "      <th>Epoch</th>\n",
              "      <th>Training Loss</th>\n",
              "      <th>Validation Loss</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <td>1</td>\n",
              "      <td>1.278800</td>\n",
              "      <td>0.765012</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table><p>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Waiting for W&B process to finish... <strong style=\"color:green\">(success).</strong>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "<style>\n",
              "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
              "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
              "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
              "    </style>\n",
              "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>eval/loss</td><td>‚ñÅ</td></tr><tr><td>eval/runtime</td><td>‚ñÅ</td></tr><tr><td>eval/samples_per_second</td><td>‚ñÅ</td></tr><tr><td>eval/steps_per_second</td><td>‚ñÅ</td></tr><tr><td>train/epoch</td><td>‚ñÅ‚ñÅ‚ñÅ</td></tr><tr><td>train/global_step</td><td>‚ñÅ‚ñÅ‚ñÅ</td></tr><tr><td>train/learning_rate</td><td>‚ñÅ</td></tr><tr><td>train/loss</td><td>‚ñÅ</td></tr><tr><td>train/total_flos</td><td>‚ñÅ</td></tr><tr><td>train/train_loss</td><td>‚ñÅ</td></tr><tr><td>train/train_runtime</td><td>‚ñÅ</td></tr><tr><td>train/train_samples_per_second</td><td>‚ñÅ</td></tr><tr><td>train/train_steps_per_second</td><td>‚ñÅ</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>eval/loss</td><td>0.76501</td></tr><tr><td>eval/runtime</td><td>2.7162</td></tr><tr><td>eval/samples_per_second</td><td>30.189</td></tr><tr><td>eval/steps_per_second</td><td>6.259</td></tr><tr><td>train/epoch</td><td>1.0</td></tr><tr><td>train/global_step</td><td>245</td></tr><tr><td>train/learning_rate</td><td>1e-05</td></tr><tr><td>train/loss</td><td>1.2788</td></tr><tr><td>train/total_flos</td><td>216590170752000.0</td></tr><tr><td>train/train_loss</td><td>1.27883</td></tr><tr><td>train/train_runtime</td><td>89.462</td></tr><tr><td>train/train_samples_per_second</td><td>8.193</td></tr><tr><td>train/train_steps_per_second</td><td>2.739</td></tr></table><br/></div></div>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              " View run <strong style=\"color:#cdcd00\">whole-sweep-12</strong> at: <a href='https://wandb.ai/oumar-kane-team/gpt2-wolof-french-translation1/runs/8le52wm6' target=\"_blank\">https://wandb.ai/oumar-kane-team/gpt2-wolof-french-translation1/runs/8le52wm6</a><br/>Synced 5 W&B file(s), 0 media file(s), 4 artifact file(s) and 0 other file(s)"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Find logs at: <code>./wandb/run-20230428_182443-8le52wm6/logs</code>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\u001b[34m\u001b[1mwandb\u001b[0m: Agent Starting Run: ik84v5nz with config:\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \tbatch_size: 5\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \tepochs: 1\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \tfr_char_p: 0.3626790920895456\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \tfr_word_p: 0.5338205729697485\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \tlearning_rate: 1.480661292209492e-05\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \trandom_state: 0\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \tweight_decay: 0.4\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \twf_char_p: 0.1691982530305743\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \twf_word_p: 0.6746588812498662\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m WANDB_NOTEBOOK_NAME should be a path to a notebook file, couldn't find training_gpt2_2.ipynb.\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "Tracking run with wandb version 0.15.0"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Run data is saved locally in <code>/content/wandb/run-20230428_182649-ik84v5nz</code>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Syncing run <strong><a href='https://wandb.ai/oumar-kane-team/gpt2-wolof-french-translation1/runs/ik84v5nz' target=\"_blank\">gentle-sweep-13</a></strong> to <a href='https://wandb.ai/oumar-kane-team/gpt2-wolof-french-translation1' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>Sweep page: <a href='https://wandb.ai/oumar-kane-team/gpt2-wolof-french-translation1/sweeps/14019vvc' target=\"_blank\">https://wandb.ai/oumar-kane-team/gpt2-wolof-french-translation1/sweeps/14019vvc</a>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              " View project at <a href='https://wandb.ai/oumar-kane-team/gpt2-wolof-french-translation1' target=\"_blank\">https://wandb.ai/oumar-kane-team/gpt2-wolof-french-translation1</a>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              " View sweep at <a href='https://wandb.ai/oumar-kane-team/gpt2-wolof-french-translation1/sweeps/14019vvc' target=\"_blank\">https://wandb.ai/oumar-kane-team/gpt2-wolof-french-translation1/sweeps/14019vvc</a>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              " View run at <a href='https://wandb.ai/oumar-kane-team/gpt2-wolof-french-translation1/runs/ik84v5nz' target=\"_blank\">https://wandb.ai/oumar-kane-team/gpt2-wolof-french-translation1/runs/ik84v5nz</a>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/optimization.py:391: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
            "  warnings.warn(\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'learning_rate' was locked by 'sweep' (ignored update).\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'weight_decay' was locked by 'sweep' (ignored update).\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='147' max='147' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [147/147 01:29, Epoch 1/1]\n",
              "    </div>\n",
              "    <table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              " <tr style=\"text-align: left;\">\n",
              "      <th>Epoch</th>\n",
              "      <th>Training Loss</th>\n",
              "      <th>Validation Loss</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <td>1</td>\n",
              "      <td>2.031600</td>\n",
              "      <td>0.909855</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table><p>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Waiting for W&B process to finish... <strong style=\"color:green\">(success).</strong>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "<style>\n",
              "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
              "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
              "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
              "    </style>\n",
              "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>eval/loss</td><td>‚ñÅ</td></tr><tr><td>eval/runtime</td><td>‚ñÅ</td></tr><tr><td>eval/samples_per_second</td><td>‚ñÅ</td></tr><tr><td>eval/steps_per_second</td><td>‚ñÅ</td></tr><tr><td>train/epoch</td><td>‚ñÅ‚ñÅ‚ñÅ</td></tr><tr><td>train/global_step</td><td>‚ñÅ‚ñÅ‚ñÅ</td></tr><tr><td>train/learning_rate</td><td>‚ñÅ</td></tr><tr><td>train/loss</td><td>‚ñÅ</td></tr><tr><td>train/total_flos</td><td>‚ñÅ</td></tr><tr><td>train/train_loss</td><td>‚ñÅ</td></tr><tr><td>train/train_runtime</td><td>‚ñÅ</td></tr><tr><td>train/train_samples_per_second</td><td>‚ñÅ</td></tr><tr><td>train/train_steps_per_second</td><td>‚ñÅ</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>eval/loss</td><td>0.90985</td></tr><tr><td>eval/runtime</td><td>2.7023</td></tr><tr><td>eval/samples_per_second</td><td>30.345</td></tr><tr><td>eval/steps_per_second</td><td>6.291</td></tr><tr><td>train/epoch</td><td>1.0</td></tr><tr><td>train/global_step</td><td>147</td></tr><tr><td>train/learning_rate</td><td>0.0</td></tr><tr><td>train/loss</td><td>2.0316</td></tr><tr><td>train/total_flos</td><td>216590170752000.0</td></tr><tr><td>train/train_loss</td><td>2.03156</td></tr><tr><td>train/train_runtime</td><td>84.3681</td></tr><tr><td>train/train_samples_per_second</td><td>8.688</td></tr><tr><td>train/train_steps_per_second</td><td>1.742</td></tr></table><br/></div></div>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              " View run <strong style=\"color:#cdcd00\">gentle-sweep-13</strong> at: <a href='https://wandb.ai/oumar-kane-team/gpt2-wolof-french-translation1/runs/ik84v5nz' target=\"_blank\">https://wandb.ai/oumar-kane-team/gpt2-wolof-french-translation1/runs/ik84v5nz</a><br/>Synced 5 W&B file(s), 0 media file(s), 4 artifact file(s) and 0 other file(s)"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Find logs at: <code>./wandb/run-20230428_182649-ik84v5nz/logs</code>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\u001b[34m\u001b[1mwandb\u001b[0m: Agent Starting Run: w6g4xc5y with config:\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \tbatch_size: 5\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \tepochs: 1\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \tfr_char_p: 0.1826025463154098\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \tfr_word_p: 0.23620689712255616\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \tlearning_rate: 0.00012545493959102943\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \trandom_state: 100\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \tweight_decay: 0.3\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \twf_char_p: 0.5243872632996286\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \twf_word_p: 0.07301709101380438\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m WANDB_NOTEBOOK_NAME should be a path to a notebook file, couldn't find training_gpt2_2.ipynb.\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "Tracking run with wandb version 0.15.0"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Run data is saved locally in <code>/content/wandb/run-20230428_182847-w6g4xc5y</code>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Syncing run <strong><a href='https://wandb.ai/oumar-kane-team/gpt2-wolof-french-translation1/runs/w6g4xc5y' target=\"_blank\">sunny-sweep-14</a></strong> to <a href='https://wandb.ai/oumar-kane-team/gpt2-wolof-french-translation1' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>Sweep page: <a href='https://wandb.ai/oumar-kane-team/gpt2-wolof-french-translation1/sweeps/14019vvc' target=\"_blank\">https://wandb.ai/oumar-kane-team/gpt2-wolof-french-translation1/sweeps/14019vvc</a>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              " View project at <a href='https://wandb.ai/oumar-kane-team/gpt2-wolof-french-translation1' target=\"_blank\">https://wandb.ai/oumar-kane-team/gpt2-wolof-french-translation1</a>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              " View sweep at <a href='https://wandb.ai/oumar-kane-team/gpt2-wolof-french-translation1/sweeps/14019vvc' target=\"_blank\">https://wandb.ai/oumar-kane-team/gpt2-wolof-french-translation1/sweeps/14019vvc</a>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              " View run at <a href='https://wandb.ai/oumar-kane-team/gpt2-wolof-french-translation1/runs/w6g4xc5y' target=\"_blank\">https://wandb.ai/oumar-kane-team/gpt2-wolof-french-translation1/runs/w6g4xc5y</a>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/optimization.py:391: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
            "  warnings.warn(\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'learning_rate' was locked by 'sweep' (ignored update).\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'weight_decay' was locked by 'sweep' (ignored update).\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='147' max='147' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [147/147 01:28, Epoch 1/1]\n",
              "    </div>\n",
              "    <table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              " <tr style=\"text-align: left;\">\n",
              "      <th>Epoch</th>\n",
              "      <th>Training Loss</th>\n",
              "      <th>Validation Loss</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <td>1</td>\n",
              "      <td>1.602400</td>\n",
              "      <td>0.861327</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table><p>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Waiting for W&B process to finish... <strong style=\"color:green\">(success).</strong>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "<style>\n",
              "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
              "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
              "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
              "    </style>\n",
              "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>eval/loss</td><td>‚ñÅ</td></tr><tr><td>eval/runtime</td><td>‚ñÅ</td></tr><tr><td>eval/samples_per_second</td><td>‚ñÅ</td></tr><tr><td>eval/steps_per_second</td><td>‚ñÅ</td></tr><tr><td>train/epoch</td><td>‚ñÅ‚ñÅ‚ñÅ</td></tr><tr><td>train/global_step</td><td>‚ñÅ‚ñÅ‚ñÅ</td></tr><tr><td>train/learning_rate</td><td>‚ñÅ</td></tr><tr><td>train/loss</td><td>‚ñÅ</td></tr><tr><td>train/total_flos</td><td>‚ñÅ</td></tr><tr><td>train/train_loss</td><td>‚ñÅ</td></tr><tr><td>train/train_runtime</td><td>‚ñÅ</td></tr><tr><td>train/train_samples_per_second</td><td>‚ñÅ</td></tr><tr><td>train/train_steps_per_second</td><td>‚ñÅ</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>eval/loss</td><td>0.86133</td></tr><tr><td>eval/runtime</td><td>2.6953</td></tr><tr><td>eval/samples_per_second</td><td>30.424</td></tr><tr><td>eval/steps_per_second</td><td>6.307</td></tr><tr><td>train/epoch</td><td>1.0</td></tr><tr><td>train/global_step</td><td>147</td></tr><tr><td>train/learning_rate</td><td>1e-05</td></tr><tr><td>train/loss</td><td>1.6024</td></tr><tr><td>train/total_flos</td><td>216590170752000.0</td></tr><tr><td>train/train_loss</td><td>1.6024</td></tr><tr><td>train/train_runtime</td><td>82.9808</td></tr><tr><td>train/train_samples_per_second</td><td>8.833</td></tr><tr><td>train/train_steps_per_second</td><td>1.771</td></tr></table><br/></div></div>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              " View run <strong style=\"color:#cdcd00\">sunny-sweep-14</strong> at: <a href='https://wandb.ai/oumar-kane-team/gpt2-wolof-french-translation1/runs/w6g4xc5y' target=\"_blank\">https://wandb.ai/oumar-kane-team/gpt2-wolof-french-translation1/runs/w6g4xc5y</a><br/>Synced 5 W&B file(s), 0 media file(s), 4 artifact file(s) and 0 other file(s)"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Find logs at: <code>./wandb/run-20230428_182847-w6g4xc5y/logs</code>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\u001b[34m\u001b[1mwandb\u001b[0m: Agent Starting Run: kle0p42n with config:\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \tbatch_size: 3\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \tepochs: 1\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \tfr_char_p: 0.5985474199581012\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \tfr_word_p: 0.07718440369801771\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \tlearning_rate: 0.0007879622799773468\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \trandom_state: 10\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \tweight_decay: 0.4\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \twf_char_p: 0.06376166584090216\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \twf_word_p: 0.40000591190905255\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m WANDB_NOTEBOOK_NAME should be a path to a notebook file, couldn't find training_gpt2_2.ipynb.\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "Tracking run with wandb version 0.15.0"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Run data is saved locally in <code>/content/wandb/run-20230428_183048-kle0p42n</code>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Syncing run <strong><a href='https://wandb.ai/oumar-kane-team/gpt2-wolof-french-translation1/runs/kle0p42n' target=\"_blank\">kind-sweep-15</a></strong> to <a href='https://wandb.ai/oumar-kane-team/gpt2-wolof-french-translation1' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>Sweep page: <a href='https://wandb.ai/oumar-kane-team/gpt2-wolof-french-translation1/sweeps/14019vvc' target=\"_blank\">https://wandb.ai/oumar-kane-team/gpt2-wolof-french-translation1/sweeps/14019vvc</a>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              " View project at <a href='https://wandb.ai/oumar-kane-team/gpt2-wolof-french-translation1' target=\"_blank\">https://wandb.ai/oumar-kane-team/gpt2-wolof-french-translation1</a>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              " View sweep at <a href='https://wandb.ai/oumar-kane-team/gpt2-wolof-french-translation1/sweeps/14019vvc' target=\"_blank\">https://wandb.ai/oumar-kane-team/gpt2-wolof-french-translation1/sweeps/14019vvc</a>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              " View run at <a href='https://wandb.ai/oumar-kane-team/gpt2-wolof-french-translation1/runs/kle0p42n' target=\"_blank\">https://wandb.ai/oumar-kane-team/gpt2-wolof-french-translation1/runs/kle0p42n</a>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/optimization.py:391: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
            "  warnings.warn(\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'learning_rate' was locked by 'sweep' (ignored update).\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'weight_decay' was locked by 'sweep' (ignored update).\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='245' max='245' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [245/245 01:35, Epoch 1/1]\n",
              "    </div>\n",
              "    <table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              " <tr style=\"text-align: left;\">\n",
              "      <th>Epoch</th>\n",
              "      <th>Training Loss</th>\n",
              "      <th>Validation Loss</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <td>1</td>\n",
              "      <td>1.334500</td>\n",
              "      <td>0.745426</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table><p>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Waiting for W&B process to finish... <strong style=\"color:green\">(success).</strong>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "<style>\n",
              "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
              "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
              "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
              "    </style>\n",
              "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>eval/loss</td><td>‚ñÅ</td></tr><tr><td>eval/runtime</td><td>‚ñÅ</td></tr><tr><td>eval/samples_per_second</td><td>‚ñÅ</td></tr><tr><td>eval/steps_per_second</td><td>‚ñÅ</td></tr><tr><td>train/epoch</td><td>‚ñÅ‚ñÅ‚ñÅ</td></tr><tr><td>train/global_step</td><td>‚ñÅ‚ñÅ‚ñÅ</td></tr><tr><td>train/learning_rate</td><td>‚ñÅ</td></tr><tr><td>train/loss</td><td>‚ñÅ</td></tr><tr><td>train/total_flos</td><td>‚ñÅ</td></tr><tr><td>train/train_loss</td><td>‚ñÅ</td></tr><tr><td>train/train_runtime</td><td>‚ñÅ</td></tr><tr><td>train/train_samples_per_second</td><td>‚ñÅ</td></tr><tr><td>train/train_steps_per_second</td><td>‚ñÅ</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>eval/loss</td><td>0.74543</td></tr><tr><td>eval/runtime</td><td>2.7172</td></tr><tr><td>eval/samples_per_second</td><td>30.178</td></tr><tr><td>eval/steps_per_second</td><td>6.257</td></tr><tr><td>train/epoch</td><td>1.0</td></tr><tr><td>train/global_step</td><td>245</td></tr><tr><td>train/learning_rate</td><td>3e-05</td></tr><tr><td>train/loss</td><td>1.3345</td></tr><tr><td>train/total_flos</td><td>216590170752000.0</td></tr><tr><td>train/train_loss</td><td>1.33447</td></tr><tr><td>train/train_runtime</td><td>89.5666</td></tr><tr><td>train/train_samples_per_second</td><td>8.184</td></tr><tr><td>train/train_steps_per_second</td><td>2.735</td></tr></table><br/></div></div>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              " View run <strong style=\"color:#cdcd00\">kind-sweep-15</strong> at: <a href='https://wandb.ai/oumar-kane-team/gpt2-wolof-french-translation1/runs/kle0p42n' target=\"_blank\">https://wandb.ai/oumar-kane-team/gpt2-wolof-french-translation1/runs/kle0p42n</a><br/>Synced 5 W&B file(s), 0 media file(s), 4 artifact file(s) and 0 other file(s)"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Find logs at: <code>./wandb/run-20230428_183048-kle0p42n/logs</code>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\u001b[34m\u001b[1mwandb\u001b[0m: Agent Starting Run: hsjf08y6 with config:\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \tbatch_size: 2\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \tepochs: 1\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \tfr_char_p: 0.6802585601283034\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \tfr_word_p: 0.37831355668841254\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \tlearning_rate: 7.784485297903175e-05\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \trandom_state: 60\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \tweight_decay: 0.3\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \twf_char_p: 0.060724252321415366\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \twf_word_p: 0.6211735926275833\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m WANDB_NOTEBOOK_NAME should be a path to a notebook file, couldn't find training_gpt2_2.ipynb.\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "Tracking run with wandb version 0.15.0"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Run data is saved locally in <code>/content/wandb/run-20230428_183254-hsjf08y6</code>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Syncing run <strong><a href='https://wandb.ai/oumar-kane-team/gpt2-wolof-french-translation1/runs/hsjf08y6' target=\"_blank\">stellar-sweep-16</a></strong> to <a href='https://wandb.ai/oumar-kane-team/gpt2-wolof-french-translation1' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>Sweep page: <a href='https://wandb.ai/oumar-kane-team/gpt2-wolof-french-translation1/sweeps/14019vvc' target=\"_blank\">https://wandb.ai/oumar-kane-team/gpt2-wolof-french-translation1/sweeps/14019vvc</a>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              " View project at <a href='https://wandb.ai/oumar-kane-team/gpt2-wolof-french-translation1' target=\"_blank\">https://wandb.ai/oumar-kane-team/gpt2-wolof-french-translation1</a>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              " View sweep at <a href='https://wandb.ai/oumar-kane-team/gpt2-wolof-french-translation1/sweeps/14019vvc' target=\"_blank\">https://wandb.ai/oumar-kane-team/gpt2-wolof-french-translation1/sweeps/14019vvc</a>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              " View run at <a href='https://wandb.ai/oumar-kane-team/gpt2-wolof-french-translation1/runs/hsjf08y6' target=\"_blank\">https://wandb.ai/oumar-kane-team/gpt2-wolof-french-translation1/runs/hsjf08y6</a>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/optimization.py:391: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
            "  warnings.warn(\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'learning_rate' was locked by 'sweep' (ignored update).\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'weight_decay' was locked by 'sweep' (ignored update).\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='367' max='367' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [367/367 01:43, Epoch 1/1]\n",
              "    </div>\n",
              "    <table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              " <tr style=\"text-align: left;\">\n",
              "      <th>Epoch</th>\n",
              "      <th>Training Loss</th>\n",
              "      <th>Validation Loss</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <td>1</td>\n",
              "      <td>1.435200</td>\n",
              "      <td>0.907959</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table><p>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Waiting for W&B process to finish... <strong style=\"color:green\">(success).</strong>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "<style>\n",
              "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
              "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
              "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
              "    </style>\n",
              "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>eval/loss</td><td>‚ñÅ</td></tr><tr><td>eval/runtime</td><td>‚ñÅ</td></tr><tr><td>eval/samples_per_second</td><td>‚ñÅ</td></tr><tr><td>eval/steps_per_second</td><td>‚ñÅ</td></tr><tr><td>train/epoch</td><td>‚ñÅ‚ñÅ‚ñÅ</td></tr><tr><td>train/global_step</td><td>‚ñÅ‚ñÅ‚ñÅ</td></tr><tr><td>train/learning_rate</td><td>‚ñÅ</td></tr><tr><td>train/loss</td><td>‚ñÅ</td></tr><tr><td>train/total_flos</td><td>‚ñÅ</td></tr><tr><td>train/train_loss</td><td>‚ñÅ</td></tr><tr><td>train/train_runtime</td><td>‚ñÅ</td></tr><tr><td>train/train_samples_per_second</td><td>‚ñÅ</td></tr><tr><td>train/train_steps_per_second</td><td>‚ñÅ</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>eval/loss</td><td>0.90796</td></tr><tr><td>eval/runtime</td><td>2.7388</td></tr><tr><td>eval/samples_per_second</td><td>29.94</td></tr><tr><td>eval/steps_per_second</td><td>6.207</td></tr><tr><td>train/epoch</td><td>1.0</td></tr><tr><td>train/global_step</td><td>367</td></tr><tr><td>train/learning_rate</td><td>0.0</td></tr><tr><td>train/loss</td><td>1.4352</td></tr><tr><td>train/total_flos</td><td>216590170752000.0</td></tr><tr><td>train/train_loss</td><td>1.43518</td></tr><tr><td>train/train_runtime</td><td>98.2703</td></tr><tr><td>train/train_samples_per_second</td><td>7.459</td></tr><tr><td>train/train_steps_per_second</td><td>3.735</td></tr></table><br/></div></div>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              " View run <strong style=\"color:#cdcd00\">stellar-sweep-16</strong> at: <a href='https://wandb.ai/oumar-kane-team/gpt2-wolof-french-translation1/runs/hsjf08y6' target=\"_blank\">https://wandb.ai/oumar-kane-team/gpt2-wolof-french-translation1/runs/hsjf08y6</a><br/>Synced 5 W&B file(s), 0 media file(s), 4 artifact file(s) and 0 other file(s)"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Find logs at: <code>./wandb/run-20230428_183254-hsjf08y6/logs</code>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\u001b[34m\u001b[1mwandb\u001b[0m: Agent Starting Run: phdrjtls with config:\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \tbatch_size: 2\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \tepochs: 1\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \tfr_char_p: 0.2574727343683873\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \tfr_word_p: 0.325848817422341\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \tlearning_rate: 0.00013694970591525185\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \trandom_state: 30\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \tweight_decay: 0.2\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \twf_char_p: 0.0515917126614958\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \twf_word_p: 0.13768465096538185\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m WANDB_NOTEBOOK_NAME should be a path to a notebook file, couldn't find training_gpt2_2.ipynb.\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "Tracking run with wandb version 0.15.0"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Run data is saved locally in <code>/content/wandb/run-20230428_183512-phdrjtls</code>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Syncing run <strong><a href='https://wandb.ai/oumar-kane-team/gpt2-wolof-french-translation1/runs/phdrjtls' target=\"_blank\">lilac-sweep-17</a></strong> to <a href='https://wandb.ai/oumar-kane-team/gpt2-wolof-french-translation1' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>Sweep page: <a href='https://wandb.ai/oumar-kane-team/gpt2-wolof-french-translation1/sweeps/14019vvc' target=\"_blank\">https://wandb.ai/oumar-kane-team/gpt2-wolof-french-translation1/sweeps/14019vvc</a>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              " View project at <a href='https://wandb.ai/oumar-kane-team/gpt2-wolof-french-translation1' target=\"_blank\">https://wandb.ai/oumar-kane-team/gpt2-wolof-french-translation1</a>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              " View sweep at <a href='https://wandb.ai/oumar-kane-team/gpt2-wolof-french-translation1/sweeps/14019vvc' target=\"_blank\">https://wandb.ai/oumar-kane-team/gpt2-wolof-french-translation1/sweeps/14019vvc</a>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              " View run at <a href='https://wandb.ai/oumar-kane-team/gpt2-wolof-french-translation1/runs/phdrjtls' target=\"_blank\">https://wandb.ai/oumar-kane-team/gpt2-wolof-french-translation1/runs/phdrjtls</a>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/optimization.py:391: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
            "  warnings.warn(\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'learning_rate' was locked by 'sweep' (ignored update).\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'weight_decay' was locked by 'sweep' (ignored update).\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='367' max='367' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [367/367 01:43, Epoch 1/1]\n",
              "    </div>\n",
              "    <table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              " <tr style=\"text-align: left;\">\n",
              "      <th>Epoch</th>\n",
              "      <th>Training Loss</th>\n",
              "      <th>Validation Loss</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <td>1</td>\n",
              "      <td>1.366900</td>\n",
              "      <td>0.895562</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table><p>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Waiting for W&B process to finish... <strong style=\"color:green\">(success).</strong>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "<style>\n",
              "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
              "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
              "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
              "    </style>\n",
              "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>eval/loss</td><td>‚ñÅ</td></tr><tr><td>eval/runtime</td><td>‚ñÅ</td></tr><tr><td>eval/samples_per_second</td><td>‚ñÅ</td></tr><tr><td>eval/steps_per_second</td><td>‚ñÅ</td></tr><tr><td>train/epoch</td><td>‚ñÅ‚ñÅ‚ñÅ</td></tr><tr><td>train/global_step</td><td>‚ñÅ‚ñÅ‚ñÅ</td></tr><tr><td>train/learning_rate</td><td>‚ñÅ</td></tr><tr><td>train/loss</td><td>‚ñÅ</td></tr><tr><td>train/total_flos</td><td>‚ñÅ</td></tr><tr><td>train/train_loss</td><td>‚ñÅ</td></tr><tr><td>train/train_runtime</td><td>‚ñÅ</td></tr><tr><td>train/train_samples_per_second</td><td>‚ñÅ</td></tr><tr><td>train/train_steps_per_second</td><td>‚ñÅ</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>eval/loss</td><td>0.89556</td></tr><tr><td>eval/runtime</td><td>2.7327</td></tr><tr><td>eval/samples_per_second</td><td>30.007</td></tr><tr><td>eval/steps_per_second</td><td>6.221</td></tr><tr><td>train/epoch</td><td>1.0</td></tr><tr><td>train/global_step</td><td>367</td></tr><tr><td>train/learning_rate</td><td>0.0</td></tr><tr><td>train/loss</td><td>1.3669</td></tr><tr><td>train/total_flos</td><td>216590170752000.0</td></tr><tr><td>train/train_loss</td><td>1.36686</td></tr><tr><td>train/train_runtime</td><td>97.7235</td></tr><tr><td>train/train_samples_per_second</td><td>7.501</td></tr><tr><td>train/train_steps_per_second</td><td>3.755</td></tr></table><br/></div></div>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              " View run <strong style=\"color:#cdcd00\">lilac-sweep-17</strong> at: <a href='https://wandb.ai/oumar-kane-team/gpt2-wolof-french-translation1/runs/phdrjtls' target=\"_blank\">https://wandb.ai/oumar-kane-team/gpt2-wolof-french-translation1/runs/phdrjtls</a><br/>Synced 5 W&B file(s), 0 media file(s), 4 artifact file(s) and 0 other file(s)"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Find logs at: <code>./wandb/run-20230428_183512-phdrjtls/logs</code>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\u001b[34m\u001b[1mwandb\u001b[0m: Agent Starting Run: np5sch0t with config:\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \tbatch_size: 5\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \tepochs: 1\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \tfr_char_p: 0.5644754223954247\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \tfr_word_p: 0.6558095944067747\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \tlearning_rate: 0.00019281695417198096\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \trandom_state: 100\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \tweight_decay: 0\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \twf_char_p: 0.47603610667853263\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \twf_word_p: 0.6193756056152349\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m WANDB_NOTEBOOK_NAME should be a path to a notebook file, couldn't find training_gpt2_2.ipynb.\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "Tracking run with wandb version 0.15.0"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Run data is saved locally in <code>/content/wandb/run-20230428_183725-np5sch0t</code>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Syncing run <strong><a href='https://wandb.ai/oumar-kane-team/gpt2-wolof-french-translation1/runs/np5sch0t' target=\"_blank\">devout-sweep-18</a></strong> to <a href='https://wandb.ai/oumar-kane-team/gpt2-wolof-french-translation1' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>Sweep page: <a href='https://wandb.ai/oumar-kane-team/gpt2-wolof-french-translation1/sweeps/14019vvc' target=\"_blank\">https://wandb.ai/oumar-kane-team/gpt2-wolof-french-translation1/sweeps/14019vvc</a>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              " View project at <a href='https://wandb.ai/oumar-kane-team/gpt2-wolof-french-translation1' target=\"_blank\">https://wandb.ai/oumar-kane-team/gpt2-wolof-french-translation1</a>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              " View sweep at <a href='https://wandb.ai/oumar-kane-team/gpt2-wolof-french-translation1/sweeps/14019vvc' target=\"_blank\">https://wandb.ai/oumar-kane-team/gpt2-wolof-french-translation1/sweeps/14019vvc</a>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              " View run at <a href='https://wandb.ai/oumar-kane-team/gpt2-wolof-french-translation1/runs/np5sch0t' target=\"_blank\">https://wandb.ai/oumar-kane-team/gpt2-wolof-french-translation1/runs/np5sch0t</a>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/optimization.py:391: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
            "  warnings.warn(\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'learning_rate' was locked by 'sweep' (ignored update).\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'weight_decay' was locked by 'sweep' (ignored update).\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='147' max='147' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [147/147 01:30, Epoch 1/1]\n",
              "    </div>\n",
              "    <table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              " <tr style=\"text-align: left;\">\n",
              "      <th>Epoch</th>\n",
              "      <th>Training Loss</th>\n",
              "      <th>Validation Loss</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <td>1</td>\n",
              "      <td>1.650400</td>\n",
              "      <td>0.851764</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table><p>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Waiting for W&B process to finish... <strong style=\"color:green\">(success).</strong>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "<style>\n",
              "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
              "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
              "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
              "    </style>\n",
              "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>eval/loss</td><td>‚ñÅ</td></tr><tr><td>eval/runtime</td><td>‚ñÅ</td></tr><tr><td>eval/samples_per_second</td><td>‚ñÅ</td></tr><tr><td>eval/steps_per_second</td><td>‚ñÅ</td></tr><tr><td>train/epoch</td><td>‚ñÅ‚ñÅ‚ñÅ</td></tr><tr><td>train/global_step</td><td>‚ñÅ‚ñÅ‚ñÅ</td></tr><tr><td>train/learning_rate</td><td>‚ñÅ</td></tr><tr><td>train/loss</td><td>‚ñÅ</td></tr><tr><td>train/total_flos</td><td>‚ñÅ</td></tr><tr><td>train/train_loss</td><td>‚ñÅ</td></tr><tr><td>train/train_runtime</td><td>‚ñÅ</td></tr><tr><td>train/train_samples_per_second</td><td>‚ñÅ</td></tr><tr><td>train/train_steps_per_second</td><td>‚ñÅ</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>eval/loss</td><td>0.85176</td></tr><tr><td>eval/runtime</td><td>2.747</td></tr><tr><td>eval/samples_per_second</td><td>29.851</td></tr><tr><td>eval/steps_per_second</td><td>6.189</td></tr><tr><td>train/epoch</td><td>1.0</td></tr><tr><td>train/global_step</td><td>147</td></tr><tr><td>train/learning_rate</td><td>1e-05</td></tr><tr><td>train/loss</td><td>1.6504</td></tr><tr><td>train/total_flos</td><td>216590170752000.0</td></tr><tr><td>train/train_loss</td><td>1.65045</td></tr><tr><td>train/train_runtime</td><td>83.0831</td></tr><tr><td>train/train_samples_per_second</td><td>8.822</td></tr><tr><td>train/train_steps_per_second</td><td>1.769</td></tr></table><br/></div></div>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              " View run <strong style=\"color:#cdcd00\">devout-sweep-18</strong> at: <a href='https://wandb.ai/oumar-kane-team/gpt2-wolof-french-translation1/runs/np5sch0t' target=\"_blank\">https://wandb.ai/oumar-kane-team/gpt2-wolof-french-translation1/runs/np5sch0t</a><br/>Synced 5 W&B file(s), 0 media file(s), 4 artifact file(s) and 0 other file(s)"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Find logs at: <code>./wandb/run-20230428_183725-np5sch0t/logs</code>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\u001b[34m\u001b[1mwandb\u001b[0m: Agent Starting Run: cqdm4q5a with config:\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \tbatch_size: 3\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \tepochs: 1\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \tfr_char_p: 0.652685626135174\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \tfr_word_p: 0.4634738497616566\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \tlearning_rate: 3.4082556870625936e-05\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \trandom_state: 30\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \tweight_decay: 0.3\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \twf_char_p: 0.5113821920270895\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \twf_word_p: 0.4690531758389533\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m WANDB_NOTEBOOK_NAME should be a path to a notebook file, couldn't find training_gpt2_2.ipynb.\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "Tracking run with wandb version 0.15.0"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Run data is saved locally in <code>/content/wandb/run-20230428_183928-cqdm4q5a</code>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Syncing run <strong><a href='https://wandb.ai/oumar-kane-team/gpt2-wolof-french-translation1/runs/cqdm4q5a' target=\"_blank\">feasible-sweep-19</a></strong> to <a href='https://wandb.ai/oumar-kane-team/gpt2-wolof-french-translation1' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>Sweep page: <a href='https://wandb.ai/oumar-kane-team/gpt2-wolof-french-translation1/sweeps/14019vvc' target=\"_blank\">https://wandb.ai/oumar-kane-team/gpt2-wolof-french-translation1/sweeps/14019vvc</a>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              " View project at <a href='https://wandb.ai/oumar-kane-team/gpt2-wolof-french-translation1' target=\"_blank\">https://wandb.ai/oumar-kane-team/gpt2-wolof-french-translation1</a>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              " View sweep at <a href='https://wandb.ai/oumar-kane-team/gpt2-wolof-french-translation1/sweeps/14019vvc' target=\"_blank\">https://wandb.ai/oumar-kane-team/gpt2-wolof-french-translation1/sweeps/14019vvc</a>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              " View run at <a href='https://wandb.ai/oumar-kane-team/gpt2-wolof-french-translation1/runs/cqdm4q5a' target=\"_blank\">https://wandb.ai/oumar-kane-team/gpt2-wolof-french-translation1/runs/cqdm4q5a</a>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/optimization.py:391: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
            "  warnings.warn(\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'learning_rate' was locked by 'sweep' (ignored update).\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'weight_decay' was locked by 'sweep' (ignored update).\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='245' max='245' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [245/245 01:38, Epoch 1/1]\n",
              "    </div>\n",
              "    <table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              " <tr style=\"text-align: left;\">\n",
              "      <th>Epoch</th>\n",
              "      <th>Training Loss</th>\n",
              "      <th>Validation Loss</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <td>1</td>\n",
              "      <td>1.618000</td>\n",
              "      <td>0.986327</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table><p>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Waiting for W&B process to finish... <strong style=\"color:green\">(success).</strong>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "<style>\n",
              "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
              "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
              "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
              "    </style>\n",
              "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>eval/loss</td><td>‚ñÅ</td></tr><tr><td>eval/runtime</td><td>‚ñÅ</td></tr><tr><td>eval/samples_per_second</td><td>‚ñÅ</td></tr><tr><td>eval/steps_per_second</td><td>‚ñÅ</td></tr><tr><td>train/epoch</td><td>‚ñÅ‚ñÅ‚ñÅ</td></tr><tr><td>train/global_step</td><td>‚ñÅ‚ñÅ‚ñÅ</td></tr><tr><td>train/learning_rate</td><td>‚ñÅ</td></tr><tr><td>train/loss</td><td>‚ñÅ</td></tr><tr><td>train/total_flos</td><td>‚ñÅ</td></tr><tr><td>train/train_loss</td><td>‚ñÅ</td></tr><tr><td>train/train_runtime</td><td>‚ñÅ</td></tr><tr><td>train/train_samples_per_second</td><td>‚ñÅ</td></tr><tr><td>train/train_steps_per_second</td><td>‚ñÅ</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>eval/loss</td><td>0.98633</td></tr><tr><td>eval/runtime</td><td>2.7112</td></tr><tr><td>eval/samples_per_second</td><td>30.244</td></tr><tr><td>eval/steps_per_second</td><td>6.27</td></tr><tr><td>train/epoch</td><td>1.0</td></tr><tr><td>train/global_step</td><td>245</td></tr><tr><td>train/learning_rate</td><td>0.0</td></tr><tr><td>train/loss</td><td>1.618</td></tr><tr><td>train/total_flos</td><td>216590170752000.0</td></tr><tr><td>train/train_loss</td><td>1.61798</td></tr><tr><td>train/train_runtime</td><td>90.9636</td></tr><tr><td>train/train_samples_per_second</td><td>8.058</td></tr><tr><td>train/train_steps_per_second</td><td>2.693</td></tr></table><br/></div></div>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              " View run <strong style=\"color:#cdcd00\">feasible-sweep-19</strong> at: <a href='https://wandb.ai/oumar-kane-team/gpt2-wolof-french-translation1/runs/cqdm4q5a' target=\"_blank\">https://wandb.ai/oumar-kane-team/gpt2-wolof-french-translation1/runs/cqdm4q5a</a><br/>Synced 5 W&B file(s), 0 media file(s), 4 artifact file(s) and 0 other file(s)"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Find logs at: <code>./wandb/run-20230428_183928-cqdm4q5a/logs</code>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\u001b[34m\u001b[1mwandb\u001b[0m: Sweep Agent: Waiting for job.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Job received.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Agent Starting Run: jo4o0j13 with config:\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \tbatch_size: 3\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \tepochs: 1\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \tfr_char_p: 0.5897261699366377\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \tfr_word_p: 0.40972011471398034\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \tlearning_rate: 3.889699482264556e-05\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \trandom_state: 60\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \tweight_decay: 0\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \twf_char_p: 0.46983715080079863\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \twf_word_p: 0.5073771947437731\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m WANDB_NOTEBOOK_NAME should be a path to a notebook file, couldn't find training_gpt2_2.ipynb.\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "Tracking run with wandb version 0.15.0"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Run data is saved locally in <code>/content/wandb/run-20230428_184146-jo4o0j13</code>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Syncing run <strong><a href='https://wandb.ai/oumar-kane-team/gpt2-wolof-french-translation1/runs/jo4o0j13' target=\"_blank\">usual-sweep-20</a></strong> to <a href='https://wandb.ai/oumar-kane-team/gpt2-wolof-french-translation1' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>Sweep page: <a href='https://wandb.ai/oumar-kane-team/gpt2-wolof-french-translation1/sweeps/14019vvc' target=\"_blank\">https://wandb.ai/oumar-kane-team/gpt2-wolof-french-translation1/sweeps/14019vvc</a>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              " View project at <a href='https://wandb.ai/oumar-kane-team/gpt2-wolof-french-translation1' target=\"_blank\">https://wandb.ai/oumar-kane-team/gpt2-wolof-french-translation1</a>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              " View sweep at <a href='https://wandb.ai/oumar-kane-team/gpt2-wolof-french-translation1/sweeps/14019vvc' target=\"_blank\">https://wandb.ai/oumar-kane-team/gpt2-wolof-french-translation1/sweeps/14019vvc</a>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              " View run at <a href='https://wandb.ai/oumar-kane-team/gpt2-wolof-french-translation1/runs/jo4o0j13' target=\"_blank\">https://wandb.ai/oumar-kane-team/gpt2-wolof-french-translation1/runs/jo4o0j13</a>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/optimization.py:391: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
            "  warnings.warn(\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'learning_rate' was locked by 'sweep' (ignored update).\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'weight_decay' was locked by 'sweep' (ignored update).\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='245' max='245' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [245/245 01:36, Epoch 1/1]\n",
              "    </div>\n",
              "    <table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              " <tr style=\"text-align: left;\">\n",
              "      <th>Epoch</th>\n",
              "      <th>Training Loss</th>\n",
              "      <th>Validation Loss</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <td>1</td>\n",
              "      <td>1.611200</td>\n",
              "      <td>0.950319</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table><p>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Waiting for W&B process to finish... <strong style=\"color:green\">(success).</strong>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "<style>\n",
              "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
              "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
              "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
              "    </style>\n",
              "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>eval/loss</td><td>‚ñÅ</td></tr><tr><td>eval/runtime</td><td>‚ñÅ</td></tr><tr><td>eval/samples_per_second</td><td>‚ñÅ</td></tr><tr><td>eval/steps_per_second</td><td>‚ñÅ</td></tr><tr><td>train/epoch</td><td>‚ñÅ‚ñÅ‚ñÅ</td></tr><tr><td>train/global_step</td><td>‚ñÅ‚ñÅ‚ñÅ</td></tr><tr><td>train/learning_rate</td><td>‚ñÅ</td></tr><tr><td>train/loss</td><td>‚ñÅ</td></tr><tr><td>train/total_flos</td><td>‚ñÅ</td></tr><tr><td>train/train_loss</td><td>‚ñÅ</td></tr><tr><td>train/train_runtime</td><td>‚ñÅ</td></tr><tr><td>train/train_samples_per_second</td><td>‚ñÅ</td></tr><tr><td>train/train_steps_per_second</td><td>‚ñÅ</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>eval/loss</td><td>0.95032</td></tr><tr><td>eval/runtime</td><td>2.7268</td></tr><tr><td>eval/samples_per_second</td><td>30.072</td></tr><tr><td>eval/steps_per_second</td><td>6.234</td></tr><tr><td>train/epoch</td><td>1.0</td></tr><tr><td>train/global_step</td><td>245</td></tr><tr><td>train/learning_rate</td><td>0.0</td></tr><tr><td>train/loss</td><td>1.6112</td></tr><tr><td>train/total_flos</td><td>216590170752000.0</td></tr><tr><td>train/train_loss</td><td>1.61115</td></tr><tr><td>train/train_runtime</td><td>90.2207</td></tr><tr><td>train/train_samples_per_second</td><td>8.125</td></tr><tr><td>train/train_steps_per_second</td><td>2.716</td></tr></table><br/></div></div>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              " View run <strong style=\"color:#cdcd00\">usual-sweep-20</strong> at: <a href='https://wandb.ai/oumar-kane-team/gpt2-wolof-french-translation1/runs/jo4o0j13' target=\"_blank\">https://wandb.ai/oumar-kane-team/gpt2-wolof-french-translation1/runs/jo4o0j13</a><br/>Synced 5 W&B file(s), 0 media file(s), 4 artifact file(s) and 0 other file(s)"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Find logs at: <code>./wandb/run-20230428_184146-jo4o0j13/logs</code>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\u001b[34m\u001b[1mwandb\u001b[0m: Agent Starting Run: e4ok3bld with config:\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \tbatch_size: 2\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \tepochs: 1\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \tfr_char_p: 0.25590210385850376\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \tfr_word_p: 0.22438627557888657\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \tlearning_rate: 0.00018345393811845188\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \trandom_state: 80\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \tweight_decay: 0.3\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \twf_char_p: 0.5224095513229281\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \twf_word_p: 0.5418942676653388\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m WANDB_NOTEBOOK_NAME should be a path to a notebook file, couldn't find training_gpt2_2.ipynb.\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "Tracking run with wandb version 0.15.0"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Run data is saved locally in <code>/content/wandb/run-20230428_184353-e4ok3bld</code>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Syncing run <strong><a href='https://wandb.ai/oumar-kane-team/gpt2-wolof-french-translation1/runs/e4ok3bld' target=\"_blank\">upbeat-sweep-21</a></strong> to <a href='https://wandb.ai/oumar-kane-team/gpt2-wolof-french-translation1' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>Sweep page: <a href='https://wandb.ai/oumar-kane-team/gpt2-wolof-french-translation1/sweeps/14019vvc' target=\"_blank\">https://wandb.ai/oumar-kane-team/gpt2-wolof-french-translation1/sweeps/14019vvc</a>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              " View project at <a href='https://wandb.ai/oumar-kane-team/gpt2-wolof-french-translation1' target=\"_blank\">https://wandb.ai/oumar-kane-team/gpt2-wolof-french-translation1</a>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              " View sweep at <a href='https://wandb.ai/oumar-kane-team/gpt2-wolof-french-translation1/sweeps/14019vvc' target=\"_blank\">https://wandb.ai/oumar-kane-team/gpt2-wolof-french-translation1/sweeps/14019vvc</a>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              " View run at <a href='https://wandb.ai/oumar-kane-team/gpt2-wolof-french-translation1/runs/e4ok3bld' target=\"_blank\">https://wandb.ai/oumar-kane-team/gpt2-wolof-french-translation1/runs/e4ok3bld</a>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/optimization.py:391: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
            "  warnings.warn(\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'learning_rate' was locked by 'sweep' (ignored update).\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'weight_decay' was locked by 'sweep' (ignored update).\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='367' max='367' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [367/367 01:45, Epoch 1/1]\n",
              "    </div>\n",
              "    <table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              " <tr style=\"text-align: left;\">\n",
              "      <th>Epoch</th>\n",
              "      <th>Training Loss</th>\n",
              "      <th>Validation Loss</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <td>1</td>\n",
              "      <td>1.328600</td>\n",
              "      <td>0.825027</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table><p>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Waiting for W&B process to finish... <strong style=\"color:green\">(success).</strong>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "<style>\n",
              "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
              "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
              "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
              "    </style>\n",
              "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>eval/loss</td><td>‚ñÅ</td></tr><tr><td>eval/runtime</td><td>‚ñÅ</td></tr><tr><td>eval/samples_per_second</td><td>‚ñÅ</td></tr><tr><td>eval/steps_per_second</td><td>‚ñÅ</td></tr><tr><td>train/epoch</td><td>‚ñÅ‚ñÅ‚ñÅ</td></tr><tr><td>train/global_step</td><td>‚ñÅ‚ñÅ‚ñÅ</td></tr><tr><td>train/learning_rate</td><td>‚ñÅ</td></tr><tr><td>train/loss</td><td>‚ñÅ</td></tr><tr><td>train/total_flos</td><td>‚ñÅ</td></tr><tr><td>train/train_loss</td><td>‚ñÅ</td></tr><tr><td>train/train_runtime</td><td>‚ñÅ</td></tr><tr><td>train/train_samples_per_second</td><td>‚ñÅ</td></tr><tr><td>train/train_steps_per_second</td><td>‚ñÅ</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>eval/loss</td><td>0.82503</td></tr><tr><td>eval/runtime</td><td>2.7529</td></tr><tr><td>eval/samples_per_second</td><td>29.786</td></tr><tr><td>eval/steps_per_second</td><td>6.175</td></tr><tr><td>train/epoch</td><td>1.0</td></tr><tr><td>train/global_step</td><td>367</td></tr><tr><td>train/learning_rate</td><td>0.0</td></tr><tr><td>train/loss</td><td>1.3286</td></tr><tr><td>train/total_flos</td><td>216590170752000.0</td></tr><tr><td>train/train_loss</td><td>1.32862</td></tr><tr><td>train/train_runtime</td><td>99.6515</td></tr><tr><td>train/train_samples_per_second</td><td>7.356</td></tr><tr><td>train/train_steps_per_second</td><td>3.683</td></tr></table><br/></div></div>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              " View run <strong style=\"color:#cdcd00\">upbeat-sweep-21</strong> at: <a href='https://wandb.ai/oumar-kane-team/gpt2-wolof-french-translation1/runs/e4ok3bld' target=\"_blank\">https://wandb.ai/oumar-kane-team/gpt2-wolof-french-translation1/runs/e4ok3bld</a><br/>Synced 5 W&B file(s), 0 media file(s), 4 artifact file(s) and 0 other file(s)"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Find logs at: <code>./wandb/run-20230428_184353-e4ok3bld/logs</code>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\u001b[34m\u001b[1mwandb\u001b[0m: Agent Starting Run: mm4mdsqo with config:\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \tbatch_size: 5\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \tepochs: 1\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \tfr_char_p: 0.5738866361745114\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \tfr_word_p: 0.620986394828583\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \tlearning_rate: 1.2452444860615832e-05\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \trandom_state: 30\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \tweight_decay: 0.1\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \twf_char_p: 0.5018859032164279\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \twf_word_p: 0.6477684239138255\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m WANDB_NOTEBOOK_NAME should be a path to a notebook file, couldn't find training_gpt2_2.ipynb.\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "Tracking run with wandb version 0.15.0"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Run data is saved locally in <code>/content/wandb/run-20230428_184612-mm4mdsqo</code>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Syncing run <strong><a href='https://wandb.ai/oumar-kane-team/gpt2-wolof-french-translation1/runs/mm4mdsqo' target=\"_blank\">pleasant-sweep-22</a></strong> to <a href='https://wandb.ai/oumar-kane-team/gpt2-wolof-french-translation1' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>Sweep page: <a href='https://wandb.ai/oumar-kane-team/gpt2-wolof-french-translation1/sweeps/14019vvc' target=\"_blank\">https://wandb.ai/oumar-kane-team/gpt2-wolof-french-translation1/sweeps/14019vvc</a>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              " View project at <a href='https://wandb.ai/oumar-kane-team/gpt2-wolof-french-translation1' target=\"_blank\">https://wandb.ai/oumar-kane-team/gpt2-wolof-french-translation1</a>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              " View sweep at <a href='https://wandb.ai/oumar-kane-team/gpt2-wolof-french-translation1/sweeps/14019vvc' target=\"_blank\">https://wandb.ai/oumar-kane-team/gpt2-wolof-french-translation1/sweeps/14019vvc</a>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              " View run at <a href='https://wandb.ai/oumar-kane-team/gpt2-wolof-french-translation1/runs/mm4mdsqo' target=\"_blank\">https://wandb.ai/oumar-kane-team/gpt2-wolof-french-translation1/runs/mm4mdsqo</a>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/optimization.py:391: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
            "  warnings.warn(\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'learning_rate' was locked by 'sweep' (ignored update).\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'weight_decay' was locked by 'sweep' (ignored update).\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='147' max='147' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [147/147 01:31, Epoch 1/1]\n",
              "    </div>\n",
              "    <table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              " <tr style=\"text-align: left;\">\n",
              "      <th>Epoch</th>\n",
              "      <th>Training Loss</th>\n",
              "      <th>Validation Loss</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <td>1</td>\n",
              "      <td>2.122100</td>\n",
              "      <td>1.082944</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table><p>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Waiting for W&B process to finish... <strong style=\"color:green\">(success).</strong>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "<style>\n",
              "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
              "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
              "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
              "    </style>\n",
              "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>eval/loss</td><td>‚ñÅ</td></tr><tr><td>eval/runtime</td><td>‚ñÅ</td></tr><tr><td>eval/samples_per_second</td><td>‚ñÅ</td></tr><tr><td>eval/steps_per_second</td><td>‚ñÅ</td></tr><tr><td>train/epoch</td><td>‚ñÅ‚ñÅ‚ñÅ</td></tr><tr><td>train/global_step</td><td>‚ñÅ‚ñÅ‚ñÅ</td></tr><tr><td>train/learning_rate</td><td>‚ñÅ</td></tr><tr><td>train/loss</td><td>‚ñÅ</td></tr><tr><td>train/total_flos</td><td>‚ñÅ</td></tr><tr><td>train/train_loss</td><td>‚ñÅ</td></tr><tr><td>train/train_runtime</td><td>‚ñÅ</td></tr><tr><td>train/train_samples_per_second</td><td>‚ñÅ</td></tr><tr><td>train/train_steps_per_second</td><td>‚ñÅ</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>eval/loss</td><td>1.08294</td></tr><tr><td>eval/runtime</td><td>2.7187</td></tr><tr><td>eval/samples_per_second</td><td>30.162</td></tr><tr><td>eval/steps_per_second</td><td>6.253</td></tr><tr><td>train/epoch</td><td>1.0</td></tr><tr><td>train/global_step</td><td>147</td></tr><tr><td>train/learning_rate</td><td>0.0</td></tr><tr><td>train/loss</td><td>2.1221</td></tr><tr><td>train/total_flos</td><td>216590170752000.0</td></tr><tr><td>train/train_loss</td><td>2.12212</td></tr><tr><td>train/train_runtime</td><td>85.1276</td></tr><tr><td>train/train_samples_per_second</td><td>8.611</td></tr><tr><td>train/train_steps_per_second</td><td>1.727</td></tr></table><br/></div></div>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              " View run <strong style=\"color:#cdcd00\">pleasant-sweep-22</strong> at: <a href='https://wandb.ai/oumar-kane-team/gpt2-wolof-french-translation1/runs/mm4mdsqo' target=\"_blank\">https://wandb.ai/oumar-kane-team/gpt2-wolof-french-translation1/runs/mm4mdsqo</a><br/>Synced 5 W&B file(s), 0 media file(s), 4 artifact file(s) and 0 other file(s)"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Find logs at: <code>./wandb/run-20230428_184612-mm4mdsqo/logs</code>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\u001b[34m\u001b[1mwandb\u001b[0m: Agent Starting Run: ypqfy2iy with config:\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \tbatch_size: 3\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \tepochs: 1\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \tfr_char_p: 0.17687337193367414\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \tfr_word_p: 0.19284716266445276\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \tlearning_rate: 0.0001288254898808281\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \trandom_state: 0\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \tweight_decay: 0.4\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \twf_char_p: 0.26615379490394614\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \twf_word_p: 0.497840262426633\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m WANDB_NOTEBOOK_NAME should be a path to a notebook file, couldn't find training_gpt2_2.ipynb.\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "Tracking run with wandb version 0.15.0"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Run data is saved locally in <code>/content/wandb/run-20230428_184816-ypqfy2iy</code>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Syncing run <strong><a href='https://wandb.ai/oumar-kane-team/gpt2-wolof-french-translation1/runs/ypqfy2iy' target=\"_blank\">swept-sweep-23</a></strong> to <a href='https://wandb.ai/oumar-kane-team/gpt2-wolof-french-translation1' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>Sweep page: <a href='https://wandb.ai/oumar-kane-team/gpt2-wolof-french-translation1/sweeps/14019vvc' target=\"_blank\">https://wandb.ai/oumar-kane-team/gpt2-wolof-french-translation1/sweeps/14019vvc</a>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              " View project at <a href='https://wandb.ai/oumar-kane-team/gpt2-wolof-french-translation1' target=\"_blank\">https://wandb.ai/oumar-kane-team/gpt2-wolof-french-translation1</a>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              " View sweep at <a href='https://wandb.ai/oumar-kane-team/gpt2-wolof-french-translation1/sweeps/14019vvc' target=\"_blank\">https://wandb.ai/oumar-kane-team/gpt2-wolof-french-translation1/sweeps/14019vvc</a>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              " View run at <a href='https://wandb.ai/oumar-kane-team/gpt2-wolof-french-translation1/runs/ypqfy2iy' target=\"_blank\">https://wandb.ai/oumar-kane-team/gpt2-wolof-french-translation1/runs/ypqfy2iy</a>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/optimization.py:391: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
            "  warnings.warn(\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'learning_rate' was locked by 'sweep' (ignored update).\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'weight_decay' was locked by 'sweep' (ignored update).\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='245' max='245' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [245/245 01:36, Epoch 1/1]\n",
              "    </div>\n",
              "    <table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              " <tr style=\"text-align: left;\">\n",
              "      <th>Epoch</th>\n",
              "      <th>Training Loss</th>\n",
              "      <th>Validation Loss</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <td>1</td>\n",
              "      <td>1.386000</td>\n",
              "      <td>0.775263</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table><p>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Waiting for W&B process to finish... <strong style=\"color:green\">(success).</strong>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "<style>\n",
              "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
              "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
              "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
              "    </style>\n",
              "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>eval/loss</td><td>‚ñÅ</td></tr><tr><td>eval/runtime</td><td>‚ñÅ</td></tr><tr><td>eval/samples_per_second</td><td>‚ñÅ</td></tr><tr><td>eval/steps_per_second</td><td>‚ñÅ</td></tr><tr><td>train/epoch</td><td>‚ñÅ‚ñÅ‚ñÅ</td></tr><tr><td>train/global_step</td><td>‚ñÅ‚ñÅ‚ñÅ</td></tr><tr><td>train/learning_rate</td><td>‚ñÅ</td></tr><tr><td>train/loss</td><td>‚ñÅ</td></tr><tr><td>train/total_flos</td><td>‚ñÅ</td></tr><tr><td>train/train_loss</td><td>‚ñÅ</td></tr><tr><td>train/train_runtime</td><td>‚ñÅ</td></tr><tr><td>train/train_samples_per_second</td><td>‚ñÅ</td></tr><tr><td>train/train_steps_per_second</td><td>‚ñÅ</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>eval/loss</td><td>0.77526</td></tr><tr><td>eval/runtime</td><td>2.7108</td></tr><tr><td>eval/samples_per_second</td><td>30.249</td></tr><tr><td>eval/steps_per_second</td><td>6.271</td></tr><tr><td>train/epoch</td><td>1.0</td></tr><tr><td>train/global_step</td><td>245</td></tr><tr><td>train/learning_rate</td><td>0.0</td></tr><tr><td>train/loss</td><td>1.386</td></tr><tr><td>train/total_flos</td><td>216590170752000.0</td></tr><tr><td>train/train_loss</td><td>1.38603</td></tr><tr><td>train/train_runtime</td><td>90.8706</td></tr><tr><td>train/train_samples_per_second</td><td>8.066</td></tr><tr><td>train/train_steps_per_second</td><td>2.696</td></tr></table><br/></div></div>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              " View run <strong style=\"color:#cdcd00\">swept-sweep-23</strong> at: <a href='https://wandb.ai/oumar-kane-team/gpt2-wolof-french-translation1/runs/ypqfy2iy' target=\"_blank\">https://wandb.ai/oumar-kane-team/gpt2-wolof-french-translation1/runs/ypqfy2iy</a><br/>Synced 5 W&B file(s), 0 media file(s), 4 artifact file(s) and 0 other file(s)"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Find logs at: <code>./wandb/run-20230428_184816-ypqfy2iy/logs</code>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\u001b[34m\u001b[1mwandb\u001b[0m: Agent Starting Run: boye36v8 with config:\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \tbatch_size: 5\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \tepochs: 1\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \tfr_char_p: 0.48264858364465457\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \tfr_word_p: 0.33448991683794504\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \tlearning_rate: 0.00031140780990074384\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \trandom_state: 40\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \tweight_decay: 0.2\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \twf_char_p: 0.060828957933087\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \twf_word_p: 0.0061826893974647685\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m WANDB_NOTEBOOK_NAME should be a path to a notebook file, couldn't find training_gpt2_2.ipynb.\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "Tracking run with wandb version 0.15.0"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Run data is saved locally in <code>/content/wandb/run-20230428_185026-boye36v8</code>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Syncing run <strong><a href='https://wandb.ai/oumar-kane-team/gpt2-wolof-french-translation1/runs/boye36v8' target=\"_blank\">royal-sweep-24</a></strong> to <a href='https://wandb.ai/oumar-kane-team/gpt2-wolof-french-translation1' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>Sweep page: <a href='https://wandb.ai/oumar-kane-team/gpt2-wolof-french-translation1/sweeps/14019vvc' target=\"_blank\">https://wandb.ai/oumar-kane-team/gpt2-wolof-french-translation1/sweeps/14019vvc</a>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              " View project at <a href='https://wandb.ai/oumar-kane-team/gpt2-wolof-french-translation1' target=\"_blank\">https://wandb.ai/oumar-kane-team/gpt2-wolof-french-translation1</a>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              " View sweep at <a href='https://wandb.ai/oumar-kane-team/gpt2-wolof-french-translation1/sweeps/14019vvc' target=\"_blank\">https://wandb.ai/oumar-kane-team/gpt2-wolof-french-translation1/sweeps/14019vvc</a>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              " View run at <a href='https://wandb.ai/oumar-kane-team/gpt2-wolof-french-translation1/runs/boye36v8' target=\"_blank\">https://wandb.ai/oumar-kane-team/gpt2-wolof-french-translation1/runs/boye36v8</a>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/optimization.py:391: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
            "  warnings.warn(\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'learning_rate' was locked by 'sweep' (ignored update).\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'weight_decay' was locked by 'sweep' (ignored update).\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='147' max='147' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [147/147 01:29, Epoch 1/1]\n",
              "    </div>\n",
              "    <table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              " <tr style=\"text-align: left;\">\n",
              "      <th>Epoch</th>\n",
              "      <th>Training Loss</th>\n",
              "      <th>Validation Loss</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <td>1</td>\n",
              "      <td>1.656500</td>\n",
              "      <td>0.821011</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table><p>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Waiting for W&B process to finish... <strong style=\"color:green\">(success).</strong>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "<style>\n",
              "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
              "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
              "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
              "    </style>\n",
              "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>eval/loss</td><td>‚ñÅ</td></tr><tr><td>eval/runtime</td><td>‚ñÅ</td></tr><tr><td>eval/samples_per_second</td><td>‚ñÅ</td></tr><tr><td>eval/steps_per_second</td><td>‚ñÅ</td></tr><tr><td>train/epoch</td><td>‚ñÅ‚ñÅ‚ñÅ</td></tr><tr><td>train/global_step</td><td>‚ñÅ‚ñÅ‚ñÅ</td></tr><tr><td>train/learning_rate</td><td>‚ñÅ</td></tr><tr><td>train/loss</td><td>‚ñÅ</td></tr><tr><td>train/total_flos</td><td>‚ñÅ</td></tr><tr><td>train/train_loss</td><td>‚ñÅ</td></tr><tr><td>train/train_runtime</td><td>‚ñÅ</td></tr><tr><td>train/train_samples_per_second</td><td>‚ñÅ</td></tr><tr><td>train/train_steps_per_second</td><td>‚ñÅ</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>eval/loss</td><td>0.82101</td></tr><tr><td>eval/runtime</td><td>2.6977</td></tr><tr><td>eval/samples_per_second</td><td>30.396</td></tr><tr><td>eval/steps_per_second</td><td>6.302</td></tr><tr><td>train/epoch</td><td>1.0</td></tr><tr><td>train/global_step</td><td>147</td></tr><tr><td>train/learning_rate</td><td>2e-05</td></tr><tr><td>train/loss</td><td>1.6565</td></tr><tr><td>train/total_flos</td><td>216590170752000.0</td></tr><tr><td>train/train_loss</td><td>1.65649</td></tr><tr><td>train/train_runtime</td><td>84.152</td></tr><tr><td>train/train_samples_per_second</td><td>8.71</td></tr><tr><td>train/train_steps_per_second</td><td>1.747</td></tr></table><br/></div></div>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              " View run <strong style=\"color:#cdcd00\">royal-sweep-24</strong> at: <a href='https://wandb.ai/oumar-kane-team/gpt2-wolof-french-translation1/runs/boye36v8' target=\"_blank\">https://wandb.ai/oumar-kane-team/gpt2-wolof-french-translation1/runs/boye36v8</a><br/>Synced 5 W&B file(s), 0 media file(s), 4 artifact file(s) and 0 other file(s)"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Find logs at: <code>./wandb/run-20230428_185026-boye36v8/logs</code>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\u001b[34m\u001b[1mwandb\u001b[0m: Agent Starting Run: pugxa5gk with config:\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \tbatch_size: 2\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \tepochs: 1\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \tfr_char_p: 0.2005369908810156\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \tfr_word_p: 0.6192113034969234\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \tlearning_rate: 8.222510622492054e-05\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \trandom_state: 50\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \tweight_decay: 0.3\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \twf_char_p: 0.34897660777730927\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \twf_word_p: 0.14382038114531173\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m WANDB_NOTEBOOK_NAME should be a path to a notebook file, couldn't find training_gpt2_2.ipynb.\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "Tracking run with wandb version 0.15.0"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Run data is saved locally in <code>/content/wandb/run-20230428_185231-pugxa5gk</code>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Syncing run <strong><a href='https://wandb.ai/oumar-kane-team/gpt2-wolof-french-translation1/runs/pugxa5gk' target=\"_blank\">pleasant-sweep-25</a></strong> to <a href='https://wandb.ai/oumar-kane-team/gpt2-wolof-french-translation1' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>Sweep page: <a href='https://wandb.ai/oumar-kane-team/gpt2-wolof-french-translation1/sweeps/14019vvc' target=\"_blank\">https://wandb.ai/oumar-kane-team/gpt2-wolof-french-translation1/sweeps/14019vvc</a>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              " View project at <a href='https://wandb.ai/oumar-kane-team/gpt2-wolof-french-translation1' target=\"_blank\">https://wandb.ai/oumar-kane-team/gpt2-wolof-french-translation1</a>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              " View sweep at <a href='https://wandb.ai/oumar-kane-team/gpt2-wolof-french-translation1/sweeps/14019vvc' target=\"_blank\">https://wandb.ai/oumar-kane-team/gpt2-wolof-french-translation1/sweeps/14019vvc</a>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              " View run at <a href='https://wandb.ai/oumar-kane-team/gpt2-wolof-french-translation1/runs/pugxa5gk' target=\"_blank\">https://wandb.ai/oumar-kane-team/gpt2-wolof-french-translation1/runs/pugxa5gk</a>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/optimization.py:391: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
            "  warnings.warn(\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'learning_rate' was locked by 'sweep' (ignored update).\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'weight_decay' was locked by 'sweep' (ignored update).\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='367' max='367' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [367/367 01:44, Epoch 1/1]\n",
              "    </div>\n",
              "    <table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              " <tr style=\"text-align: left;\">\n",
              "      <th>Epoch</th>\n",
              "      <th>Training Loss</th>\n",
              "      <th>Validation Loss</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <td>1</td>\n",
              "      <td>1.394400</td>\n",
              "      <td>0.880709</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table><p>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Waiting for W&B process to finish... <strong style=\"color:green\">(success).</strong>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "<style>\n",
              "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
              "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
              "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
              "    </style>\n",
              "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>eval/loss</td><td>‚ñÅ</td></tr><tr><td>eval/runtime</td><td>‚ñÅ</td></tr><tr><td>eval/samples_per_second</td><td>‚ñÅ</td></tr><tr><td>eval/steps_per_second</td><td>‚ñÅ</td></tr><tr><td>train/epoch</td><td>‚ñÅ‚ñÅ‚ñÅ</td></tr><tr><td>train/global_step</td><td>‚ñÅ‚ñÅ‚ñÅ</td></tr><tr><td>train/learning_rate</td><td>‚ñÅ</td></tr><tr><td>train/loss</td><td>‚ñÅ</td></tr><tr><td>train/total_flos</td><td>‚ñÅ</td></tr><tr><td>train/train_loss</td><td>‚ñÅ</td></tr><tr><td>train/train_runtime</td><td>‚ñÅ</td></tr><tr><td>train/train_samples_per_second</td><td>‚ñÅ</td></tr><tr><td>train/train_steps_per_second</td><td>‚ñÅ</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>eval/loss</td><td>0.88071</td></tr><tr><td>eval/runtime</td><td>2.7231</td></tr><tr><td>eval/samples_per_second</td><td>30.112</td></tr><tr><td>eval/steps_per_second</td><td>6.243</td></tr><tr><td>train/epoch</td><td>1.0</td></tr><tr><td>train/global_step</td><td>367</td></tr><tr><td>train/learning_rate</td><td>0.0</td></tr><tr><td>train/loss</td><td>1.3944</td></tr><tr><td>train/total_flos</td><td>216590170752000.0</td></tr><tr><td>train/train_loss</td><td>1.39437</td></tr><tr><td>train/train_runtime</td><td>98.2322</td></tr><tr><td>train/train_samples_per_second</td><td>7.462</td></tr><tr><td>train/train_steps_per_second</td><td>3.736</td></tr></table><br/></div></div>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              " View run <strong style=\"color:#cdcd00\">pleasant-sweep-25</strong> at: <a href='https://wandb.ai/oumar-kane-team/gpt2-wolof-french-translation1/runs/pugxa5gk' target=\"_blank\">https://wandb.ai/oumar-kane-team/gpt2-wolof-french-translation1/runs/pugxa5gk</a><br/>Synced 5 W&B file(s), 0 media file(s), 4 artifact file(s) and 0 other file(s)"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Find logs at: <code>./wandb/run-20230428_185231-pugxa5gk/logs</code>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\u001b[34m\u001b[1mwandb\u001b[0m: Agent Starting Run: qgktshex with config:\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \tbatch_size: 3\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \tepochs: 1\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \tfr_char_p: 0.2005777699232024\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \tfr_word_p: 0.4385247110587816\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \tlearning_rate: 1.8573037368967597e-05\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \trandom_state: 20\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \tweight_decay: 0.5\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \twf_char_p: 0.6186729799794922\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \twf_word_p: 0.11024648884950464\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m WANDB_NOTEBOOK_NAME should be a path to a notebook file, couldn't find training_gpt2_2.ipynb.\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "6342d7b19cf24413a7e6abc021730a7a",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "VBox(children=(Label(value='Waiting for wandb.init()...\\r'), FloatProgress(value=0.01666909818333503, max=1.0)‚Ä¶"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Tracking run with wandb version 0.15.0"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Run data is saved locally in <code>/content/wandb/run-20230428_185445-qgktshex</code>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Syncing run <strong><a href='https://wandb.ai/oumar-kane-team/gpt2-wolof-french-translation1/runs/qgktshex' target=\"_blank\">solar-sweep-26</a></strong> to <a href='https://wandb.ai/oumar-kane-team/gpt2-wolof-french-translation1' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>Sweep page: <a href='https://wandb.ai/oumar-kane-team/gpt2-wolof-french-translation1/sweeps/14019vvc' target=\"_blank\">https://wandb.ai/oumar-kane-team/gpt2-wolof-french-translation1/sweeps/14019vvc</a>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              " View project at <a href='https://wandb.ai/oumar-kane-team/gpt2-wolof-french-translation1' target=\"_blank\">https://wandb.ai/oumar-kane-team/gpt2-wolof-french-translation1</a>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              " View sweep at <a href='https://wandb.ai/oumar-kane-team/gpt2-wolof-french-translation1/sweeps/14019vvc' target=\"_blank\">https://wandb.ai/oumar-kane-team/gpt2-wolof-french-translation1/sweeps/14019vvc</a>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              " View run at <a href='https://wandb.ai/oumar-kane-team/gpt2-wolof-french-translation1/runs/qgktshex' target=\"_blank\">https://wandb.ai/oumar-kane-team/gpt2-wolof-french-translation1/runs/qgktshex</a>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/optimization.py:391: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
            "  warnings.warn(\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'learning_rate' was locked by 'sweep' (ignored update).\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'weight_decay' was locked by 'sweep' (ignored update).\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='245' max='245' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [245/245 01:38, Epoch 1/1]\n",
              "    </div>\n",
              "    <table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              " <tr style=\"text-align: left;\">\n",
              "      <th>Epoch</th>\n",
              "      <th>Training Loss</th>\n",
              "      <th>Validation Loss</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <td>1</td>\n",
              "      <td>1.645100</td>\n",
              "      <td>0.976688</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table><p>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Waiting for W&B process to finish... <strong style=\"color:green\">(success).</strong>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "<style>\n",
              "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
              "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
              "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
              "    </style>\n",
              "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>eval/loss</td><td>‚ñÅ</td></tr><tr><td>eval/runtime</td><td>‚ñÅ</td></tr><tr><td>eval/samples_per_second</td><td>‚ñÅ</td></tr><tr><td>eval/steps_per_second</td><td>‚ñÅ</td></tr><tr><td>train/epoch</td><td>‚ñÅ‚ñÅ‚ñÅ</td></tr><tr><td>train/global_step</td><td>‚ñÅ‚ñÅ‚ñÅ</td></tr><tr><td>train/learning_rate</td><td>‚ñÅ</td></tr><tr><td>train/loss</td><td>‚ñÅ</td></tr><tr><td>train/total_flos</td><td>‚ñÅ</td></tr><tr><td>train/train_loss</td><td>‚ñÅ</td></tr><tr><td>train/train_runtime</td><td>‚ñÅ</td></tr><tr><td>train/train_samples_per_second</td><td>‚ñÅ</td></tr><tr><td>train/train_steps_per_second</td><td>‚ñÅ</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>eval/loss</td><td>0.97669</td></tr><tr><td>eval/runtime</td><td>2.7517</td></tr><tr><td>eval/samples_per_second</td><td>29.8</td></tr><tr><td>eval/steps_per_second</td><td>6.178</td></tr><tr><td>train/epoch</td><td>1.0</td></tr><tr><td>train/global_step</td><td>245</td></tr><tr><td>train/learning_rate</td><td>0.0</td></tr><tr><td>train/loss</td><td>1.6451</td></tr><tr><td>train/total_flos</td><td>216590170752000.0</td></tr><tr><td>train/train_loss</td><td>1.64512</td></tr><tr><td>train/train_runtime</td><td>90.9331</td></tr><tr><td>train/train_samples_per_second</td><td>8.061</td></tr><tr><td>train/train_steps_per_second</td><td>2.694</td></tr></table><br/></div></div>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              " View run <strong style=\"color:#cdcd00\">solar-sweep-26</strong> at: <a href='https://wandb.ai/oumar-kane-team/gpt2-wolof-french-translation1/runs/qgktshex' target=\"_blank\">https://wandb.ai/oumar-kane-team/gpt2-wolof-french-translation1/runs/qgktshex</a><br/>Synced 5 W&B file(s), 0 media file(s), 4 artifact file(s) and 0 other file(s)"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Find logs at: <code>./wandb/run-20230428_185445-qgktshex/logs</code>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\u001b[34m\u001b[1mwandb\u001b[0m: Agent Starting Run: imvxju8c with config:\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \tbatch_size: 5\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \tepochs: 1\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \tfr_char_p: 0.2429679815237644\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \tfr_word_p: 0.6715100689164555\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \tlearning_rate: 3.574388375356532e-05\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \trandom_state: 60\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \tweight_decay: 0.3\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \twf_char_p: 0.38070327933194215\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \twf_word_p: 0.33645061220489525\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m WANDB_NOTEBOOK_NAME should be a path to a notebook file, couldn't find training_gpt2_2.ipynb.\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "Tracking run with wandb version 0.15.0"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Run data is saved locally in <code>/content/wandb/run-20230428_185657-imvxju8c</code>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Syncing run <strong><a href='https://wandb.ai/oumar-kane-team/gpt2-wolof-french-translation1/runs/imvxju8c' target=\"_blank\">worldly-sweep-27</a></strong> to <a href='https://wandb.ai/oumar-kane-team/gpt2-wolof-french-translation1' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>Sweep page: <a href='https://wandb.ai/oumar-kane-team/gpt2-wolof-french-translation1/sweeps/14019vvc' target=\"_blank\">https://wandb.ai/oumar-kane-team/gpt2-wolof-french-translation1/sweeps/14019vvc</a>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              " View project at <a href='https://wandb.ai/oumar-kane-team/gpt2-wolof-french-translation1' target=\"_blank\">https://wandb.ai/oumar-kane-team/gpt2-wolof-french-translation1</a>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              " View sweep at <a href='https://wandb.ai/oumar-kane-team/gpt2-wolof-french-translation1/sweeps/14019vvc' target=\"_blank\">https://wandb.ai/oumar-kane-team/gpt2-wolof-french-translation1/sweeps/14019vvc</a>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              " View run at <a href='https://wandb.ai/oumar-kane-team/gpt2-wolof-french-translation1/runs/imvxju8c' target=\"_blank\">https://wandb.ai/oumar-kane-team/gpt2-wolof-french-translation1/runs/imvxju8c</a>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/optimization.py:391: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
            "  warnings.warn(\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'learning_rate' was locked by 'sweep' (ignored update).\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'weight_decay' was locked by 'sweep' (ignored update).\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='147' max='147' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [147/147 01:31, Epoch 1/1]\n",
              "    </div>\n",
              "    <table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              " <tr style=\"text-align: left;\">\n",
              "      <th>Epoch</th>\n",
              "      <th>Training Loss</th>\n",
              "      <th>Validation Loss</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <td>1</td>\n",
              "      <td>1.808600</td>\n",
              "      <td>0.959918</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table><p>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Waiting for W&B process to finish... <strong style=\"color:green\">(success).</strong>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "<style>\n",
              "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
              "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
              "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
              "    </style>\n",
              "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>eval/loss</td><td>‚ñÅ</td></tr><tr><td>eval/runtime</td><td>‚ñÅ</td></tr><tr><td>eval/samples_per_second</td><td>‚ñÅ</td></tr><tr><td>eval/steps_per_second</td><td>‚ñÅ</td></tr><tr><td>train/epoch</td><td>‚ñÅ‚ñÅ‚ñÅ</td></tr><tr><td>train/global_step</td><td>‚ñÅ‚ñÅ‚ñÅ</td></tr><tr><td>train/learning_rate</td><td>‚ñÅ</td></tr><tr><td>train/loss</td><td>‚ñÅ</td></tr><tr><td>train/total_flos</td><td>‚ñÅ</td></tr><tr><td>train/train_loss</td><td>‚ñÅ</td></tr><tr><td>train/train_runtime</td><td>‚ñÅ</td></tr><tr><td>train/train_samples_per_second</td><td>‚ñÅ</td></tr><tr><td>train/train_steps_per_second</td><td>‚ñÅ</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>eval/loss</td><td>0.95992</td></tr><tr><td>eval/runtime</td><td>2.7306</td></tr><tr><td>eval/samples_per_second</td><td>30.03</td></tr><tr><td>eval/steps_per_second</td><td>6.226</td></tr><tr><td>train/epoch</td><td>1.0</td></tr><tr><td>train/global_step</td><td>147</td></tr><tr><td>train/learning_rate</td><td>0.0</td></tr><tr><td>train/loss</td><td>1.8086</td></tr><tr><td>train/total_flos</td><td>216590170752000.0</td></tr><tr><td>train/train_loss</td><td>1.80862</td></tr><tr><td>train/train_runtime</td><td>84.5346</td></tr><tr><td>train/train_samples_per_second</td><td>8.671</td></tr><tr><td>train/train_steps_per_second</td><td>1.739</td></tr></table><br/></div></div>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              " View run <strong style=\"color:#cdcd00\">worldly-sweep-27</strong> at: <a href='https://wandb.ai/oumar-kane-team/gpt2-wolof-french-translation1/runs/imvxju8c' target=\"_blank\">https://wandb.ai/oumar-kane-team/gpt2-wolof-french-translation1/runs/imvxju8c</a><br/>Synced 5 W&B file(s), 0 media file(s), 4 artifact file(s) and 0 other file(s)"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Find logs at: <code>./wandb/run-20230428_185657-imvxju8c/logs</code>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\u001b[34m\u001b[1mwandb\u001b[0m: Agent Starting Run: bo8m8jrx with config:\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \tbatch_size: 3\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \tepochs: 1\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \tfr_char_p: 0.4701006275945654\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \tfr_word_p: 0.29401382444657836\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \tlearning_rate: 0.00017980663294666269\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \trandom_state: 40\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \tweight_decay: 0.5\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \twf_char_p: 0.02956569463186749\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \twf_word_p: 0.5762135150235679\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m WANDB_NOTEBOOK_NAME should be a path to a notebook file, couldn't find training_gpt2_2.ipynb.\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "Tracking run with wandb version 0.15.0"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Run data is saved locally in <code>/content/wandb/run-20230428_185906-bo8m8jrx</code>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Syncing run <strong><a href='https://wandb.ai/oumar-kane-team/gpt2-wolof-french-translation1/runs/bo8m8jrx' target=\"_blank\">rare-sweep-28</a></strong> to <a href='https://wandb.ai/oumar-kane-team/gpt2-wolof-french-translation1' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>Sweep page: <a href='https://wandb.ai/oumar-kane-team/gpt2-wolof-french-translation1/sweeps/14019vvc' target=\"_blank\">https://wandb.ai/oumar-kane-team/gpt2-wolof-french-translation1/sweeps/14019vvc</a>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              " View project at <a href='https://wandb.ai/oumar-kane-team/gpt2-wolof-french-translation1' target=\"_blank\">https://wandb.ai/oumar-kane-team/gpt2-wolof-french-translation1</a>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              " View sweep at <a href='https://wandb.ai/oumar-kane-team/gpt2-wolof-french-translation1/sweeps/14019vvc' target=\"_blank\">https://wandb.ai/oumar-kane-team/gpt2-wolof-french-translation1/sweeps/14019vvc</a>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              " View run at <a href='https://wandb.ai/oumar-kane-team/gpt2-wolof-french-translation1/runs/bo8m8jrx' target=\"_blank\">https://wandb.ai/oumar-kane-team/gpt2-wolof-french-translation1/runs/bo8m8jrx</a>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/optimization.py:391: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
            "  warnings.warn(\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'learning_rate' was locked by 'sweep' (ignored update).\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'weight_decay' was locked by 'sweep' (ignored update).\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='245' max='245' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [245/245 01:37, Epoch 1/1]\n",
              "    </div>\n",
              "    <table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              " <tr style=\"text-align: left;\">\n",
              "      <th>Epoch</th>\n",
              "      <th>Training Loss</th>\n",
              "      <th>Validation Loss</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <td>1</td>\n",
              "      <td>1.448900</td>\n",
              "      <td>0.823548</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table><p>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Waiting for W&B process to finish... <strong style=\"color:green\">(success).</strong>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "<style>\n",
              "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
              "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
              "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
              "    </style>\n",
              "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>eval/loss</td><td>‚ñÅ</td></tr><tr><td>eval/runtime</td><td>‚ñÅ</td></tr><tr><td>eval/samples_per_second</td><td>‚ñÅ</td></tr><tr><td>eval/steps_per_second</td><td>‚ñÅ</td></tr><tr><td>train/epoch</td><td>‚ñÅ‚ñÅ‚ñÅ</td></tr><tr><td>train/global_step</td><td>‚ñÅ‚ñÅ‚ñÅ</td></tr><tr><td>train/learning_rate</td><td>‚ñÅ</td></tr><tr><td>train/loss</td><td>‚ñÅ</td></tr><tr><td>train/total_flos</td><td>‚ñÅ</td></tr><tr><td>train/train_loss</td><td>‚ñÅ</td></tr><tr><td>train/train_runtime</td><td>‚ñÅ</td></tr><tr><td>train/train_samples_per_second</td><td>‚ñÅ</td></tr><tr><td>train/train_steps_per_second</td><td>‚ñÅ</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>eval/loss</td><td>0.82355</td></tr><tr><td>eval/runtime</td><td>2.729</td></tr><tr><td>eval/samples_per_second</td><td>30.047</td></tr><tr><td>eval/steps_per_second</td><td>6.229</td></tr><tr><td>train/epoch</td><td>1.0</td></tr><tr><td>train/global_step</td><td>245</td></tr><tr><td>train/learning_rate</td><td>1e-05</td></tr><tr><td>train/loss</td><td>1.4489</td></tr><tr><td>train/total_flos</td><td>216590170752000.0</td></tr><tr><td>train/train_loss</td><td>1.44893</td></tr><tr><td>train/train_runtime</td><td>91.1441</td></tr><tr><td>train/train_samples_per_second</td><td>8.042</td></tr><tr><td>train/train_steps_per_second</td><td>2.688</td></tr></table><br/></div></div>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              " View run <strong style=\"color:#cdcd00\">rare-sweep-28</strong> at: <a href='https://wandb.ai/oumar-kane-team/gpt2-wolof-french-translation1/runs/bo8m8jrx' target=\"_blank\">https://wandb.ai/oumar-kane-team/gpt2-wolof-french-translation1/runs/bo8m8jrx</a><br/>Synced 5 W&B file(s), 0 media file(s), 4 artifact file(s) and 0 other file(s)"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Find logs at: <code>./wandb/run-20230428_185906-bo8m8jrx/logs</code>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\u001b[34m\u001b[1mwandb\u001b[0m: Agent Starting Run: 95x0lt2y with config:\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \tbatch_size: 2\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \tepochs: 1\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \tfr_char_p: 0.1543890127501567\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \tfr_word_p: 0.299038502866073\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \tlearning_rate: 1.5099229296726838e-05\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \trandom_state: 50\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \tweight_decay: 0\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \twf_char_p: 0.5008911488024025\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \twf_word_p: 0.2541906580982826\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m WANDB_NOTEBOOK_NAME should be a path to a notebook file, couldn't find training_gpt2_2.ipynb.\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "Tracking run with wandb version 0.15.0"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Run data is saved locally in <code>/content/wandb/run-20230428_190119-95x0lt2y</code>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Syncing run <strong><a href='https://wandb.ai/oumar-kane-team/gpt2-wolof-french-translation1/runs/95x0lt2y' target=\"_blank\">proud-sweep-29</a></strong> to <a href='https://wandb.ai/oumar-kane-team/gpt2-wolof-french-translation1' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>Sweep page: <a href='https://wandb.ai/oumar-kane-team/gpt2-wolof-french-translation1/sweeps/14019vvc' target=\"_blank\">https://wandb.ai/oumar-kane-team/gpt2-wolof-french-translation1/sweeps/14019vvc</a>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              " View project at <a href='https://wandb.ai/oumar-kane-team/gpt2-wolof-french-translation1' target=\"_blank\">https://wandb.ai/oumar-kane-team/gpt2-wolof-french-translation1</a>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              " View sweep at <a href='https://wandb.ai/oumar-kane-team/gpt2-wolof-french-translation1/sweeps/14019vvc' target=\"_blank\">https://wandb.ai/oumar-kane-team/gpt2-wolof-french-translation1/sweeps/14019vvc</a>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              " View run at <a href='https://wandb.ai/oumar-kane-team/gpt2-wolof-french-translation1/runs/95x0lt2y' target=\"_blank\">https://wandb.ai/oumar-kane-team/gpt2-wolof-french-translation1/runs/95x0lt2y</a>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/optimization.py:391: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
            "  warnings.warn(\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'learning_rate' was locked by 'sweep' (ignored update).\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'weight_decay' was locked by 'sweep' (ignored update).\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='367' max='367' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [367/367 01:43, Epoch 1/1]\n",
              "    </div>\n",
              "    <table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              " <tr style=\"text-align: left;\">\n",
              "      <th>Epoch</th>\n",
              "      <th>Training Loss</th>\n",
              "      <th>Validation Loss</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <td>1</td>\n",
              "      <td>1.515700</td>\n",
              "      <td>0.938820</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table><p>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Waiting for W&B process to finish... <strong style=\"color:green\">(success).</strong>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "<style>\n",
              "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
              "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
              "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
              "    </style>\n",
              "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>eval/loss</td><td>‚ñÅ</td></tr><tr><td>eval/runtime</td><td>‚ñÅ</td></tr><tr><td>eval/samples_per_second</td><td>‚ñÅ</td></tr><tr><td>eval/steps_per_second</td><td>‚ñÅ</td></tr><tr><td>train/epoch</td><td>‚ñÅ‚ñÅ‚ñÅ</td></tr><tr><td>train/global_step</td><td>‚ñÅ‚ñÅ‚ñÅ</td></tr><tr><td>train/learning_rate</td><td>‚ñÅ</td></tr><tr><td>train/loss</td><td>‚ñÅ</td></tr><tr><td>train/total_flos</td><td>‚ñÅ</td></tr><tr><td>train/train_loss</td><td>‚ñÅ</td></tr><tr><td>train/train_runtime</td><td>‚ñÅ</td></tr><tr><td>train/train_samples_per_second</td><td>‚ñÅ</td></tr><tr><td>train/train_steps_per_second</td><td>‚ñÅ</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>eval/loss</td><td>0.93882</td></tr><tr><td>eval/runtime</td><td>2.7383</td></tr><tr><td>eval/samples_per_second</td><td>29.945</td></tr><tr><td>eval/steps_per_second</td><td>6.208</td></tr><tr><td>train/epoch</td><td>1.0</td></tr><tr><td>train/global_step</td><td>367</td></tr><tr><td>train/learning_rate</td><td>0.0</td></tr><tr><td>train/loss</td><td>1.5157</td></tr><tr><td>train/total_flos</td><td>216590170752000.0</td></tr><tr><td>train/train_loss</td><td>1.51569</td></tr><tr><td>train/train_runtime</td><td>97.7831</td></tr><tr><td>train/train_samples_per_second</td><td>7.496</td></tr><tr><td>train/train_steps_per_second</td><td>3.753</td></tr></table><br/></div></div>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              " View run <strong style=\"color:#cdcd00\">proud-sweep-29</strong> at: <a href='https://wandb.ai/oumar-kane-team/gpt2-wolof-french-translation1/runs/95x0lt2y' target=\"_blank\">https://wandb.ai/oumar-kane-team/gpt2-wolof-french-translation1/runs/95x0lt2y</a><br/>Synced 5 W&B file(s), 0 media file(s), 4 artifact file(s) and 0 other file(s)"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Find logs at: <code>./wandb/run-20230428_190119-95x0lt2y/logs</code>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\u001b[34m\u001b[1mwandb\u001b[0m: Agent Starting Run: 6fqqnqtl with config:\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \tbatch_size: 5\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \tepochs: 1\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \tfr_char_p: 0.4706006414197762\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \tfr_word_p: 0.3779312323679532\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \tlearning_rate: 0.00018704899051687955\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \trandom_state: 50\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \tweight_decay: 0.5\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \twf_char_p: 0.6900740729270312\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \twf_word_p: 0.3272944144812937\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m WANDB_NOTEBOOK_NAME should be a path to a notebook file, couldn't find training_gpt2_2.ipynb.\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "Tracking run with wandb version 0.15.0"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Run data is saved locally in <code>/content/wandb/run-20230428_190340-6fqqnqtl</code>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Syncing run <strong><a href='https://wandb.ai/oumar-kane-team/gpt2-wolof-french-translation1/runs/6fqqnqtl' target=\"_blank\">crisp-sweep-30</a></strong> to <a href='https://wandb.ai/oumar-kane-team/gpt2-wolof-french-translation1' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>Sweep page: <a href='https://wandb.ai/oumar-kane-team/gpt2-wolof-french-translation1/sweeps/14019vvc' target=\"_blank\">https://wandb.ai/oumar-kane-team/gpt2-wolof-french-translation1/sweeps/14019vvc</a>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              " View project at <a href='https://wandb.ai/oumar-kane-team/gpt2-wolof-french-translation1' target=\"_blank\">https://wandb.ai/oumar-kane-team/gpt2-wolof-french-translation1</a>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              " View sweep at <a href='https://wandb.ai/oumar-kane-team/gpt2-wolof-french-translation1/sweeps/14019vvc' target=\"_blank\">https://wandb.ai/oumar-kane-team/gpt2-wolof-french-translation1/sweeps/14019vvc</a>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              " View run at <a href='https://wandb.ai/oumar-kane-team/gpt2-wolof-french-translation1/runs/6fqqnqtl' target=\"_blank\">https://wandb.ai/oumar-kane-team/gpt2-wolof-french-translation1/runs/6fqqnqtl</a>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/optimization.py:391: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
            "  warnings.warn(\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'learning_rate' was locked by 'sweep' (ignored update).\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Config item 'weight_decay' was locked by 'sweep' (ignored update).\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='147' max='147' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [147/147 01:30, Epoch 1/1]\n",
              "    </div>\n",
              "    <table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              " <tr style=\"text-align: left;\">\n",
              "      <th>Epoch</th>\n",
              "      <th>Training Loss</th>\n",
              "      <th>Validation Loss</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <td>1</td>\n",
              "      <td>1.678200</td>\n",
              "      <td>0.869228</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table><p>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Waiting for W&B process to finish... <strong style=\"color:green\">(success).</strong>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "<style>\n",
              "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
              "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
              "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
              "    </style>\n",
              "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>eval/loss</td><td>‚ñÅ</td></tr><tr><td>eval/runtime</td><td>‚ñÅ</td></tr><tr><td>eval/samples_per_second</td><td>‚ñÅ</td></tr><tr><td>eval/steps_per_second</td><td>‚ñÅ</td></tr><tr><td>train/epoch</td><td>‚ñÅ‚ñÅ‚ñÅ</td></tr><tr><td>train/global_step</td><td>‚ñÅ‚ñÅ‚ñÅ</td></tr><tr><td>train/learning_rate</td><td>‚ñÅ</td></tr><tr><td>train/loss</td><td>‚ñÅ</td></tr><tr><td>train/total_flos</td><td>‚ñÅ</td></tr><tr><td>train/train_loss</td><td>‚ñÅ</td></tr><tr><td>train/train_runtime</td><td>‚ñÅ</td></tr><tr><td>train/train_samples_per_second</td><td>‚ñÅ</td></tr><tr><td>train/train_steps_per_second</td><td>‚ñÅ</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>eval/loss</td><td>0.86923</td></tr><tr><td>eval/runtime</td><td>2.7436</td></tr><tr><td>eval/samples_per_second</td><td>29.887</td></tr><tr><td>eval/steps_per_second</td><td>6.196</td></tr><tr><td>train/epoch</td><td>1.0</td></tr><tr><td>train/global_step</td><td>147</td></tr><tr><td>train/learning_rate</td><td>1e-05</td></tr><tr><td>train/loss</td><td>1.6782</td></tr><tr><td>train/total_flos</td><td>216590170752000.0</td></tr><tr><td>train/train_loss</td><td>1.67821</td></tr><tr><td>train/train_runtime</td><td>83.6572</td></tr><tr><td>train/train_samples_per_second</td><td>8.762</td></tr><tr><td>train/train_steps_per_second</td><td>1.757</td></tr></table><br/></div></div>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              " View run <strong style=\"color:#cdcd00\">crisp-sweep-30</strong> at: <a href='https://wandb.ai/oumar-kane-team/gpt2-wolof-french-translation1/runs/6fqqnqtl' target=\"_blank\">https://wandb.ai/oumar-kane-team/gpt2-wolof-french-translation1/runs/6fqqnqtl</a><br/>Synced 5 W&B file(s), 0 media file(s), 4 artifact file(s) and 0 other file(s)"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Find logs at: <code>./wandb/run-20230428_190340-6fqqnqtl/logs</code>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "# %%wandb\n",
        "\n",
        "def train(config = None):\n",
        "\n",
        "  \n",
        "\n",
        "  with wandb.init(config = config):\n",
        "\n",
        "    # seed\n",
        "    torch.manual_seed(50)\n",
        "\n",
        "    # set sweep configuration\n",
        "    config = wandb.config\n",
        "\n",
        "    # split the data\n",
        "    split_data(config.random_state)\n",
        "\n",
        "    # let us recuperate the datasets\n",
        "    train_dataset, test_dataset = recuperate_datasets(config.fr_char_p, config.wf_char_p, \n",
        "                                   config.fr_word_p, config.wf_word_p)\n",
        "\n",
        "    # get train and test datasets according to the config\n",
        "\n",
        "    # train_dataset = datasets[config.dataset_aug]['train_dataset']\n",
        "\n",
        "    # test_dataset = datasets[config.dataset_aug]['test_dataset']\n",
        "\n",
        "    # set training arguments\n",
        "    training_args = TrainingArguments(f\"{path}training2/results1\",\n",
        "                                      report_to = f\"wandb\",\n",
        "                                      num_train_epochs=config.epochs,\n",
        "                                      # logging_steps=100,\n",
        "                                      load_best_model_at_end=True,\n",
        "                                      save_strategy=\"epoch\",\n",
        "                                      evaluation_strategy=\"epoch\",\n",
        "                                      logging_strategy = 'epoch',\n",
        "                                      per_device_train_batch_size=config.batch_size, \n",
        "                                      per_device_eval_batch_size=5,\n",
        "                                      learning_rate=config.learning_rate,\n",
        "                                      weight_decay=config.weight_decay,\n",
        "                                      logging_dir=f'{path}gpt2_training_logs2',\n",
        "                                      remove_unused_columns = False,\n",
        "                                      fp16 = True,\n",
        "                                      )   \n",
        "\n",
        "    # define training loop\n",
        "    trainer = Trainer(model_init=partial(gpt2_model_init, tokenizer = train_dataset.tokenizer),\n",
        "                      args=training_args,\n",
        "                      train_dataset=train_dataset, \n",
        "                      eval_dataset=test_dataset,\n",
        "                      data_collator=data_collator,\n",
        "                      # compute_metrics=translation_eval.compute_metrics\n",
        "                      )\n",
        "\n",
        "    # start training loop\n",
        "    trainer.train()\n",
        "\n",
        "agent = wandb.agent(sweep_id, train, count = 30)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "u7NPlNlPgRz9"
      },
      "source": [
        "------------------"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DO_49vgmTu8B"
      },
      "source": [
        "## Wolof to french"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "The only thing that we will change is the order of sentences. The wolof sentence is the first one to write."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Configure dataset üî†"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Let us recuperate the datasets."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {},
      "outputs": [],
      "source": [
        "def recuperate_datasets(fr_char_p: float, wf_char_p: float, fr_word_p: float, wf_word_p):\n",
        "\n",
        "  # with augmentation\n",
        "  wf_augmentation = TransformerSequences(nac.KeyboardAug(aug_char_p=fr_char_p, aug_word_p=fr_word_p),\n",
        "                                        remove_mark_space, delete_guillemet_space)\n",
        "\n",
        "  train_dataset_aug = SentenceDataset(f\"{path}new_data/train_set.csv\", \n",
        "                                  tokenizer_path = f\"{path}wolof-translate/wolof_translate/tokenizers/tokenizer_v1.json\",\n",
        "                                  corpus_1=\"wolof_corpus\",\n",
        "                                  corpus_2=\"french_corpus\",\n",
        "                                  cp1_transformer=wf_augmentation, truncation=True,\n",
        "                                  max_len=579)\n",
        "\n",
        "  test_dataset = SentenceDataset(f\"{path}new_data/test_set.csv\",\n",
        "                                tokenizer_path = f\"{path}wolof-translate/wolof_translate/tokenizers/tokenizer_v1.json\",\n",
        "                                corpus_1=\"wolof_corpus\",\n",
        "                                corpus_2=\"french_corpus\",\n",
        "                                truncation=True, max_len=579)\n",
        "  \n",
        "  return train_dataset_aug, test_dataset"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Configure hyperparameter search ‚öôÔ∏è"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "We have to configure the search space and the search method (\"random\" in our case). ."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Failed to detect the name of this notebook, you can set it manually with the WANDB_NOTEBOOK_NAME environment variable to enable code saving.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33moumar-kane\u001b[0m (\u001b[33moumar-kane-team\u001b[0m). Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m If you're specifying your api key in code, ensure this code is not shared publicly.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Consider setting the WANDB_API_KEY environment variable, or running `wandb login` from the command line.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Appending key for api.wandb.ai to your netrc file: C:\\Users\\Oumar Kane/.netrc\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Create sweep with ID: 12vgs0ak\n",
            "Sweep URL: https://wandb.ai/oumar-kane-team/gpt2-wolof-french-translation1/sweeps/12vgs0ak\n"
          ]
        }
      ],
      "source": [
        "import wandb\n",
        "wandb.login(key=\"237a8450cd2568ea1c8e1f8e0400708e79b6b4ee\")\n",
        "\n",
        "# hyperparameters\n",
        "sweep_config = {\n",
        "    'method': 'random',\n",
        "    'metric':{\n",
        "          'goal': 'minimize',\n",
        "          'name': 'eval_loss'\n",
        "      },\n",
        "    'parameters':\n",
        "    {\n",
        "      'epochs': {\n",
        "          'value': 1\n",
        "      },\n",
        "      'batch_size': {\n",
        "          'values': [2, 3, 5]\n",
        "      },\n",
        "      'learning_rate': {\n",
        "          'distribution': 'log_uniform_values',\n",
        "          'min': 1e-5,\n",
        "          'max': 1e-3\n",
        "      },\n",
        "      'weight_decay': {\n",
        "          'values': [0.0, 0.1, 0.2, 0.3, 0.4, 0.5]\n",
        "      },\n",
        "     'fr_char_p': {\n",
        "         'min': 0.0,\n",
        "         'max': 0.7\n",
        "     },\n",
        "     'fr_word_p': {\n",
        "          'min': 0.0,\n",
        "          'max': 0.7\n",
        "     },\n",
        "     'wf_char_p': {\n",
        "          'min': 0.0,\n",
        "          'max': 0.7\n",
        "     },\n",
        "     'wf_word_p': {\n",
        "          'min': 0.0,\n",
        "          'max': 0.7\n",
        "     },\n",
        "     'random_state': {\n",
        "         'values': [0, 10, 20, 30, 40, 50, 60, 70, 80, 100]\n",
        "     }\n",
        "    }\n",
        "}\n",
        "\n",
        "# Initialize the hyperparameter search\n",
        "sweep_id = wandb.sweep(sweep_config, project = \"gpt2-wolof-French-translation1\")\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Configure the model and the evaluation function ‚öôÔ∏è"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Let us recuperate the model and resize the token embeddings."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def gpt2_model_init(tokenizer):\n",
        "  # set the mode name\n",
        "  model_name = \"gpt2\"\n",
        "\n",
        "  # recuperate the tokenizer from the dataset\n",
        "  tokenizer = tokenizer\n",
        "\n",
        "  # configure the model\n",
        "  model = GPT2LMHeadModel.from_pretrained(model_name).cuda()\n",
        "\n",
        "  # resize the token embeddings\n",
        "  model.resize_token_embeddings(len(tokenizer))\n",
        "\n",
        "  return model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Let us evaluate the predictions with the `bleu` metric."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# %%writefile wolof-translate/wolof_translate/utils/evaluation.py\n",
        "from tokenizers import Tokenizer\n",
        "from typing import *\n",
        "import numpy as np\n",
        "import evaluate\n",
        "\n",
        "class TranslationEvaluation:\n",
        "    \n",
        "    def __init__(self, \n",
        "                 tokenizer: Tokenizer,\n",
        "                 decoder: Union[Callable, None] = None,\n",
        "                 metric = evaluate.load('sacrebleu'),\n",
        "                 ):\n",
        "        \n",
        "        self.tokenizer = tokenizer\n",
        "        \n",
        "        self.decoder = decoder\n",
        "        \n",
        "        self.metric = metric\n",
        "    \n",
        "    def postprocess_text(self, preds, labels):\n",
        "        \n",
        "        preds = [pred.strip() for pred in preds]\n",
        "        \n",
        "        labels = [[label.strip()] for label in labels]\n",
        "        \n",
        "        return preds, labels\n",
        "\n",
        "    def compute_metrics(self, eval_preds):\n",
        "        \n",
        "        preds, labels = eval_preds.preds.detach().cpu(), labels.detach().cpu()\n",
        "        \n",
        "        if isinstance(preds, tuple):\n",
        "            \n",
        "            preds = preds[0]\n",
        "        \n",
        "        if self.decoder is None:\n",
        "            \n",
        "            decoded_preds = self.tokenizer.batch_decode(preds, skip_special_tokens=True)\n",
        "            \n",
        "            decoded_labels = self.tokenizer.batch_decode(labels, skip_special_tokens=True)\n",
        "            \n",
        "            decoded_preds, decoded_labels = self.postprocess_text(decoded_preds, decoded_labels)\n",
        "            \n",
        "            result = self.metric.compute(predictions=decoded_preds, references=decoded_labels)\n",
        "            \n",
        "            result = {\"bleu\": result[\"score\"]}\n",
        "            \n",
        "            prediction_lens = [np.count_nonzero(pred != self.tokenizer.pad_token_id) for pred in preds]\n",
        "            \n",
        "            result[\"gen_len\"] = np.mean(prediction_lens)\n",
        "        \n",
        "        else:\n",
        "            \n",
        "            predictions = list(self.decoder(preds))\n",
        "            \n",
        "            labels = list(self.decoder(labels))\n",
        "      \n",
        "            decoded_preds, decoded_labels = self.postprocess_text(predictions, labels)\n",
        "            \n",
        "            result = self.metric.compute(predictions=predictions, references=labels)\n",
        "            \n",
        "            result = {\"bleu\": result[\"score\"]}\n",
        "        \n",
        "        result = {k:round(v, 4) for k, v in result.items()}\n",
        "\n",
        "        wandb.log(\"bleu\", result[\"bleu\"])\n",
        "            \n",
        "        return result"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# %run wolof-translate/wolof_translate/utils/evaluation.py"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Let us initialize the evaluation object."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# translation_eval = TranslationEvaluation(test_dataset.tokenizer)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Searching for the best parameters üïñ"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Let us define the data collator."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def data_collator(batch):\n",
        "    \"\"\"Generate a batch of data to provide to trainer\n",
        "\n",
        "    Args:\n",
        "        batch (_type_): The batch\n",
        "\n",
        "    Returns:\n",
        "        dict: A dictionary containing the ids, the attention mask and the labels\n",
        "    \"\"\"\n",
        "    input_ids = torch.stack([b[0] for b in batch])\n",
        "    \n",
        "    attention_mask = torch.stack([b[1] for b in batch])\n",
        "    \n",
        "    labels = torch.stack([b[0] for b in batch])\n",
        "    \n",
        "    return {'input_ids': input_ids, 'attention_mask': attention_mask,\n",
        "            'labels': labels}"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Let us initialize the training arguments and make random search."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# %%wandb\n",
        "\n",
        "def train(config = None):\n",
        "\n",
        "  with wandb.init(config = config):\n",
        "\n",
        "    # seed\n",
        "    torch.manual_seed(50)\n",
        "\n",
        "    # set sweep configuration\n",
        "    config = wandb.config\n",
        "\n",
        "    # split the data\n",
        "    split_data(config.random_state)\n",
        "\n",
        "    # let us recuperate the datasets\n",
        "    train_dataset, test_dataset = recuperate_datasets(config.fr_char_p, config.wf_char_p, \n",
        "                                   config.fr_word_p, config.wf_word_p)\n",
        "\n",
        "    # get train and test datasets according to the config\n",
        "\n",
        "    # train_dataset = datasets[config.dataset_aug]['train_dataset']\n",
        "\n",
        "    # test_dataset = datasets[config.dataset_aug]['test_dataset']\n",
        "\n",
        "    # set training arguments\n",
        "    training_args = TrainingArguments(f\"{path}training2/results1\",\n",
        "                                      report_to = f\"wandb\",\n",
        "                                      num_train_epochs=config.epochs,\n",
        "                                      # logging_steps=100,\n",
        "                                      load_best_model_at_end=True,\n",
        "                                      save_strategy=\"epoch\",\n",
        "                                      evaluation_strategy=\"epoch\",\n",
        "                                      logging_strategy = 'epoch',\n",
        "                                      per_device_train_batch_size=config.batch_size, \n",
        "                                      per_device_eval_batch_size=5,\n",
        "                                      learning_rate=config.learning_rate,\n",
        "                                      weight_decay=config.weight_decay,\n",
        "                                      logging_dir=f'{path}gpt2_training_logs2',\n",
        "                                      remove_unused_columns = False,\n",
        "                                      fp16 = True,\n",
        "                                      )   \n",
        "\n",
        "    # define training loop\n",
        "    trainer = Trainer(model_init=partial(gpt2_model_init, tokenizer = train_dataset.tokenizer),\n",
        "                      args=training_args,\n",
        "                      train_dataset=train_dataset, \n",
        "                      eval_dataset=test_dataset,\n",
        "                      data_collator=data_collator,\n",
        "                      # compute_metrics=translation_eval.compute_metrics\n",
        "                      )\n",
        "\n",
        "    # start training loop\n",
        "    trainer.train()\n",
        "\n",
        "agent = wandb.agent(sweep_id, train, count = 30)\n"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "-----------"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Colab download and remove step"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import shutil\n",
        "\n",
        "# shutil.rmtree('/content/drive/MyDrive/Memoire/subject2/training2/results1/checkpoint-147')\n",
        "# shutil.rmtree('wandb')\n",
        "# shutil.make_archive('wandb', 'zip', 'wanbd')"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "provenance": []
    },
    "gpuClass": "standard",
    "kernelspec": {
      "display_name": "pytorch1-HleOW5am-py3.10",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.8"
    },
    "orig_nbformat": 4,
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "236123f576194b84b19fe8ad0cbb9529": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "249040adcb494e02960fb0df640d935a": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "3d16fe0f000741bd98cbc13d48359816": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "5b9e6e1bb7194446838b1e5d8efc2874": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "FloatProgressModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_d85038d56365464095f65a20da8b4add",
            "max": 1,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_f2073de3f26e469593f0732e681cf8e2",
            "value": 1
          }
        },
        "6342d7b19cf24413a7e6abc021730a7a": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "VBoxModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "VBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "VBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_d540120cbcac4649a26f8197b73a673f",
              "IPY_MODEL_5b9e6e1bb7194446838b1e5d8efc2874"
            ],
            "layout": "IPY_MODEL_249040adcb494e02960fb0df640d935a"
          }
        },
        "d540120cbcac4649a26f8197b73a673f": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "LabelModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "LabelModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "LabelView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_236123f576194b84b19fe8ad0cbb9529",
            "placeholder": "‚Äã",
            "style": "IPY_MODEL_3d16fe0f000741bd98cbc13d48359816",
            "value": "Waiting for wandb.init()...\r"
          }
        },
        "d85038d56365464095f65a20da8b4add": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "f2073de3f26e469593f0732e681cf8e2": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "ProgressStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        }
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
