{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Generate sentences to augment the dataset\n",
    "-------------------------------------------------"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this notebook we will try to create a generative-adversarial network which will generate for us new sentences in order to augment the corpora size. We will use the `pytorch-lightning` module to improve the training fastness. \n",
    "\n",
    "- The generative model will understand the following characteristics:\n",
    "    - we will provide the `size of the sequences` to a first model to generate a output of the same size that the given sequences\n",
    "    - the output will be rounded in order to be transmit to the discriminator\n",
    "    - it will use `leaky-relu` as activation function and batch normalization to avoid over-fitting\n",
    "    - some rules will used on the decoded output in order to obtain the textual sentences\n",
    "\n",
    "- The discriminative model will be used to verify if the output is close to the true sentences:\n",
    "    - we will use for that a pre-trained BERT Model to discriminate of the output\n",
    "    - we will tokenize the GAN inputs with a WordPiece tokenizer without normalizer because we want to generate texts\n",
    "\n",
    "\n",
    "    "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Steps"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following steps will be required:\n",
    "\n",
    "- Create a custom dataset to recuperate the sentences\n",
    "- Create the generator\n",
    "- Create the discriminator\n",
    "- Create the GAN\n",
    "- Train the model and evaluate it"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create a custom dataset"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let us use the already trained tokenizer to recuperate the encoded sequences. Note that this dataset is different from that we want to use to train the translation model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %%writefile wolof-translate/wolof_translate/data/gan_dataset.py\n",
    "import torch\n",
    "import pandas as pd\n",
    "from torch import nn\n",
    "from tokenizers import Tokenizer\n",
    "from torch.utils.data import Dataset\n",
    "\n",
    "class SentenceDatasetGAN(Dataset):\n",
    "    \n",
    "    def __init__(self, file_path: str, corpus_1: str = \"french_corpus\", corpus_2: str = \"wolof_corpus\",\n",
    "                 tokenizer_path: str = \"wolof-translate/wolof_translate/tokenizers/adverse_tokenizer.json\",\n",
    "                 cls_token: str = \"[CLS]\", sep_token: str = \"[SEP]\", sep: str = \",\", **kwargs):\n",
    "        \n",
    "        # let us recuperate the data frame\n",
    "        self.__sentences = pd.read_csv(file_path, sep=sep, **kwargs)\n",
    "        \n",
    "        # let us recuperate the tokenizer\n",
    "        self.__tokenizer = Tokenizer.from_file(tokenizer_path)\n",
    "        \n",
    "        # recuperate the first corpus' sentences\n",
    "        self.__sentences_1 = self.__sentences[corpus_1].to_list()\n",
    "        \n",
    "        # recuperate the second corpus' sentences\n",
    "        self.__sentences_2 = self.__sentences[corpus_2].to_list()\n",
    "        \n",
    "        # recuperate the special tokens\n",
    "        self.cls_token = cls_token\n",
    "        \n",
    "        self.sep_token = sep_token\n",
    "        \n",
    "        # recuperate the length\n",
    "        self.__length = len(self.__sentences_1)\n",
    "        \n",
    "        # let us recuperate the max len\n",
    "        self.max_len = 0\n",
    "        \n",
    "        for i in range(self.__length):\n",
    "            \n",
    "            sentence = f\"{self.cls_token}{self.__sentences_1[i]}{self.sep_token}{self.__sentences_2[i]}{self.sep_token}\"\n",
    "            \n",
    "            encoding = self.__tokenizer.encode(sentence)\n",
    "            \n",
    "            if len(encoding.ids) > self.max_len:\n",
    "                \n",
    "                self.max_len = len(encoding.ids)    \n",
    "        \n",
    "    def __getitem__(self, index):\n",
    "        \n",
    "        sentence_1 = self.__sentences_1[index]\n",
    "        \n",
    "        sentence_2 = self.__sentences_2[index]\n",
    "        \n",
    "        # let us create the sentence with special tokens\n",
    "        sentence = f\"{self.cls_token}{sentence_1}{self.sep_token}{sentence_2}{self.sep_token}\"\n",
    "        \n",
    "        # let us encode the sentence\n",
    "        encoding = self.__tokenizer.encode(sentence)\n",
    "        \n",
    "        # it will return the padded ids and attention mask\n",
    "        padding = self.max_len - len(encoding.ids)\n",
    "        \n",
    "        ids = torch.tensor(encoding.ids + [0] * padding)\n",
    "        \n",
    "        return ids, (ids > 0).float()\n",
    "        \n",
    "    def __len__(self):\n",
    "        \n",
    "        return self.__length"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The data loader will generate the padded sequences of ids and the attention masks. Let us test it bellow."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = SentenceDatasetGAN(\"data/extractions/new_data/sent_extraction.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ids:\n",
      "tensor([[   2, 1059,  688,  ...,    0,    0,    0],\n",
      "        [   2, 1386,  704,  ...,    0,    0,    0],\n",
      "        [   2,   51,  382,  ...,    0,    0,    0],\n",
      "        ...,\n",
      "        [   2,  909, 1606,  ...,    0,    0,    0],\n",
      "        [   2,   36,    7,  ...,    0,    0,    0],\n",
      "        [   2, 4825,   64,  ...,    0,    0,    0]])\n",
      "\n",
      "Mask:\n",
      "tensor([[1., 1., 1.,  ..., 0., 0., 0.],\n",
      "        [1., 1., 1.,  ..., 0., 0., 0.],\n",
      "        [1., 1., 1.,  ..., 0., 0., 0.],\n",
      "        ...,\n",
      "        [1., 1., 1.,  ..., 0., 0., 0.],\n",
      "        [1., 1., 1.,  ..., 0., 0., 0.],\n",
      "        [1., 1., 1.,  ..., 0., 0., 0.]])\n"
     ]
    }
   ],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "\n",
    "# let us generate 10 sentences\n",
    "ids, mask = next(iter(DataLoader(dataset, batch_size=10, shuffle=True)))\n",
    "\n",
    "print(\"Ids:\")\n",
    "print(ids)\n",
    "\n",
    "print(\"\\nMask:\")\n",
    "print(mask)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generator"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The generator is a Multi Layers Perceptron with a number of features and layers specified as arguments. It will also require. A drop out rate can be specified to verify if the generator is over-fitting."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %%writefile wolof-translate/wolof_translate/models/generative_model.py\n",
    "from torch.nn import functional as F\n",
    "from typing import *\n",
    "from torch import nn\n",
    "\n",
    "class GenerativeSequence(nn.Module):\n",
    "    \n",
    "    def __init__(self, \n",
    "                 input_dim,\n",
    "                 num_features,\n",
    "                 negative_slope: float = 0.01,\n",
    "                 drop_out: float = 0.0,\n",
    "                 eps: float = 0.00001,\n",
    "                 momentum: float = 0.1):\n",
    "        \n",
    "        super(GenerativeSequence, self).__init__()\n",
    "        \n",
    "        self.batch_norm = nn.BatchNorm1d(input_dim, eps, momentum)\n",
    "        \n",
    "        self.linear = nn.Linear(input_dim, num_features)\n",
    "        \n",
    "        self.drop_out = nn.Dropout1d(drop_out)\n",
    "        \n",
    "        self.activation = nn.LeakyReLU(negative_slope)\n",
    "        \n",
    "        \n",
    "    def forward(self, input_):\n",
    "        \n",
    "        out = self.batch_norm(input_)\n",
    "        \n",
    "        out = self.activation(self.drop_out(self.linear(out)))\n",
    "        \n",
    "        return out\n",
    "\n",
    "class SentenceGenerativeNet(nn.Module):\n",
    "    \n",
    "    def __init__(self, \n",
    "                 latent_dim: int,\n",
    "                 output_size: int,\n",
    "                 num_features: Union[int, List] = 300,\n",
    "                 num_layers: int = 5,\n",
    "                 negative_slope: float = 0.01,\n",
    "                 drop_out: float = 0.0,\n",
    "                 eps: float = 0.00001,\n",
    "                 momentum: float = 0.1):\n",
    "        \n",
    "        super(SentenceGenerativeNet, self).__init__()\n",
    "        \n",
    "        self.latent_dim = latent_dim\n",
    "        \n",
    "        self.num_features = [num_features] * num_layers if type(num_features) is int else num_features\n",
    "        \n",
    "        assert len(self.num_features) == num_layers\n",
    "        \n",
    "        self.num_layers = num_layers\n",
    "        \n",
    "        self.output_size = output_size\n",
    "        \n",
    "        self.sequences = nn.ModuleList()\n",
    "        \n",
    "        self.sequences.append(GenerativeSequence(latent_dim, self.num_features[0], negative_slope, drop_out, eps, momentum))\n",
    "        \n",
    "        for l in range(1, num_layers):\n",
    "            \n",
    "            self.sequences.append(GenerativeSequence(self.num_features[l-1], self.num_features[l], negative_slope, drop_out, eps, momentum))\n",
    "        \n",
    "        self.output_layer = nn.Linear(self.num_features[-1], output_size)\n",
    "        \n",
    "    def forward(self, input_):\n",
    "        \n",
    "        out = input_\n",
    "        \n",
    "        for sequence in self.sequences:\n",
    "            \n",
    "            out = sequence(out)\n",
    "            \n",
    "        return self.output_layer(out)\n",
    "        "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let us test our generative model with dummy input."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "generative_model = SentenceGenerativeNet(latent_dim = 300, output_size=dataset.max_len)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([10, 379])"
      ]
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "generative_model(torch.randn((10, 300))).round().size()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pytorch1-HleOW5am-py3.10",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
