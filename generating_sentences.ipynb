{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Generate sentences to augment the dataset\n",
    "-------------------------------------------------"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this notebook we will try to create a generative-adversarial network which will generate for us new sentences in order to augment the corpora size. We will use the `pytorch-lightning` module to improve the training fastness. \n",
    "\n",
    "- The generative model will understand the following characteristics:\n",
    "    - we will provide the `size of the sequences` to a first model to generate a output of the same size that the given sequences\n",
    "    - the output will be rounded in order to be transmit to the discriminator\n",
    "    - we will use a transformer encoder to the generate sentence ids in place of a simple `RNN` module\n",
    "    - some rules will used on the decoded output in order to obtain the textual sentences\n",
    "\n",
    "- The discriminative model will be used to verify if the output is close to the true sentences:\n",
    "    - ~~we will use for that a pre-trained BERT Model to discriminate of the output~~\n",
    "    - A Multi-Layers Perceptron will be sufficient to discriminate the output\n",
    "    - we will tokenize the GAN inputs with a WordPiece tokenizer without normalizer because we want to generate texts\n",
    "\n",
    "\n",
    "    "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Steps"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following steps will be required:\n",
    "\n",
    "- Create a custom dataset to recuperate the sentences\n",
    "- Create the generator\n",
    "- Create the discriminator\n",
    "- ~~Create the GAN~~\n",
    "- Create Trainer \n",
    "- Search for the best parameters\n",
    "- Train the model and evaluate it"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create a custom dataset"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let us use the already trained tokenizer to recuperate the encoded sequences. Note that this dataset is different from that we want to use to train the translation model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Oumar Kane\\AppData\\Local\\pypoetry\\Cache\\virtualenvs\\pytorch1-HleOW5am-py3.10\\lib\\site-packages\\tqdm\\auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "# %%writefile wolof-translate/wolof_translate/data/gan_dataset.py\n",
    "import torch\n",
    "import pandas as pd\n",
    "from torch import nn\n",
    "from tokenizers import Tokenizer\n",
    "from torch.utils.data import Dataset\n",
    "\n",
    "class SentenceDatasetGAN(Dataset):\n",
    "    \n",
    "    def __init__(self, file_path: str, corpus_1: str = \"french_corpus\", corpus_2: str = \"wolof_corpus\",\n",
    "                 tokenizer_path: str = \"wolof-translate/wolof_translate/tokenizers/adverse_tokenizer.json\",\n",
    "                 cls_token: str = \"[CLS]\", sep_token: str = \"[SEP]\", sep: str = \",\", **kwargs):\n",
    "        \n",
    "        # let us recuperate the data frame\n",
    "        self.__sentences = pd.read_csv(file_path, sep=sep, **kwargs)\n",
    "        \n",
    "        # let us recuperate the tokenizer\n",
    "        self.tokenizer = Tokenizer.from_file(tokenizer_path)\n",
    "        \n",
    "        # recuperate the first corpus' sentences\n",
    "        self.__sentences_1 = self.__sentences[corpus_1].to_list()\n",
    "        \n",
    "        # recuperate the second corpus' sentences\n",
    "        self.__sentences_2 = self.__sentences[corpus_2].to_list()\n",
    "        \n",
    "        # recuperate the special tokens\n",
    "        self.cls_token = cls_token\n",
    "        \n",
    "        self.sep_token = sep_token\n",
    "        \n",
    "        # recuperate the length\n",
    "        self.__length = len(self.__sentences_1)\n",
    "        \n",
    "        # recuperate the max id\n",
    "        self.max_id = self.tokenizer.get_vocab_size() - 1\n",
    "        \n",
    "        # let us recuperate the max len\n",
    "        self.max_len = 0\n",
    "        \n",
    "        for i in range(self.__length):\n",
    "            \n",
    "            sentence = f\"{self.cls_token}{self.__sentences_1[i]}{self.sep_token}{self.__sentences_2[i]}{self.sep_token}\"\n",
    "            \n",
    "            encoding = self.tokenizer.encode(sentence)\n",
    "            \n",
    "            if len(encoding.ids) > self.max_len:\n",
    "                \n",
    "                self.max_len = len(encoding.ids)    \n",
    "        \n",
    "    def __getitem__(self, index):\n",
    "        \n",
    "        sentence_1 = self.__sentences_1[index]\n",
    "        \n",
    "        sentence_2 = self.__sentences_2[index]\n",
    "        \n",
    "        # let us create the sentence with special tokens\n",
    "        sentence = f\"{self.cls_token}{sentence_1}{self.sep_token}{sentence_2}{self.sep_token}\"\n",
    "        \n",
    "        # let us encode the sentence\n",
    "        encoding = self.tokenizer.encode(sentence)\n",
    "        \n",
    "        # it will return the padded ids and attention mask\n",
    "        padding = self.max_len - len(encoding.ids)\n",
    "        \n",
    "        ids = torch.tensor(encoding.ids + [0] * padding)\n",
    "        \n",
    "        return ids.float(), (ids > 0).float()\n",
    "        \n",
    "    def __len__(self):\n",
    "        \n",
    "        return self.__length"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The data loader will generate the padded sequences of ids and the attention masks. Let us test it bellow."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = SentenceDatasetGAN(\"data/extractions/new_data/sent_extraction.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ids:\n",
      "tensor([[2.0000e+00, 8.0400e+02, 3.2400e+02,  ..., 0.0000e+00, 0.0000e+00,\n",
      "         0.0000e+00],\n",
      "        [2.0000e+00, 5.7200e+02, 1.5280e+03,  ..., 0.0000e+00, 0.0000e+00,\n",
      "         0.0000e+00],\n",
      "        [2.0000e+00, 2.0060e+03, 1.1000e+01,  ..., 0.0000e+00, 0.0000e+00,\n",
      "         0.0000e+00],\n",
      "        ...,\n",
      "        [2.0000e+00, 3.7700e+02, 2.4300e+02,  ..., 0.0000e+00, 0.0000e+00,\n",
      "         0.0000e+00],\n",
      "        [2.0000e+00, 3.0260e+03, 1.1000e+01,  ..., 0.0000e+00, 0.0000e+00,\n",
      "         0.0000e+00],\n",
      "        [2.0000e+00, 1.3860e+03, 1.8560e+03,  ..., 0.0000e+00, 0.0000e+00,\n",
      "         0.0000e+00]])\n",
      "\n",
      "Mask:\n",
      "tensor([[1., 1., 1.,  ..., 0., 0., 0.],\n",
      "        [1., 1., 1.,  ..., 0., 0., 0.],\n",
      "        [1., 1., 1.,  ..., 0., 0., 0.],\n",
      "        ...,\n",
      "        [1., 1., 1.,  ..., 0., 0., 0.],\n",
      "        [1., 1., 1.,  ..., 0., 0., 0.],\n",
      "        [1., 1., 1.,  ..., 0., 0., 0.]])\n"
     ]
    }
   ],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "\n",
    "# let us generate 10 sentences\n",
    "ids, mask = next(iter(DataLoader(dataset, batch_size=10, shuffle=True)))\n",
    "\n",
    "print(\"Ids:\")\n",
    "print(ids)\n",
    "\n",
    "print(\"\\nMask:\")\n",
    "print(mask)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generator"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The generator use a transformer encoder with a d_model, a number of layers, a number of features and activation function specified as arguments. We can also specify a drop out. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %%writefile wolof-translate/wolof_translate/models/generative_model.py\n",
    "from torch.nn import functional as F\n",
    "from custom_rnn.transformers.add_position import PositionalEncoding\n",
    "from typing import *\n",
    "from torch import nn\n",
    "\n",
    "class SentenceGenerator(nn.Module):\n",
    "    \n",
    "    def __init__(self, \n",
    "                 output_size: int,\n",
    "                 d_model: int = 512,\n",
    "                 latent_dim: Union[int, None] = None,\n",
    "                 num_features: int = 2048,\n",
    "                 n_heads: int = 8,\n",
    "                 dropout: float = 0.0,\n",
    "                 activation = F.relu,\n",
    "                 num_layers: int = 6,\n",
    "                 min: int = 0, max: int = 100):\n",
    "        \n",
    "        super(SentenceGenerator, self).__init__()\n",
    "        \n",
    "        self.min, self.max = min, max\n",
    "        \n",
    "        self.d_model = d_model\n",
    "        \n",
    "        self.n_heads = n_heads\n",
    "        \n",
    "        self.dropout = dropout\n",
    "        \n",
    "        self.activation = activation\n",
    "        \n",
    "        \n",
    "        self.num_layers = num_layers\n",
    "        \n",
    "        self.num_features = num_features\n",
    "        \n",
    "        self.output_size = output_size\n",
    "        \n",
    "        self.latent_dim = latent_dim if not latent_dim is None else self.output_size\n",
    "        \n",
    "        \n",
    "        self.pe = PositionalEncoding(self.latent_dim, self.d_model)\n",
    "        \n",
    "        self.encoder_layer = nn.TransformerEncoderLayer(self.d_model,\n",
    "                                                        self.n_heads,\n",
    "                                                        self.num_features,\n",
    "                                                        self.dropout,\n",
    "                                                        self.activation,\n",
    "                                                        batch_first=True)\n",
    "        \n",
    "        self.encoder = nn.TransformerEncoder(self.encoder_layer, self.num_layers)\n",
    "        \n",
    "        self.output_layer = nn.Linear(self.d_model * self.latent_dim, output_size)\n",
    "        \n",
    "    def forward(self, input_, attention_mask):\n",
    "        \n",
    "        out = self.pe(input_).type_as(next(self.encoder.parameters()))\n",
    "        \n",
    "        out = self.encoder(out, src_key_padding_mask = attention_mask).view(-1, self.latent_dim * self.d_model)\n",
    "            \n",
    "        out = torch.clip(self.output_layer(out), self.min, self.max).round()\n",
    "        \n",
    "        return out\n",
    "        "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let us test our generative model with dummy input."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "generative_model = SentenceGenerator(output_size=dataset.max_len)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([10, 379])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# the output must be rounded to the nearest integer and clipped between the lowest and the highest ids\n",
    "g_output = generative_model(torch.randn((10, 379, 512)), mask)\n",
    "\n",
    "g_output.size()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Discriminator"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-----------------------------"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Remark**: We will use another type of discriminator than the BERT since the BERT embed the input. Then we will lose the gradient of the generator output when making the forward pass through the BERT model. See the nextly, another discriminator model implemented as the generator model."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Like we specified earlier the discriminator will be the pre-trained base BERT Model. Let us import the necessary libraries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# try:\n",
    "\n",
    "#     from transformers import AdamW, BertForSequenceClassification, get_linear_schedule_with_warmup\n",
    "\n",
    "# except ImportError:\n",
    "    \n",
    "#     !pip install transformers\n",
    "    \n",
    "#     from transformers import AdamW, BertForSequenceClassification, get_linear_schedule_with_warmup\n",
    "    \n",
    "# try:\n",
    "    \n",
    "#     from pytorch_pretrained_bert.optimization import BertAdam\n",
    "\n",
    "# except ImportError:\n",
    "    \n",
    "#     !pip install pytorch_pretrained_bert\n",
    "    \n",
    "#     from pytorch_pretrained_bert.optimization import BertAdam\n",
    "\n",
    "# from tqdm import tqdm, trange"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let us load the pre-trained Bert Model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# discriminator = BertForSequenceClassification.from_pretrained(\"bert-base-uncased\", num_labels = 1)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let us initialize the optimizer group parameters. We need to add a weight decay rate to some parameters to avoid over-fitting."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # recuperate the named parameters\n",
    "# param_optimizer = list(discriminator.named_parameters())\n",
    "\n",
    "# # identify the parameters with no decay\n",
    "# no_decay = ['bias', 'LayerNorm.Weight']\n",
    "\n",
    "# # Filter the parameters\n",
    "# optimizer_group_parameters = [\n",
    "#     {\n",
    "#         'params': [p for n, p in param_optimizer if not any(nd in n for nd in no_decay)],\n",
    "#         'weight_decay_rate': 0.1\n",
    "#     },\n",
    "#     {\n",
    "#         'params': [p for n, p in param_optimizer if any(nd in n for nd in no_decay)],\n",
    "#         'weight_decay_rate': 0.0\n",
    "#     }\n",
    "# ]\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let us now configure the optimizer. We will use the BERT version of the Adam optimizer, `BertAdam`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# opt_d = BertAdam(optimizer_group_parameters,\n",
    "#                  lr=2e-5,\n",
    "#                  warmup=.1)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will reply the above configuration in the `GAN` Model."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let us test the discriminator with the generator output."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# discriminator(g_output, attention_mask = None)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It gave us a object containing the loss (we must specify the labels to obtain them) and the logits. The latter are the most important. We will give to the discriminator the attention mask in addition to the output of the generator."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We must create a final discriminator model including the Sigmoid to obtain probabilities in place of logits."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # %%writefile wolof-translate/wolof_translate/models/discriminative_model.py\n",
    "# from transformers import BertForSequenceClassification, get_linear_schedule_with_warmup\n",
    "# from pytorch_pretrained_bert.optimization import BertAdam\n",
    "# from tqdm import tqdm, trange\n",
    "# from torch import nn\n",
    "\n",
    "\n",
    "# class SentenceDiscriminator(nn.Module):\n",
    "    \n",
    "#     def __init__(self):\n",
    "        \n",
    "#         super(SentenceDiscriminator, self).__init__()\n",
    "        \n",
    "#         self.bert_model = BertForSequenceClassification.from_pretrained('bert-base-uncased', num_labels = 1)\n",
    "        \n",
    "#         self.sigmoid = nn.Sigmoid()\n",
    "    \n",
    "#     def forward(self, input_, attention_mask = None):\n",
    "        \n",
    "#         out = self.bert_model(input_, attention_mask = attention_mask).logits\n",
    "        \n",
    "#         out = self.sigmoid(out)\n",
    "        \n",
    "#         return out\n",
    "        "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---------------------------------------------------"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let us create a new discriminator model different from the BERT Model. It will take output of the generator without converting it to a long tensor since doing so will make us losing the gradient."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %%writefile wolof-translate/wolof_translate/models/discriminative_model.py\n",
    "from torch.nn import functional as F\n",
    "from typing import *\n",
    "from torch import nn\n",
    "\n",
    "class DiscriminatorSequence(nn.Module):\n",
    "    \n",
    "    def __init__(self, \n",
    "                 input_dim,\n",
    "                 num_features,\n",
    "                 negative_slope: float = 0.01,\n",
    "                 drop_out: float = 0.0,\n",
    "                 eps: float = 0.00001,\n",
    "                 momentum: float = 0.1):\n",
    "        \n",
    "        super(DiscriminatorSequence, self).__init__()\n",
    "        \n",
    "        self.batch_norm = nn.BatchNorm1d(input_dim, eps, momentum)\n",
    "        \n",
    "        self.linear = nn.Linear(input_dim, num_features)\n",
    "        \n",
    "        self.drop_out = nn.Dropout1d(drop_out)\n",
    "        \n",
    "        self.activation = nn.LeakyReLU(negative_slope)\n",
    "        \n",
    "        \n",
    "    def forward(self, input_):\n",
    "        \n",
    "        out = self.batch_norm(input_)\n",
    "        \n",
    "        out = self.activation(self.drop_out(self.linear(out)))\n",
    "        \n",
    "        return out\n",
    "\n",
    "class SentenceDiscriminator(nn.Module):\n",
    "    \n",
    "    def __init__(self, \n",
    "                 input_dim: int,\n",
    "                 num_features: Union[int, List] = 300,\n",
    "                 num_layers: int = 5,\n",
    "                 negative_slope: float = 0.01,\n",
    "                 drop_out: float = 0.0,\n",
    "                 eps: float = 0.00001,\n",
    "                 momentum: float = 0.1):\n",
    "        \n",
    "        super(SentenceDiscriminator, self).__init__()\n",
    "        \n",
    "        self.input_dim = input_dim\n",
    "        \n",
    "        self.num_features = [num_features] * num_layers if type(num_features) is int else num_features\n",
    "        \n",
    "        assert len(self.num_features) == num_layers\n",
    "        \n",
    "        self.num_layers = num_layers\n",
    "        \n",
    "        self.sequences = nn.ModuleList()\n",
    "        \n",
    "        self.sequences.append(DiscriminatorSequence(input_dim, self.num_features[0], negative_slope, drop_out, eps, momentum))\n",
    "        \n",
    "        for l in range(1, num_layers):\n",
    "            \n",
    "            self.sequences.append(DiscriminatorSequence(self.num_features[l-1], self.num_features[l], negative_slope, drop_out, eps, momentum))\n",
    "        \n",
    "        self.output_layer = nn.Linear(self.num_features[-1], 1)\n",
    "        \n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "        \n",
    "    def forward(self, input_: torch.Tensor):\n",
    "        \n",
    "        out = input_\n",
    "        \n",
    "        for sequence in self.sequences:\n",
    "            \n",
    "            out = sequence(out)\n",
    "        \n",
    "        out = self.sigmoid(self.output_layer(out))\n",
    "        \n",
    "        return out\n",
    "        "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### GAN Model"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will use the `pytorch-lightning` to create and train the GAN Model:"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-----------------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # %%writefile wolof-translate/wolof_translate/models/gan_model.py\n",
    "# from pytorch_lightning import LightningModule, Trainer\n",
    "# from torch.nn import functional as F\n",
    "# from torch.utils.data import Dataset\n",
    "# from torch.optim import Adam \n",
    "# import IPython.display as ipd\n",
    "# from typing import *\n",
    "# from torch import nn\n",
    "# import torch\n",
    "# import os\n",
    "\n",
    "# class SentenceGAN(LightningModule):\n",
    "    \n",
    "#     def __init__(\n",
    "#         self,\n",
    "#         latent_dim: int,\n",
    "#         dataset: Dataset,\n",
    "#         g_learning_rate: float,\n",
    "#         d_learning_rate: float,\n",
    "#         g_num_features: Union[int, list] = 300,\n",
    "#         g_num_layers: int = 5,\n",
    "#         g_negative_slope: float = 0.01,\n",
    "#         g_drop_out: float = 0,\n",
    "#         g_eps: float = 0.00001,\n",
    "#         g_momentum: float = 0.1,\n",
    "#         d_warmup: float = .1,\n",
    "#         d_decay: float = .1,\n",
    "#     ):\n",
    "#         super().__init__()\n",
    "        \n",
    "#         self.save_hyperparameters()\n",
    "        \n",
    "#         # Initialize the dataset\n",
    "#         self.dataset = dataset\n",
    "        \n",
    "#         # Initialize the generator and the discriminator\n",
    "#         self.generator = SentenceGenerator(latent_dim,\n",
    "#                                            dataset.max_len,\n",
    "#                                            g_num_features,\n",
    "#                                            g_num_layers,\n",
    "#                                            g_negative_slope,\n",
    "#                                            g_drop_out,\n",
    "#                                            g_eps,\n",
    "#                                            g_momentum\n",
    "#                                            )\n",
    "        \n",
    "#         self.discriminator = SentenceDiscriminator()\n",
    "    \n",
    "#         # Generate a batch of 10 noisy data\n",
    "#         self.noisy_data = torch.randn(10, self.hparams.latent_dim)\n",
    "    \n",
    "#     def forward(self, x):\n",
    "        \n",
    "#         return self.generator(x)\n",
    "    \n",
    "#     def adversarial_loss(self, y_pred, y):\n",
    "        \n",
    "#         return F.binary_cross_entropy(y_pred, y) # we can also use the binary cross entropy with logits if logits were returned\n",
    "    \n",
    "#     def training_step(self, batch, batch_idx, optimizer_idx):\n",
    "        \n",
    "#         # Recuperate the real ids and the masks from the batch\n",
    "#         real_ids, attention_mask = batch\n",
    "        \n",
    "#         # Generate noisy data\n",
    "#         noisy_data = torch.randn(real_ids.size(0), self.hparams.latent_dim)\n",
    "        \n",
    "#         noisy_data = noisy_data.type_as(next(self.parameters()))\n",
    "        \n",
    "#         if optimizer_idx == 0:\n",
    "            \n",
    "#             # Generate fake ids\n",
    "#             fake_ids = self.convert(self(noisy_data))\n",
    "#             print(\"fake ids 1\")\n",
    "#             print(fake_ids.requires_grad)\n",
    "#             # We consider that the fake ids are real\n",
    "#             y = torch.ones(real_ids.size(0), 1)\n",
    "            \n",
    "#             y = y.type_as(next(self.parameters()))\n",
    "            \n",
    "#             # Predict the veracity of the fake ids\n",
    "#             y_pred = self.discriminator(fake_ids, attention_mask = attention_mask)\n",
    "            \n",
    "#             # Calculate the loss\n",
    "#             loss = self.adversarial_loss(y_pred, y)\n",
    "            \n",
    "#             # Print the loss\n",
    "#             self.log(\"g loss\", loss, on_step=True, on_epoch=False)\n",
    "            \n",
    "#             return loss\n",
    "        \n",
    "#         elif optimizer_idx == 1:\n",
    "            \n",
    "#             # Generate fake ids\n",
    "#             fake_ids = self.convert(self(noisy_data))\n",
    "#             print(\"fake_ids 2\")\n",
    "#             print(fake_ids.requires_grad)\n",
    "#             # We consider that the real ids as the true data\n",
    "#             y_true = torch.ones(real_ids.size(0), 1).float()\n",
    "            \n",
    "#             y_true = y_true.type_as(next(self.parameters()))\n",
    "            \n",
    "#             # Predict the veracity of the true data\n",
    "#             y_pred_true = self.discriminator(self.convert(real_ids), attention_mask = attention_mask)\n",
    "            \n",
    "#             if y_pred_true.ndim > 2:\n",
    "                \n",
    "#                 y_pred_true = y_pred_true.view(y_pred_true.size(0), 1)\n",
    "            \n",
    "#             # Calculate the loss on the real ids\n",
    "#             real_loss = self.adversarial_loss(y_pred_true, y_true)\n",
    "            \n",
    "#             # Consider the fake ids to be false\n",
    "#             y_false = torch.zeros(real_ids.size(0), 1)\n",
    "            \n",
    "#             y_false = y_false.type_as(next(self.parameters()))\n",
    "            \n",
    "#             # Predict the veracity of the false data\n",
    "#             y_pred_false = self.discriminator(fake_ids, attention_mask = attention_mask)\n",
    "            \n",
    "#             # Calculate the loss on the fake ids\n",
    "#             fake_loss = self.adversarial_loss(y_pred_false, y_false)\n",
    "            \n",
    "#             # Calculate the average loss\n",
    "#             loss = (real_loss + fake_loss) / 2\n",
    "            \n",
    "#             # Print the loss\n",
    "#             self.log(\"d loss\", loss, on_step=True, on_epoch=False)\n",
    "            \n",
    "#             return loss\n",
    "    \n",
    "#     def configure_optimizers(self):\n",
    "        \n",
    "#         G_LR = self.hparams.g_learning_rate\n",
    "#         D_LR = self.hparams.d_learning_rate\n",
    "        \n",
    "#         WARMUP = self.hparams.d_warmup\n",
    "        \n",
    "#         DECAY = self.hparams.d_decay\n",
    "        \n",
    "#         # recuperate the named parameters\n",
    "#         param_optimizer = list(self.discriminator.bert_model.named_parameters())\n",
    "\n",
    "#         # identify the parameters with no decay\n",
    "#         no_decay = ['bias', 'LayerNorm.Weight']\n",
    "\n",
    "#         # Filter the parameters\n",
    "#         optimizer_group_parameters = [\n",
    "#             {\n",
    "#                 'params': [p for n, p in param_optimizer if not any(nd in n for nd in no_decay)],\n",
    "#                 'weight_decay_rate': DECAY\n",
    "#             },\n",
    "#             {\n",
    "#                 'params': [p for n, p in param_optimizer if any(nd in n for nd in no_decay)],\n",
    "#                 'weight_decay_rate': 0.0\n",
    "#             }\n",
    "#         ]\n",
    "        \n",
    "#         opt_g = Adam(self.generator.parameters(), lr = G_LR)\n",
    "        \n",
    "#         opt_d = BertAdam(optimizer_group_parameters, lr = D_LR, warmup = WARMUP)\n",
    "        \n",
    "#         return [opt_g, opt_d], []\n",
    "    \n",
    "#     def on_train_epoch_end(self):\n",
    "        \n",
    "#         # recuperate the noisy data for prediction\n",
    "#         noisy_data = self.noisy_data.type_as(self.generator.sequences[0].linear.weight)\n",
    "        \n",
    "#         generated_data = self(noisy_data).cpu().detach()\n",
    "            \n",
    "#         generated_data = generated_data.tolist()\n",
    "        \n",
    "#         print(f\"Generated sentences at {self.current_epoch}\")\n",
    "        \n",
    "#         for data in generated_data:\n",
    "            \n",
    "#             sentence = self.hparams.dataset.tokenizer.decode(data)\n",
    "            \n",
    "#             print(sentence)\n",
    "        \n",
    "#     def generate(self, number: int = 10):\n",
    "        \n",
    "#         # Generate noisy data\n",
    "#         noisy_data = torch.randn(number, self.hparams.latent_dim)\n",
    "        \n",
    "#         noisy_data = noisy_data.type_as(self.generator.sequence[0].linear.weight)\n",
    "        \n",
    "#         # decode and return the decode sentences\n",
    "#         generated_data = self(noisy_data).cpu().detach().tolist()\n",
    "        \n",
    "#         return [self.hparams.tokenizer.decode(data) for data in generated_data]\n",
    "    \n",
    "#     def convert(self, logits: torch.Tensor):\n",
    "#         print(\"Before convert\")\n",
    "#         print(logits.requires_grad)\n",
    "#         return logits.round().clip(0, self.hparams.dataset.max_id)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "--------------------------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %%writefile wolof-translate/wolof_translate/models/gan_model.py\n",
    "from pytorch_lightning import LightningModule, Trainer\n",
    "from torch.nn import functional as F\n",
    "from torch.utils.data import Dataset\n",
    "from torch.optim import Adam \n",
    "import IPython.display as ipd\n",
    "from typing import *\n",
    "from torch import nn\n",
    "import torch\n",
    "import os\n",
    "\n",
    "class SentenceGAN(LightningModule):\n",
    "    \n",
    "    def __init__(\n",
    "        self,\n",
    "        config: dict,\n",
    "        dataset: Dataset,\n",
    "        d_model: int = 512,\n",
    "        latent_dim: Union[int, None] = None,\n",
    "        g_num_features: int = 2048,\n",
    "        n_heads: int = 8,\n",
    "        g_dropout: float = 0.0,\n",
    "        g_activation = F.relu,\n",
    "        g_num_layers: int = 6,\n",
    "        d_num_features: Union[int, list] = 500,\n",
    "        d_num_layers: int = 3,\n",
    "        d_negative_slope: float = 0.01,\n",
    "        d_drop_out: float = 0,\n",
    "        d_eps: float = 0.00001,\n",
    "        d_momentum: float = 0.1\n",
    "    ):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.save_hyperparameters()\n",
    "        \n",
    "        # Initialize the dataset\n",
    "        self.dataset = dataset\n",
    "        \n",
    "        # Initialize the generator and the discriminator\n",
    "        self.generator = SentenceGenerator(\n",
    "            dataset.max_len,\n",
    "            d_model,\n",
    "            latent_dim,\n",
    "            g_num_features,\n",
    "            n_heads,\n",
    "            g_dropout,\n",
    "            g_activation,\n",
    "            g_num_layers, min = 0, max = dataset.max_id\n",
    "        )\n",
    "        \n",
    "        self.discriminator = SentenceDiscriminator(dataset.max_len,\n",
    "                                                   d_num_features,\n",
    "                                                   d_num_layers,\n",
    "                                                   d_negative_slope,\n",
    "                                                   d_drop_out,\n",
    "                                                   d_eps,\n",
    "                                                   d_momentum)\n",
    "    \n",
    "        # Generate a batch of 10 noisy data\n",
    "        self.noisy_data = torch.randn(10, self.generator.latent_dim, self.hparams.d_model)\n",
    "    \n",
    "    def forward(self, x, mask):\n",
    "        \n",
    "        return self.generator(x, mask)\n",
    "    \n",
    "    def adversarial_loss(self, y_pred, y):\n",
    "        \n",
    "        return F.binary_cross_entropy(y_pred, y) # we can also use the binary cross entropy with logits if logits were returned\n",
    "    \n",
    "    def training_step(self, batch, batch_idx, optimizer_idx):\n",
    "        \n",
    "        # Recuperate the real ids and the masks from the batch\n",
    "        real_ids, attention_mask = batch\n",
    "        \n",
    "        # Generate noisy data\n",
    "        noisy_data = torch.randn(real_ids.size(0), self.generator.latent_dim, self.hparams.d_model)\n",
    "        \n",
    "        noisy_data = noisy_data.type_as(next(self.parameters()))\n",
    "        \n",
    "        if optimizer_idx == 0:\n",
    "            \n",
    "            # Generate fake ids\n",
    "            fake_ids = self(noisy_data, attention_mask)\n",
    "           \n",
    "            # We consider that the fake ids are real\n",
    "            y = torch.ones(real_ids.size(0), 1)\n",
    "            \n",
    "            y = y.type_as(next(self.parameters()))\n",
    "            \n",
    "            # Predict the veracity of the fake ids\n",
    "            y_pred = self.discriminator(fake_ids)\n",
    "            \n",
    "            # Calculate the loss\n",
    "            loss = self.adversarial_loss(y_pred, y)\n",
    "            \n",
    "            # Print the loss\n",
    "            self.log(\"g_loss\", loss, on_step=True, on_epoch=False)\n",
    "            \n",
    "            return loss\n",
    "        \n",
    "        elif optimizer_idx == 1:\n",
    "            \n",
    "            # Generate fake ids\n",
    "            fake_ids = self(noisy_data, attention_mask)\n",
    "          \n",
    "            # We consider that the real ids as the true data\n",
    "            y_true = torch.ones(real_ids.size(0), 1)\n",
    "            \n",
    "            y_true = y_true.type_as(next(self.parameters()))\n",
    "            \n",
    "            # Predict the veracity of the true data\n",
    "            y_pred_true = self.discriminator(real_ids)\n",
    "            \n",
    "            if y_pred_true.ndim > 2:\n",
    "                \n",
    "                y_pred_true = y_pred_true.view(y_pred_true.size(0), 1)\n",
    "            \n",
    "            # Calculate the loss on the real ids\n",
    "            real_loss = self.adversarial_loss(y_pred_true, y_true)\n",
    "            \n",
    "            # Consider the fake ids to be false\n",
    "            y_false = torch.zeros(real_ids.size(0), 1)\n",
    "            \n",
    "            y_false = y_false.type_as(next(self.parameters()))\n",
    "            \n",
    "            # Predict the veracity of the false data\n",
    "            y_pred_false = self.discriminator(fake_ids)\n",
    "            \n",
    "            # Calculate the loss on the fake ids\n",
    "            fake_loss = self.adversarial_loss(y_pred_false, y_false)\n",
    "            \n",
    "            # Calculate the average loss\n",
    "            loss = (real_loss + fake_loss) / 2\n",
    "            \n",
    "            # Print the loss\n",
    "            self.log(\"d_loss\", loss, on_step=True, on_epoch=False)\n",
    "            \n",
    "            return loss\n",
    "    \n",
    "    def configure_optimizers(self):\n",
    "        \n",
    "        G_LR = self.hparams.config['g_learning_rate']\n",
    "        D_LR = self.hparams.config['d_learning_rate']\n",
    "        \n",
    "        opt_g = Adam(self.generator.parameters(), lr = G_LR)\n",
    "        \n",
    "        opt_d = Adam(self.discriminator.parameters(), lr = D_LR)\n",
    "        \n",
    "        return [opt_g, opt_d], []\n",
    "    \n",
    "    def on_train_epoch_end(self):\n",
    "        \n",
    "        if (self.current_epoch + 1) % 3 == 0:\n",
    "            \n",
    "            # recuperate the noisy data for prediction\n",
    "            noisy_data = self.noisy_data.type_as(next(self.parameters()))\n",
    "            \n",
    "            generated_data = self(noisy_data).cpu().detach().long()\n",
    "                \n",
    "            generated_data = generated_data.tolist()\n",
    "            \n",
    "            print(f\"\\nGenerated sentences at epoch {self.current_epoch}\")\n",
    "            \n",
    "            for data in generated_data:\n",
    "                \n",
    "                sentence = self.hparams.dataset.tokenizer.decode(data)\n",
    "                \n",
    "                print(sentence)\n",
    "        \n",
    "    def generate(self, number: int = 10):\n",
    "        \n",
    "        # Generate noisy data\n",
    "        noisy_data = torch.randn(number, self.generator.latent_dim, self.hparams.d_model)\n",
    "        \n",
    "        noisy_data = noisy_data.type_as(next(self.parameters()))\n",
    "        \n",
    "        # decode and return the decode sentences\n",
    "        generated_data = self(noisy_data).cpu().detach().long().tolist()\n",
    "        \n",
    "        return [self.hparams.tokenizer.decode(data) for data in generated_data]\n",
    "    \n",
    "    def apply_mask(self, input_: torch.Tensor, attention_mask: torch.Tensor):\n",
    "        \n",
    "        return input_.masked_fill(attention_mask == 0, 0)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training of the GAN and evaluation"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let us train the `GAN` Model. It will generate after each epoch 10 sentences. We will check if they are correct. It will require eventually many iterations so let us initialize the max number of epochs to 50 and increase it if necessary. "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let us import all the necessary libraries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ray.tune.integration.pytorch_lightning import TuneReportCallback\n",
    "from pytorch_lightning.loggers import TensorBoardLogger\n",
    "from pytorch_lightning.trainer import Trainer\n",
    "import ray.tune as tune\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize the gan model\n",
    "gan_model = SentenceGAN(dataset = dataset, config = {'g_learning_rate': 1e-5, 'd_learning_rate': 1e-5})"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let us initialize the tensor boar logger."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "LOGGER = TensorBoardLogger(save_dir=\"gan_logs\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let us configure the trainer. It will automatically save the parameters locally to make us continue the training at any time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# this code will make us loading the checkpoints\n",
    "# gan_model = SentenceGAN.load_from_checkpoint(\"data/checkpoints/generator/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n"
     ]
    }
   ],
   "source": [
    "\n",
    "trainer = Trainer(\n",
    "    logger=LOGGER,\n",
    "    accelerator=\"gpu\",\n",
    "    max_epochs=300,\n",
    "    devices=1 if torch.cuda.is_available() else None,\n",
    "    log_every_n_steps=300,\n",
    "    default_root_dir=\"data/checkpoints/generator/\"\n",
    ")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let us train the `GAN` Model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_SIZE = 5\n",
    "\n",
    "# trainer.fit(gan_model, train_dataloaders=DataLoader(dataset, batch_size=BATCH_SIZE, shuffle=True))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let us use the pytorch lightning runer to search for the best parameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# let us create the trainer \n",
    "def train_generator(\n",
    "    config, \n",
    "    dataset = dataset,\n",
    "    num_epochs = 5,\n",
    "    d_model: int = 512,\n",
    "    latent_dim: Union[int, None] = None,\n",
    "    g_num_features: int = 2048,\n",
    "    n_heads: int = 8,\n",
    "    g_dropout: float = 0.0,\n",
    "    g_activation = F.relu,\n",
    "    g_num_layers: int = 6,\n",
    "    d_num_features: Union[int, list] = 500,\n",
    "    d_num_layers: int = 3,\n",
    "    d_negative_slope: float = 0.01,\n",
    "    d_drop_out: float = 0,\n",
    "    d_eps: float = 0.00001,\n",
    "    d_momentum: float = 0.1\n",
    "    ):\n",
    "    \n",
    "    gan_model = SentenceGAN(\n",
    "        config, \n",
    "        dataset, \n",
    "        d_model, \n",
    "        latent_dim,\n",
    "        g_num_features,\n",
    "        n_heads,\n",
    "        g_dropout,\n",
    "        g_activation,\n",
    "        g_num_layers,\n",
    "        d_num_features,\n",
    "        d_num_layers,\n",
    "        d_negative_slope,\n",
    "        d_drop_out,\n",
    "        d_eps,\n",
    "        d_momentum)\n",
    "    \n",
    "    loader = DataLoader(dataset, batch_size=10, shuffle=True)\n",
    "    \n",
    "    metrics = {'d_loss': 'd_loss', 'g_loss': 'g_loss'}\n",
    "    \n",
    "    trainer = Trainer(\n",
    "        logger=LOGGER,\n",
    "        max_epochs=num_epochs,\n",
    "        accelerator=\"gpu\",\n",
    "        devices=1 if torch.cuda.is_available() else None,\n",
    "        callbacks=[TuneReportCallback(metrics, on=\"training_end\")]\n",
    "    )\n",
    "    \n",
    "    trainer.fit(gan_model, loader)\n",
    "    \n",
    "    \n",
    "    \n",
    "    "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let us define a search space."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-04-24 17:00:48,894\tINFO worker.py:1553 -- Started a local Ray instance.\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[33m(raylet)\u001b[0m C:\\Python\\Python310\\python.exe: can't open file 'c:\\\\Users\\\\Oumar\\\\ Kane\\\\AppData\\\\Local\\\\pypoetry\\\\Cache\\\\virtualenvs\\\\pytorch1-HleOW5am-py3.10\\\\lib\\\\site-packages\\\\ray\\\\_private\\\\workers\\\\default_worker.py': [Errno 2] No such file or directory\n",
      "\u001b[2m\u001b[33m(raylet)\u001b[0m [2023-04-24 17:02:00,085 E 39400 26796] (raylet.exe) worker_pool.cc:525: Some workers of the worker process(38480) have not registered within the timeout. The process is dead, probably it crashed during start.\n",
      "\u001b[2m\u001b[33m(raylet)\u001b[0m C:\\Python\\Python310\\python.exe: can't open file 'c:\\\\Users\\\\Oumar\\\\ Kane\\\\AppData\\\\Local\\\\pypoetry\\\\Cache\\\\virtualenvs\\\\pytorch1-HleOW5am-py3.10\\\\lib\\\\site-packages\\\\ray\\\\_private\\\\workers\\\\default_worker.py': [Errno 2] No such file or directory\n",
      "\u001b[2m\u001b[33m(raylet)\u001b[0m [2023-04-24 17:03:00,115 E 39400 26796] (raylet.exe) worker_pool.cc:525: Some workers of the worker process(9560) have not registered within the timeout. The process is dead, probably it crashed during start.\n",
      "\u001b[2m\u001b[33m(raylet)\u001b[0m C:\\Python\\Python310\\python.exe: can't open file 'c:\\\\Users\\\\Oumar\\\\ Kane\\\\AppData\\\\Local\\\\pypoetry\\\\Cache\\\\virtualenvs\\\\pytorch1-HleOW5am-py3.10\\\\lib\\\\site-packages\\\\ray\\\\_private\\\\workers\\\\default_worker.py': [Errno 2] No such file or directory\n",
      "\u001b[2m\u001b[33m(raylet)\u001b[0m [2023-04-24 17:04:00,128 E 39400 26796] (raylet.exe) worker_pool.cc:525: Some workers of the worker process(52820) have not registered within the timeout. The process is dead, probably it crashed during start.\n",
      "\u001b[2m\u001b[33m(raylet)\u001b[0m C:\\Python\\Python310\\python.exe: can't open file 'c:\\\\Users\\\\Oumar\\\\ Kane\\\\AppData\\\\Local\\\\pypoetry\\\\Cache\\\\virtualenvs\\\\pytorch1-HleOW5am-py3.10\\\\lib\\\\site-packages\\\\ray\\\\_private\\\\workers\\\\default_worker.py': [Errno 2] No such file or directory\n",
      "\u001b[2m\u001b[33m(raylet)\u001b[0m [2023-04-24 17:05:00,136 E 39400 26796] (raylet.exe) worker_pool.cc:525: Some workers of the worker process(12408) have not registered within the timeout. The process is dead, probably it crashed during start.\n",
      "\u001b[2m\u001b[33m(raylet)\u001b[0m C:\\Python\\Python310\\python.exe: can't open file 'c:\\\\Users\\\\Oumar\\\\ Kane\\\\AppData\\\\Local\\\\pypoetry\\\\Cache\\\\virtualenvs\\\\pytorch1-HleOW5am-py3.10\\\\lib\\\\site-packages\\\\ray\\\\_private\\\\workers\\\\default_worker.py': [Errno 2] No such file or directory\n",
      "\u001b[2m\u001b[33m(raylet)\u001b[0m [2023-04-24 17:06:00,159 E 39400 26796] (raylet.exe) worker_pool.cc:525: Some workers of the worker process(2072) have not registered within the timeout. The process is dead, probably it crashed during start.\n",
      "\u001b[2m\u001b[33m(raylet)\u001b[0m C:\\Python\\Python310\\python.exe: can't open file 'c:\\\\Users\\\\Oumar\\\\ Kane\\\\AppData\\\\Local\\\\pypoetry\\\\Cache\\\\virtualenvs\\\\pytorch1-HleOW5am-py3.10\\\\lib\\\\site-packages\\\\ray\\\\_private\\\\workers\\\\default_worker.py': [Errno 2] No such file or directory\n",
      "\u001b[2m\u001b[33m(raylet)\u001b[0m [2023-04-24 17:07:00,186 E 39400 26796] (raylet.exe) worker_pool.cc:525: Some workers of the worker process(14968) have not registered within the timeout. The process is dead, probably it crashed during start.\n",
      "\u001b[2m\u001b[33m(raylet)\u001b[0m C:\\Python\\Python310\\python.exe: can't open file 'c:\\\\Users\\\\Oumar\\\\ Kane\\\\AppData\\\\Local\\\\pypoetry\\\\Cache\\\\virtualenvs\\\\pytorch1-HleOW5am-py3.10\\\\lib\\\\site-packages\\\\ray\\\\_private\\\\workers\\\\default_worker.py': [Errno 2] No such file or directory\n",
      "2023-04-24 17:07:53,536\tWARNING tune.py:146 -- Stop signal received (e.g. via SIGINT/Ctrl+C), ending Ray Tune run. This will try to checkpoint the experiment state one last time. Press CTRL+C (or send SIGINT/SIGKILL/SIGTERM) to skip. \n",
      "\u001b[2m\u001b[33m(raylet)\u001b[0m [2023-04-24 17:08:00,207 E 39400 26796] (raylet.exe) worker_pool.cc:525: Some workers of the worker process(32064) have not registered within the timeout. The process is dead, probably it crashed during start.\n",
      "\u001b[2m\u001b[33m(raylet)\u001b[0m C:\\Python\\Python310\\python.exe: can't open file 'c:\\\\Users\\\\Oumar\\\\ Kane\\\\AppData\\\\Local\\\\pypoetry\\\\Cache\\\\virtualenvs\\\\pytorch1-HleOW5am-py3.10\\\\lib\\\\site-packages\\\\ray\\\\_private\\\\workers\\\\default_worker.py': [Errno 2] No such file or directory\n"
     ]
    }
   ],
   "source": [
    "num_samples = 10\n",
    "\n",
    "num_epochs = 5\n",
    "\n",
    "config = {\n",
    "    'g_learning_rate': tune.loguniform(1e-5, 1e-1),\n",
    "    'd_learning_rate': tune.loguniform(1e-5, 1e-1)\n",
    "}\n",
    "\n",
    "trainable = tune.with_parameters(\n",
    "    train_generator, dataset = dataset, num_epochs = num_epochs\n",
    ")\n",
    "\n",
    "analysis = tune.run(\n",
    "    trainable,\n",
    "    metric=\"loss\",\n",
    "    mode = 'min',\n",
    "    resources_per_trial={\n",
    "        'cpu': 1,\n",
    "        'gpu': 1\n",
    "    },\n",
    "    config = config,\n",
    "    num_samples=num_samples,\n",
    "    name=\"tune_generator\" \n",
    ")\n",
    "\n",
    "print(analysis.best_config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pytorch1-HleOW5am-py3.10",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
