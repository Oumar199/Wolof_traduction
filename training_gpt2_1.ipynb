{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First Training with the GPT-2 decoder \n",
    "-----------------------------------\n",
    "\n",
    "In this notebook, we will train the pre-trained GPT-2 model provided by OPEN-AI. It only tests how the model can be accurate on the corpora we extracted. It is undoubtedly a partial model. If necessary, we will make grid-search as we did with the `GAN model` to find the best parameters (this task will require creating a new runner class). For the test, we will follow the fine-tuning tutorial available at the following medium link [fine-tuning-transformers](https://medium.com/towards-data-science/guide-to-fine-tuning-text-generation-models-gpt-2-gpt-neo-and-t5-dc5de6b3bc5e).\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Objective**: On the GPT-2 model we want to obtain an evaluation loss between 0.0 and 0.30 for the moment.  "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will make this with and without augmentation and see where we obtain better results. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install evaluate -qq\n",
    "!pip install sacrebleu -qq\n",
    "!pip install optuna -qq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Oumar Kane\\AppData\\Local\\pypoetry\\Cache\\virtualenvs\\pytorch1-HleOW5am-py3.10\\lib\\site-packages\\tqdm\\auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "# let us import all necessary libraries\n",
    "from wolof_translate.utils.sent_transformers import TransformerSequences\n",
    "from transformers import GPT2LMHeadModel, TrainingArguments, Trainer\n",
    "from wolof_translate.data.dataset_v1 import SentenceDataset\n",
    "from wolof_translate.utils.sent_corrections import *\n",
    "from sklearn.model_selection import train_test_split\n",
    "from nlpaug.augmenter import char as nac\n",
    "from torch.utils.data import DataLoader\n",
    "# from datasets  import load_metric # make pip install evaluate instead\n",
    "# and pip install sacrebleu for instance\n",
    "from functools import partial\n",
    "from tqdm import tqdm\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import evaluate\n",
    "import torch\n",
    "\n",
    "# For searching the best parameters\n",
    "#---------\n",
    "import ray\n",
    "from ray import tune\n",
    "from ray.tune import CLIReporter\n",
    "from ray.tune.schedulers import PopulationBasedTraining\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will create two models: \n",
    "\n",
    "- One translating the french corpus to a wolof corpus [french_to_wolof](#french-to-wolof)\n",
    "- One translating the wolof corpus to a french corpus [wolof_to_french](#wolof-to-french)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "--------------"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## French to wolof"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Configure dataset"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can use the same custom dataset that we created in [text_augmentation](text_augmentation.ipynb). But we need to split the data between train and test sets and save them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load the corpora and split into train and test sets\n",
    "corpora = pd.read_csv(\"data/extractions/new_data/sent_extraction.csv\")\n",
    "\n",
    "train_set, test_set = train_test_split(corpora, test_size=0.1, random_state=50)\n",
    "\n",
    "# let us save the sets\n",
    "train_set.to_csv(\"data/extractions/new_data/train_set.csv\", index=False)\n",
    "\n",
    "test_set.to_csv(\"data/extractions/new_data/test_set.csv\", index=False)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let us recuperate the datasets with and without augmentation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# without augmentation\n",
    "train_dataset = SentenceDataset(\"data/extractions/new_data/train_set.csv\")\n",
    "\n",
    "test_dataset = SentenceDataset(\"data/extractions/new_data/test_set.csv\")\n",
    "\n",
    "# with augmentation\n",
    "fr_augmentation = TransformerSequences(nac.KeyboardAug(aug_char_p=0.2, aug_word_p=0.2),\n",
    "                                       remove_mark_space, delete_guillemet_space)\n",
    "\n",
    "wf_augmentation = TransformerSequences(nac.KeyboardAug(aug_char_p=0.2, aug_word_p=0.2),\n",
    "                                       remove_mark_space, delete_guillemet_space)\n",
    "\n",
    "train_dataset_aug = SentenceDataset(\"data/extractions/new_data/train_set.csv\", \n",
    "                                cp1_transformer=fr_augmentation, truncation=True,\n",
    "                                cp2_transformer=wf_augmentation, max_len=579)\n",
    "\n",
    "test_dataset_aug = SentenceDataset(\"data/extractions/new_data/test_set.csv\",\n",
    "                               cp1_transformer=fr_augmentation, truncation=True,\n",
    "                               cp2_transformer=wf_augmentation, max_len=579)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Configure the model"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let us define the data collator."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def data_collator(batch):\n",
    "    \"\"\"Generate a batch of data to provide to trainer\n",
    "\n",
    "    Args:\n",
    "        batch (_type_): The batch\n",
    "\n",
    "    Returns:\n",
    "        dict: A dictionary containing the ids, the attention mask and the labels\n",
    "    \"\"\"\n",
    "    input_ids = torch.stack([b[0] for b in batch])\n",
    "    \n",
    "    attention_mask = torch.stack([b[1] for b in batch])\n",
    "    \n",
    "    labels = torch.stack([b[0] for b in batch])\n",
    "    \n",
    "    return {'input_ids': input_ids, 'attention_mask': attention_mask,\n",
    "            'labels': labels}"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let us recuperate the model and resize the token embeddings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Embedding(15696, 768)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# def gpt2_model_init():\n",
    "# set the mode name\n",
    "model_name = \"gpt2\"\n",
    "\n",
    "# recuperate the tokenizer from the dataset\n",
    "tokenizer = train_dataset.tokenizer\n",
    "\n",
    "# configure the model\n",
    "model = GPT2LMHeadModel.from_pretrained(model_name).cuda()\n",
    "\n",
    "# resize the token embeddings\n",
    "model.resize_token_embeddings(len(tokenizer))\n",
    "\n",
    "# return model"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let us evaluate the predictions with the `bleu` metric."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting wolof-translate/wolof_translate/utils/evaluation.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile wolof-translate/wolof_translate/utils/evaluation.py\n",
    "from tokenizers import Tokenizer\n",
    "from typing import *\n",
    "import numpy as np\n",
    "import evaluate\n",
    "\n",
    "class TranslationEvaluation:\n",
    "    \n",
    "    def __init__(self, \n",
    "                 tokenizer: Tokenizer,\n",
    "                 decoder: Union[Callable, None] = None,\n",
    "                 metric = evaluate.load('sacrebleu'),\n",
    "                 ):\n",
    "        \n",
    "        self.tokenizer = tokenizer\n",
    "        \n",
    "        self.decoder = decoder\n",
    "        \n",
    "        self.metric = metric\n",
    "    \n",
    "    def postprocess_text(self, preds, labels):\n",
    "        \n",
    "        preds = [pred.strip() for pred in preds]\n",
    "        \n",
    "        labels = [[label.strip()] for label in labels]\n",
    "        \n",
    "        return preds, labels\n",
    "\n",
    "    def compute_metrics(self, eval_preds):\n",
    "        \n",
    "        preds, labels = eval_preds\n",
    "        \n",
    "        if isinstance(preds, tuple):\n",
    "            \n",
    "            preds = preds[0]\n",
    "        \n",
    "        if self.decoder is None:\n",
    "            \n",
    "            decoded_preds = tokenizer.batch_decode(preds, skip_special_tokens=True)\n",
    "            \n",
    "            decoded_labels = tokenizer.batch_decode(labels, skip_special_tokens=True)\n",
    "            \n",
    "            decoded_preds, decoded_labels = self.postprocess_text(decoded_preds, decoded_labels)\n",
    "            \n",
    "            result = self.metric.compute(predictions=decoded_preds, references=decoded_labels)\n",
    "            \n",
    "            result = {\"bleu\": result[\"score\"]}\n",
    "            \n",
    "            prediction_lens = [np.count_nonzero(pred != tokenizer.pad_token_id) for pred in preds]\n",
    "            \n",
    "            result[\"gen_len\"] = np.mean(prediction_lens)\n",
    "        \n",
    "        else:\n",
    "            \n",
    "            predictions = list(self.decoder(preds))\n",
    "            \n",
    "            labels = list(self.decoder(labels))\n",
    "      \n",
    "            decoded_preds, decoded_labels = self.postprocess_text(predictions, labels)\n",
    "            \n",
    "            result = self.metric.compute(predictions=predictions, references=labels)\n",
    "            \n",
    "            result = {\"bleu\": result[\"score\"]}\n",
    "        \n",
    "        result = {k:round(v, 4) for k, v in result.items()}\n",
    "            \n",
    "        return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "%run wolof-translate/wolof_translate/utils/evaluation.py"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let us initialize the evaluation object."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "translation_eval = TranslationEvaluation(tokenizer, \n",
    "                                         partial(test_dataset.decode, for_prediction = True))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training without augmentation"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let us initialize the training arguments and fine-tune the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# seed\n",
    "torch.manual_seed(50)\n",
    "\n",
    "# create training arguments\n",
    "training_args = TrainingArguments(\"data/training1/results\",\n",
    "                                  overwrite_output_dir=True,\n",
    "                                  num_train_epochs=3,\n",
    "                                  logging_steps=100,\n",
    "                                  load_best_model_at_end=True,\n",
    "                                  save_strategy=\"epoch\",\n",
    "                                  evaluation_strategy=\"epoch\",\n",
    "                                  per_device_train_batch_size=2, \n",
    "                                  per_device_eval_batch_size=2,\n",
    "                                  learning_rate=2e-5,\n",
    "                                #   warmup_steps=120,\n",
    "                                  weight_decay=0.05,\n",
    "                                  logging_dir='gpt2_training_logs1')\n",
    "\n",
    "# start training\n",
    "trainer = Trainer(model=model,\n",
    "                  args=training_args,\n",
    "                  train_dataset=train_dataset, \n",
    "                  eval_dataset=test_dataset,\n",
    "                  data_collator=data_collator,\n",
    "                  # compute_metrics=translation_eval.compute_metrics\n",
    "                  )\n",
    "\n",
    "# def optuna_hp_space(trial):\n",
    "  \n",
    "#   return {\n",
    "#     \"learning_rate\": trial.suggest_float(\"learning_rate\", 1e-5, 5e-5, log = True),\n",
    "#     \"num_train_epochs\": trial.suggest_int(\"num_train_epochs\", 1, 5),\n",
    "#     \"seed\": trial.suggest_int(\"seed\", 1, 40),\n",
    "#     \"per_device_train_batch_size\": trial.suggest_categorical(\"per_device_train_batch_size\", [2, 3, 4, 5, 6])\n",
    "#   }\n",
    "\n",
    "# # Searching for the best hyperparameters\n",
    "# trainer.hyperparameter_search(\n",
    "#   direction = \"maximize\",\n",
    "#   backend = \"optuna\",\n",
    "#   hp_space = optuna_hp_space,\n",
    "#   n_trials = 10,\n",
    "# )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Oumar Kane\\AppData\\Local\\pypoetry\\Cache\\virtualenvs\\pytorch1-HleOW5am-py3.10\\lib\\site-packages\\transformers\\optimization.py:391: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n",
      "  9%|▉         | 101/1101 [00:21<03:27,  4.82it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.3157, 'learning_rate': 1.818346957311535e-05, 'epoch': 0.27}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 18%|█▊        | 200/1101 [00:41<03:17,  4.55it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.3862, 'learning_rate': 1.63669391462307e-05, 'epoch': 0.54}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 27%|██▋       | 300/1101 [01:06<03:19,  4.01it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.224, 'learning_rate': 1.455040871934605e-05, 'epoch': 0.82}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                  \n",
      " 33%|███▎      | 367/1101 [01:26<02:45,  4.42it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 1.267166256904602, 'eval_runtime': 2.8523, 'eval_samples_per_second': 28.749, 'eval_steps_per_second': 14.374, 'epoch': 1.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 36%|███▋      | 400/1101 [01:45<02:50,  4.11it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.3129, 'learning_rate': 1.27338782924614e-05, 'epoch': 1.09}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 45%|████▌     | 500/1101 [02:11<02:32,  3.93it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.299, 'learning_rate': 1.0917347865576748e-05, 'epoch': 1.36}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 54%|█████▍    | 600/1101 [02:37<02:07,  3.93it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.2032, 'learning_rate': 9.1008174386921e-06, 'epoch': 1.63}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 64%|██████▎   | 700/1101 [03:02<01:42,  3.93it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.1972, 'learning_rate': 7.284287011807448e-06, 'epoch': 1.91}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                  \n",
      " 67%|██████▋   | 734/1101 [03:14<01:23,  4.40it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 1.2623374462127686, 'eval_runtime': 2.856, 'eval_samples_per_second': 28.712, 'eval_steps_per_second': 14.356, 'epoch': 2.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 73%|███████▎  | 800/1101 [03:43<01:16,  3.93it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.2231, 'learning_rate': 5.467756584922798e-06, 'epoch': 2.18}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 82%|████████▏ | 900/1101 [04:08<00:51,  3.90it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.2427, 'learning_rate': 3.6512261580381475e-06, 'epoch': 2.45}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 91%|█████████ | 1000/1101 [04:34<00:25,  3.90it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.2359, 'learning_rate': 1.8346957311534968e-06, 'epoch': 2.72}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1101/1101 [05:00<00:00,  4.30it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.1939, 'learning_rate': 1.8165304268846506e-08, 'epoch': 3.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                   \n",
      "100%|██████████| 1101/1101 [05:03<00:00,  4.30it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 1.2587969303131104, 'eval_runtime': 2.922, 'eval_samples_per_second': 28.063, 'eval_steps_per_second': 14.032, 'epoch': 3.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1101/1101 [05:15<00:00,  3.49it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'train_runtime': 315.4911, 'train_samples_per_second': 6.97, 'train_steps_per_second': 3.49, 'train_loss': 1.2575635269054166, 'epoch': 3.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=1101, training_loss=1.2575635269054166, metrics={'train_runtime': 315.4911, 'train_samples_per_second': 6.97, 'train_steps_per_second': 3.49, 'train_loss': 1.2575635269054166, 'epoch': 3.0})"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# load from a checkpoint and continue the training\n",
    "# trainer._load_from_checkpoint('data/training1/results/checkpoint-734/')\n",
    "\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# trainer.evaluate(test_dataset)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We see that the model is over-fitted. We must fine-tune the model and augment it to add some noise into the training step."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let us evaluate the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  1%|          | 1/82 [00:06<09:20,  6.91s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Muy nag, nag, nag, nag, nag, nag, nag, nag, nag, nag, ci sama yaay daan wax dëgg-dëgg, ci sama biir yu kowe ya.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  2%|▏         | 2/82 [00:12<08:10,  6.13s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Nettali sax, su ma ci sama yaay moom, di ma ni sama yaay daan nettali nii, di ma koy jekk-xare, di sama yaay daan nettali nii. Mu ni sama yaay daan nettali nii, sama yaay daan nettali nii, sama yaay daan nettali nii.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  4%|▎         | 3/82 [00:18<08:17,  6.30s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ngalla, su ma sama yaay moom, sama yaay moom, sama yaay moom, sama yaay moom, sama yaay moom, sama yaay daan wax.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  5%|▍         | 4/82 [00:24<07:35,  5.84s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Nettali askan wi, ay way-jur, ay ndell, ay ndell, ay ndell, ay ndell, ay ndell, ay ndell, ay ndell, ay ndell, ay ndell, ay ndell, ay ndell, ay ndell, ay ndell, ay ndell, ay ndell, ay ndell yu dóomu-tóor yu ňuul, ay ndell ci seen biir yu ňuul, ay ndell, ay ndell, ay ndell ci seen biir yu ňuul, ay ndell, ay ndell, ay ndell, ay ndell, ay ndell, ay ndell, ay ndell, ay ndell ci seen biir yu dóomu-tóor yu dóomu-tóor yu dóomu-tóor yu dóomu-tóor yu dóomu-tóor yu ňuul.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  6%|▌         | 5/82 [00:29<07:22,  5.75s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Nettali sax, du sax, du sax, du sax, du sax, du sax, du sax, du ci sama yaay a ngi janook moom, du woon.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  7%|▋         | 6/82 [00:35<07:29,  5.92s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Baay na, ci sama yaay daan nettali nii, di sama yaay daan nettali nii.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  9%|▊         | 7/82 [00:41<07:18,  5.84s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Nit ki, Baay jël bés bi, di ma ni ma ci sama yaay daan wax dëgg-dëgg, di ma ni ma ni ma ni ma ni ma ni sama xel.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 10%|▉         | 8/82 [00:46<06:43,  5.46s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mbóot, su ma Baay, su ma ci sama yaay, su ma, su ma ni, su ma ci sama yaay daan nettali nii, su ma ci sama yaay daan nettali nii, su ma tolloo ci sama yaay daan nettali nii, du dox lu mu ma tolloo ci sama yaay daan nettali nii.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 11%|█         | 9/82 [00:52<06:53,  5.67s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Li ci sama yaay dund, du sax, du woon dara ci sama yaay dund Afrig.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 12%|█▏        | 10/82 [01:00<07:48,  6.50s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Nettali na, nag, nag, nag, nag, nag, nag, nag, nag, nag, nag, nag, di ko ko nga xam ni mel ni i xob.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 13%|█▎        | 11/82 [01:08<08:07,  6.86s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ngir ma ni ma ni ma ni ma ni ma ni ma ni sama yaay, du woon dara ci sama mag ju góor gu ndaw.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 15%|█▍        | 12/82 [01:17<08:41,  7.45s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Muy nag, nag, nag, nag, nag, nag, nag, nag, nag, nag.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 16%|█▌        | 13/82 [01:24<08:21,  7.26s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Nettali askan wi, du sax, du sax, du sax, du sax, du sax, du sax, du sax, du sax, du sax, du sax, du sax, du sax, du sax, du sax, du sax, du sax, du sax, du sax, du dox ci sama yaay a ngi, du dox ci sama yaay.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 17%|█▋        | 14/82 [01:30<07:55,  6.99s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Nettali sax, du sax, du sax, du sax, du sax, du sax, du sax, du sax, du sax, du sax, du sax, du sax, du sax, du sax, du sax, du sax, du sax, du dox ci sama yaay a ngi gisaat, du dox ci sama yaay.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 18%|█▊        | 15/82 [01:38<08:05,  7.24s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Muy nag, du dox ci sama yaay daan nettali nii.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 20%|█▉        | 16/82 [01:44<07:45,  7.05s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Nettali askan wi, du sax, du sax, du sax, du sax, du sax, du ci seen biir yu kowe ya. Mu mel ni mel ni mel ni mel ni mel ni mel ni mel.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 21%|██        | 17/82 [01:51<07:36,  7.03s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mu ma sama yaay daan wax, du woon dara ci sama mag ju góor.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 22%|██▏       | 18/82 [01:58<07:30,  7.03s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ngir, su ma ni ci sama yaay daan wax dëgg-dëgg, ci sama yaay daan wax dëgg-dëgg, ci sama yaay daan wax dëgg-dëgg, ci sama yaay daan wax dëgg-dëgg, ci sama yaay daan wax dëgg-dëgg, di ma daan wax dëgg-dëgg.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 23%|██▎       | 19/82 [02:05<07:15,  6.92s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Li ci sama yaay dund Afrig, du sax, du sax, du sax, du sax, du sax, du sax, du sax, du sax, du sax, du sax, du sax, du sax, du ci sama yaay.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 24%|██▍       | 20/82 [02:12<07:03,  6.83s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Muy nag, nag, nag, nag, nag, nag, nag, nag, nag.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 26%|██▌       | 21/82 [02:18<06:56,  6.82s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Nettali askan wi, du sax, du sax, du sax, du sax, du sax, du ci sama yaay daan nettali nii.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 27%|██▋       | 22/82 [02:24<06:34,  6.57s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Nettali askan wi, ci sama yaay moom, sama yaay daan nettali nii. Mu ni ko, sama yaay daan nettali nii, sama yaay daan nettali nii.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 28%|██▊       | 23/82 [02:31<06:18,  6.42s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Muy nag, nag, nag, nag, nag, nag, nag, nag, nag, nag, nag, nag, nag, nag, nag, nag, nag, nag.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 29%|██▉       | 24/82 [02:38<06:24,  6.63s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ngiraal ya mu ngi nii, di ko ko daan nettali nii.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 30%|███       | 25/82 [02:46<06:39,  7.00s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Nettali askan wi, di ko ci sama yaay dund, di ko nu daan nettali nii.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 32%|███▏      | 26/82 [02:55<07:20,  7.87s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Muy nag, nag, nag, nag mooy ñaari gone yi ci sama yaay daan nettali nii.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 33%|███▎      | 27/82 [03:02<06:50,  7.46s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Liñ na ma ni ma ni ma ni Baay, ci sama yaay di sama yaay daan def.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 34%|███▍      | 28/82 [03:09<06:38,  7.39s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Nettali sax, nag, nag, nag, nag, nag, nag, nag, nag, nag, nag, nag, nag, nag, nag, nag, nag, nag, nag, nag, nag, nag, nag, nag, nag, nag, nag, nag, nag, nag, nag, nag, nag, nag, nag, nag, nag, nag, nag, nag, nag, nag, nag, nag, nag, nag, nag, nag, nag, nag, nag, nag.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 35%|███▌      | 29/82 [03:16<06:27,  7.31s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ngir sax, du dox lu mu mel ni mel ni mel ni mel ni mel ni mel.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 37%|███▋      | 30/82 [03:23<06:10,  7.12s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Li ci sama yaay dund, du sax, du woon dara lu dul woon lu dul woon.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 38%|███▊      | 31/82 [03:30<06:01,  7.08s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ngir, ci sama yaay dund, di nu daan nettali nii, di nu daan nu daan nettali nii.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 39%|███▉      | 32/82 [03:37<05:46,  6.94s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Li ci sama yaay dund époque-saalumu Afrig, du sax, du sax, du sax, du sax, du sax, du sax, du sax, du ci sama yaay.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 40%|████      | 33/82 [03:43<05:36,  6.87s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Baay naa ni naa ni Baay jël, ci sama yaay daan def.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 41%|████▏     | 34/82 [03:49<05:14,  6.56s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Nettali na mu ngi fàttaliku, moo xam ni, di mu ngi fàttaliku, di mu ngi janook fi ci sama yaay, di ngi janook moom, di mu ngi janook fi ci sama yaay.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 43%|████▎     | 35/82 [03:56<05:16,  6.73s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Muy nag, nag, nag, nag, nag, nag, nag, nag, nag, nag, nag, nag, nag, nag, nag.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 44%|████▍     | 36/82 [04:03<05:13,  6.81s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Nettali askan wi, di ko, di ko, di ko daan def. Mu ni ko, di ko daan def.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 45%|████▌     | 37/82 [04:11<05:13,  6.98s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ngir sax, di ko ci sama yaay moom, di ko ko ko ko ko daan def.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 46%|████▋     | 38/82 [04:17<05:03,  6.90s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ngir, su ma neexaan géej gi, du dox lu dul jeex.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 48%|████▊     | 39/82 [04:24<04:56,  6.89s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ngir, du dox ci sama yaay daan nettali nii.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 49%|████▉     | 40/82 [04:31<04:45,  6.79s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Li ci sama yaay dund, du sax, du sax, du sax, du sax, du sax, du sax, du ci sama yaay.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 50%|█████     | 41/82 [04:37<04:32,  6.65s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Nataal ya mu ngi fàttaliku, di ma ni ma ni ma ni ma ni sama yaay daan def, ci sama yaay daan def, di ma ni ma ni ma ni ma ni sama yaay daan ma daan ma daan ma daan ma daan ma daan ma daan ma daan ma daan ma daan ma daan ma daan ma ci sama yaay daan nettali nii.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 51%|█████     | 42/82 [04:44<04:31,  6.80s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Muy nag, du woon a nga xam ni ci sama yaay.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 52%|█████▏    | 43/82 [04:50<04:12,  6.47s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Nettali askan solo, moo xam naa, di ko, di sama yaay daan nettali nii, di sama yaay daan nettali nii, di sama yaay daan nettali nii, di sama yaay daan nettali nii, di faj la woon.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 54%|█████▎    | 44/82 [04:56<03:59,  6.31s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Nettali askan wi, du sax, du sax, du sax, du sax, du sax, du sax, du sax, du ci sama yaay, du dox lu dul jeex, du dox ci biir yax.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 55%|█████▍    | 45/82 [05:03<04:03,  6.59s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Muy nag, du woon a ngi janook moom, du woon dara.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 56%|█████▌    | 46/82 [05:10<03:58,  6.62s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Biñ na ma nee bëggoon a ngi janook moom, du dox ci sama yaay daan nettali nii.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 57%|█████▋    | 47/82 [05:16<03:43,  6.39s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Nettali askan wi, ci sama yaay daan nettali nii, di ko ci sama yaay daan nettali nii. Mu doon sama yaay daan nettali nii, di ngi janook moom, di ko ci sama yaay daan nettali nii.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 59%|█████▊    | 48/82 [05:21<03:30,  6.20s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Nettali askan wi, du sax, du sax, du sax, du sax, du sax, du ci sama yaay daan wax dëgg, du fa, du fa, du fa, du fa, du fa, du fa, du fa, du fa, du fa, du fa, du fa, du fa, du fa, du fa, du fa, du fa, du fa, du fa, du fa, du fa, du fa, du fa, du fa, du fa, du fa, du fa, du fa, du fa, du fa, du fa, du fa, du fa, du fa, du fa, du fa, du fa, du fa, du fa, du fa, du fa, du fa, du fa, du fa, du fa, du fa, du fa, du fa, du fa, du fa, du fa, du fa, du fa, du fa, du fa, du fa, du fa, du fa, du fa, du fa, du fa, du fa, du fa, du fa, du fa, du fa, du fa, du fa, du fa, du fa, du fa, du fa, du fa, du fa, du fa, du fa, du fa, du fa, du fa, du fa, du fa, du fa, du fa, du fa, du fa, du fa, du fa, du fa, du fa, du fa, du fa, du fa, du fa, du fa, du fa, du\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 60%|█████▉    | 49/82 [05:29<03:38,  6.62s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ngiraal ya, di ngi nii, di ngi nii, di ngi nii, di ngi nii.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 61%|██████    | 50/82 [05:37<03:47,  7.10s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Nettali na, nag, nag, nag, nag, nag, nag, nag, nag, nag, nag, nag, nag, nag, nag, nag, nag, nag, nag, nag, nag, nag, nag, nag, nag, nag.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 62%|██████▏   | 51/82 [06:07<07:08, 13.84s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Li ci sama yaay moom, du dox lu mu daan nettali nii.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 63%|██████▎   | 52/82 [06:27<07:53, 15.78s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Nettali sax, du dox lu dul jeex. Mu ni mel ni mel ni mel ni mel ni mel ni mel ni mel ni mel.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 65%|██████▍   | 53/82 [06:48<08:23, 17.35s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Nettali askan wi, di ma ni ma ni ma ni ma ci sama yaay, du sax, du sax, du sax, du ci sama yaay daan nettali nii.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 66%|██████▌   | 54/82 [07:17<09:42, 20.80s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Nettali sax, su ma sama yaay moom, sama yaay moom, sama yaay daan jéem a ngi janook moom.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 67%|██████▋   | 55/82 [07:34<08:48, 19.58s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Nettali askan wi, sama yaay moom, sama yaay moom, sama yaay moom, sama yaay moom, sama yaay daan nettali nii, di sama yaay daan nettali nii, di sama yaay daan nettali nii.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 68%|██████▊   | 56/82 [07:53<08:29, 19.59s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Liñ sama yaay dund, du sax, du sax, du sax, du sax, du sax, du sax, du sax, du sax, du ci sama yaay.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 70%|██████▉   | 57/82 [08:14<08:16, 19.87s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Nettali askan wi, du sax, du sax, du sax, du sax, du sax, du sax, du ci sama yaay.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 71%|███████   | 58/82 [08:37<08:19, 20.81s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Muy nag, ci sama yaay daan wax dëgg-dëgg, ci sama yaay.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 72%|███████▏  | 59/82 [08:53<07:27, 19.46s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Nettali askan wi, du sax, du sax, du sax, du sax, du ci sama yaay daan nettali nii. Mu doon sama yaay daan nettali nii, du woon a ngi gisaat, du dox ci sama yaay daan nettali nii.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 73%|███████▎  | 60/82 [09:14<07:16, 19.84s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Muy nag, nag, nag, nag mooy ñaari gone gu ndaw-gisu Baay ca kow, ci sama mag.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 74%|███████▍  | 61/82 [09:31<06:41, 19.11s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Xam naa xam naa xam naa xam naa xam ni Baay, du sax, du sax, du sax, du sax, du sax, du sax, du sax, du sax, du sax, du sax, du sax, du ci sama yaay.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 76%|███████▌  | 62/82 [09:52<06:32, 19.64s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Muy nag, du sax, du woon a nga xam ni ci sama yaay.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 77%|███████▋  | 63/82 [10:12<06:15, 19.76s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Xam naa ni ko ko ci sama yaay, ci sama yaay daan wax dëgg-dëgg, ci sama yaay.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 78%|███████▊  | 64/82 [10:42<06:50, 22.83s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Muy nag, nag, nag, nag, nag, nag, nag, nag.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 79%|███████▉  | 65/82 [11:05<06:26, 22.72s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Nu nekk, di ko ko mel ni mel ni mel ni mel ni mel ni mel ni mel ni mel.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 80%|████████  | 66/82 [11:35<06:40, 25.03s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Muy nag, nag, nag, nag, nag, nag, nag, nag, nag, nag, nag, nag, nag, nag, nag, nag, nag, nag, ñu ànd ak seen bopp, ñu ànd ak seen biir yu kowe, seen biir yu kowe, seen biir yu kowe ya.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 82%|████████▏ | 67/82 [11:55<05:50, 23.37s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Muy nag, nag, nag, nag, nag, nag.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 83%|████████▎ | 68/82 [12:15<05:16, 22.64s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ngir sax, ci sama yaay di ko ci sama yaay.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 84%|████████▍ | 69/82 [12:48<05:32, 25.58s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Liñ na ci sama yaay dund, du sax, du sax, du sax, du sax, du sax, du sax, du sax, du sax, du sax, du sax, du sax, du sax, du ci sama yaay du sax, du sax, du sax, du sax, du sax, du sax, du sax, du ci sama yaay.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 85%|████████▌ | 70/82 [13:13<05:03, 25.32s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Nu nekk Afrig, du dox lu mu mel ni mel ni mel ni mel ni mel ni mel.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 87%|████████▋ | 71/82 [13:37<04:36, 25.10s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Nettali askan wi, du sax, du sax, du sax, du dox ci sama yaay dund Afrig, du dox ci sama yaay.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 88%|████████▊ | 72/82 [14:04<04:16, 25.60s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Muy nag, nag, nag, nag, nag, nag, nag, nag, nag, nag, nag, nag.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 89%|████████▉ | 73/82 [14:31<03:54, 26.10s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ngir, du dox lu mu mel ni mel ni mel ni mel.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 90%|█████████ | 74/82 [14:58<03:29, 26.19s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ngir, di ko muy def, di ko ci sama yaay, di sama yaay daan nettali nii.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 91%|█████████▏| 75/82 [15:21<02:57, 25.41s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Nettali askan wi, ci seen biir yu kowe, seen biir yu kowe ya, seen bopp, seen biir yu kowe ya mu mel ni mel ni mel ni mel ni mel niy baat.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 93%|█████████▎| 76/82 [15:48<02:35, 25.92s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Muy nag, nag, nag, nag, nag, nag, nag, nag, nag, nag.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 94%|█████████▍| 77/82 [16:13<02:07, 25.54s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Nettali askan wi, ci sama yaay, sama yaay daan wax dëgg-dëgg, di ma ni ma ni ma ni ma ni ma ni ma koy jekk-jant.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 95%|█████████▌| 78/82 [16:41<01:45, 26.29s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Biñ yeggee ci sama yaay dund, du sax, du sax, du sax, du sax, du sax, du sax, du sax, du sax, du sax, du ci sama yaay.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 96%|█████████▋| 79/82 [17:06<01:17, 25.83s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Nettali sax, nag, nag, nag, nag, nag, nag, nag, nag, nag, nag, nag, nag, nag, nag, nag, nag, nag, nag, nag, nag, nag, nag, nag, nag, nag, nag, nag, nag, nag, nag, nag, nag, nag, nag, nag, nag, nag, nag, nag, nag, nag, nag, nag, ñu ànd ak seen bopp, seen bopp, seen bopp, seen bopp, seen bopp, seen bopp, seen bopp, seen bopp, seen bopp, seen bopp, seen bopp, seen bopp, seen bopp, seen bopp, seen bopp, seen bopp, seen bopp, seen bopp, seen bopp, seen bopp, seen bopp, seen bopp, seen bopp, seen bopp, seen bopp, seen bopp, seen bopp, seen bopp seen bopp, seen bopp, seen bopp seen bopp, seen bopp seen bopp, seen bopp, seen bopp seen bopp, seen bopp, seen bopp, seen bopp seen bopp seen bopp seen bopp, seen bopp, seen bopp seen bopp, seen bopp, seen bopp seen bopp seen bopp, seen bopp, seen bopp, seen bopp seen bopp, seen bopp, seen bopp, seen bopp, seen bopp seen bopp, seen bopp, seen bopp seen bopp, seen bopp seen bopp, seen bopp seen bopp, seen bopp, seen bopp seen bopp, seen bopp seen bopp seen bopp, seen bopp seen bopp seen bopp seen bopp seen bopp, seen bopp seen bopp, seen bopp seen bopp seen bopp seen bopp seen bopp seen\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 98%|█████████▊| 80/82 [17:31<00:51, 25.61s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Muy nag, nag, nag, nag, nag, nag, nag, nag, nag.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 99%|█████████▉| 81/82 [17:56<00:25, 25.53s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Baay na ko ko ko daan def, di ko ci sama yaay daan def.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 82/82 [18:23<00:00, 13.45s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Nettali sax, du dox ci seen biir yu dóomu-gisu Baay, seen biir yu dóomu-gisu Baay, seen biir yu mag, seen biir yu mag, seen biir yu mag, seen biir yu ñu ànd ak seen biir yu ñu ànd ak seeni dàll, seen bopp, seen bopp, seen bopp, seen bopp, seen bopp, seen bopp, seen bopp, seen bopp, seen bopp, seen bopp, seen bopp, seen bopp, seen bopp, seen bopp, seen bopp, seen bopp, seen bopp, seen bopp, seen bopp, seen bopp, seen bopp, seen bopp, seen bopp, seen bopp, seen bopp, seen bopp, seen bopp, seen bopp, seen bopp, seen bopp, seen bopp, seen bopp, seen bopp, seen bopp, seen bopp, seen bopp, seen bopp, seen bopp, seen bopp, seen bopp, seen bopp, seen bopp, seen bopp, seen bopp, seen bopp, seen bopp seen bopp seen bopp, seen bopp, seen bopp, seen bopp seen bopp, seen bopp seen bopp, seen bopp seen bopp seen bopp, seen bopp, seen bopp seen bopp, seen bopp seen bopp, seen bopp, seen bopp seen bopp, seen bopp, seen bopp, seen bopp seen bopp, seen bopp, seen bopp seen bopp, seen bopp seen bopp, seen bopp, seen bopp, seen bopp, seen bopp seen bopp, seen bopp, seen bopp, seen bopp, seen bopp, seen bopp, seen bopp, seen bopp, seen bopp, seen bopp, seen bopp, seen bopp, seen bopp, seen bopp, seen bopp, seen bopp, seen\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>original_text</th>\n",
       "      <th>original_label</th>\n",
       "      <th>predicted_label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Hommes, femmes, enfants sont pris dans un pièg...</td>\n",
       "      <td>Mag, ndaw, góor ak jigéen jàq, song àll bi te ...</td>\n",
       "      <td>Muy nag, nag, nag, nag, nag, nag, nag, nag, na...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Les portes ouvertes à l'émigration, ces cohort...</td>\n",
       "      <td>Loolu doyul, biñ duggee ci atum 60, ñu ubbil A...</td>\n",
       "      <td>Nettali sax, su ma ci sama yaay moom, di ma ni...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Les sulfamides sont rares, les poudres et les ...</td>\n",
       "      <td>Garab yu néew lañuy yóbbale, diy puudar ak i t...</td>\n",
       "      <td>Ngalla, su ma sama yaay moom, sama yaay moom, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Je peux ressentir l'émotion qu'il éprouve à tr...</td>\n",
       "      <td>Li koy yëngal noonu, xam naa ko. Lan moo ko dà...</td>\n",
       "      <td>Nettali askan wi, ay way-jur, ay ndell, ay nde...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Tout ce que j'ai pu savoir de cette période, c...</td>\n",
       "      <td>Sunu yaay daal moo daan sànni léeg-léeg ci wax...</td>\n",
       "      <td>Nettali sax, du sax, du sax, du sax, du sax, d...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                       original_text  \\\n",
       "0  Hommes, femmes, enfants sont pris dans un pièg...   \n",
       "1  Les portes ouvertes à l'émigration, ces cohort...   \n",
       "2  Les sulfamides sont rares, les poudres et les ...   \n",
       "3  Je peux ressentir l'émotion qu'il éprouve à tr...   \n",
       "4  Tout ce que j'ai pu savoir de cette période, c...   \n",
       "\n",
       "                                      original_label  \\\n",
       "0  Mag, ndaw, góor ak jigéen jàq, song àll bi te ...   \n",
       "1  Loolu doyul, biñ duggee ci atum 60, ñu ubbil A...   \n",
       "2  Garab yu néew lañuy yóbbale, diy puudar ak i t...   \n",
       "3  Li koy yëngal noonu, xam naa ko. Lan moo ko dà...   \n",
       "4  Sunu yaay daal moo daan sànni léeg-léeg ci wax...   \n",
       "\n",
       "                                     predicted_label  \n",
       "0  Muy nag, nag, nag, nag, nag, nag, nag, nag, na...  \n",
       "1  Nettali sax, su ma ci sama yaay moom, di ma ni...  \n",
       "2  Ngalla, su ma sama yaay moom, sama yaay moom, ...  \n",
       "3  Nettali askan wi, ay way-jur, ay ndell, ay nde...  \n",
       "4  Nettali sax, du sax, du sax, du sax, du sax, d...  "
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "# set the model to eval mode\n",
    "_ = model.eval()\n",
    "\n",
    "# run model inference on all test data\n",
    "original_traduction, predicted_traduction, original_text, scores = [], [], [], {}\n",
    "\n",
    "for data in tqdm(DataLoader(test_dataset)):\n",
    "    \n",
    "    # recuperate the two part of the sentence\n",
    "    sents = list(test_dataset.decode(data[0]))\n",
    "    \n",
    "    cp1_sent, cp2_sent = sents[0][0], sents[0][1] \n",
    "    \n",
    "    # create the sentence to traduce\n",
    "    sent1 = f'{test_dataset.cls_token}{cp1_sent}{test_dataset.sep_token}'\n",
    "    \n",
    "    # generate tokens\n",
    "    encoding = tokenizer(sent1, return_tensors='pt')\n",
    "    \n",
    "    generated = encoding.input_ids.cuda()\n",
    "    \n",
    "    attention_mask = encoding.attention_mask.cuda()\n",
    "    \n",
    "    # recuperate the pad token id\n",
    "    pad_token_id = tokenizer.pad_token_id\n",
    "    \n",
    "    # perform prediction\n",
    "    sample_outputs = model.generate(generated, do_sample = False, top_k = 50, max_length = test_dataset.max_len, top_p = 0.90,\n",
    "                                    temperature = 0, num_return_sequences = 0, attention_mask = attention_mask, pad_token_id = pad_token_id)\n",
    "    \n",
    "    # calculate the score and add it to the score\n",
    "    result = translation_eval.compute_metrics((sample_outputs, generated))\n",
    "    \n",
    "    if not scores: scores.update({k: v for k, v in result.items()})\n",
    "    \n",
    "    else: scores.update({k: round((scores[k] + v) / 2, 4) for k, v in result.items()})\n",
    "    \n",
    "    # decode the predicted tokens into texts\n",
    "    sent2 = list(test_dataset.decode(sample_outputs, True))[0]\n",
    "    \n",
    "    print(sent2)\n",
    "    # append results\n",
    "    original_traduction.append(cp2_sent)\n",
    "    predicted_traduction.append(sent2)\n",
    "    original_text.append(cp1_sent)\n",
    "\n",
    "# transform result into data frame\n",
    "df_ft_to_wf = pd.DataFrame({'original_text': original_text,\n",
    "                            'original_label': original_traduction,\n",
    "                            'predicted_label': predicted_traduction})\n",
    "\n",
    "# print the result\n",
    "df_ft_to_wf.head()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Like we specified in conclusion of the first processing on the corpora, some words appear a too important number of times in the text and we can see that their arrive in the large part of the translations. We can add augmentation and see if we obtain better results on the evaluation set."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training with augmentation"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We must initialize a new model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Embedding(15696, 768)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# set the mode name\n",
    "model_name = \"gpt2\"\n",
    "\n",
    "# recuperate the tokenizer from the dataset\n",
    "tokenizer = train_dataset_aug.tokenizer\n",
    "\n",
    "# configure the model\n",
    "model_aug = GPT2LMHeadModel.from_pretrained(model_name).cuda()\n",
    "\n",
    "# resize the token embeddings\n",
    "model_aug.resize_token_embeddings(len(tokenizer))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let us initialize the training arguments and fine-tune the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# seed\n",
    "torch.manual_seed(50)\n",
    "\n",
    "# create training arguments\n",
    "training_args = TrainingArguments(\"data/training1/results_aug\",\n",
    "                                  overwrite_output_dir=True,\n",
    "                                  num_train_epochs=3,\n",
    "                                  logging_steps=100,\n",
    "                                  load_best_model_at_end=True,\n",
    "                                  save_strategy=\"epoch\",\n",
    "                                  evaluation_strategy=\"epoch\",\n",
    "                                  per_device_train_batch_size=2, \n",
    "                                  per_device_eval_batch_size=2,\n",
    "                                  learning_rate=2e-5,\n",
    "                                #   warmup_steps=120,\n",
    "                                  weight_decay=0.05,\n",
    "                                  logging_dir='gpt2_training_aug_logs1')\n",
    "\n",
    "# start training\n",
    "trainer = Trainer(model=model_aug,\n",
    "                  args=training_args,\n",
    "                  train_dataset=train_dataset_aug, \n",
    "                  eval_dataset=test_dataset_aug,\n",
    "                  data_collator=data_collator,\n",
    "                  # compute_metrics=translation_eval.compute_metrics\n",
    "                  )\n",
    "\n",
    "# def optuna_hp_space(trial):\n",
    "  \n",
    "#   return {\n",
    "#     \"learning_rate\": trial.suggest_float(\"learning_rate\", 1e-5, 5e-5, log = True),\n",
    "#     \"num_train_epochs\": trial.suggest_int(\"num_train_epochs\", 1, 5),\n",
    "#     \"seed\": trial.suggest_int(\"seed\", 1, 40),\n",
    "#     \"per_device_train_batch_size\": trial.suggest_categorical(\"per_device_train_batch_size\", [2, 3, 4, 5, 6])\n",
    "#   }\n",
    "\n",
    "# # Searching for the best hyperparameters\n",
    "# trainer.hyperparameter_search(\n",
    "#   direction = \"maximize\",\n",
    "#   backend = \"optuna\",\n",
    "#   hp_space = optuna_hp_space,\n",
    "#   n_trials = 10,\n",
    "# )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Oumar Kane\\AppData\\Local\\pypoetry\\Cache\\virtualenvs\\pytorch1-HleOW5am-py3.10\\lib\\site-packages\\transformers\\optimization.py:391: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n",
      "  1%|▏         | 15/1101 [00:29<31:12,  1.72s/it] "
     ]
    }
   ],
   "source": [
    "# load from a checkpoint and continue the training\n",
    "# trainer._load_from_checkpoint('data/training1/results/checkpoint-734/')\n",
    "\n",
    "trainer.train()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pytorch1-HleOW5am-py3.10",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
