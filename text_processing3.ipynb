{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Discriminator tokenizer\n",
    "----------------------------\n",
    "\n",
    "We will create a WordPiece Tokenizer (for which the tutorial can be find at the following link [WordPieceTokenizer](https://huggingface.co/learn/nlp-course/chapter6/6?fw=pt)) to compare a generated sequence from a true sequence in the GAN network. We will only test the capacity of the WordPiece Tokenizer to provide good results: In our case it is generating the original sequence. \n",
    "\n",
    "The WordPiece is chosen because the discriminator is considered to be the BERT Model. \n",
    "\n",
    "- Creating a sentences' generator \n",
    "- Instantiating the `WordPiece tokenizer` \n",
    "- ~~Instantiating the `normalizer`~~\n",
    "- Instantiating the `BertPre-tokenizer`\n",
    "- Instantiating the trainer with `20000` tokens and the following special tokens `\"[UNK]\", \"[PAD]\", \"[CLS]\", \"[SEP]\", and \"[MASK]\"`\n",
    "- Training the tokenizer\n",
    "- Instantiate the pos-processor by specifying the format of the two passed sequences (sequence from the French corpus and sequence from the Wolof corpus)\n",
    "- Initialize the decoder\n",
    "- Save the tokenizer\n",
    "- Make a test\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let us import the necessary libraries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for creating the tokenizer\n",
    "from tokenizers import (\n",
    "    normalizers,\n",
    "    decoders,\n",
    "    models,\n",
    "    pre_tokenizers,\n",
    "    processors,\n",
    "    trainers,\n",
    "    Tokenizer,\n",
    ")\n",
    "\n",
    "# for importing and manipulating the sentences\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Load dataset and create generator"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will create two tokenizers: one for each corpus since we different languages and so different vocabularies. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load sentences\n",
    "sentences = pd.read_csv(\"data/extractions/new_data/sent_extraction.csv\")\n",
    "\n",
    "# initialize a batch size\n",
    "BATCH_SIZE = 50\n",
    "\n",
    "# create generators (for the corpora)\n",
    "def generate_sentences():\n",
    "    \n",
    "    # stacking the sentences\n",
    "    concat_sentences = lambda line_index: sentences.loc[line_index, \"french_corpus\"] + \" \" + sentences.loc[line_index, \"wolof_corpus\"]  \n",
    "    \n",
    "    sentences[\"corpora\"] = sentences.index.map(concat_sentences)\n",
    "    \n",
    "    sents = sentences[\"corpora\"].to_list()\n",
    "    \n",
    "    for i in range(1, len(sents), BATCH_SIZE):\n",
    "        \n",
    "        yield sents[i:i+BATCH_SIZE]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Initialize the tokenizers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = Tokenizer(models.WordPiece(unk_token=\"[UNK]\"))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Specify normalizers (No normalizer will be required)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "pass"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Specify a bert pre-tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer.pre_tokenizer = pre_tokenizers.Whitespace()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Specify a trainer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "special_tokens = [\"[UNK]\", \"[PAD]\", \"[CLS]\", \"[SEP]\", \"[MASK]\"]\n",
    "trainer = trainers.WordPieceTrainer(vocab_size=20000, special_tokens=special_tokens)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Train the tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer.train_from_iterator(generate_sentences(), trainer)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let us print the vocabulary size."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "15023"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.get_vocab_size()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Add post-processing (Not required)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2 3\n"
     ]
    }
   ],
   "source": [
    "# let us recuperate the special tokens ids\n",
    "cls_token_id = tokenizer.token_to_id(\"[CLS]\")\n",
    "sep_token_id = tokenizer.token_to_id(\"[SEP]\")\n",
    "print(cls_token_id, sep_token_id)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will add the special tokens for differentiating the French sentences from the Wolof sentences."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer.post_process = processors.TemplateProcessing(\n",
    "    single=f\"[CLS]:0 $A:0 [SEP]:0\",\n",
    "    pair=f\"[CLS]:0 $A:0 [SEP]:0 $B:1 [SEP]:1\",\n",
    "    special_tokens=[(\"[CLS]\", cls_token_id), (\"[SEP]\", sep_token_id)]\n",
    ")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Specify the decoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer.decoder = decoders.WordPiece(prefix=\"##\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Save the tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer.save(\"wolof-translate/wolof_translate/tokenizers/adverse_tokenizer.json\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Make a test with a sentence"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let us recuperate a random French sentence and her corresponding Wolof translation in order to verify if we obtain the expected tokenization result."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "\n",
    "line = random.randint(0, sentences.shape[0])\n",
    "\n",
    "sentence = sentences.loc[line, :]\n",
    "\n",
    "fr_sentence = sentence['french_corpus'].replace(\"’\", \"'\")\n",
    "\n",
    "wf_sentence = sentence['wolof_corpus'].replace(\"’\", \"'\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let us encode the sentences."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "encoding = tokenizer.encode(f\"[CLS]{fr_sentence}[SEP]{wf_sentence}[SEP]\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let us print the french encoding characteristics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokens:\n",
      "['[CLS]', 'Ce', 'n', \"'\", 'est', 'que', 'longtemps', 'après', ',', 'quand', 'l', \"'\", 'égoïsme', 'naturel', 'aux', 'enfants', 's', \"'\", 'est', 'estompé', ',', 'que', 'j', \"'\", 'ai', 'compris', ':', 'ma', 'mère', ',', 'en', 'vivant', 'loin', 'de', 'mon', 'père', ',', 'avait', 'pratiqué', 'du', 'fait', 'de', 'la', 'guerre', 'un', 'héroïsme', 'sans', 'emphase', ',', 'non', 'par', 'inconscience', 'ni', 'par', 'résignation', '(', 'même', 'si', 'la', 'foi', 'religieuse', 'avait', 'pu', 'lui', 'être', 'd', \"'\", 'un', 'grand', 'secours', '),', 'mais', 'par', 'la', 'force', 'que', 'faisait', 'naître', 'en', 'elle', 'une', 'telle', 'inhumanité', '.', '[SEP]', 'Teg', 'nañ', 'ciy', 'ati', '-', 'at', 'ma', 'door', 'a', 'jëli', 'ni', 'jigéen', ',', 'ni', 'góor', 'di', 'wonee', 'njàmbaar', 'ci', 'toolu', '-', 'xare', ',', 'la', 'mën', 'a', 'toog', 'biir', 'këram', 'moom', 'tamit', ',', 'wone', 'fa', 'njàmbaar', 'gu', 'ni', 'tollu', '.', 'Ni', 'sama', 'yaay', 'daan', 'doxalee', 'ci', 'geer', 'bi', ',', 'firnde', 'la', 'ci', '.', '[SEP]']\n",
      "IDS:\n",
      "[2, 1381, 66, 7, 289, 268, 2366, 742, 10, 1029, 64, 7, 14063, 11809, 576, 556, 71, 7, 289, 14628, 10, 268, 62, 7, 410, 3306, 24, 200, 422, 10, 204, 3813, 832, 177, 269, 324, 10, 382, 13342, 213, 744, 177, 186, 627, 207, 13970, 470, 6523, 10, 937, 252, 11821, 218, 252, 5221, 8, 527, 338, 186, 8020, 12884, 382, 1079, 453, 553, 56, 7, 207, 560, 4900, 1102, 455, 252, 186, 1972, 268, 1861, 3718, 204, 577, 279, 2463, 14662, 12, 3, 4738, 749, 765, 2181, 11, 491, 200, 1289, 53, 6190, 218, 703, 10, 218, 1073, 223, 11383, 5013, 189, 3845, 11, 679, 10, 186, 398, 53, 956, 569, 2129, 413, 975, 10, 1797, 287, 5013, 302, 218, 1640, 12, 702, 325, 591, 280, 10361, 189, 5431, 234, 10, 6973, 186, 189, 12, 3]\n"
     ]
    }
   ],
   "source": [
    "print(\"Tokens:\")\n",
    "print(encoding.tokens)\n",
    "\n",
    "print(\"IDS:\")\n",
    "print(encoding.ids)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let us decode the tokens."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"Ce n ' est que longtemps après, quand l ' égoïsme naturel aux enfants s ' est estompé, que j ' ai compris : ma mère, en vivant loin de mon père, avait pratiqué du fait de la guerre un héroïsme sans emphase, non par inconscience ni par résignation ( même si la foi religieuse avait pu lui être d ' un grand secours ), mais par la force que faisait naître en elle une telle inhumanité. Teg nañ ciy ati - at ma door a jëli ni jigéen, ni góor di wonee njàmbaar ci toolu - xare, la mën a toog biir këram moom tamit, wone fa njàmbaar gu ni tollu. Ni sama yaay daan doxalee ci geer bi, firnde la ci.\""
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.decode(encoding.ids)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pytorch1-HleOW5am-py3.10",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
