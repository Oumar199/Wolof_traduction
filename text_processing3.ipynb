{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Discriminator tokenizer\n",
    "----------------------------\n",
    "\n",
    "We will create a WordPiece Tokenizer (for which the tutorial can be find at the following link [WordPieceTokenizer](https://huggingface.co/learn/nlp-course/chapter6/6?fw=pt)) to compare a generated sequence from a true sequence in the GAN network. We will only test the capacity of the WordPiece Tokenizer to provide good results: In our case it is generating the original sequence. \n",
    "\n",
    "The WordPiece is chosen because the discriminator is considered to be the BERT Model. \n",
    "\n",
    "- Creating a sentences' generator \n",
    "- Instantiating the `WordPiece tokenizer` \n",
    "- ~~Instantiating the `normalizer`~~\n",
    "- Instantiating the `BertPre-tokenizer`\n",
    "- Instantiating the trainer with `20000` tokens and the following special tokens `\"[UNK]\", \"[PAD]\", \"[CLS]\", \"[SEP]\", and \"[MASK]\"`\n",
    "- Training the tokenizer\n",
    "- Instantiate the pos-processor by specifying the format of the two passed sequences (sequence from the French corpus and sequence from the Wolof corpus)\n",
    "- Initialize the decoder\n",
    "- Save the tokenizer\n",
    "- Make a test\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let us import the necessary libraries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for creating the tokenizer\n",
    "from tokenizers import (\n",
    "    normalizers,\n",
    "    decoders,\n",
    "    models,\n",
    "    pre_tokenizers,\n",
    "    processors,\n",
    "    trainers,\n",
    "    Tokenizer,\n",
    ")\n",
    "\n",
    "# for importing and manipulating the sentences\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Load dataset and create generator"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will create two tokenizers: one for each corpus since we different languages and so different vocabularies. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load sentences\n",
    "sentences = pd.read_csv(\"data/extractions/new_data/sent_extraction.csv\")\n",
    "\n",
    "# initialize a batch size\n",
    "BATCH_SIZE = 50\n",
    "\n",
    "# create generators (for the corpora)\n",
    "def generate_sentences():\n",
    "    \n",
    "    # stacking the sentences\n",
    "    concat_sentences = lambda line_index: sentences.loc[line_index, \"french_corpus\"] + \" \" + sentences.loc[line_index, \"wolof_corpus\"]  \n",
    "    \n",
    "    sentences[\"corpora\"] = sentences.index.map(concat_sentences)\n",
    "    \n",
    "    sents = sentences[\"corpora\"].to_list()\n",
    "    \n",
    "    for i in range(1, len(sents), BATCH_SIZE):\n",
    "        \n",
    "        yield sents[i:i+BATCH_SIZE]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Initialize the tokenizers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = Tokenizer(models.WordPiece(unk_token=\"[UNK]\"))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Specify normalizers (No normalizer will be required)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "pass"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Specify a bert pre-tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer.pre_tokenizer = pre_tokenizers.Whitespace()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Specify a trainer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "special_tokens = [\"[UNK]\", \"[PAD]\", \"[CLS]\", \"[SEP]\", \"[MASK]\"]\n",
    "trainer = trainers.WordPieceTrainer(vocab_size=20000, special_tokens=special_tokens)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Train the tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer.train_from_iterator(generate_sentences(), trainer)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let us print the vocabulary size."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "15115"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.get_vocab_size()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Add post-processing (Not required)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2 3\n"
     ]
    }
   ],
   "source": [
    "# let us recuperate the special tokens ids\n",
    "cls_token_id = tokenizer.token_to_id(\"[CLS]\")\n",
    "sep_token_id = tokenizer.token_to_id(\"[SEP]\")\n",
    "print(cls_token_id, sep_token_id)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will add the special tokens for differentiating the French sentences from the Wolof sentences."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer.post_process = processors.TemplateProcessing(\n",
    "    single=f\"[CLS]:0 $A:0 [SEP]:0\",\n",
    "    pair=f\"[CLS]:0 $A:0 [SEP]:0 $B:1 [SEP]:1\",\n",
    "    special_tokens=[(\"[CLS]\", cls_token_id), (\"[SEP]\", sep_token_id)]\n",
    ")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Specify the decoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer.decoder = decoders.WordPiece(prefix=\"##\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Save the tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer.save(\"wolof-translate/wolof_translate/tokenizers/adverse_tokenizer.json\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Make a test with a sentence"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let us recuperate a random French sentence and her corresponding Wolof translation in order to verify if we obtain the expected tokenization result."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "\n",
    "random.seed(100)\n",
    "\n",
    "line = random.randint(0, sentences.shape[0])\n",
    "\n",
    "sentence = sentences.loc[line, :]\n",
    "\n",
    "fr_sentence = sentence['french_corpus'].replace(\"’\", \"'\")\n",
    "\n",
    "wf_sentence = sentence['wolof_corpus'].replace(\"’\", \"'\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let us encode the sentences."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "encoding = tokenizer.encode(f\"[CLS]{fr_sentence}[SEP]{wf_sentence}[SEP]\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let us print the french encoding characteristics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokens:\n",
      "['[CLS]', 'Nous', 'frappions', 'à', 'nouveau', ',', 'jusqu', \"'\", 'à', 'en', 'avoir', 'mal', 'aux', 'mains', ',', 'comme', 'si', 'nous', 'combattions', 'un', 'ennemi', 'invisible', '.', '[SEP]', 'Loolu', 'gën', 'noo', 'ràkkaajuloo', ',', 'nuy', 'dóor', ',', 'di', 'dóor', ',', 'di', 'dóor', 'ba', 'sunuy', 'loxoy', 'metti', '.', '[SEP]']\n",
      "IDS:\n",
      "[2, 911, 4007, 88, 2374, 10, 828, 7, 88, 204, 1437, 727, 571, 1890, 10, 481, 321, 417, 14054, 207, 4079, 5070, 12, 3, 1201, 498, 1893, 14071, 10, 1062, 1150, 10, 222, 1150, 10, 222, 1150, 223, 1840, 4765, 1032, 12, 3]\n"
     ]
    }
   ],
   "source": [
    "print(\"Tokens:\")\n",
    "print(encoding.tokens)\n",
    "\n",
    "print(\"IDS:\")\n",
    "print(encoding.ids)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let us decode the tokens."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"Nous frappions à nouveau, jusqu ' à en avoir mal aux mains, comme si nous combattions un ennemi invisible. Loolu gën noo ràkkaajuloo, nuy dóor, di dóor, di dóor ba sunuy loxoy metti.\""
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.decode(encoding.ids)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We remark that some the marks are badly separated from their letters (example of the guillemet from the letter 'u' and 'à'). We must identify all of them to produce recombination rules for the generated sentences."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pytorch1-HleOW5am-py3.10",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
